{"file_name": "quickstart-tutorial.html", "content": "Tutorial to Get Started with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Tutorial to Get Started with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Tutorial to Get Started with MindsDB Before we start, install MindsDB locally via Docker or Docker Desktop . Get started with MindsDB in a few simple steps: 1 Connect a data source Explore all available data sources here . 2 Configure an AI engine Explore all available AI engines here . 3 Create and deploy an AI model MindsDB abstracts AI Models as AI Tables . This step uses the configured AI engine. 4 Query for predictions Join the data table with the AI table to get predictions. 5 Automate customized workflows Use Jobs or Triggers to automate workflows. ​ Step 1. Connect a data source Use the CREATE DATABASE statement to connect a data source to MindsDB. CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; This is the input data used in the following steps: SELECT * FROM mysql_demo_db . questions LIMIT 3 ; Here is the output: +------------------+--------------------------------------------------------+-------------+ | article_title | question | true_answer | +------------------+--------------------------------------------------------+-------------+ | Alessandro_Volta | Was Volta an Italian physicist? | yes | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | no | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | yes | +------------------+--------------------------------------------------------+-------------+ ​ Step 2. Configure an AI engine Use the CREATE ML_ENGINE command to configure an AI engine. Here we use the OpenAI engine. CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; ​ Step 3. Create and deploy an AI model Use the CREATE MODEL statement to create, train, and deploy an AI model based on the AI engine created in step 2. CREATE MODEL question_answering_model PREDICT answer USING engine = 'openai_engine' , prompt_template = 'answer the question of text:{{question}} about text:{{article_title}}' ; This model expects {{question}} and {{article_title}} as input, and generates answer as output. ​ Step 4. Query for predictions Query for predictions by joining the AI Table (from step 3) with the data table (from step 1). SELECT input . article_title , input . question , output . answer FROM mysql_demo_db . questions AS input JOIN question_answering_model AS output LIMIT 3 ; Here is the output data: +------------------+--------------------------------------------------------+--------------------------------------------------------+ | article_title | question | answer | +------------------+--------------------------------------------------------+--------------------------------------------------------+ | Alessandro_Volta | Was Volta an Italian physicist? | Yes, Volta was an Italian physicist. | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | No, Volta is not buried in the city of Pittsburgh. | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | Yes, Volta had a passion for the study of electricity. | +------------------+--------------------------------------------------------+--------------------------------------------------------+ ​ Step 5. Automate customized workflows With MindsDB, you can create custom automation workflows. Let’s set up a workflow that uses Jobs and (re)creates a table with predicted answers to all questions. CREATE JOB answer_questions ( CREATE OR REPLACE TABLE data_source . questions_answers ( SELECT input . article_title , input . question , output . answer FROM mysql_demo_db . questions AS input JOIN question_answering_model AS output ) ) EVERY 1 day ; The data_source connection should be made using the CREATE DATABASE statement to a data source with a user that has the write access. This job creates the questions_answers table inside the connected data source. This table is filled with questions from the input data table and answers generated by the AI table. Considering that new questions are added daily to the input data table, this job executes once a day. Alternatively, you could use the LAST keyword to fetch only the newly added questions to the input data table. That would enable you to insert new question-answer pairs into the questions_answers table instead of recreating it. But to do that, the input data table must provide either a date/time or integer/float column that would be used in the condition like datetime > LAST . Learn more about the LAST keyword here . Next Steps Follow the links below to explore MindsDB. Connect data sources Configure AI engines Deploy models Get predictions Finetune models Automate with jobs Automate with triggers Create knowledge bases Create chatbots Was this page helpful? Yes No Suggest edits Raise issue Introduction Docker github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Step 1. Connect a data source Step 2. Configure an AI engine Step 3. Create and deploy an AI model Step 4. Query for predictions Step 5. Automate customized workflows"}
{"file_name": "generative-ai-tables.html", "content": "Generative AI Tables - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more Generative AI Tables Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more Generative AI Tables MindsDB empowers organizations to harness the power of AI by abstracting AI models as Generative AI Tables. These tables are capable of learning from the input data and generating predictions from the underlying model upon being queried. This abstraction makes AI highly accessible, enabling development teams to use their existing SQL skills to build applications powered by AI. MindsDB integrates with numerous AI frameworks. Learn more here . ​ What are Generative AI Tables? Generative AI is a subfield of artificial intelligence that trains AI models to create new content, such as realistic text, forecasts, images, and more, by learning patterns from existing data. MindsDB revolutionizes machine learning within enterprise databases by introducing the concept of Generative AI tables . These essentially abstract AI models as virtual AI tables, capable of producing output when given certain input. ​ How to Use Generative AI Tables AI tables, introduced by MindsDB, abstract AI models as virtual tables so you can simply query AI models for predictions. With MindsDB, you can join multiple AI tables (that abstract AI models) with multiple data tables (that provide input to the models) to get all predictions at once. Let’s look at some examples. ​ Deploy AI Models as AI Tables You can deploy an AI model as a virtual AI table using the CREATE MODEL statement. Here we create a model that classifies sentiment of customer reviews as instructed in the prompt template message. The required input is the review and output is the sentiment predicted by the model. CREATE MODEL sentiment_classifier_model PREDICT sentiment USING engine = 'openai_engine' , model_name = 'gpt-4' , prompt_template = ' describe the sentiment of the reviews strictly as \"positive\" , \"neutral\" , or \"negative\" . \"I love the product\" :positive \"It is a scam\" :negative \"{{review}}.\" :' ; Next we create a model that generates responses to the reviews. The required input includes review, product name, and sold product quantity, and output is the response generated by the model. CREATE MODEL response_generator_model PREDICT response USING engine = 'openai_engine' , model_name = 'gpt-4' , prompt_template = 'briefly respond to the customer review: {{review}}, added by a customer after buying {{product_name}} in quantity {{quantity}}' ; Follow this doc page to configure the OpenAI engine in MindsDB. Now let’s look at the data tables that we’ll use to provide input data to the AI tables. ​ Prepare Input Data The amazon_reviews table stores the following columns: + ----------------------------+-----------------------------+------------------------+-------------+ | created_at | product_name | review | customer_id | + ----------------------------+-----------------------------+------------------------+-------------+ | 2023 - 10 - 03 16 : 30 : 00.000000 | Power Adapter | It is a great product . | 1 | | 2023 - 10 - 03 16 : 31 : 00.000000 | Bluetooth and Wi - Fi Speaker | It is ok . | 2 | | 2023 - 10 - 03 16 : 32 : 00.000000 | Kindle eReader | It doesn’t work . | 3 | + ----------------------------+-----------------------------+------------------------+-------------+ It provides sufficient input data for the sentiment_classifier_model , but not for the response_generator_model . The products_sold table stores the following columns: + ----------------------------+-----------------------------+-------------+----------+ | sale_date | product_name | customer_id | quantity | + ----------------------------+-----------------------------+-------------+----------+ | 2023 - 10 - 03 16 : 30 : 00.000000 | Power Adapter | 1 | 20 | | 2023 - 10 - 03 16 : 31 : 00.000000 | Bluetooth and Wi - Fi Speaker | 2 | 5 | | 2023 - 10 - 03 16 : 32 : 00.000000 | Kindle eReader | 3 | 10 | + ----------------------------+-----------------------------+-------------+----------+ The response_generator_model requires the two tables to be joined to provide it with sufficient input data. ​ Make Predictions You can query the AI tables directly or join AI tables with data tables to get the predictions. There are two ways you can provide input to the models: If you query the AI table directly, you can provide input data in the WHERE clause, like this: SELECT review , sentiment FROM sentiment_classifier_model WHERE review = 'I like it' ; You can provide input data to AI tables from the joined data tables, like this: SELECT inp . product_name , inp . review , m1 . sentiment , m2 . response FROM data_integration_conn . amazon_reviews2 AS inp JOIN data_integration_conn . products_sold AS inp2 ON inp . customer_id = inp2 . customer_id JOIN sentiment_classifier_model AS m1 JOIN response_generator_model AS m2 ; The sentiment_classifier_model requires a parameter named review , so the data table should contain a column named review , which is picked up by the model. Note that, when joining data tables, you must provide the ON clause condition, which is implemented implicitly when joining the AI tables. Moreover, you can combine these two options and provide the input data to the AI tables partially from the data tables and partially from the WHERE clause, like this: SELECT inp . product_name , inp . review , m1 . sentiment , m2 . response FROM data_integration_conn . amazon_reviews2 AS inp JOIN sentiment_classifier_model AS m1 JOIN response_generator_model AS m2 WHERE m2 . quantity = 5 ; Here the sentiment_classifier_model takes input data from the amazon_review table, while the response_generator_model takes input data from the amazon_reviews table and from the WHERE clause. Furthermore, you can make use of subqueries to provide input data to the models via the WHERE clause, like this: SELECT inp . product_name , inp . review , m1 . sentiment , m2 . response FROM data_integration_conn . amazon_reviews2 AS inp JOIN sentiment_classifier_model AS m1 JOIN response_generator_model AS m2 WHERE m2 . quantity = ( SELECT quantity FROM data_integration_conn . products_sold WHERE customer_id = 2 ) ; ​ Difference between AI Tables and Standard Tables To understand the difference, let’s go over a simpler example. Here we will see how traditional database tables are designed to give you a deterministic response given some input, and how Generative AI Tables are designed to generate an approximate response given some input. Let’s consider the following income_table table that stores the income and debt values. SELECT income , debt FROM income_table ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 60000 | 20000 | | 80000 | 25100 | | 100000 | 30040 | | 120000 | 36010 | + ------+-----+ A simple visualization of the data present in the income_table table is as follows: Querying the income table to get the debt value for a particular income value results in the following: SELECT income , debt FROM income_table WHERE income = 80000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 80000 | 25100 | + ------+-----+ And here is what we get: But what happens when querying the table for an income value that is not present there? SELECT income , debt FROM income_table WHERE income = 90000 ; On execution, we get: Empty set ( 0.00 sec ) When the WHERE clause condition is not fulfilled for any of the rows, no value is returned. When a table doesn’t have an exact match, the query returns an empty set or null value. This is where the AI Tables come into play! Let’s create a debt_model model that allows us to approximate the debt value for any income value. We train the debt_model model using the data from the income_table table. CREATE MODEL mindsdb . debt_model FROM income_table PREDICT debt ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) MindsDB provides the CREATE MODEL statement. On execution of this statement, the predictive model works in the background, automatically creating a vector representation of the data that can be visualized as follows: Let’s now look for the debt value of some random income value. To get the approximated debt value, we query the mindsdb.debt_model model instead of the income_table table. SELECT income , debt FROM mindsdb . debt_model WHERE income = 90000 ; On execution, we get: + ------+-----+ | income | debt | + ------+-----+ | 90000 | 27820 | + ------+-----+ And here is how it looks: Was this page helpful? Yes No Suggest edits Raise issue MindsDB Projects Feature Engineering github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What are Generative AI Tables? How to Use Generative AI Tables Deploy AI Models as AI Tables Prepare Input Data Make Predictions Difference between AI Tables and Standard Tables"}
{"file_name": "minds-demo.html", "content": "Demo - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Demo Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Demo Was this page helpful? Yes No Suggest edits Raise issue Environment Variables Model Management github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "quickstart.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "docs-rules.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "model-management.html", "content": "Model Management - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Features Model Management Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Features Model Management MindsDB abstracts AI models, making them accessible from enterprise data environments. MindsDB enables you to manage every aspect of AI models. With MindsDB, you can CREATE MODEL , FINETUNE , RETRAIN , and more. Deploy You can create, train, and deploy AI models based on popular AI/ML frameworks within MindsDB. Fine-tune You can fine-tune models with data from various data sources connected to MindsDB. Check out examples here . Automate You can automate tasks, including retraining or fine-tuning of AI models, to keep your AI system up-to-date. See examples here . Go ahead and create an AI model! Use SQL API , REST API , or one of the SDKs to create and deploy AI models within MindsDB. Was this page helpful? Yes No Suggest edits Raise issue Demo AI Integrations github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "tutorials.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "mindsdb-handlers.html", "content": "Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more Handlers ​ What are Handlers? At the heart of the MindsDB philosophy lies the belief that predictive insights are best leveraged when produced as close as possible to the data layer. Usually, this data layer is a SQL-compatible database, but it could also be a non-SQL database, data stream, or any other tool that interacts with data stored somewhere else. The above description fits an enormous set of tools used across the software industry. The complexity increases further by bringing Machine Learning into the equation, as the set of popular ML tools is similarly huge. We aim to support most technology stacks, requiring a simple integration procedure so that anyone can easily contribute the necessary glue to enable any predictive system for usage within data layers. This motivates the concept of handlers , which is an abstraction for the two types of entities mentioned above: data layers and ML frameworks . Handlers are meant to enforce a common and sufficient set of behaviors that all MindsDB-compatible entities should support. By creating a handler, the target system is effectively integrated into the wider MindsDB ecosystem. ​ Types of Handlers ​ Data Handlers Data handlers act as a bridge to any database. You use data handlers to connect data sources, including databases and applications, using the CREATE DATABASE command . So you can reach data from any database that has its handler implemented within MindsDB. Go ahead and implement a handler for the database of your choice! Here you’ll find instructions on how to implement a data handler. ​ Machine Learning (ML) Handlers ML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command . So you can expose ML models from any supported ML engine as an AI table. Go ahead and implement a handler for the ML library or framework of your choice! Here you’ll find instructions on how to implement an ML handler. ​ Handlers in the MindsDB Repository The source code for integrations is located in the main MindsDB repository under the /integrations directory. integrations # Contains handlers' source codes ├─ handlers/ # Each handler has its own handler directory │ ├─ mysql_handler/ # MySQL integration code │ ├─ lightwood_handler/ # Lightwood integration code │ ├─ .../ # Other handlers ├─ handlers_client/ # Handler clients directory │ ├─ db_client/ │ ├─ ml_client/ ├─ libs/ # Handler libraries directory │ ├─ base.py # Each handler class inherits from one of the base classes └─ utilities # Handler utility directory │ ├─ install.py # Script that installs all handler dependencies ​ Structure of a Handler In technical terms, a handler is a self-contained Python package having everything required for MindsDB to interact with it. It includes aspects like dependencies, unit tests, and continuous integration logic. It is up to the author to determine the nature of the package, for example, closed or open source, version control, and more. Although, we encourage opening pull requests to expand the default set of supported tools. The entry point for a data handler is a class definition that should inherit directly from the mindsdb.integrations.libs.base.DatabaseHandler class and thus indirectly from the mindsdb.integrations.libs.base.BaseHandler class, which defines all the methods that must be overwritten in order to achieve a functional implementation. And the entry point for an ML handler is a class definition that should inherit from the mindsdb.integrations.libs.base.BaseMLEngine class, which defines all the methods that must be overwritten in order to achieve a functional implementation. All other details of the handler’s structure are not enforced, so the author can decide on its design. ​ Things to Remember Here are a few points to keep in mind while implementing a handler. ​ Handlers Inherit from Base Classes Inherit from the DatabaseHandler class when adding a new data handler. For more info, visit our doc page here . Inherit from the BaseMLEngine class when adding a new ML handler. For more info, visit our doc page here . ​ Parsing SQL Whenever you want to parse a string that contains SQL, we strongly recommend using the mindsdb_sql package. It provides a parser that fully supports the MindsDB SQL dialect and partially the standard SQL dialect. There is also a render feature to map other dialects into the already supported ones. ​ Formatting Output In the case of data handlers, when it comes to building the response of the public methods, the output should be wrapped by the mindsdb.integrations.libs.response.HandlerResponse or mindsdb.integrations.libs.response.HandlerStatusResponse class. These classes are used by the MindsDB executioner to orchestrate and coordinate multiple handler instances in parallel. And in the case of ML handlers, output wrapping is automatically done by an intermediate wrapper, the BaseMLEngineExec class, so the contributor wouldn’t need to worry about it. Next Steps Below are the links to help you explore further. Data Handler Here is how to build a data handler. App Handler Here is how to build an app handler. ML Handler Here is how to build an ML handler. Was this page helpful? Yes No Suggest edits Raise issue Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What are Handlers? Types of Handlers Data Handlers Machine Learning (ML) Handlers Handlers in the MindsDB Repository Structure of a Handler Things to Remember Handlers Inherit from Base Classes Parsing SQL Formatting Output"}
{"file_name": "index.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "mindsdb-gui.html", "content": "Navigating the MindsDB GUI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more Navigating the MindsDB GUI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more Navigating the MindsDB GUI MindsDB offers a user-friendly graphical interface that allows users to execute SQL commands, view their outputs, and easily navigate connected data sources, projects, and their contents. Let’s explore the features and usage of the MindsDB editor. ​ Accessing the MindsDB GUI Editor Install MindsDB locally via Docker or Docker Desktop . ​ Exploring the MindsDB GUI Editor ​ Query Editor This is the primary component where users can input SQL commands and queries. It provides a code editor environment where users can write, edit, and execute SQL statements. It is located in the top center of the MindsDB GUI. You can open multiple query editor tabs by clicking the plus button next to the current tab, like this: ​ Results Viewer Once a query is executed, the results viewer displays the output of the query. It presents the results in a tabular format, showing rows and columns of data. It is located in the bottom center of the MindsDB GUI. MindsDB supports additional features such as the following: The Data Insights feature provides useful data visualization charts. The Export feature lets you export the query output as a CSV or Markdown file. ​ Object Explorer The object explorer provides an overview of the projects, models, views, connected data sources, and tables. Users can navigate through the available objects by expanding the tree structure items. Upon hovering over the tables, you can query their content using the provided SELECT statement, as below. ​ Model Progress Bar MindsDB provides a custom SQL statement to create and deploy models as virtual tables. Upon executing the CREATE MODEL statement, you can monitor the training progress at the bottom-left corner below the object explorer. Once the model is ready, its status updates to complete. ​ Add New Data Sources You can connect a data source to MindsDB by clicking the Add button and choosing New Datasource . It takes you to a page that lists all available data sources, including, databases, data warehouses, applications, and more. Here, you can search for a data source you want to connect to and follow the instructions. For more information, visit the Data Sources section of the docs. ​ Upload Files You can upload a file to MindsDB by clicking the Add button and choosing Upload File . It takes you to a form where you can upload a file and give it a name. For more information, visit our docs here . ​ Upload Custom Models MindsDB offers a way to upload your custom model in the form of Python code and incorporate it into the MindsDB ecosystem. You can do that by clicking the Add button and choosing Upload custom model . For more information, visit our docs here . ​ Learning Hub This is the best place to start exploring MindsDB. In the Learning Hub, you can find various tutorials and follow them by copying and pasting the code into the query editor and executing it. The Learning Hub lists useful links, including documentation and customer support contact. Here is a video that showcases all the features of the MindsDB GUI editor. Was this page helpful? Yes No Suggest edits Raise issue Feature Importance Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Accessing the MindsDB GUI Editor Exploring the MindsDB GUI Editor Query Editor Results Viewer Object Explorer Model Progress Bar Add New Data Sources Upload Files Upload Custom Models Learning Hub"}
{"file_name": "model-types.html", "content": "Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more Models In this section, we go through the AI models types available in MindsDB. These are regression models, classification models, time series models, and large language models (LLMs). Disclaimer In this section, we describe the default behavior using the Lightwood ML engine for regression, classification, and time series models. Other ML handlers may behave differently. For example, some may not perform validation automatically when creating a model, as numerous behaviors are handler-specific. ​ What is an AI models? A machine learning (ML) model is a program trained using the available data in order to learn how to recognize patterns and behaviors to predict future data. There are various types of AI models that use different learning paradigms, but MindsDB models are all supervised because they learn from pairs of input data and expected output. You input data into an AI models. It processes this input data, searching for patterns and correlations. After that, the AI models returns output data defined based on the input data. ​ Features Features are variables that an AI models uses as input data to search for patterns and predict the target variable. In tabular datasets, features usually correspond to single columns. ​ Target The target is a variable of interest that an AI models predicts based on the information fetched out of the features. ​ Training Dataset The training dataset is used during the training phase of an AI models. It contains both feature variables and a target variable. As its name indicates, it is used to train an AI model. The AI model takes the entire training dataset as input. It learns the patterns and relationships between feature variables and target values. Once the training process is complete, one can move on to the validation phase. ​ Validation Dataset The validation dataset is used during the validation phase of an AI models. It contains both feature variables and a target variable, like the training dataset. But as its name indicates, it is used to validate the predictions made by an AI models. It has no overlap with the training dataset, as it is a held-out set to simulate a real scenario where the model generates predictions for novel input data. The AI models takes only the feature variables from the validation dataset as input. Based on what the model learns during the training process, it makes predictions for the values of a target variable. Now comes the validation step. To assess the accuracy of the AI models, one compares the target variable values from the validation dataset with the target variable values predicted by the AI models. The closer these values are to each other, the better accuracy of the AI models. ​ Input Dataset After completing the training and validation phases, one can provide the input dataset consisting of only the feature variables to predict the target variable values. ​ How is an AI Model Created? In MindsDB, we use the CREATE MODEL statement to create, train, and validate a model. ​ Training Phase Let’s look at our training dataset. It contains both features and a target. SELECT * FROM files . salary_dataset LIMIT 5 ; On execution, we get: + ---------+--------------+-----------+---------+--------+---------------+-------------------+------+ | companyId | jobType | degree | major | industry | yearsExperience | milesFromMetropolis | salary | + ---------+--------------+-----------+---------+--------+---------------+-------------------+------+ | COMP37 | CFO | MASTERS | MATH | HEALTH | 10 | 83 | 130 | | COMP19 | CEO | HIGH_SCHOOL | NONE | WEB | 3 | 73 | 101 | | COMP52 | VICE_PRESIDENT | DOCTORAL | PHYSICS | HEALTH | 10 | 38 | 137 | | COMP38 | MANAGER | DOCTORAL | CHEMISTRY | AUTO | 8 | 17 | 142 | | COMP7 | VICE_PRESIDENT | BACHELORS | PHYSICS | FINANCE | 8 | 16 | 163 | + ---------+--------------+-----------+---------+--------+---------------+-------------------+------+ Here, the features are companyId , jobType , degree , major , industry , yearsExperience , and milesFromMetropolis . And the target variable is salary . Let’s create and train an AI models using this training dataset. CREATE MODEL salary_predictor FROM files ( SELECT * FROM salary_dataset ) PREDICT salary ; On execution, we get: Query successfully completed ​ Progress Here is how to check whether the training process is completed: DESCRIBE salary_predictor ; Once the status is complete , the training phase is completed. ​ Validation Phase By default, the CREATE MODEL statement performs validation of the model. Additionally, we can validate the model manually by querying it and providing the feature values in the WHERE clause like this: SELECT salary , salary_explain FROM mindsdb . salary_predictor WHERE companyId = 'COMP37' AND jobType = 'MANAGER' AND degree = 'DOCTORAL' AND major = 'MATH' AND industry = 'FINANCE' AND yearsExperience = 5 AND milesFromMetropolis = 50 ; On execution, we get: + ------+------------------------------------------------------------------------------------------------------------------------------------------+ | salary | salary_explain | + ------+------------------------------------------------------------------------------------------------------------------------------------------+ | 128 | { \"predicted_value\" : 128 , \"confidence\" : 0.67 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 109 , \"confidence_upper_bound\" : 147 } | + ------+------------------------------------------------------------------------------------------------------------------------------------------+ By comparing the real salary values for the defined individuals and the predicted salary values, one can figure out the accuracy of the AI models. Please note that MindsDB calculates the model’s accuracy by default while running the CREATE MODEL statement. However, it is not guaranteed that all ML engines do this. By default, the CREATE MODEL statement does the following: it creates a model, it divides the input data into training and validation datasets, it trains a model using the training dataset, it validates a model using the validation dataset, it compares the true and predicted values of a target to define the model’s accuracy. Let’s look at the basic types of AI models. ​ AI Model Types ​ Regression Models Regression is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted. In the case of regression models, the target variable belongs to a set of continuous values. For example, having data on real estates, such as the number of rooms, location, and rental price, one can predict the rental price using regression. The rental price is predicted based on the input data, and its value is any value from a range between minimum and maximum rental price values from the training data. ​ Example First, let’s look at our input data. SELECT * FROM example_db . demo_data . home_rentals LIMIT 5 ; On execution, we get: + ---------------+-------------------+----+--------+--------------+--------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+--------------+------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + ---------------+-------------------+----+--------+--------------+--------------+------------+ Here, the features are number_of_rooms , number_of_bathrooms , sqft , location , days_on_market , and neighborhood . And the target variable is rental_price . Let’s create and train an AI models. CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; On execution, we get: Query successfully completed Once the training process is completed, we can query for predictions. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ For details, check out this tutorial . ​ Classification Models Classification is a type of predictive modeling that analyses input data, including relationships between dependent and independent variables and the target variable that is to be predicted. In the case of classification models, the target variable belongs to a set of discrete values. For example, having data on each customer of a telecom company, one can predict the churn possibility using classification. The churn is predicted based on the input data, and its value is either Yes or No . This is a special case called binary classification. ​ Example First, let’s look at our input data. SELECT * FROM example_db . demo_data . customer_churn LIMIT 5 ; On execution, we get: + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+ | customerid | gender | seniorcitizen | partner | dependents | tenure | phoneservice | multiplelines | internetservice | onlinesecurity | onlinebackup | deviceprotection | techsupport | streamingtv | streamingmovies | contract | paperlessbilling | paymentmethod | monthlycharges | totalcharges | churn | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+ | 7590 - VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month - to - month | Yes | Electronic check | $ 29.85 | $ 29.85 | No | | 5575 - GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | $ 56.95 | $ 1 , 889.50 | No | | 3668 - QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month - to - month | Yes | Mailed check | $ 53.85 | $ 108.15 | Yes | | 7795 - CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer ( automatic ) | $ 42.30 | $ 1 , 840.75 | No | | 9237 - HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month - to - month | Yes | Electronic check | $ 70.70 | $ 151.65 | Yes | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+------+ Here, the features are customerid , gender , seniorcitizen , partner , dependents , tenure , phoneservice , multiplelines , internetservice , onlinesecurity , onlinebackup , deviceprotection , techsupport , streamingtv , streamingmovies , contract , paperlessbilling , paymentmethod , monthlycharges , and totalcharges . And the target variable is churn . Let’s create and train an AI models. CREATE MODEL churn_predictor FROM example_db ( SELECT * FROM demo_data . customer_churn ) PREDICT churn ; On execution, we get: Query successfully completed Once the training process is completed, we can query for predictions. SELECT churn , churn_confidence , churn_explain FROM mindsdb . customer_churn_predictor WHERE seniorcitizen = 0 AND partner = 'Yes' AND dependents = 'No' AND tenure = 1 AND phoneservice = 'No' AND multiplelines = 'No phone service' AND internetservice = 'DSL' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0.7752808988764045 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0.7752808988764045 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.4756 , \"probability_class_Yes\" : 0.5244 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ For details, check out this tutorial . ​ Time Series Models Time series models fall under the regression or classification category. But what’s distinct about them is that we order data by date, time, or any value defining sequential order of events. Usually, predictions made by time series models are referred to as forecasts . A time series model predicts a target that comes from a continuous set (regression) or a discrete set (classification). There is a mandatory ORDER BY clause followed by a sequential column, such as a date. It orders all the rows accordingly. If you want to group your predictions, there is an optional GROUP BY clause. By following this clause with a column name, or multiple column names, one can make predictions for partitions of data defined by these columns. In the case of time series models, one should define how many data rows are used to train the model. The WINDOW clause followed by an integer does just that. There is an optional HORIZON clause where you can define how many rows, or how far into the future, you want to predict. By default, it is one. ​ Example First, let’s look at our input data. SELECT * FROM example_db . demo_data . house_sales LIMIT 5 ; On execution, we get: + ----------+------+-----+--------+ | saledate | ma | type | bedrooms | + ----------+------+-----+--------+ | 2007 - 09 - 30 | 441854 | house | 2 | | 2007 - 12 - 31 | 441854 | house | 2 | | 2008 - 03 - 31 | 441854 | house | 2 | | 2008 - 06 - 30 | 441854 | house | 2 | | 2008 - 09 - 30 | 451583 | house | 2 | + ----------+------+-----+--------+ Here, the features are saledate , type , and bedrooms . And the target variable is ma . Let’s create and train an AI models. CREATE MODEL mindsdb . house_sales_predictor FROM files ( SELECT * FROM house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms , type -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) On execution, we get: Query successfully completed Once the training process is completed, we can query for predictions. SELECT m . saledate AS date , m . MA AS forecast , MA_explain FROM mindsdb . house_sales_predictor AS m JOIN files . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' AND t . bedrooms = 2 LIMIT 4 ; On execution, we get: + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | date | forecast | MA_explain | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | 2019 - 12 - 31 | 441413.5849598734 | { \"predicted_value\" : 441413.5849598734 , \"confidence\" : 0.99 , \"anomaly\" : true , \"truth\" : null , \"confidence_lower_bound\" : 440046.28237074096 , \"confidence_upper_bound\" : 442780.88754900586 } | | 2020 - 04 - 01 | 443292.5194586229 | { \"predicted_value\" : 443292.5194586229 , \"confidence\" : 0.9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 427609.3325864327 , \"confidence_upper_bound\" : 458975.7063308131 } | | 2020 - 07 - 02 | 443292.5194585953 | { \"predicted_value\" : 443292.5194585953 , \"confidence\" : 0.9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501.59192981094 , \"confidence_upper_bound\" : 462083.4469873797 } | | 2020 - 10 - 02 | 443292.5194585953 | { \"predicted_value\" : 443292.5194585953 , \"confidence\" : 0.9991 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 424501.59192981094 , \"confidence_upper_bound\" : 462083.4469873797 } | + -------------+-------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ For details, check out this tutorial . ​ Large Language Models Large language models are advanced artificial intelligence systems designed to process and generate human-like language. These models leverage deep learning techniques, such as transformer architectures, to analyze vast amounts of text data and learn complex patterns and relationships within the language. Large language models have applications in chatbots, content generation, language translation, sentiment analysis, and various natural language processing tasks. ​ Example Check out examples here: Slack chatbot Sentiment analysis NLP examples ​ How it Works in the Background MindsDB uses the Lightwood ML engine by default. This section takes a closer look at how this package automatically chooses what type of model to use. Models in Lightwood follow an encoder-mixer-decoder pattern, where refined, or encoded, representations of all features are mixed to produce target predictions. Here are the mixers used by Lightwood . Please note that there is an ensembling step after training all mixers in case multiple mixers are used. Read on to learn more. To give you some details on how MindsDB creates a model using different mixers, here is the full code . And here comes the breakdown: This piece of code adds mixers to the submodels array depending on the model type and the data type of the target variable. And here , we choose the best of submodels to be used to create, train, and validate our AI models. Let’s dive into the details of how MindsDB picks the mixers. Case 1 Case 2 Case 3 Here is the piece of code being analyzed. If we deal with a simple encoder/decoder pair performing the task , we use the Unit mixer that can be thought of as a bypass mixer. A good example is the Spam Classifier model of Hugging Face because it uses a single column as input. Otherwise , we choose from a range of other mixers depending on the following conditions: If it is not a time series case , we use the Neural mixer. A good example is the Customer Churn model . If it is a time series case , we use the NeuralTs mixer. A good example is the House Sales model . MindsDB may use one or multiple mixers while preparing a model. Depending on the model type and the data type of the target variable, one mixer is chosen or a set of mixers are ensembled to create, train, and validate an AI models. The three cases above describe how MindsDB chooses the mixer candidates and stores them in the submodels array. By default, after training all relevant mixers in the submodels array, MindsDB uses the BestOf ensemble to single out the best mixer as the final model . But you can always use a different ensemble that may aggregate multiple mixers per model, such as the MeanEnsemble , ModeEnsemble , StackedEnsemble , TsStackedEnsemble , or WeightedMeanEnsemble ensemble type. Here , you’ll find implementations of all ensemble types. Next Steps Below are the links to help you explore further. CREATE MODEL Custom SQL syntax. db.predictors.insertOne() Custom Mongo-QL syntax. project.create_model() Directly in Python. MindsDB.Models.trainModel() Directly in JavaScript. Train a Model Using REST APIs. Was this page helpful? Yes No Suggest edits Raise issue MindsDB GUI Overview Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What is an AI models? Features Target Training Dataset Validation Dataset Input Dataset How is an AI Model Created? Training Phase Progress Validation Phase AI Model Types Regression Models Example Classification Models Example Time Series Models Example Large Language Models Example How it Works in the Background"}
{"file_name": "project.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "database.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "jobs.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "usage.html", "content": "Usage - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect Usage Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Usage Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Connect Usage Here is how to connect and use REST API to MindsDB. ​ Local MindsDB Here is an example in Python: import requests # connect url = 'http://127.0.0.1:47334/api/sql/query' # query resp = requests.post(url, json={'query': 'SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;'}) # response print(resp.text) # alternative: print(resp.json()) Was this page helpful? Yes No Suggest edits Raise issue Overview Connect a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Local MindsDB"}
{"file_name": "overview.html", "content": "REST API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation REST API REST API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents REST API REST API MindsDB provides REST API endpoints, enabling incorporation of AI building blocks into applications. This section introduces REST API endpoints provided by MindsDB to bring data and AI together. Follow these steps to get started: 1 Set up the development environment Learn more about usage here . 2 Connect a data source Connect your data source to MindsDB via this endpoint . Explore all available data sources here . 3 Create and deploy an AI/ML model Create, train, and deploy AI/ML models within MindsDB via this endpoint . Explore all available AI engines here . 4 Get predictions Query for predictions via this endpoint . Was this page helpful? Yes No Suggest edits Raise issue Usage github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "sql.html", "content": "Query - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Query Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Query Shell Python curl --request POST \\ --url https://cloud.mindsdb.com/api/sql/query \\ --header 'Content-Type: application/json' \\ --cookie '{session=273trgsehgrui3i2riurwehe}' \\ --data ' { \"query\" : \"SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;\" } Response { \"column_names\" : [ \"sqft\" , \"rental_price\" ] , \"context\" : { \"db\" : \"mindsdb\" } , \"data\" : [ [ 917 , 3901 ] , [ 194 , 2042 ] ] , \"type\" : \"table\" } ​ Description This API provides a REST endpoint for executing the SQL queries. Note: This endpoint is a HTTP POST method. This endpoint accept data via application/json request body. The only required key is the query which has the SQL statement value. ​ Body ​ query string required String that contains the SQL query that needs to be executed. ​ Response ​ column_names array required A list with the column names returned ​ context object required The database where the query is executed ​ data array The actual data returned by the query in case of the table response type ​ type string The type of the response table | error | ok Shell Python curl --request POST \\ --url https://cloud.mindsdb.com/api/sql/query \\ --header 'Content-Type: application/json' \\ --cookie '{session=273trgsehgrui3i2riurwehe}' \\ --data ' { \"query\" : \"SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;\" } Response { \"column_names\" : [ \"sqft\" , \"rental_price\" ] , \"context\" : { \"db\" : \"mindsdb\" } , \"data\" : [ [ 917 , 3901 ] , [ 194 , 2042 ] ] , \"type\" : \"table\" } Was this page helpful? Yes No Suggest edits Raise issue Delete From a Table Create a View github facebook twitter slack linkedin youtube medium Powered by Mintlify Shell Python curl --request POST \\ --url https://cloud.mindsdb.com/api/sql/query \\ --header 'Content-Type: application/json' \\ --cookie '{session=273trgsehgrui3i2riurwehe}' \\ --data ' { \"query\" : \"SELECT * FROM example_db.demo_data.home_rentals LIMIT 10;\" } Response { \"column_names\" : [ \"sqft\" , \"rental_price\" ] , \"context\" : { \"db\" : \"mindsdb\" } , \"data\" : [ [ 917 , 3901 ] , [ 194 , 2042 ] ] , \"type\" : \"table\" }"}
{"file_name": "drop.html", "content": "Remove a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Remove a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project GET List Projects GET Get a Project Models Tables Views Files Jobs AI Agents Projects Remove a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Create a Project List Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project GET List Projects GET Get a Project Models Tables Views Files Jobs AI Agents Projects Create a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "get-project.html", "content": "Get a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Get a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project GET List Projects GET Get a Project Models Tables Views Files Jobs AI Agents Projects Get a Project GET / api / projects / {projectName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } 200 401 404 500 { \"name\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project Response 200 200 401 404 500 application/json The returned project ​ name string Was this page helpful? Yes No Suggest edits Raise issue List Projects Create, Train, and Deploy a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } 200 401 404 500 { \"name\" : \"<string>\" }"}
{"file_name": "get-projects.html", "content": "List Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects List Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project GET List Projects GET Get a Project Models Tables Views Files Jobs AI Agents Projects List Projects GET / api / projects Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects 200 401 500 [ { \"name\" : \"<string>\" } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Response 200 200 401 500 application/json A JSON array of projects ​ name string Was this page helpful? Yes No Suggest edits Raise issue Remove a Project Get a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects 200 401 500 [ { \"name\" : \"<string>\" } ]"}
{"file_name": "delete.html", "content": "Remove a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Remove a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs POST Create a Job GET List Jobs GET Get a Job DEL Remove a Job AI Agents Jobs Remove a Job DELETE / api / projects / {projectName} / jobs / {jobName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /jobs/ { jobName } 200 401 404 500 \"<string>\" The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project ​ jobName string required The name of the job Response 200 200 401 404 500 application/json Empty response if delete is successful The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Get a Job Create an Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /jobs/ { jobName } 200 401 404 500 \"<string>\""}
{"file_name": "delete-table.html", "content": "Remove a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Remove a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Remove a Table DELETE / api / databases / {databaseName} / tables / {tableName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables/ { tableName } 200 401 404 500 { } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required The name of the database ​ tableName string required Table name to delete Response 200 200 401 404 500 application/json Successfully deleted The response is of type object . Was this page helpful? Yes No Suggest edits Raise issue Create a Table List Tables github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables/ { tableName } 200 401 404 500 { }"}
{"file_name": "list-table.html", "content": "Get a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Get a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Get a Table GET / api / databases / {databaseName} / tables / {tableName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables/ { tableName } 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required The name of the database ​ tableName string required The name of the table Response 200 200 401 404 500 application/json A JSON object with SELECT data ​ name string ​ type string Type of table (data | view) Was this page helpful? Yes No Suggest edits Raise issue List Tables Update a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables/ { tableName } 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" }"}
{"file_name": "create-table.html", "content": "Create a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Create a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Create a Table POST / api / databases / {databaseName} / tables Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables \\ --header 'Content-Type: application/json' \\ --data '{ \"table\": { \"name\": \"<string>\", \"select\": \"<string>\", \"replace\": true } }' 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required Name of the database Body application/json ​ table object Show child attributes ​ table. name string ​ table. select string SELECT statement to create the table from ​ table. replace boolean Whether or not to delete a pre-existing table before creating it Response 200 200 401 404 500 application/json Table created ​ name string ​ type string Type of table (data | view) Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Remove a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables \\ --header 'Content-Type: application/json' \\ --data '{ \"table\": { \"name\": \"<string>\", \"select\": \"<string>\", \"replace\": true } }' 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" }"}
{"file_name": "insert.html", "content": "Insert Into a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Insert Into a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Insert Into a Table This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Update a Table Delete From a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list-tables.html", "content": "List Tables - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables List Tables Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables List Tables GET / api / databases / {databaseName} / tables Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables 200 401 404 500 [ { \"name\" : \"<string>\" , \"type\" : \"<string>\" } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required The name of the database Response 200 200 401 404 500 application/json A JSON array of tables ​ name string ​ type string Type of table (data | view) Was this page helpful? Yes No Suggest edits Raise issue Remove a Table Get a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } /tables 200 401 404 500 [ { \"name\" : \"<string>\" , \"type\" : \"<string>\" } ]"}
{"file_name": "delete.html", "content": "Delete From a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Delete From a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Delete From a Table This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Insert Into a Table Query github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "update.html", "content": "Update a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables Update a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables POST Create a Table DEL Remove a Table GET List Tables GET Get a Table Update a Table Insert Into a Table Delete From a Table POST Query Views Files Jobs AI Agents Tables Update a Table This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Get a Table Insert Into a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create-databases.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources POST Connect a Data Source PUT Update a Data Source DEL Remove a Data Source GET List Data Sources GET Get a Data Source AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Data Sources Connect a Data Source POST / api / databases Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/databases \\ --header 'Content-Type: application/json' \\ --data '{ \"database\": { \"name\": \"<string>\", \"engine\": \"<string>\", \"parameters\": { \"user\": \"<string>\", \"password\": \"<string>\", \"host\": \"<string>\", \"port\": \"<string>\", \"database\": \"<string>\" } } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Body application/json ​ database object Show child attributes ​ database. name string ​ database. engine string ​ database. parameters object Parameters used to connect to your data source. These example parameters are for connecting to MySQL . See here for which parameters to use for your datasource. Show child attributes ​ database.parameters. user string ​ database.parameters. password string ​ database.parameters. host string ​ database.parameters. port string ​ database.parameters. database string Response 200 200 400 401 409 500 application/json The created database ​ name string ​ engine string Handler used to create this database (e.g. postgres) ​ type string Type of database (data | project | system) Was this page helpful? Yes No Suggest edits Raise issue Usage Update a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/databases \\ --header 'Content-Type: application/json' \\ --data '{ \"database\": { \"name\": \"<string>\", \"engine\": \"<string>\", \"parameters\": { \"user\": \"<string>\", \"password\": \"<string>\", \"host\": \"<string>\", \"port\": \"<string>\", \"database\": \"<string>\" } } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" }"}
{"file_name": "list-databases.html", "content": "List Data Sources - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Sources Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources POST Connect a Data Source PUT Update a Data Source DEL Remove a Data Source GET List Data Sources GET Get a Data Source AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Data Sources List Data Sources GET / api / databases Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases 200 401 500 [ { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Response 200 200 401 500 application/json A JSON array of database names ​ name string ​ engine string Handler used to create this database (e.g. postgres) ​ type string Type of database (data | project | system) Was this page helpful? Yes No Suggest edits Raise issue Remove a Data Source Get a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases 200 401 500 [ { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" } ]"}
{"file_name": "list-database.html", "content": "Get a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Get a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources POST Connect a Data Source PUT Update a Data Source DEL Remove a Data Source GET List Data Sources GET Get a Data Source AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Data Sources Get a Data Source GET / api / databases / {databaseName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } 200 401 404 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required Name of existing database Response 200 200 401 404 500 application/json A JSON object with database informations ​ name string ​ engine string Handler used to create this database (e.g. postgres) ​ type string Type of database (data | project | system) Was this page helpful? Yes No Suggest edits Raise issue List Data Sources Configure an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } 200 401 404 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" }"}
{"file_name": "update-databases.html", "content": "Update a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Update a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources POST Connect a Data Source PUT Update a Data Source DEL Remove a Data Source GET List Data Sources GET Get a Data Source AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Data Sources Update a Data Source PUT / api / databases / {databaseName} Try it cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } \\ --header 'Content-Type: application/json' \\ --data '{ \"database\": { \"engine\": \"<string>\", \"parameters\": { \"user\": \"<string>\", \"password\": \"<string>\", \"host\": \"<string>\", \"port\": \"<string>\", \"database\": \"<string>\" } } }' 200 400 401 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required The name of the project Body application/json ​ database object Show child attributes ​ database. engine string ​ database. parameters object Parameters used to connect to your data source. These example parameters are for connecting to MySQL . See here for which parameters to use for your datasource. Show child attributes ​ database.parameters. user string ​ database.parameters. password string ​ database.parameters. host string ​ database.parameters. port string ​ database.parameters. database string Response 200 200 400 401 500 application/json Database was successfully updated ​ name string ​ engine string Handler used to create this database (e.g. postgres) ​ type string Type of database (data | project | system) Was this page helpful? Yes No Suggest edits Raise issue Connect a Data Source Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } \\ --header 'Content-Type: application/json' \\ --data '{ \"database\": { \"engine\": \"<string>\", \"parameters\": { \"user\": \"<string>\", \"password\": \"<string>\", \"host\": \"<string>\", \"port\": \"<string>\", \"database\": \"<string>\" } } }' 200 400 401 500 { \"name\" : \"<string>\" , \"engine\" : \"<string>\" , \"type\" : \"<string>\" }"}
{"file_name": "delete-databases.html", "content": "Remove a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Remove a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources POST Connect a Data Source PUT Update a Data Source DEL Remove a Data Source GET List Data Sources GET Get a Data Source AI/ML Engines Projects Models Tables Views Files Jobs AI Agents Data Sources Remove a Data Source DELETE / api / databases / {databaseName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } 200 401 404 500 \"<string>\" The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ databaseName string required Name of existing database to delete Response 200 200 401 404 500 application/json An empty response indicates success The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Update a Data Source List Data Sources github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/databases/ { databaseName } 200 401 404 500 \"<string>\""}
{"file_name": "list-chatbots.html", "content": "List All Chatbots - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents List All Chatbots Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents List All Chatbots GET / api / projects / {projectName} / chatbots Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots 200 401 500 [ { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" } ] Path Parameters ​ projectName string required The name of the project where agent resides Response 200 200 401 500 application/json A JSON array of skill names ​ name string ​ database_name string Name of the connection to a chat app like Slack or MS Teams ​ agent_name string Agent object created beforehand. Alternatively, provide a large language model (LLM) using the model_name parameter. Was this page helpful? Yes No Suggest edits Raise issue Create a Chatbot Get a Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots 200 401 500 [ { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" } ]"}
{"file_name": "update-skill.html", "content": "Update a Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Update a Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Update a Skill PUT / api / projects / {projectName} / skills / {skillName} Try it cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } \\ --header 'Content-Type: application/json' \\ --data '{ \"skill\": { \"name\": \"<string>\", \"type\": \"<string>\", \"source\": \"<string>\", \"database\": \"<string>\", \"tables\": [ \"<string>\" ], \"description\": \"<string>\" } }' 200 400 401 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides ​ skillName string required The name of the skill Body application/json ​ skill object Show child attributes ​ skill. name string ​ skill. type string Type of skill (text2sql | knowledge_base). ​ skill. source string Used to store a knowledge_base object when type is set to knowledge_base. ​ skill. database string Used to store a data source connection when type is set to text2sql. ​ skill. tables string[] Used to store table(s) names when type is set to text2sql. ​ skill. description string Skill description is important for an agent to decide which skill to use. Response 200 200 400 401 500 application/json Skill was successfully updated ​ name string ​ type string Type of skill (text2sql | knowledge_base). ​ source string Used to store a knowledge_base object when type is set to knowledge_base. ​ database string Used to store a data source connection when type is set to text2sql. ​ tables string[] Used to store table(s) names when type is set to text2sql. ​ description string Skill description is important for an agent to decide which skill to use. Was this page helpful? Yes No Suggest edits Raise issue Get a Skill Delete a Skill github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } \\ --header 'Content-Type: application/json' \\ --data '{ \"skill\": { \"name\": \"<string>\", \"type\": \"<string>\", \"source\": \"<string>\", \"database\": \"<string>\", \"tables\": [ \"<string>\" ], \"description\": \"<string>\" } }' 200 400 401 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" }"}
{"file_name": "get-chatbot.html", "content": "Get a Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Get a Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Get a Chatbot GET / api / projects / {projectName} / chatbots / {chatbotName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } 200 401 404 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides ​ chatbotName string required Name of existing chatbot Response 200 200 401 404 500 application/json A JSON object with chatbots ​ name string ​ database_name string Name of the connection to a chat app like Slack or MS Teams ​ agent_name string Agent object created beforehand. Alternatively, provide a large language model (LLM) using the model_name parameter. Was this page helpful? Yes No Suggest edits Raise issue List All Chatbots Update a Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } 200 401 404 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" }"}
{"file_name": "create-agent.html", "content": "Create an Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Create an Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Create an Agent POST / api / projects / {projectName} / agents Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents \\ --header 'Content-Type: application/json' \\ --data '{ \"agent\": { \"name\": \"<string>\", \"model\": \"<string>\", \"skills\": [ \"<string>\" ] } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] } Path Parameters ​ projectName string required The name of the project where agent resides Body application/json ​ agent object Show child attributes ​ agent. name string ​ agent. model string A conversational model used by an agent ​ agent. skills string[] One or more skills that an agent can use Response 200 200 400 401 409 500 application/json Created an agent ​ name string ​ model string A conversational model used by an agent ​ skills string[] One or more skills that an agent can use Was this page helpful? Yes No Suggest edits Raise issue Remove a Job List All Agents github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents \\ --header 'Content-Type: application/json' \\ --data '{ \"agent\": { \"name\": \"<string>\", \"model\": \"<string>\", \"skills\": [ \"<string>\" ] } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] }"}
{"file_name": "create-skill.html", "content": "Create a Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Create a Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Create a Skill POST / api / projects / {projectName} / skills Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills \\ --header 'Content-Type: application/json' \\ --data '{ \"skill\": { \"name\": \"<string>\", \"type\": \"<string>\", \"source\": \"<string>\", \"database\": \"<string>\", \"tables\": [ \"<string>\" ], \"description\": \"<string>\" } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides Body application/json ​ skill object Show child attributes ​ skill. name string ​ skill. type string Type of skill (text2sql | knowledge_base). ​ skill. source string Used to store a knowledge_base object when type is set to knowledge_base. ​ skill. database string Used to store a data source connection when type is set to text2sql. ​ skill. tables string[] Used to store table(s) names when type is set to text2sql. ​ skill. description string Skill description is important for an agent to decide which skill to use. Response 200 200 400 401 409 500 application/json Created a skill ​ name string ​ type string Type of skill (text2sql | knowledge_base). ​ source string Used to store a knowledge_base object when type is set to knowledge_base. ​ database string Used to store a data source connection when type is set to text2sql. ​ tables string[] Used to store table(s) names when type is set to text2sql. ​ description string Skill description is important for an agent to decide which skill to use. Was this page helpful? Yes No Suggest edits Raise issue Delete an Agent List All Skills github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills \\ --header 'Content-Type: application/json' \\ --data '{ \"skill\": { \"name\": \"<string>\", \"type\": \"<string>\", \"source\": \"<string>\", \"database\": \"<string>\", \"tables\": [ \"<string>\" ], \"description\": \"<string>\" } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" }"}
{"file_name": "delete-chatbot.html", "content": "Delete a Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Delete a Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Delete a Chatbot DELETE / api / projects / {projectName} / chatbots / {chatbotName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } 200 401 404 500 \"<string>\" Path Parameters ​ projectName string required The name of the project where agent resides ​ chatbotName string required Name of existing chatbot to delete Response 200 200 401 404 500 application/json An empty response indicates success The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Update a Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } 200 401 404 500 \"<string>\""}
{"file_name": "update-chatbot.html", "content": "Update a Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Update a Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Update a Chatbot PUT / api / projects / {projectName} / chatbots / {chatbotName} Try it cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } \\ --header 'Content-Type: application/json' \\ --data '{ \"chatbot\": { \"name\": \"<string>\", \"database\": \"<string>\", \"agent\": \"<string>\" } }' 200 400 401 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides ​ chatbotName string required The name of the chatbot Body application/json ​ chatbot object Show child attributes ​ chatbot. name string ​ chatbot. database string Connection to a chat app like Slack or MS Teams ​ chatbot. agent string Agent object created beforehand Response 200 200 400 401 500 application/json Chatbot was successfully updated ​ name string ​ database_name string Name of the connection to a chat app like Slack or MS Teams ​ agent_name string Agent object created beforehand. Alternatively, provide a large language model (LLM) using the model_name parameter. Was this page helpful? Yes No Suggest edits Raise issue Get a Chatbot Delete a Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots/ { chatbotName } \\ --header 'Content-Type: application/json' \\ --data '{ \"chatbot\": { \"name\": \"<string>\", \"database\": \"<string>\", \"agent\": \"<string>\" } }' 200 400 401 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" }"}
{"file_name": "delete-agent.html", "content": "Delete an Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Delete an Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Delete an Agent DELETE / api / projects / {projectName} / agents / {agentName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } 200 401 404 500 \"<string>\" Path Parameters ​ projectName string required The name of the project where agent resides ​ agentName string required Name of existing agent to delete Response 200 200 401 404 500 application/json An empty response indicates success The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Update an Agent Create a Skill github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } 200 401 404 500 \"<string>\""}
{"file_name": "get-agent.html", "content": "Get an Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Get an Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Get an Agent GET / api / projects / {projectName} / agents / {agentName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } 200 401 404 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] } Path Parameters ​ projectName string required The name of the project where agent resides ​ agentName string required Name of existing agent Response 200 200 401 404 500 application/json A JSON object with skills ​ name string ​ model string A conversational model used by an agent ​ skills string[] One or more skills that an agent can use Was this page helpful? Yes No Suggest edits Raise issue List All Agents Query an Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } 200 401 404 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] }"}
{"file_name": "update-agent.html", "content": "Update an Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Update an Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Update an Agent PUT / api / projects / {projectName} / agents / {agentName} Try it cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } \\ --header 'Content-Type: application/json' \\ --data '{ \"agent\": { \"name\": \"<string>\", \"model\": \"<string>\", \"skills\": [ \"<string>\" ] } }' 200 400 401 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] } Path Parameters ​ projectName string required The name of the project where agent resides ​ agentName string required The name of the agent Body application/json ​ agent object Show child attributes ​ agent. name string ​ agent. model string A conversational model used by an agent ​ agent. skills string[] One or more skills that an agent can use Response 200 200 400 401 500 application/json Agent was successfully updated ​ name string ​ model string A conversational model used by an agent ​ skills string[] One or more skills that an agent can use Was this page helpful? Yes No Suggest edits Raise issue Query an Agent Delete an Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request PUT \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } \\ --header 'Content-Type: application/json' \\ --data '{ \"agent\": { \"name\": \"<string>\", \"model\": \"<string>\", \"skills\": [ \"<string>\" ] } }' 200 400 401 500 { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] }"}
{"file_name": "list-skills.html", "content": "List All Skills - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents List All Skills Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents List All Skills GET / api / projects / {projectName} / skills Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills 200 401 500 [ { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" } ] Path Parameters ​ projectName string required The name of the project where agent resides Response 200 200 401 500 application/json A JSON array of skill names ​ name string ​ type string Type of skill (text2sql | knowledge_base). ​ source string Used to store a knowledge_base object when type is set to knowledge_base. ​ database string Used to store a data source connection when type is set to text2sql. ​ tables string[] Used to store table(s) names when type is set to text2sql. ​ description string Skill description is important for an agent to decide which skill to use. Was this page helpful? Yes No Suggest edits Raise issue Create a Skill Get a Skill github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills 200 401 500 [ { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" } ]"}
{"file_name": "get-skill.html", "content": "Get a Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Get a Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Get a Skill GET / api / projects / {projectName} / skills / {skillName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides ​ skillName string required Name of existing skill Response 200 200 401 404 500 application/json A JSON object with skills ​ name string ​ type string Type of skill (text2sql | knowledge_base). ​ source string Used to store a knowledge_base object when type is set to knowledge_base. ​ database string Used to store a data source connection when type is set to text2sql. ​ tables string[] Used to store table(s) names when type is set to text2sql. ​ description string Skill description is important for an agent to decide which skill to use. Was this page helpful? Yes No Suggest edits Raise issue List All Skills Update a Skill github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } 200 401 404 500 { \"name\" : \"<string>\" , \"type\" : \"<string>\" , \"source\" : \"<string>\" , \"database\" : \"<string>\" , \"tables\" : [ \"<string>\" ] , \"description\" : \"<string>\" }"}
{"file_name": "list-agents.html", "content": "List All Agents - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents List All Agents Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents List All Agents GET / api / projects / {projectName} / agents Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents 200 401 500 [ { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] } ] Path Parameters ​ projectName string required The name of the project where agent resides Response 200 200 401 500 application/json A JSON array of skill names ​ name string ​ model string A conversational model used by an agent ​ skills string[] One or more skills that an agent can use Was this page helpful? Yes No Suggest edits Raise issue Create an Agent Get an Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents 200 401 500 [ { \"name\" : \"<string>\" , \"model\" : \"<string>\" , \"skills\" : [ \"<string>\" ] } ]"}
{"file_name": "create-chatbot.html", "content": "Create a Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Create a Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Create a Chatbot POST / api / projects / {projectName} / chatbots Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots \\ --header 'Content-Type: application/json' \\ --data '{ \"chatbot\": { \"name\": \"<string>\", \"database_name\": \"<string>\", \"agent_name\": \"<string>\" } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" } Path Parameters ​ projectName string required The name of the project where agent resides Body application/json ​ chatbot object Show child attributes ​ chatbot. name string ​ chatbot. database_name string Name of the connection to a chat app like Slack or MS Teams ​ chatbot. agent_name string Agent object created beforehand. Alternatively, provide a large language model (LLM) using the model_name parameter. Response 200 200 400 401 409 500 application/json Created a chatbot ​ name string ​ database_name string Name of the connection to a chat app like Slack or MS Teams ​ agent_name string Agent object created beforehand. Alternatively, provide a large language model (LLM) using the model_name parameter. Was this page helpful? Yes No Suggest edits Raise issue Delete a Skill List All Chatbots github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /chatbots \\ --header 'Content-Type: application/json' \\ --data '{ \"chatbot\": { \"name\": \"<string>\", \"database_name\": \"<string>\", \"agent_name\": \"<string>\" } }' 200 400 401 409 500 { \"name\" : \"<string>\" , \"database_name\" : \"<string>\" , \"agent_name\" : \"<string>\" }"}
{"file_name": "delete-skill.html", "content": "Delete a Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Delete a Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Delete a Skill DELETE / api / projects / {projectName} / skills / {skillName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } 200 401 404 500 \"<string>\" Path Parameters ​ projectName string required The name of the project where agent resides ​ skillName string required Name of existing skill to delete Response 200 200 401 404 500 application/json An empty response indicates success The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Update a Skill Create a Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /skills/ { skillName } 200 401 404 500 \"<string>\""}
{"file_name": "query-agent.html", "content": "Query an Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Query an Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views Files Jobs AI Agents POST Create an Agent GET List All Agents GET Get an Agent POST Query an Agent PUT Update an Agent DEL Delete an Agent POST Create a Skill GET List All Skills GET Get a Skill PUT Update a Skill DEL Delete a Skill POST Create a Chatbot GET List All Chatbots GET Get a Chatbot PUT Update a Chatbot DEL Delete a Chatbot AI Agents Query an Agent POST / api / projects / {projectName} / agents / {agentName} / completions Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } /completions \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"question\": \"<string>\", \"answer\": \"<string>\" } ] }' 200 400 401 500 { \"message\" : { \"role\" : \"<string>\" , \"content\" : \"<string>\" } } Path Parameters ​ projectName string required The name of the project where agent resides ​ agentName string required The name of the agent Body application/json ​ messages object[] Show child attributes ​ messages. question string The question being asked ​ messages. answer string | null The answer to the question (can be null) Response 200 200 400 401 500 application/json Message submission successful ​ message object Show child attributes ​ message. role string The role of the responder ​ message. content string The content of the response Was this page helpful? Yes No Suggest edits Raise issue Get an Agent Update an Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /agents/ { agentName } /completions \\ --header 'Content-Type: application/json' \\ --data '{ \"messages\": [ { \"question\": \"<string>\", \"answer\": \"<string>\" } ] }' 200 400 401 500 { \"message\" : { \"role\" : \"<string>\" , \"content\" : \"<string>\" } }"}
{"file_name": "create.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Tables Views Files Jobs AI Agents AI/ML Engines Configure an ML Engine This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Get a Data Source Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "delete.html", "content": "Remove an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Remove an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Tables Views Files Jobs AI Agents AI/ML Engines Remove an ML Engine This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Configure an ML Engine List ML Engines github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list.html", "content": "List ML Engines - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Engines Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Tables Views Files Jobs AI Agents AI/ML Engines List ML Engines This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Remove an ML Engine Create a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create-view.html", "content": "Create a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Views Create a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models Tables Views POST Create a View DEL Remove a View PUT Update a View GET List Views GET Get a View Files Jobs AI Agents Views Create a View POST / api / projects / {projectName} / views Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /views \\ --header 'Content-Type: application/json' \\ --data '{ \"view\": { \"name\": \"<string>\", \"query\": \"<string>\" } }' 200 401 404 500 [ { \"name\" : \"<string>\" , \"query\" : \"<string>\" } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project Body application/json ​ view object Show child attributes ​ view. name string Name of the view ​ view. query string The SQL query that will save the result-set in a view. Response 200 200 401 404 500 application/json View created ​ name string ​ query string SELECT query used to create the view Was this page helpful? Yes No Suggest edits Raise issue Query Remove a View github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /views \\ --header 'Content-Type: application/json' \\ --data '{ \"view\": { \"name\": \"<string>\", \"query\": \"<string>\" } }' 200 401 404 500 [ { \"name\" : \"<string>\" , \"query\" : \"<string>\" } ]"}
{"file_name": "list-models.html", "content": "List Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models List Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models List Models GET / api / projects / {projectName} / models Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models 200 401 404 500 [ { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project Response 200 200 401 404 500 application/json A JSON array of models names ​ name string ​ accuracy number Accuracy of trained model between 0 and 1 ​ active boolean Whether or not this model is currently the active version ​ version number Version of this model ​ status string Current status of this model (generating | creating | complete | error) ​ predict string Column name that this model predicts ​ mindsdb_version string MindsDB version associated with this model ​ error string Error encountered during training, if applicable ​ fetch_data_query string SQL query used to fetch training data for this model ​ created_at string Time model was created at in YYYY-MM-DD HH:MM:SS format (trained models only) ​ training_time string How long training this model took in HH:MM:SS format (trained models only) ​ update string Set to \"available\" when a new version of MindsDB is available that makes the model obsolete, or when new data is available in the data that was used to train the model (trained models only). Was this page helpful? Yes No Suggest edits Raise issue Remove a Model Describe a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models 200 401 404 500 [ { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" } ]"}
{"file_name": "train-model.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Create, Train, and Deploy a Model POST / api / projects / {projectName} / models Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models \\ --header 'Content-Type: application/json' \\ --data '{ \"query\": \"<string>\" }' 200 400 401 404 500 { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project Body application/json ​ query string The SQL CREATE MODEL statement used to train this model. See the CREATE MODEL statement Response 200 200 400 401 404 500 application/json Model training started ​ name string ​ accuracy number Accuracy of trained model between 0 and 1 ​ active boolean Whether or not this model is currently the active version ​ version number Version of this model ​ status string Current status of this model (generating | creating | complete | error) ​ predict string Column name that this model predicts ​ mindsdb_version string MindsDB version associated with this model ​ error string Error encountered during training, if applicable ​ fetch_data_query string SQL query used to fetch training data for this model ​ created_at string Time model was created at in YYYY-MM-DD HH:MM:SS format (trained models only) ​ training_time string How long training this model took in HH:MM:SS format (trained models only) ​ update string Set to \"available\" when a new version of MindsDB is available that makes the model obsolete, or when new data is available in the data that was used to train the model (trained models only). Was this page helpful? Yes No Suggest edits Raise issue Get a Project Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models \\ --header 'Content-Type: application/json' \\ --data '{ \"query\": \"<string>\" }' 200 400 401 404 500 { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" }"}
{"file_name": "query-model.html", "content": "Get a Single Prediction - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Get a Single Prediction Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Get a Single Prediction POST / api / projects / {projectName} / models / {modelName} / predict Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /predict \\ --header 'Content-Type: application/json' \\ --data '[ \"<any>\" ]' Request Response curl --request POST \\ --url http : //127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict \\ --header 'Content-Type : application/json' \\ --data ' { \"data\" : [ { \"sqft\" : \"823\" , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : \"10\" } ] } ' Request Response curl --request POST \\ --url http : //127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict \\ --header 'Content-Type : application/json' \\ --data ' { \"data\" : [ { \"sqft\" : \"823\" , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : \"10\" } ] } ' The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Note that data sent in the request is specific to the model. For instance, the home_rentals_model has been trained on data that includes details of rental properties, and we want to predict the rental_price value for a specific property that matches data passed in the request. Path Parameters ​ projectName string required The name of the project ​ modelName string required The name of the model Body application/json · any[] The body is of type any[] . Response 200 200 401 404 500 application/json Model queried succesfully The response is of type object . Was this page helpful? Yes No Suggest edits Raise issue Manage Model Versions Get Batch Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /predict \\ --header 'Content-Type: application/json' \\ --data '[ \"<any>\" ]' Request Response curl --request POST \\ --url http : //127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict \\ --header 'Content-Type : application/json' \\ --data ' { \"data\" : [ { \"sqft\" : \"823\" , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : \"10\" } ] } '"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Finetune a Model This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "manage-model-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Manage Model Versions This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "delete-model.html", "content": "Remove a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Remove a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Remove a Model DELETE / api / projects / {projectName} / models / {modelName} Try it cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } 200 401 404 500 \"<string>\" The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project ​ modelName string required The name of the model Response 200 200 401 404 500 application/json Empty response if delete is successful The response is of type string . Was this page helpful? Yes No Suggest edits Raise issue Create, Train, and Deploy a Model List Models github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request DELETE \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } 200 401 404 500 \"<string>\""}
{"file_name": "query-model-joined-with-data.html", "content": "Get Batch Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Get Batch Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Get Batch Predictions POST / api / projects / {projectName} / models / {modelName} / predict Try it cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /predict \\ --header 'Content-Type: application/json' \\ --data '[ \"<any>\" ]' Request Response import requests url = 'http://127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict' data = [ { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 917 , 'location' : 'great' , 'days_on_market' : 13 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 3901 , 'created_at' : '2024-02-24 02:28:21.746167' } , { 'number_of_rooms' : 0 , 'number_of_bathrooms' : 1 , 'sqft' : 194 , 'location' : 'great' , 'days_on_market' : 10 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 2042 , 'created_at' : '2024-02-19 06:10:59.693052' } , { 'number_of_rooms' : 1 , 'number_of_bathrooms' : 1 , 'sqft' : 543 , 'location' : 'poor' , 'days_on_market' : 18 , 'neighborhood' : 'westbrae' , 'rental_price' : 1871 , 'created_at' : '2024-02-12 07:53:45.914146' } ] r = requests . post ( url , json = { 'data' : data } ) print ( r . json ( ) ) There are two ways to get batch predictions: Join the model with data tables and use the query endpoint to query for batch predictions. Query the model using this endpoint and provide data to be used by the model in the request. Request Response import requests url = 'http://127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict' data = [ { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 917 , 'location' : 'great' , 'days_on_market' : 13 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 3901 , 'created_at' : '2024-02-24 02:28:21.746167' } , { 'number_of_rooms' : 0 , 'number_of_bathrooms' : 1 , 'sqft' : 194 , 'location' : 'great' , 'days_on_market' : 10 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 2042 , 'created_at' : '2024-02-19 06:10:59.693052' } , { 'number_of_rooms' : 1 , 'number_of_bathrooms' : 1 , 'sqft' : 543 , 'location' : 'poor' , 'days_on_market' : 18 , 'neighborhood' : 'westbrae' , 'rental_price' : 1871 , 'created_at' : '2024-02-12 07:53:45.914146' } ] r = requests . post ( url , json = { 'data' : data } ) print ( r . json ( ) ) The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project ​ modelName string required The name of the model Body application/json · any[] The body is of type any[] . Response 200 200 401 404 500 application/json Model queried succesfully The response is of type object . Was this page helpful? Yes No Suggest edits Raise issue Get a Single Prediction Create a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request POST \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /predict \\ --header 'Content-Type: application/json' \\ --data '[ \"<any>\" ]' Request Response import requests url = 'http://127.0.0.1:47334/api/projects/mindsdb/models/home_rentals_model/predict' data = [ { 'number_of_rooms' : 2 , 'number_of_bathrooms' : 1 , 'sqft' : 917 , 'location' : 'great' , 'days_on_market' : 13 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 3901 , 'created_at' : '2024-02-24 02:28:21.746167' } , { 'number_of_rooms' : 0 , 'number_of_bathrooms' : 1 , 'sqft' : 194 , 'location' : 'great' , 'days_on_market' : 10 , 'neighborhood' : 'berkeley_hills' , 'rental_price' : 2042 , 'created_at' : '2024-02-19 06:10:59.693052' } , { 'number_of_rooms' : 1 , 'number_of_bathrooms' : 1 , 'sqft' : 543 , 'location' : 'poor' , 'days_on_market' : 18 , 'neighborhood' : 'westbrae' , 'rental_price' : 1871 , 'created_at' : '2024-02-12 07:53:45.914146' } ] r = requests . post ( url , json = { 'data' : data } ) print ( r . json ( ) )"}
{"file_name": "list-model.html", "content": "Get a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Get a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Get a Model GET / api / projects / {projectName} / models / {modelName} Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } 200 401 404 500 { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" } The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project ​ modelName string required The name of the model Response 200 200 401 404 500 application/json Model information ​ name string ​ accuracy number Accuracy of trained model between 0 and 1 ​ active boolean Whether or not this model is currently the active version ​ version number Version of this model ​ status string Current status of this model (generating | creating | complete | error) ​ predict string Column name that this model predicts ​ mindsdb_version string MindsDB version associated with this model ​ error string Error encountered during training, if applicable ​ fetch_data_query string SQL query used to fetch training data for this model ​ created_at string Time model was created at in YYYY-MM-DD HH:MM:SS format (trained models only) ​ training_time string How long training this model took in HH:MM:SS format (trained models only) ​ update string Set to \"available\" when a new version of MindsDB is available that makes the model obsolete, or when new data is available in the data that was used to train the model (trained models only). Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } 200 401 404 500 { \"name\" : \"<string>\" , \"accuracy\" : 123 , \"active\" : true , \"version\" : 123 , \"status\" : \"<string>\" , \"predict\" : \"<string>\" , \"mindsdb_version\" : \"<string>\" , \"error\" : \"<string>\" , \"fetch_data_query\" : \"<string>\" , \"created_at\" : \"<string>\" , \"training_time\" : \"<string>\" , \"update\" : \"<string>\" }"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Retrain a Model This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Get a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "describe-model.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website REST API Overview Connect Data Sources AI/ML Engines Projects Models POST Create, Train, and Deploy a Model DEL Remove a Model GET List Models GET Describe a Model GET Get a Model Retrain a Model Finetune a Model Manage Model Versions POST Get a Single Prediction POST Get Batch Predictions Tables Views Files Jobs AI Agents Models Describe a Model GET / api / projects / {projectName} / models / {modelName} / describe Try it cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /describe \\ --header 'Content-Type: application/json' \\ --data '{ \"attribute\": \"<string>\" }' Response [ { \"accuracies\" : { \"r2_score\" : 0.999 } , \"column_importances\" : { \"days_on_market\" : 0.116 , \"location\" : 0.054 , \"neighborhood\" : 0.0 , \"number_of_bathrooms\" : 0.009 , \"number_of_rooms\" : 0.297 , \"sqft\" : 1.037 } , \"inputs\" : [ \"number_of_rooms\" , \"number_of_bathrooms\" , \"sqft\" , \"location\" , \"days_on_market\" , \"neighborhood\" ] , \"model\" : \"encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> accuracy_functions\" , \"outputs\" : [ \"rental_price\" ] } ] Response [ { \"accuracies\" : { \"r2_score\" : 0.999 } , \"column_importances\" : { \"days_on_market\" : 0.116 , \"location\" : 0.054 , \"neighborhood\" : 0.0 , \"number_of_bathrooms\" : 0.009 , \"number_of_rooms\" : 0.297 , \"sqft\" : 1.037 } , \"inputs\" : [ \"number_of_rooms\" , \"number_of_bathrooms\" , \"sqft\" , \"location\" , \"days_on_market\" , \"neighborhood\" ] , \"model\" : \"encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> accuracy_functions\" , \"outputs\" : [ \"rental_price\" ] } ] The REST API endpoints can be used with MindsDB running locally at http://127.0.0.1:47334/api . Path Parameters ​ projectName string required The name of the project ​ modelName string required The name of the model Body application/json ​ attribute string Attribute of the model to describe. See our DESCRIBE documentation . E.g. info, features, model Response 200 200 401 404 500 application/json A JSON object with model informations The response is of type object . Was this page helpful? Yes No Suggest edits Raise issue List Models Get a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify cURL Python JavaScript PHP Go Java curl --request GET \\ --url http://127.0.0.1:47334/api/projects/ { projectName } /models/ { modelName } /describe \\ --header 'Content-Type: application/json' \\ --data '{ \"attribute\": \"<string>\" }' Response [ { \"accuracies\" : { \"r2_score\" : 0.999 } , \"column_importances\" : { \"days_on_market\" : 0.116 , \"location\" : 0.054 , \"neighborhood\" : 0.0 , \"number_of_bathrooms\" : 0.009 , \"number_of_rooms\" : 0.297 , \"sqft\" : 1.037 } , \"inputs\" : [ \"number_of_rooms\" , \"number_of_bathrooms\" , \"sqft\" , \"location\" , \"days_on_market\" , \"neighborhood\" ] , \"model\" : \"encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> accuracy_functions\" , \"outputs\" : [ \"rental_price\" ] } ]"}
{"file_name": "huggingface.html", "content": "Hugging Face - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Hugging Face Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Hugging Face This documentation describes the integration of MindsDB with Hugging Face , a company that develops computer tools for building applications using machine learning. The integration allows for the deployment of Hugging Face models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Hugging Face within MindsDB, install the required dependencies following this instruction . ​ Setup Create an AI engine from the Hugging Face handler . CREATE ML_ENGINE huggingface_engine FROM huggingface USING huggingface_api_api_key = 'hf_xxx' ; Create a model using huggingface_engine as an engine. CREATE MODEL huggingface_model PREDICT target_column USING engine = 'huggingface_engine' , -- engine name as created via CREATE ML_ENGINE model_name = 'hf_hub_model_name' , -- choose one of PyTorch models from the Hugging Face Hub task = 'task_name' , -- choose one of 'text-classification', 'text-generation', 'zero-shot-classification', 'translation', 'summarization', 'text2text-generation', 'fill-mask' input_column = 'column_name' , -- column that stores input/question to the model labels = [ 'label 1' , 'label 2' ] ; -- labels used to classify data (used for classification tasks) ​ Usage The following usage examples utilize huggingface_engine to create a model with the CREATE MODEL statement. Create a model to classify input text as spam or ham. CREATE MODEL spam_classifier PREDICT spam_or_ham USING engine = 'huggingface_engine' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , task = 'text-classification' , input_column = 'text' , labels = [ 'ham' , 'spam' ] ; Query the model to get predictions. SELECT text , spam_or_ham FROM spam_classifier WHERE text = 'Subscribe to this channel asap' ; Here is the output: + --------------------------------+-------------+ | text | spam_or_ham | + --------------------------------+-------------+ | Subscribe to this channel asap | spam | + --------------------------------+-------------+ Next Steps Follow this link to see more use case examples. Was this page helpful? Yes No Suggest edits Raise issue Google Gemini Hugging Face Inference API github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "byom.html", "content": "Bring Your Own Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Bring Your Own Models Bring Your Own Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models BYOM MLflow Ray Serve Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Bring Your Own Models Bring Your Own Model The Bring Your Own Model (BYOM) feature lets you upload your own models in the form of Python code and use them within MindsDB. ​ How It Works You can upload your custom model via the MindsDB editor by clicking Add and Upload custom model , like this: Here is the form that needs to be filled out in order to bring your model to MindsDB: Let’s briefly go over the files that need to be uploaded: The Python file stores an implementation of your model. It should contain the class with the implementation for the train and predict methods. Here is the sample format: class CustomPredictor ( ) : ​ def train ( self , df , target_col , args = None ) : < implementation goes here > return '' def predict ( self , df ) : < implementation goes here > return df Example import os import pandas as pd ​ from sklearn . cross_decomposition import PLSRegression from sklearn import preprocessing ​ class CustomPredictor ( ) : ​ def train ( self , df , target_col , args = None ) : print ( args , '1111' ) ​ self . target_col = target_col y = df [ self . target_col ] x = df . drop ( columns = self . target_col ) x_cols = list ( x . columns ) ​ x_scaler = preprocessing . StandardScaler ( ) . fit ( x ) y_scaler = preprocessing . StandardScaler ( ) . fit ( y . values . reshape ( - 1 , 1 ) ) ​ xs = x_scaler . transform ( x ) ys = y_scaler . transform ( y . values . reshape ( - 1 , 1 ) ) ​ pls = PLSRegression ( n_components = 1 ) pls . fit ( xs , ys ) ​ T = pls . x_scores_ W = pls . x_weights_ P = pls . x_loadings_ R = pls . x_rotations_ ​ self . x_cols = x_cols self . x_scaler = x_scaler self . P = P ​ def calc_limit ( df ) : res = None for column in df . columns : if column == self . target_col : continue tbl = df . groupby ( self . target_col ) . agg ( { column : [ 'mean' , 'min' , 'max' , 'std' ] } ) tbl . columns = tbl . columns . get_level_values ( 1 ) tbl [ 'name' ] = column tbl [ 'std' ] = tbl [ 'std' ] . fillna ( 0 ) tbl [ 'lower' ] = tbl [ 'mean' ] - 3 * tbl [ 'std' ] tbl [ 'upper' ] = tbl [ 'mean' ] + 3 * tbl [ 'std' ] tbl [ 'lower' ] = tbl [ [ \"lower\" , \"min\" ] ] . max ( axis = 1 ) # lower >= min tbl [ 'upper' ] = tbl [ [ \"upper\" , \"max\" ] ] . min ( axis = 1 ) # upper <= max tbl = tbl [ [ 'name' , 'lower' , 'mean' , 'upper' ] ] try : res = pd . concat ( [ res , tbl ] ) except : res = tbl return res ​ trdf = pd . DataFrame ( ) trdf [ self . target_col ] = y . values trdf [ 'T1' ] = T . squeeze ( ) limit = calc_limit ( trdf ) . reset_index ( ) ​ self . limit = limit ​ return \"Trained predictor ready to be stored\" ​ def predict ( self , df ) : ​ yt = df [ self . target_col ] . values xt = df [ self . x_cols ] ​ xt = self . x_scaler . transform ( xt ) ​ excess_cols = list ( set ( df . columns ) - set ( self . x_cols ) ) ​ pred_df = df [ excess_cols ] . copy ( ) ​ pred_df [ self . target_col ] = yt pred_df [ 'T1' ] = ( xt @ self . P ) . squeeze ( ) ​ pred_df = pd . merge ( pred_df , self . limit [ [ self . target_col , 'lower' , 'upper' ] ] , how = 'left' , on = self . target_col ) ​ return pred_df The optional requirements file, or requirements.txt , stores all dependencies along with their versions. Here is the sample format: dependency_package_1 = = version dependency_package_2 >= version dependency_package_3 >= version , < version . . . Example pandas scikit - learn Once you upload the above files, please provide an engine name. Please note that your custom model is uploaded to MindsDB as an engine. Then you can use this engine to create a model. Let’s look at an example. ​ Example We upload the custom model, as below: Here we upload the model.py file that stores an implementation of the model and the requirements.txt file that stores all the dependencies. Once the model is uploaded, it becomes an ML engine within MindsDB. Now we use this custom_model_engine to create a model as follows: CREATE MODEL custom_model FROM my_integration ( SELECT * FROM my_table ) PREDICT target USING ENGINE = 'custom_model_engine' ; Let’s query for predictions by joining the custom model with the data table. SELECT input . feature_column , model_target_column FROM my_integration . my_table as input JOIN custom_model as model ; Check out the BYOM handler folder to see the implementation details. Was this page helpful? Yes No Suggest edits Raise issue Vertex AI MLflow github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How It Works Example"}
{"file_name": "ai-integrations.html", "content": "AI Integrations - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Engines AI Integrations Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models AI Engines AI Integrations MindsDB integrates with numerous AI frameworks, facilitating deployment and management of AI models . MindsDB offers a wide range of AI engines used to create models and incorporate them in the data landscape as virtual AI tables . MindsDB abstracts AI models as virtual tables, or Generative AI Tables, that can generate data from the underlying model upon being queried. This section contains instructions on how to create and deploy models within MindsDB, utilizing different AI/ML frameworks. ​ Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Minds Endpoint Ollama OpenAI Portkey Replicate (LLM) Vertex AI ​ Bring Your Own Models BYOM MLflow Ray Serve ​ Anomaly Detection Anomaly Detection ​ AutoML Lightwood PyCaret ​ Time Series Models NeuralForecast StatsForecast TimeGPT ​ Recommender Models LightFM Popularity Recommender ​ Audio Models Replicate (Audio) ​ Image Models Clipdrop Replicate (Text2Img) Replicate (Img2Text) ​ Video Models TweleveLabs (Video Semantic Search) Replicate (Text2Video) If you don’t find an AI/ML framework of your interest, you can request a feature here or build an AI/ML handler following this instruction . Metadata about AI handlers and AI engines AI handlers represent a raw implementation of the integration between MindsDB and an AI/ML framework. These are used to create AI engines. Here is how you can query for all the available AI handlers used to create AI engines. SELECT * FROM information_schema . handlers WHERE type = 'ml' ; Or, alternatively: SHOW HANDLERS WHERE type = 'ml' ; And here is how you can query for all the created AI engines: SELECT * FROM information_schema . ml_engines ; Or, alternatively: SHOW ML_ENGINES ; Was this page helpful? Yes No Suggest edits Raise issue Parquet Anthropic github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Audio Models Image Models Video Models"}
{"file_name": "data-integrations.html", "content": "Data Integrations - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Data Integrations Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Data Sources Data Integrations MindsDB integrates with numerous data sources, including databases, vector stores, and applications, making data available to AI models by connecting data sources to MindsDB. This section contains instructions on how to connect data sources to MindsDB. Note that MindsDB doesn’t store or copy your data. Instead, it fetches data directly from your connected sources each time you make a query, ensuring that any changes to the data are instantly reflected. This means your data remains in its original location, and MindsDB always works with the most up-to-date information. ​ Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft OneDrive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube ​ Databases Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB Elasticsearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB ​ Vector Stores ChromaDB Couchbase PGVector PineCone Weaviate If you don’t find a data source of your interest, you can request a feature here or build a handler following this instruction for data handlers and this instruction for applications . Metadata about data handlers and data sources Data handlers represent a raw implementation of the integration between MindsDB and a data source. Here is how you can query for all the available data handlers used to connect data sources to MindsDB. SELECT * FROM information_schema . handlers WHERE type = 'data' ; Or, alternatively: SHOW HANDLERS WHERE type = 'data' ; And here is how you can query for all the created AI engines: SELECT * FROM information_schema . databases ; Or, alternatively: SHOW DATABASES ; Was this page helpful? Yes No Suggest edits Raise issue Supported Integrations Sample Database github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Applications Databases Vector Stores"}
{"file_name": "model-management.html", "content": "Model Management - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Features Model Management Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Features Model Management MindsDB abstracts AI models, making them accessible from enterprise data environments. MindsDB enables you to manage every aspect of AI models. With MindsDB, you can CREATE MODEL , FINETUNE , RETRAIN , and more. Deploy You can create, train, and deploy AI models based on popular AI/ML frameworks within MindsDB. Fine-tune You can fine-tune models with data from various data sources connected to MindsDB. Check out examples here . Automate You can automate tasks, including retraining or fine-tuning of AI models, to keep your AI system up-to-date. See examples here . Go ahead and create an AI model! Use SQL API , REST API , or one of the SDKs to create and deploy AI models within MindsDB. Was this page helpful? Yes No Suggest edits Raise issue Demo AI Integrations github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "automation.html", "content": "Automation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Features Automation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Features Automation MindsDB provides mechanisms to automate tasks. These include jobs , triggers , and chatbots . Use AI automation to keep your AI systems up-to-date by continuously retraining or fine-tuning them with real-time data. Follow this use case to learn how to build it with MindsDB. Use AI automation to create chatbots. Follow this use case to learn how to build custom chatbots with MindsDB. Was this page helpful? Yes No Suggest edits Raise issue Data Integrations MindsDB Schema github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "tidb.html", "content": "TiDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TiDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TiDB This is the implementation of the TiDB data handler for MindsDB. TiDB is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing workloads. It is MySQL-compatible and can provide horizontal scalability, strong consistency, and high availability. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TiDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to TiDB. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the TiDB database in MindsDB, the following syntax can be used: CREATE DATABASE tidb_datasource WITH ENGINE = 'tidb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4000 , \"database\" : \"tidb\" , \"user\" : \"root\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM tidb_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue Teradata TimescaleDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "d0lt.html", "content": "D0lt - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases D0lt Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases D0lt This is the implementation of the D0lt data handler for MindsDB. D0lt is a single-node and embedded DBMS that incorporates Git-style versioning as a first-class entity. D0lt behaves like Git - it is a content-addressable local database where the main objects are tables instead of files. In D0lt, a user creates a database locally. The database contains tables that can be read and updated using SQL. Similar to Git, writes are staged until the user issues a commit. Upon commit, the writes are appended to permanent storage. Branch and merge semantics are supported allowing for the tables to evolve at a different pace for multiple users. This allows for loose collaboration on data as well as multiple views on the same core data. Merge conflicts are detected for schema and data conflicts. Data conflicts are cell-based, not line-based. Remote repositories allow for cooperation among repository instances. Clone, push, and pull semantics are all available. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect D0lt to MindsDB, install the required dependencies following this instruction . Install or ensure access to D0lt. ​ Implementation This handler is implemented using mysql-connector , a Python library that allows you to use Python code to run SQL commands on the D0lt database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP address of the server. port is the port through which a TCP/IP connection is to be made. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the D0lt database in MindsDB, the following syntax can be used: CREATE DATABASE d0lt_datasource WITH engine = 'd0lt' , parameters = { \"user\" : \"root\" , \"password\" : \"\" , \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"information_schema\" } ; You can use this established connection to query your table as follows: SELECT * FROM D0lt_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue CrateDB Databend github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ckan.html", "content": "Ckan - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Ckan ​ CKAN Integration handler This handler facilitates integration with CKAN . an open-source data catalog platform for managing and publishing open data. CKAN organizes datasets and stores data in its DataStore .To retrieve data from CKAN, the CKANAPI must be used. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SAP HANA to MindsDB, install the required dependencies following this instruction . The CKAN handler is included with MindsDB by default, so no additional installation is required. ​ Configuration To use the CKAN handler, you need to provide the URL of the CKAN instance you want to connect to. You can do this by setting the CKAN_URL environment variable. For example: CREATE DATABASE ckan_datasource WITH ENGINE = 'ckan' , PARAMETERS = { \"url\" : \"https://your-ckan-instance-url.com\" , \"api_key\" : \"your-api-key-if-required\" } ; NOTE: Some CKAN instances will require you to provide an API Token. You can create one in the CKAN user panel. ​ Usage The CKAN handler provides three main tables: datasets : Lists all datasets in the CKAN instance. resources : Lists all resources metadata across all packages. datastore : Allows querying individual datastore resources. ​ Example Queries List all datasets: SELECT * FROM ` your-datasource ` . datasets ; List all resources: SELECT * FROM ` your-datasource ` . resources ; Query a specific datastore resource: SELECT * FROM ` your-datasource ` . datastore WHERE resource_id = 'your-resource-id' ; Replace your-resource-id-here with the actual resource ID you want to query. ​ Querying Large Resources The CKAN handler supports automatic pagination when querying datastore resources. This allows you to retrieve large datasets without worrying about API limits. You can still use the LIMIT clause to limit the number of rows returned by the query. For example: SELECT * FROM ckan_datasource . datastore WHERE resource_id = 'your-resource-id-here' LIMIT 1000 ; ​ Limitations The handler currently supports read operations only. Write operations are not supported. Performance may vary depending on the size of the CKAN instance and the complexity of your queries. The handler may not work with all CKAN instances, especially those with custom configurations. The handler does not support all CKAN API features. Some advanced features may not be available. The datastore search will return limited records up to 32000. Please refer to the CKAN API documentation for more information. Was this page helpful? Yes No Suggest edits Raise issue Apache Solr ClickHouse github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page CKAN Integration handler Prerequisites Configuration Usage Example Queries Querying Large Resources Limitations"}
{"file_name": "microsoft-sql-server.html", "content": "Microsoft SQL Server - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Microsoft SQL Server Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Microsoft SQL Server This documentation describes the integration of MindsDB with Microsoft SQL Server, a relational database management system developed by Microsoft. The integration allows for advanced SQL functionalities, extending Microsoft SQL Server’s capabilities with MindsDB’s features. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Microsoft SQL Server to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Microsoft SQL Server database from MindsDB by executing the following SQL command: CREATE DATABASE mssql_datasource WITH ENGINE = 'mssql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1433 , \"user\" : \"sa\" , \"password\" : \"password\" , \"database\" : \"master\" } ; Required connection parameters include the following: user : The username for the Microsoft SQL Server. password : The password for the Microsoft SQL Server. host The hostname, IP address, or URL of the Microsoft SQL Server. database The name of the Microsoft SQL Server database to connect to. Optional connection parameters include the following: port : The port number for connecting to the Microsoft SQL Server. Default is 1433. server : The server name to connect to. Typically only used with named instances or Azure SQL Database. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM mssql_datasource . schema_name . table_name LIMIT 10 ; Run T-SQL queries directly on the connected Microsoft SQL Server database: SELECT * FROM mssql_datasource ( --Native Query Goes Here SELECT SUM ( orderqty ) total FROM Product p JOIN SalesOrderDetail sd ON p . productid = sd . productid JOIN SalesOrderHeader sh ON sd . salesorderid = sh . salesorderid JOIN Customer c ON sh . customerid = c . customerid WHERE ( Name = 'Racing Socks, L' ) AND ( companyname = 'Riding Cycles' ) ; ) ; The above examples utilize mssql_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Microsoft SQL Server database. Checklist : Make sure the Microsoft SQL Server is active. Confirm that host, port, user, and password are correct. Try a direct Microsoft SQL Server connection using a client like SQL Server Management Studio or DBeaver. Ensure a stable network between MindsDB and Microsoft SQL Server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Microsoft Access MonetDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "orioledb.html", "content": "OrioleDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OrioleDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OrioleDB This is the implementation of the OrioleDB data handler for MindsDB. OrioleDB is a new storage engine for PostgreSQL, bringing a modern approach to database capacity, capabilities, and performance to the world’s most-loved database platform. It consists of an extension, building on the innovative table access method framework and other standard Postgres extension interfaces. By extending and enhancing the current table access methods, OrioleDB opens the door to a future of more powerful storage models that are optimized for cloud and modern hardware architectures. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OrioleDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to OrioleDB. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. server is the OrioleDB server. database is the database name. ​ Usage In order to make use of this handler and connect to the OrioleDB server in MindsDB, the following syntax can be used: CREATE DATABASE orioledb_datasource WITH ENGINE = 'orioledb' , PARAMETERS = { \"user\" : \"orioledb_user\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 55505 , \"server\" : \"server_name\" , \"database\" : \"oriole_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM orioledb_data . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Oracle PlanetScale github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cratedb.html", "content": "CrateDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases CrateDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases CrateDB This is the implementation of the CrateDB data handler for MindsDB. CrateDB is a distributed SQL database management system that integrates a fully searchable document-oriented data store. It is open-source, written in Java, based on a shared-nothing architecture, and designed for high scalability. CrateDB includes components from Lucene, Elasticsearch and Netty. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect CrateDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to CrateDB. ​ Implementation This handler is implemented using crate , a Python library that allows you to use Python code to run SQL commands on CrateDB. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP adress of the server. port is the port through which connection is to be made. schema_name is schema name to get tables from. Defaults to doc . ​ Usage In order to make use of this handler and connect to the CrateDB database in MindsDB, the following syntax can be used: CREATE DATABASE crate_datasource WITH engine = 'crate' , parameters = { \"user\" : \"crate\" , \"password\" : \"\" , \"host\" : \"127.0.0.1\" , \"port\" : 4200 , \"schema_name\" : \"doc\" } ; You can use this established connection to query your table as follows: SELECT * FROM crate_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue Couchbase D0lt github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "scylladb.html", "content": "ScyllaDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ScyllaDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ScyllaDB This is the implementation of the ScyllaDB data handler for MindsDB. ScyllaDB is an open-source distributed NoSQL wide-column data store. It was purposefully designed to offer compatibility with Apache Cassandra while outperforming it with higher throughputs and reduced latencies. For a comprehensive understanding of ScyllaDB, visit ScyllaDB’s official website. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ScyllaDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to ScyllaDB. ​ Implementation The ScyllaDB handler for MindsDB was developed using the scylla-driver library for Python. The required arguments to establish a connection are as follows: host : Host name or IP address of ScyllaDB. port : Connection port. user : Authentication username. Optional; required only if authentication is enabled. password : Authentication password. Optional; required only if authentication is enabled. keyspace : The specific keyspace (top-level container for tables) to connect to. protocol_version : Optional. Defaults to 4. secure_connect_bundle : Optional. Needed only for connections to DataStax Astra. ​ Usage To set up a connection between MindsDB and a Scylla server, utilize the following SQL syntax: CREATE DATABASE scylladb_datasource WITH ENGINE = 'scylladb' , PARAMETERS = { \"user\" : \"user@mindsdb.com\" , \"password\" : \"pass\" , \"host\" : \"127.0.0.1\" , \"port\" : \"9042\" , \"keyspace\" : \"test_data\" } ; The protocol version is set to 4 by default. Should you wish to modify it, simply include “protocol_version”: 5 within the PARAMETERS dictionary in the query above. With the connection established, you can execute queries on your keyspace as demonstrated below: SELECT * FROM scylladb_datasource . keystore . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue SAP SQL Anywhere SingleStore github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "duckdb.html", "content": "DuckDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases DuckDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases DuckDB This is the implementation of the DuckDB data handler for MindsDB. DuckDB is an open-source analytical database system. It is designed for fast execution of analytical queries. There are no external dependencies and the DBMS runs completely embedded within a host process, similar to SQLite. DuckDB provides a rich SQL dialect with support for complex queries with transactional guarantees (ACID). ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect DuckDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to DuckDB. ​ Implementation This handler is implemented using the duckdb Python client library. The DuckDB handler is currently using the 0.7.1.dev187 pre-relase version of the Python client library. In case of issues, make sure your DuckDB database is compatible with this version. See the requirements.txt for details. The required arguments to establish a connection are as follows: database is the name of the DuckDB database file. It can be set to :memory: to create an in-memory database. The optional arguments are as follows: read_only is a flag that specifies whether the connection is in the read-only mode. This is required if multiple processes want to access the same database file at the same time. ​ Usage In order to make use of this handler and connect to the DuckDB database in MindsDB, the following syntax can be used: CREATE DATABASE duckdb_datasource WITH engine = 'duckdb' , parameters = { \"database\" : \"db.duckdb\" } ; You can use this established connection to query your table as follows: SELECT * FROM duckdb_datasource . my_table ; Was this page helpful? Yes No Suggest edits Raise issue DataStax EdgelessDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "teradata.html", "content": "Teradata - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Teradata Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Teradata This documentation describes the integration of MindsDB with Teradata , the complete cloud analytics and data platform for Trusted AI. The integration allows MindsDB to access data from Teradata and enhance Teradata with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Teradata to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Teradata from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE teradata_datasource WITH ENGINE = 'teradata' , PARAMETERS = { \"host\" : \"192.168.0.41\" , \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"database\" : \"example_db\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the Teradata server. user : The username for the Teradata database. password : The password for the Teradata database. Optional connection parameters include the following: database : The name of the Teradata database to connect to. Defaults is the user’s default database. ​ Usage Retrieve data from a specified table by providing the integration, database and table names: SELECT * FROM teradata_datasource . database_name . table_name LIMIT 10 ; Run Teradata SQL queries directly on the connected Teradata database: SELECT * FROM teradata_datasource ( --Native Query Goes Here SELECT emp_id , emp_name , job_duration AS tsp FROM employee EXPAND ON job_duration AS tsp BY INTERVAL '1' YEAR FOR PERIOD ( DATE '2006-01-01' , DATE '2008-01-01' ) ; ) ; The above examples utilize teradata_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the Teradata database. Checklist : Make sure the Teradata database is active. Confirm that host, user and password are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and Teradata. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Connection Timeout Error Symptoms : Connection to the Teradata database times out or queries take too long to execute. Checklist : Ensure the Teradata server is running and accessible (if the server has been idle for a long time, it may have shut down automatically). Was this page helpful? Yes No Suggest edits Raise issue TDengine TiDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "mysql.html", "content": "MySQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MySQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MySQL This documentation describes the integration of MindsDB with MySQL , a fast, reliable, and scalable open-source database. The integration allows MindsDB to access data from MySQL and enhance MySQL with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MySQL to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to MySQL from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE mysql_conn WITH ENGINE = 'mysql' , PARAMETERS = { \"host\" : \"host-name\" , \"port\" : 3306 , \"database\" : \"db-name\" , \"user\" : \"user-name\" , \"password\" : \"password\" } ; Or: CREATE DATABASE mysql_datasource WITH ENGINE = 'mysql' , PARAMETERS = { \"url\" : \"mysql://user-name@host-name:3306\" } ; Required connection parameters include the following: user : The username for the MySQL database. password : The password for the MySQL database. host : The hostname, IP address, or URL of the MySQL server. port : The port number for connecting to the MySQL server. database : The name of the MySQL database to connect to. Or: url : You can specify a connection to MySQL Server using a URI-like string, as an alternative connection option. Optional connection parameters include the following: ssl : Boolean parameter that indicates whether SSL encryption is enabled for the connection. Set to True to enable SSL and enhance connection security, or set to False to use the default non-encrypted connection. ssl_ca : Specifies the path to the Certificate Authority (CA) file in PEM format. ssl_cert : Specifies the path to the SSL certificate file. This certificate should be signed by a trusted CA specified in the ssl_ca file or be a self-signed certificate trusted by the server. ssl_key : Specifies the path to the private key file (in PEM format). ​ Usage The following usage examples utilize the connection to MySQL made via the CREATE DATABASE statement and named mysql_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM mysql_conn . table_name LIMIT 10 ; Next Steps Follow this tutorial to see more use case examples. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the MySQL database. Checklist : Ensure that the MySQL server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the MySQL server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue MongoDB OceanBase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "apache-cassandra.html", "content": "Apache Cassandra - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Cassandra Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Cassandra This is the implementation of the Cassandra data handler for MindsDB. Cassandra is a free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Cassandra to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Cassandra. ​ Implementation As ScyllaDB is API-compatible with Apache Cassandra, the Cassandra data handler extends the ScyllaDB handler and uses the scylla-driver Python library. The required arguments to establish a connection are as follows: host is the host name or IP address of the Cassandra database. port is the port to use when connecting. user is the user to authenticate. password is the password to authenticate the user. keyspace is the keyspace to connect, the top level container for tables. protocol_version is not required and defaults to 4. ​ Usage In order to make use of this handler and connect to the Cassandra server in MindsDB, the following syntax can be used: CREATE DATABASE sc WITH engine = \"cassandra\" , parameters = { \"host\" : \"127.0.0.1\" , \"port\" : \"9043\" , \"user\" : \"user\" , \"password\" : \"pass\" , \"keyspace\" : \"test_data\" , \"protocol_version\" : 4 } ; You can use this established connection to query your table as follows: SELECT * FROM cassandra_datasource . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Amazon S3 Apache Druid github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "sql-anywhere.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "mongodb.html", "content": "MongoDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MongoDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MongoDB This documentation describes the integration of MindsDB with MongoDB , a document database with the scalability and flexibility that you want with the querying and indexing that you need. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . ​ Connection Establish a connection to MongoDB from MindsDB by executing the following SQL command: CREATE DATABASE mongodb_datasource WITH ENGINE = 'mongodb' , PARAMETERS = { \"host\" : \"mongodb+srv://admin:admin_pass@demo.mongodb.net/public\" } ; Use the following parameters to establish the connection: host : The connection string of the MongoDB server that includes user ( admin ), password ( admin_pass ), host and port ( demo.mongodb.net ), and database ( public ). database : If the connection string does not include the /database path, provide it in this parameter. Alternatively, the following set of connection parameters can be used: username : The username associated with the database. password : The password to authenticate your access. host : The host of the MongoDB server. port : The port through which TCP/IP connection is to be made. database : The database name to be connected. ​ Usage Retrieve data from a specified collection by providing the integration name and collection name: SELECT * FROM mongodb_datasource . my_collection LIMIT 10 ; The above examples utilize mongodb_datasource as the datasource name, which is defined in the CREATE DATABASE command. At the moment, this integration only supports SELECT and UPDATE queries. For this connection, we strongly suggest using the Mongo API instead of the SQL API. MindsDB has a dedicated Mongo API that allows you to use the full power of the MindsDB platform. Using the Mongo API feels more natural for MongoDB users and allows you to use all the features of MindsDB. You can find the instructions on how to connect MindsDB to MongoDB Compass or MongoDB Shell and proceed with the Mongo API documentation for further details. Once you connected MindsDB to MongoDB Compass or MongoDB Shell, you can run this command to connect your database to MindsDB: test > use mindsdb mindsdb > db . databases . insertOne ( { name: \"mongo_datasource\" , engine : \"mongodb\" , connection_args: { \"host\" : \"mongodb+srv://user:pass@db.xxxyyy.mongodb.net/\" } } ) ; Then you can query your data, like this: mindsdb > use mongo_datasource mongo_datasource > db . demo . find ( {} ) . limit ( 3 ) ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the MongoDB server. Checklist : Make sure the MongoDB server is active. Confirm that host and credentials provided are correct. Try a direct MongoDB connection using a client like MongoDB Compass. Ensure a stable network between MindsDB and MongoDB. For example, if you are using MongoDB Atlas, ensure that the IP address of the machine running MindsDB is whitelisted. Unknown statement Symptoms : Errors related to the issuing of unsupported queries to MongoDB via the integration. Checklist : Ensure the query is a SELECT or UPDATE query. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing collection names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue MonetDB MySQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "apache-solr.html", "content": "Apache Solr - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Solr Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Solr This is the implementation of the Solr data handler for MindsDB. Apache Solr is a highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration, and more. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Solr to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Solr. ​ Implementation This handler is implemented using the sqlalchemy-solr library, which provides a Python/SQLAlchemy interface. The required arguments to establish a connection are as follows: username is the username used to authenticate with the Solr server. This parameter is optional. password is the password to authenticate the user with the Solr server. This parameter is optional. host is the host name or IP address of the Solr server. port is the port number of the Solr server. server_path defaults to solr if not provided. collection is the Solr Collection name. use_ssl defaults to false if not provided. Further reference: https://pypi.org/project/sqlalchemy-solr/ . ​ Usage In order to make use of this handler and connect to the Solr database in MindsDB, the following syntax can be used: CREATE DATABASE solr_datasource WITH engine = 'solr' , parameters = { \"username\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"127.0.0.1\" , \"port\" : \"8981\" , \"server_path\" : \"solr\" , \"collection\" : \"gettingstarted\" , \"use_ssl\" : \"false\" } ; You can use this established connection to query your table as follows: SELECT * FROM solr_datasource . gettingstarted LIMIT 10000 ; Requirements A Solr instance with a Parallel SQL supported up and running. There are certain limitations that need to be taken into account when issuing queries to Solr. Refer to https://solr.apache.org/guide/solr/latest/query-guide/sql-query.html#parallel-sql-queries . Don’t forget to put limit in the end of the SQL statement Was this page helpful? Yes No Suggest edits Raise issue Apache Pinot Ckan github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "supabase.html", "content": "Supabase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Supabase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Supabase This is the implementation of the Supabase data handler for MindsDB. Supabase is an open-source Firebase alternative. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Supabase to MindsDB, install the required dependencies following this instruction . Install or ensure access to Supabase. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the Supabase server in MindsDB, the following syntax can be used: CREATE DATABASE supabase_datasource WITH ENGINE = 'supabase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 54321 , \"database\" : \"test\" , \"user\" : \"supabase\" , \"password\" : \"password\" } ; You can use this established connection to query your database as follows: SELECT * FROM supabase_datasource . public . rentals LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue StarRocks SurrealDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cloud-spanner.html", "content": "Cloud Spanner - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Cloud Spanner Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Cloud Spanner This is the implementation of the Cloud Spanner data handler for MindsDB. Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, automatic, synchronous replication for high availability. It supports two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Cloud Spanner to MindsDB, install the required dependencies following this instruction . Install or ensure access to Cloud Spanner. ​ Implementation This handler was implemented using the google-cloud-spanner Python client library. The required arguments to establish a connection are as follows: instance_id is the instance identifier. database_id is the database identifier. project is the identifier of the project that owns the resources. credentials is a stringified GCP service account key JSON. ​ Usage In order to make use of this handler and connect to the Cloud Spanner database in MindsDB, the following syntax can be used: CREATE DATABASE cloud_spanner_datasource WITH engine = 'cloud_spanner' , parameters = { \"instance_id\" : \"my-instance\" , \"database_id\" : \"example-id\" , \"project\" : \"my-project\" , \"credentials\" : \"{...}\" } ; You can use this established connection to query your table as follows: SELECT * FROM cloud_spanner_datasource . my_table ; Cloud Spanner supports both PostgreSQL and GoogleSQL dialects. However, not all PostgresSQL features are supported. Was this page helpful? Yes No Suggest edits Raise issue ClickHouse CockroachDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "vitess.html", "content": "Vitess - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Vitess Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Vitess This is the implementation of the Vitess data handler for MindsDB. Vitess is a database solution for deploying, scaling, and managing large clusters of open-source database instances. It currently supports MySQL and Percona Server for MySQL. It’s architected to run as effectively in a public or private cloud architecture as it does on dedicated hardware. It combines and extends many important SQL features with the scalability of a NoSQL database. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Vitess to MindsDB, install the required dependencies following this instruction . Install or ensure access to Vitess. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the Vitess server in MindsDB, the following syntax can be used: CREATE DATABASE vitess_datasource WITH ENGINE = \"vitess\" , PARAMETERS = { \"user\" : \"root\" , \"password\" : \"\" , \"host\" : \"localhost\" , \"port\" : 33577 , \"database\" : \"commerce\" } ; You can use this established connection to query your table as follows: SELECT * FROM vitess_datasource . product LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Vertica YugabyteDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "microsoft-access.html", "content": "Microsoft Access - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Microsoft Access Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Microsoft Access This is the implementation of the Microsoft Access data handler for MindsDB. Microsoft Access is a pseudo-relational database engine from Microsoft. It is part of the Microsoft Office suite of applications that also includes Word, Outlook, and Excel, among others. Access is also available for purchase as a stand-alone product. It uses the Jet Database Engine for data storage. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Microsoft Access to MindsDB, install the required dependencies following this instruction . Install or ensure access to Microsoft Access. ​ Implementation This handler is implemented using pyodbc , the Python ODBC bridge. The only required argument to establish a connection is db_file that points to a database file to be queried. ​ Usage In order to make use of this handler and connect to the Access database in MindsDB, the following syntax can be used: CREATE DATABASE access_datasource WITH engine = 'access' , parameters = { \"db_file\" : \"C:\\\\Users\\\\minurap\\\\Documents\\\\example_db.accdb\" } ; You can use this established connection to query your table as follows: SELECT * FROM access_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue MatrixOne Microsoft SQL Server github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "google-sheets.html", "content": "Google Sheets - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Google Sheets Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Google Sheets This is the implementation of the Google Sheets data handler for MindsDB. Google Sheets is a spreadsheet program included as a part of the free, web-based Google Docs Editors suite offered by Google. Please note that the integration of MindsDB with Google Sheets works for public sheets only. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Google Sheets to MindsDB, install the required dependencies following this instruction . Install or ensure access to Google Sheets. ​ Implementation This handler is implemented using duckdb , a library that allows SQL queries to be executed on pandas DataFrames. In essence, when querying a particular sheet, the entire sheet is first pulled into a pandas DataFrame using the Google Visualization API . Once this is done, SQL queries can be run on the DataFrame using duckdb . Since the entire sheet needs to be pulled into memory first (DataFrame), it is recommended to be somewhat careful when querying large datasets so as not to overload your machine. The required arguments to establish a connection are as follows: spreadsheet_id is the unique ID of the Google Sheet. sheet_name is the name of the sheet within the Google Sheet. ​ Usage In order to make use of this handler and connect to a Google Sheet in MindsDB, the following syntax can be used: CREATE DATABASE sheets_datasource WITH engine = 'sheets' , parameters = { \"spreadsheet_id\" : \"12wgS-1KJ9ymUM-6VYzQ0nJYGitONxay7cMKLnEE2_d0\" , \"sheet_name\" : \"iris\" } ; You can use this established connection to query your table as follows: SELECT * FROM sheets_datasource . example_tbl ; The name of the table will be the name of the relevant sheet, provided as an input to the sheet_name parameter. At the moment, only the SELECT statemet is allowed to be executed through duckdb . This, however, has no restriction on running machine learning algorithms against your data in Google Sheets using the CREATE MODEL statement. Was this page helpful? Yes No Suggest edits Raise issue Google Cloud SQL GreptimeDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "sap-hana.html", "content": "SAP HANA - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SAP HANA Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SAP HANA This documentation describes the integration of MindsDB with SAP HANA , a multi-model database with a column-oriented in-memory design that stores data in its memory instead of keeping it on a disk. The integration allows MindsDB to access data from SAP HANA and enhance SAP HANA with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SAP HANA to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to SAP HANA from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE sap_hana_datasource WITH ENGINE = 'hana' , PARAMETERS = { \"address\" : \"123e4567-e89b-12d3-a456-426614174000.hana.trial-us10.hanacloud.ondemand.com\" , \"port\" : \"443\" , \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"encrypt\" : true } ; Required connection parameters include the following: address : The hostname, IP address, or URL of the SAP HANA database. port : The port number for connecting to the SAP HANA database. user : The username for the SAP HANA database. password : The password for the SAP HANA database. Optional connection parameters include the following: ‘database’: The name of the database to connect to. This parameter is not used for SAP HANA Cloud. schema : The database schema to use. Defaults to the user’s default schema. encrypt : The setting to enable or disable encryption. Defaults to `True’ ​ Usage Retrieve data from a specified table by providing the integration, schema and table names: SELECT * FROM sap_hana_datasource . schema_name . table_name LIMIT 10 ; Run Teradata SQL queries directly on the connected Teradata database: SELECT * FROM sap_hana_datasource ( --Native Query Goes Here SELECT customer , year , SUM ( sales ) FROM t1 GROUP BY ROLLUP ( customer , year ) ; SELECT customer , year , SUM ( sales ) FROM t1 GROUP BY GROUPING SETS ( ( customer , year ) , ( customer ) ) UNION ALL SELECT NULL , NULL , SUM ( sales ) FROM t1 ; ) ; The above examples utilize sap_hana_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the SAP HANA database. Checklist : Make sure the SAP HANA database is active. Confirm that address, port, user and password are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and SAP HANA. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue QuestDB SAP SQL Anywhere github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "airtable.html", "content": "Airtable - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Airtable Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Airtable This is the implementation of the Airtable data handler for MindsDB. Airtable is a platform that makes it easy to build powerful, custom applications. These tools can streamline just about any process, workflow, or project. And best of all, you can build them without ever learning to write a single line of code. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Airtable to MindsDB, install the required dependencies following this instruction . Install or ensure access to Airtable. ​ Implementation This handler is implemented using duckdb , a library that allows SQL queries to be executed on pandas DataFrames. In essence, when querying a particular table, the entire table is first pulled into a pandas DataFrame using the Airtable API . Once this is done, SQL queries can be run on the DataFrame using duckdb . The required arguments to establish a connection are as follows: base_id is the Airtable base ID. table_name is the Airtable table name. api_key is the API key for the Airtable API. ​ Usage In order to make use of this handler and connect to the Airtable database in MindsDB, the following syntax can be used: CREATE DATABASE airtable_datasource WITH engine = 'airtable' , parameters = { \"base_id\" : \"dqweqweqrwwqq\" , \"table_name\" : \"iris\" , \"api_key\" : \"knlsndlknslk\" } ; You can use this established connection to query your table as follows: SELECT * FROM airtable_datasource . example_tbl ; At the moment, only the SELECT statement is allowed to be executed through duckdb . This, however, has no restriction on running machine learning algorithms against your data in Airtable using the CREATE MODEL statement. Was this page helpful? Yes No Suggest edits Raise issue Supported Integrations Amazon Aurora github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "yugabytedb.html", "content": "YugabyteDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases YugabyteDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases YugabyteDB This is the implementation of the YugabyteDB data handler for MindsDB. YugabyteDB is a high-performance, cloud-native distributed SQL database that aims to support all PostgreSQL features. It is best fit for cloud-native OLTP (i.e. real-time, business-critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, or globally-distributed deployments. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect YugabyteDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to YugabyteDB. ​ Implementation This handler is implemented using psycopg2 , a Python library that allows you to use Python code to run SQL commands on the YugabyteDB database. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. schema is the schema to which your table belongs. ​ Usage In order to make use of this handler and connect to the YugabyteDB database in MindsDB, the following syntax can be used: CREATE DATABASE yugabyte_datasource WITH engine = 'yugabyte' , parameters = { \"user\" : \"admin\" , \"password\" : \"1234\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"yugabyte\" , \"schema\" : \"your_schema_name\" } ; You can use this established connection to query your table as follows: SELECT * FROM yugabyte_datasource . demo ; NOTE : If you are using YugabyteDB Cloud with MindsDB Cloud website you need to add below 3 static IPs of MindsDB Cloud to allow IP list for accessing it publicly. 18.220.205.95 3.19.152.46 52.14.91.162 Was this page helpful? Yes No Suggest edits Raise issue Vitess ChromaDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "matrixone.html", "content": "MatrixOne - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MatrixOne Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MatrixOne This is the implementation of the MatrixOne data handler for MindsDB. MatrixOne is a future-oriented hyper-converged cloud and edge native DBMS that supports transactional, analytical, and streaming workloads with a simplified and distributed database engine, across multiple data centers, clouds, edges, and other heterogeneous infrastructures. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MatrixOne to MindsDB, install the required dependencies following this instruction . Install or ensure access to MatrixOne. ​ Implementation This handler is implemented using PyMySQL , a Python library that allows you to use Python code to run SQL commands on the MatrixOne database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP address of the database. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. There are several optional arguments that can be used as well. ssl indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores the SSL certificates. ssl_key stores the SSL keys. ​ Usage In order to make use of this handler and connect to the MatrixOne database in MindsDB, the following syntax can be used: CREATE DATABASE matrixone_datasource WITH engine = 'matrixone' , parameters = { \"user\" : \"dump\" , \"password\" : \"111\" , \"host\" : \"127.0.0.1\" , \"port\" : 6001 , \"database\" : \"mo_catalog\" } ; You can use this established connection to query your table as follows: SELECT * FROM Matrixone_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue MariaDB Microsoft Access github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-pinot.html", "content": "Apache Pinot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Pinot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Pinot This is the implementation of the Pinot data handler for MindsDB. Apache Pinot is a real-time distributed OLAP database designed for low-latency query execution even at extremely high throughput. Apache Pinot can ingest directly from streaming sources like Apache Kafka and make events available for querying immediately. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Pinot to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Pinot. ​ Implementation This handler was implemented using the pinotdb library, the Python DB-API and SQLAlchemy dialect for Pinot. The required arguments to establish a connection are as follows: host is the host name or IP address of the Apache Pinot cluster. broker_port is the port that the Broker of the Apache Pinot cluster is running on. controller_port is the port that the Controller of the Apache Pinot cluster is running on. path is the query path. ​ Usage In order to make use of this handler and connect to the Pinot cluster in MindsDB, the following syntax can be used: CREATE DATABASE pinot_datasource WITH engine = 'pinot' , parameters = { \"host\" : \"localhost\" , \"broker_port\" : 8000 , \"controller_port\" : 9000 , \"path\" : \"/query/sql\" , \"scheme\" : \"http\" } ; You can use this established connection to query your table as follows: SELECT * FROM pinot_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Apache Impala Apache Solr github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "starrocks.html", "content": "StarRocks - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases StarRocks Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases StarRocks This is the implementation of the StarRocks data handler for MindsDB. StarRocks is the next-generation data platform designed to make data-intensive real-time analytics fast and easy. It delivers query speeds 5 to 10 times faster than other popular solutions. StarRocks can perform real-time analytics well while updating historical records. It can also enhance real-time analytics with historical data from data lakes easily. With StarRocks, you can get rid of the de-normalized tables and get the best performance and flexibility. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect StarRocks to MindsDB, install the required dependencies following this instruction . Install or ensure access to StarRocks. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the StarRocks server in MindsDB, the following syntax can be used: CREATE DATABASE starrocks_datasource WITH ENGINE = 'starrocks' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"starrocks_user\" , \"password\" : \"password\" , \"port\" : 8030 , \"database\" : \"starrocks_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM starrocks_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue SQLite Supabase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "timescaledb.html", "content": "TimescaleDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TimescaleDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TimescaleDB This is the implementation of the TimescaleDB data handler for MindsDB. TimescaleDB is an open-source relational database that is optimized for time-series data. It is designed to handle large volumes of data. It enables you to query and analyze data in real-time. TimescaleDB can be used for a wide range of applications, including IoT, finance, and monitoring. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TimescaleDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to TimescaleDB. ​ Implementation This handler is implemented using the psycopg2 library, which is a PostgreSQL adapter for the Python programming language. TimescaleDB is built on top of PostgreSQL and therefore can be accessed using the same client libraries and APIs. The required arguments to establish a connection are as follows: * host is the the host name or IP address of the TimescaleDB server. * port is the port to use when connecting with the TimescaleDB server. * database is the database name to use when connecting with the TimescaleDB server. * user is the user to authenticate the user with the TimescaleDB server. * password is the password to authenticate the user with the TimescaleDB server. ​ Usage Before attempting to connect to a TimescaleDB server using MindsDB, ensure that it accepts incoming connections using this guide . In order to make use of this handler and connect to the TimescaleDB server in MindsDB, the following syntax can be used: CREATE DATABASE timescaledb_datasource WITH engine = 'timescaledb' , parameters = { \"host\" : \"examplehost.timescaledb.com\" , \"port\" : 5432 , \"user\" : \"example_user\" , \"password\" : \"my_password\" , \"database\" : \"tsdb\" } ; You can use this established connection to query your table as follows, SELECT * FROM timescaledb_datasource . sensor ; Was this page helpful? Yes No Suggest edits Raise issue TiDB Trino github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "tdengine.html", "content": "TDengine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TDengine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TDengine This is the implementation of the TDEngine data handler for MindsDB. TDengine is an open source, high-performance, cloud native time-series database optimized for Internet of Things (IoT), Connected Cars, and Industrial IoT. It enables efficient, real-time data ingestion, processing, and monitoring of TB and even PB scale data per day, generated by billions of sensors and data collectors. TDengine differentiates itself from other time-series databases with numerous advantages, such as high performance, simplified solution, cloud-native, ease of use, easy data analytics, and open-source. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TDengine to MindsDB, install the required dependencies following this instruction . Install or ensure access to TDengine. ​ Implementation This handler is implemented using taos/taosrest , a Python library that allows you to use Python code to run SQL commands on the TDEngine server. The required arguments to establish a connection are as follows: user is the username associated with the server. password is the password to authenticate your access. url is the URL to the TDEngine server. For local server, the URL is localhost:6041 by default. token is the unique token provided while using TDEngine Cloud. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the TDEngine database in MindsDB, the following syntax can be used: CREATE DATABASE tdengine_datasource WITH ENGINE = 'tdengine' , PARAMETERS = { \"user\" : \"tdengine_user\" , \"password\" : \"password\" , \"url\" : \"localhost:6041\" , \"token\" : \"token\" , \"database\" : \"tdengine_db\" } ; You can specify token instead of user and password while using TDEngine. You can use this established connection to query your table as follows: SELECT * FROM tdengine_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue SurrealDB Teradata github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-druid.html", "content": "Apache Druid - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Druid Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Druid This is the implementation of the Druid data handler for MindsDB. Apache Druid is a real-time analytics database designed for fast slice-and-dice analytics ( OLAP queries) on large data sets. Most often, Druid powers use cases where real-time ingestion, fast query performance, and high uptime are important. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Druid to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Druid. ​ Implementation This handler was implemented using the pydruid library, the Python API for Apache Druid. The required arguments to establish a connection are as follows: host is the host name or IP address of the Apache Druid database. port is the port that Apache Druid is running on. path is the query path. scheme is the URI schema. This parameter is optional and defaults to http . user is the username used to authenticate with Apache Druid. This parameter is optional. password is the password used to authenticate with Apache Druid. This parameter is optional. ​ Usage In order to make use of this handler and connect to Apache Druid in MindsDB, the following syntax can be used: CREATE DATABASE druid_datasource WITH engine = 'druid' , parameters = { \"host\" : \"localhost\" , \"port\" : 8888 , \"path\" : \"/druid/v2/sql/\" , \"scheme\" : \"http\" } ; You can use this established connection to query your table as follows: SELECT * FROM druid_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Apache Cassandra Apache Hive github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "greptimedb.html", "content": "GreptimeDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases GreptimeDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases GreptimeDB This is the implementation of the GreptimeDB data handler for MindsDB. GreptimeDB is an open-source, cloud-native time series database features analytical capabilities, scalebility and open protocols support. ​ Implementation This handler is implemented by extending the MySQLHandler. Connect GreptimeDB to MindsDB by providing the following parameters: host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. user is the database user. password is the database password. There are several optional parameters that can be used as well. ssl is the ssl parameter value that indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores SSL certificates. ssl_key stores SSL keys. ​ Usage In order to make use of this handler and connect to the GreptimeDB database in MindsDB, the following syntax can be used: CREATE DATABASE greptimedb_datasource WITH engine = 'greptimedb' , parameters = { \"host\" : \"127.0.0.1\" , \"port\" : 4002 , \"database\" : \"public\" , \"user\" : \"username\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows. SELECT * FROM greptimedb_datasource . example_table ; Was this page helpful? Yes No Suggest edits Raise issue Google Sheets IBM Db2 github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Implementation Usage"}
{"file_name": "opengauss.html", "content": "OpenGauss - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OpenGauss Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OpenGauss This is the implementation of the OpenGauss data handler for MindsDB. OpenGauss is an open-source relational database management system released with the Mulan PSL v2 and the kernel built on Huawei’s years of experience in the database field. It continuously provides competitive features tailored to enterprise-grade scenarios. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OpenGauss to MindsDB, install the required dependencies following this instruction . Install or ensure access to OpenGauss. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the OpenGauss database in MindsDB, the following syntax can be used: CREATE DATABASE opengauss_datasource WITH ENGINE = 'opengauss' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"opengauss\" , \"user\" : \"mindsdb\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM opengauss_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue OceanBase Oracle github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ibm-db2.html", "content": "IBM Db2 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases IBM Db2 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases IBM Db2 This documentation describes the integration of MindsDB with IBM Db2 , the cloud-native database built to power low-latency transactions, real-time analytics and AI applications at scale. The integration allows MindsDB to access data stored in the IBM Db2 database and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect IBM Db2 to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your IBM Db2 database from MindsDB by executing the following SQL command: CREATE DATABASE db2_datasource WITH engine = 'db2' , parameters = { \"host\" : \"127.0.0.1\" , \"user\" : \"db2inst1\" , \"password\" : \"password\" , \"database\" : \"example_db\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the IBM Db2 database. user : The username for the IBM Db2 database. password : The password for the IBM Db2 database. database : The name of the IBM Db2 database to connect to. Optional connection parameters include the following: port : The port number for connecting to the IBM Db2 database. Default is 50000 . schema : The database schema to use within the IBM Db2 database. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM db2_datasource . schema_name . table_name LIMIT 10 ; Run IBM Db2 native queries directly on the connected database: SELECT * FROM db2_datasource ( --Native Query Goes Here WITH DINFO ( DEPTNO , AVGSALARY , EMPCOUNT ) AS ( SELECT OTHERS . WORKDEPT , AVG ( OTHERS . SALARY ) , COUNT ( * ) FROM EMPLOYEE OTHERS GROUP BY OTHERS . WORKDEPT ) , DINFOMAX AS ( SELECT MAX ( AVGSALARY ) AS AVGMAX FROM DINFO ) SELECT THIS_EMP . EMPNO , THIS_EMP . SALARY , DINFO . AVGSALARY , DINFO . EMPCOUNT , DINFOMAX . AVGMAX FROM EMPLOYEE THIS_EMP , DINFO , DINFOMAX WHERE THIS_EMP . JOB = 'SALESREP' AND THIS_EMP . WORKDEPT = DINFO . DEPTNO ) ; The above examples utilize db2_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the IBM Db2 database. Checklist : Make sure the IBM Db2 database is active. Confirm that host, user, password and database are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and the IBM Db2 database. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` This guide of common connection Db2 connection issues provided by IBM might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue GreptimeDB IBM Informix github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "monetdb.html", "content": "MonetDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MonetDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MonetDB This is the implementation of the MonetDB data handler for MindsDB. MonetDB is an open-source column-oriented relational database management system originally developed at the Centrum Wiskunde & Informatica in the Netherlands. It is designed to provide high performance on complex queries against large databases, such as combining tables with hundreds of columns and millions of rows. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MonetDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to MonetDB. ​ Implementation This handler is implemented using pymonetdb , a Python library that allows you to use Python code to run SQL commands on the MonetDB database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the host name or IP address. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema_name is the schema name to get tables. It is optional and defaults to the current schema if not provided. ​ Usage In order to make use of this handler and connect to the MonetDB database in MindsDB, the following syntax can be used: CREATE DATABASE monetdb_datasource WITH engine = 'monetdb' , parameters = { \"user\" : \"monetdb\" , \"password\" : \"monetdb\" , \"host\" : \"127.0.0.1\" , \"port\" : 50000 , \"schema_name\" : \"sys\" , \"database\" : \"demo\" } ; You can use this established connection to query your table as follows: SELECT * FROM monetdb_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue Microsoft SQL Server MongoDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ibm-informix.html", "content": "IBM Informix - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases IBM Informix Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases IBM Informix This is the implementation of the IBM Informix data handler for MindsDB. IBM Informix is a product family within IBM’s Information Management division that is centered on several relational database management system (RDBMS) offerings. The Informix server supports object–relational models and (through extensions) data types that are not a part of the SQL standard. The most widely used of these are the JSON, BSON, time series, and spatial extensions, which provide both data type support and language extensions that permit high-performance domain-specific queries and efficient storage for data sets based on semi-structured, time series, and spatial data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect IBM Informix to MindsDB, install the required dependencies following this instruction . Install or ensure access to IBM Informix. ​ Implementation This handler is implemented using IfxPy/IfxPyDbi , a Python library that allows you to use Python code to run SQL commands on the Informix database. The required arguments to establish a connection are as follows: user is the username associated with database. password is the password to authenticate your access. host is the hostname or IP address of the server. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema_name is the schema name to get tables. server is the name of server you want connect. logging_enabled defines whether logging is enabled or not. Defaults to True if not provided. ​ Usage In order to make use of this handler and connect to the Informix database in MindsDB, the following syntax can be used: CREATE DATABASE informix_datasource WITH engine = 'informix' , parameters = { \"server\" : \"server\" , \"host\" : \"127.0.0.1\" , \"port\" : 9091 , \"user\" : \"informix\" , \"password\" : \"in4mix\" , \"database\" : \"stores_demo\" , \"schema_name\" : \"love\" , \"loging_enabled\" : False } ; You can use this established connection to query your table as follows: SELECT * FROM informix_datasource . items ; This integration uses IfxPy . As it is in development stage, it can be install using pip install IfxPy . However, it doesn’t work with higher versions of Python, therefore, you have to build it from source. On Linux This code downloads and extracts the onedb-ODBC driver used to make connection: cd $HOME mkdir Informix cd Informix mkdir -p home/informix/cli wget https://hcl-onedb.github.io/odbc/OneDB-Linux64-ODBC-Driver.tar sudo tar xvf OneDB-Linux64-ODBC-Driver.tar -C ./home/informix/cli rm OneDB-Linux64-ODBC-Driver.tar Add enviroment variables in the .bashrc file: export INFORMIXDIR = $HOME /Informix/home/informix/cli/onedb-odbc-driver export LD_LIBRARY_PATH = ${LD_LIBRARY_PATH} ${INFORMIXDIR} /lib: ${INFORMIXDIR} /lib/esql: ${INFORMIXDIR} /lib/cli This code clones the IfxPy repo, builds a wheel, and installs it: pip install wheel mkdir Temp cd Temp git clone https://github.com/OpenInformix/IfxPy.git cd IfxPy/IfxPy python setup.py bdist_wheel pip install --find-links = ./dist IfxPy cd .. cd .. cd .. rm -rf Temp On Windows This code downloads and extracts the onedb-ODBC driver used to make connection: cd $HOME mkdir Informix cd Informix mkdir /home/informix/cli wget https://hcl-onedb.github.io/odbc/OneDB-Win64-ODBC-Driver.zip tar xvf OneDB-Win64-ODBC-Driver.zip -C ./home/informix/cli del OneDB-Win64-ODBC-Driver.zip Add an enviroment variable: set INFORMIXDIR = $HOME /Informix/home/informix/cli/onedb-odbc-driver Add %INFORMIXDIR%\\bin to the PATH environment variable. This code clones the IfxPy repo, builds a wheel, and installs it: pip install wheel mkdir Temp cd Temp git clone https://github.com/OpenInformix/IfxPy.git cd IfxPy/IfxPy python setup.py bdist_wheel pip install --find-links = ./dist IfxPy cd .. cd .. cd .. rmdir Temp Was this page helpful? Yes No Suggest edits Raise issue IBM Db2 InfluxDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-redshift.html", "content": "Amazon Redshift - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon Redshift Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon Redshift This documentation describes the integration of MindsDB with Amazon Redshift , a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more, enabling you to use your data to acquire new insights for your business and customers. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Redshift to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Redshift database from MindsDB by executing the following SQL command: CREATE DATABASE redshift_datasource WITH engine = 'redshift' , parameters = { \"host\" : \"examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com\" , \"port\" : 5439 , \"database\" : \"example_db\" , \"user\" : \"awsuser\" , \"password\" : \"my_password\" } ; Required connection parameters include the following: host : The host name or IP address of the Redshift cluster. port : The port to use when connecting with the Redshift cluster. database : The database name to use when connecting with the Redshift cluster. user : The username to authenticate the user with the Redshift cluster. password : The password to authenticate the user with the Redshift cluster. Optional connection parameters include the following: schema : The database schema to use. Default is public. sslmode : The SSL mode for the connection. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM redshift_datasource . schema_name . table_name LIMIT 10 ; Run Amazon Redshift SQL queries directly on the connected Redshift database: SELECT * FROM redshift_datasource ( --Native Query Goes Here WITH VENUECOPY AS ( SELECT * FROM VENUE ) SELECT * FROM VENUECOPY ORDER BY 1 LIMIT 10 ; ) ; The above examples utilize redshift_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon Redshift cluster. Checklist : Make sure the Redshift cluster is active. Confirm that host, port, user, password and database are correct. Try a direct Redshift connection using a client like DBeaver. Ensure that the security settings of the Redshift cluster allow connections from MindsDB. Ensure a stable network between MindsDB and Redshift. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` This troubleshooting guide provided by AWS might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue Amazon DynamoDB Amazon S3 github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "clickhouse.html", "content": "ClickHouse - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ClickHouse Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ClickHouse This documentation describes the integration of MindsDB with ClickHouse , a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). The integration allows MindsDB to access data from ClickHouse and enhance ClickHouse with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ClickHouse to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to ClickHouse from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE clickhouse_conn WITH ENGINE = 'clickhouse' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : \"8443\" , \"user\" : \"root\" , \"password\" : \"mypass\" , \"database\" : \"test_data\" , \"protocol\" : \"https\" } Required connection parameters include the following: host : is the hostname or IP address of the ClickHouse server. port : is the TCP/IP port of the ClickHouse server. user : is the username used to authenticate with the ClickHouse server. password : is the password to authenticate the user with the ClickHouse server. database : defaults to default . It is the database name to use when connecting with the ClickHouse server. protocol : defaults to native . It is an optional parameter. Its supported values are native , http and https . ​ Usage The following usage examples utilize the connection to ClickHouse made via the CREATE DATABASE statement and named clickhouse_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM clickhouse_conn . table_name LIMIT 10 ; ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the ClickHouse database. Checklist : Ensure that the ClickHouse server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the ClickHouse server. Slow Connection Initialization Symptoms : Connecting to the ClickHouse server takes an exceptionally long time, or connections hang without completing Checklist : Ensure that you are using the appropriate protocol (http, https, or native) for your ClickHouse setup. Misconfigurations here can lead to significant delays. Ensure that firewalls or security groups (in cloud environments) are properly configured to allow traffic on the necessary ports (as 8123 for HTTP or 9000 for native). SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Ckan Cloud Spanner github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "databend.html", "content": "Databend - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Databend Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Databend This is the implementation of the Databend data handler for MindsDB. Databend is a modern cloud data warehouse that empowers your object storage for real-time analytics. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Databend to MindsDB, install the required dependencies following this instruction . Install or ensure access to Databend. ​ Implementation This handler is implemented by extending the ClickHouse handler. The required arguments to establish a connection are as follows: protocol is the protocol to query Databend. Supported values include native , http , https . It defaults to native if not provided. host is the host name or IP address of the Databend warehouse. port is the TCP/IP port of the Databend warehouse. user is the username used to authenticate with the Databend warehouse. password is the password to authenticate the user with the Databend warehouse. database is the database name to use when connecting with the Databend warehouse. ​ Usage In order to make use of this handler and connect to the Databend database in MindsDB, the following syntax can be used: CREATE DATABASE databend_datasource WITH engine = 'databend' , parameters = { \"protocol\" : \"https\" , \"user\" : \"root\" , \"port\" : 443 , \"password\" : \"password\" , \"host\" : \"some-url.aws-us-east-2.default.databend.com\" , \"database\" : \"test_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM databend_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue D0lt Databricks github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "oracle.html", "content": "Oracle - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Oracle Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Oracle This documentation describes the integration of MindsDB with Oracle , one of the most trusted and widely used relational database engines for storing, organizing and retrieving data by type while still maintaining relationships between the various types. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Oracle to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Oracle database from MindsDB by executing the following SQL command: CREATE DATABASE oracle_datasource WITH ENGINE = 'oracle' , PARAMETERS = { \"host\" : \"localhost\" , \"service_name\" : \"FREEPDB1\" , \"user\" : \"SYSTEM\" , \"password\" : \"password\" } ; Required connection parameters include the following: user : The username for the Oracle database. password : The password for the Oracle database. dsn : The data source name (DSN) for the Oracle database. OR host : The hostname, IP address, or URL of the Oracle server. AND sid : The system identifier (SID) of the Oracle database. OR service_name : The service name of the Oracle database. Optional connection parameters include the following: port : The port number for connecting to the Oracle database. Default is 1521. disable_oob : The boolean parameter to disable out-of-band breaks. Default is false . auth_mode : The authorization mode to use. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM oracle_datasource . schema_name . table_name LIMIT 10 ; Run PL/SQL queries directly on the connected Oracle database: SELECT * FROM oracle_datasource ( --Native Query Goes Here SELECT employee_id , first_name , last_name , email , hire_date FROM oracle_datasource . hr . employees WHERE department_id = 10 ORDER BY hire_date DESC ; ) ; The above examples utilize oracle_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Oracle database. Checklist : Make sure the Oracle database is active. Confirm that the connection parameters provided (DSN, host, SID, service_name) and the credentials (user, password) are correct. Ensure a stable network between MindsDB and Oracle. This troubleshooting guide provided by Oracle might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue OpenGauss OrioleDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "datastax.html", "content": "DataStax - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases DataStax Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases DataStax This is the implementation of the DataStax data handler for MindsDB. DataStax Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect DataStax to MindsDB, install the required dependencies following this instruction . Install or ensure access to DataStax. ​ Implementation DataStax Astra DB is API-compatible with Apache Cassandra and ScyllaDB. Therefore, its implementation extends the ScyllaDB handler and is using the scylla-driver Python library. The required arguments to establish a connection are as follows: user is the user to authenticate. password is the password to authenticate the user. secure_connect_bundle is the path to the secure_connect_bundle zip file. ​ Usage In order to make use of this handler and connect to the Astra DB database in MindsDB, the following syntax can be used: CREATE DATABASE astra_connection WITH engine = \"astra\" , parameters = { \"user\" : \"user\" , \"password\" : \"pass\" , \"secure_connect_bundle\" : \"/home/Downloads/file.zip\" } ; or, reference the bundle from Datastax s3 as: CREATE DATABASE astra_connection WITH ENGINE = \"astra\" , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"pass\" , \"secure_connect_bundle\" : \"https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/32312-b9eb-4e09-a641-213eaesa12-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AK...\" } You can use this established connection to query your table as follows: SELECT * FROM astra_connection . keystore . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Databricks DuckDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cockroachdb.html", "content": "CockroachDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases CockroachDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases CockroachDB This is the implementation of the CockroachDB data handler for MindsDB. CockroachDB was architected for complex, high performant distributed writes and delivers scale-out read capability. CockroachDB delivers simple relational SQL transactions and obscures complexity away from developers. It is wire-compatible with PostgreSQL and provides a familiar and easy interface for developers. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect CockroachDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to CockroachDB. ​ Implementation CockroachDB is wire-compatible with PostgreSQL. Therefore, its implementation extends the PostgreSQL handler. The required arguments to establish a connection are as follows: host is the host name or IP address of the CockroachDB. database is the name of the database to connect to. user is the user to authenticate with the CockroachDB. port is the port to use when connecting. password is the password to authenticate the user. In order to make use of this handler and connect to the CockroachDB server in MindsDB, the following syntax can be used: CREATE DATABASE cockroachdb WITH engine = 'cockroachdb' , parameters = { \"host\" : \"localhost\" , \"database\" : \"dbname\" , \"user\" : \"admin\" , \"password\" : \"password\" , \"port\" : \"5432\" } ; ​ Usage You can use this established connection to query your table as follows: SELECT * FROM cockroachdb . public . db ; Was this page helpful? Yes No Suggest edits Raise issue Cloud Spanner Couchbase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-s3.html", "content": "Amazon S3 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon S3 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon S3 This documentation describes the integration of MindsDB with Amazon S3 , an object storage service that offers industry-leading scalability, data availability, security, and performance. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure that MindsDB is installed locally via Docker or Docker Desktop . ​ Connection Establish a connection to your Amazon S3 bucket from MindsDB by executing the following SQL command: CREATE DATABASE s3_datasource WITH engine = 's3' , parameters = { \"aws_access_key_id\" : \"AQAXEQK89OX07YS34OP\" , \"aws_secret_access_key\" : \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" , \"bucket\" : \"my-bucket\" } ; Note that sample parameter values are provided here for reference, and you should replace them with your connection parameters. Required connection parameters include the following: aws_access_key_id : The AWS access key that identifies the user or IAM role. aws_secret_access_key : The AWS secret access key that identifies the user or IAM role. Optional connection parameters include the following: aws_session_token : The AWS session token that identifies the user or IAM role. This becomes necessary when using temporary security credentials. bucket : The name of the Amazon S3 bucket. If not provided, all available buckets can be queried, however, this can affect performance, especially when listing all of the available objects. ​ Usage Retrieve data from a specified object (file) in a S3 bucket by providing the integration name and the object key: SELECT * FROM s3_datasource . ` my-file.csv ` ; LIMIT 10 ; If a bucket name is provided in the CREATE DATABASE command, querying will be limited to that bucket and the bucket name can be ommitted from the object key as shown in the example above. However, if the bucket name is not provided, the object key must include the bucket name, such as s3_datasource. my-bucket/my-folder/my-file.csv`. Wrap the object key in backticks (`) to avoid any issues parsing the SQL statements provided. This is especially important when the object key contains spaces, special characters or prefixes, such as my-folder/my-file.csv . At the moment, the supported file formats are CSV, TSV, JSON, and Parquet. The above examples utilize s3_datasource as the datasource name, which is defined in the CREATE DATABASE command. The special files table can be used to list all objects available in the specified bucket or all buckets if the bucket name is not provided: SELECT * FROM s3_datasource . files LIMIT 10 The content of files can also be retrieved by explicitly requesting the content column. This column is empty by default to avoid unnecessary data transfer: SELECT path , content FROM s3_datasource . files LIMIT 10 This table will return all objects regardless of the file format, however, only the supported file formats mentioned above can be queried. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon S3 bucket. Checklist : Make sure the Amazon S3 bucket exists. Confirm that provided AWS credentials are correct. Try making a direct connection to the S3 bucket using the AWS CLI. Ensure a stable network between MindsDB and AWS. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing object names containing spaces, special characters or prefixes. Checklist : Ensure object names with spaces, special characters or prefixes are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel/travel_data.csv Incorrect: SELECT * FROM integration.‘travel/travel_data.csv’ Correct: SELECT * FROM integration.`travel/travel_data.csv` Was this page helpful? Yes No Suggest edits Raise issue Amazon Redshift Apache Cassandra github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "sqlite.html", "content": "SQLite - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SQLite Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SQLite This is the implementation of the SQLite data handler for MindsDB. SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain and is thus free to use for either commercial or private purpose. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SQLite to MindsDB, install the required dependencies following this instruction . Install or ensure access to SQLite. ​ Implementation This handler is implemented using the standard sqlite3 library that comes with Python. The only required argument to establish a connection is db_file that points to the database file that the connection is to be made to. Optionally, this may also be set to :memory: to create an in-memory database. ​ Usage In order to make use of this handler and connect to the SQLite database in MindsDB, the following syntax can be used: CREATE DATABASE sqlite_datasource WITH engine = 'sqlite' , parameters = { \"db_file\" : \"example.db\" } ; You can use this established connection to query your table as follows: SELECT * FROM sqlite_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Snowflake StarRocks github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "singlestore.html", "content": "SingleStore - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SingleStore Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SingleStore This is the implementation of the SingleStore data handler for MindsDB. SingleStore is a proprietary, cloud-native database designed for data-intensive applications. A distributed, relational, SQL database management system that features ANSI SQL support. It is known for speed in data ingest, transaction processing, and query processing. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SingleStore to MindsDB, install the required dependencies following this instruction . Install or ensure access to SingleStore. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. There are several optional arguments that can be used as well. ssl is the ssl parameter value that indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores SSL certificates. ssl_key stores SSL keys. ​ Usage In order to make use of this handler and connect to the SingleStore database in MindsDB, the following syntax can be used: CREATE DATABASE singlestore_datasource WITH ENGINE = 'singlestore' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"singlestore\" , \"user\" : \"root\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM singlestore_datasource . example_table ; Was this page helpful? Yes No Suggest edits Raise issue ScyllaDB Snowflake github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "postgresql.html", "content": "PostgreSQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases PostgreSQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases PostgreSQL This documentation describes the integration of MindsDB with PostgreSQL , a powerful, open-source, object-relational database system. The integration allows MindsDB to access data stored in the PostgreSQL database and enhance PostgreSQL with AI capabilities. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect PostgreSQL to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your PostgreSQL database from MindsDB by executing the following SQL command: CREATE DATABASE postgresql_conn WITH ENGINE = 'postgres' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"postgres\" , \"user\" : \"postgres\" , \"schema\" : \"data\" , \"password\" : \"password\" } ; Required connection parameters include the following: user : The username for the PostgreSQL database. password : The password for the PostgreSQL database. host : The hostname, IP address, or URL of the PostgreSQL server. port : The port number for connecting to the PostgreSQL server. database : The name of the PostgreSQL database to connect to. Optional connection parameters include the following: schema : The database schema to use. Default is public. sslmode : The SSL mode for the connection. ​ Usage The following usage examples utilize the connection to PostgreSQL made via the CREATE DATABASE statement and named postgresql_conn . Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM postgresql_conn . table_name LIMIT 10 ; Run PostgreSQL-native queries directly on the connected PostgreSQL database: SELECT * FROM postgresql_conn ( --Native Query Goes Here SELECT model , COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price FROM used_car_price ) ; Next Steps Follow this tutorial to see more use case examples. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the PostgreSQL database. Checklist : Make sure the PostgreSQL server is active. Confirm that host, port, user, schema, and password are correct. Try a direct PostgreSQL connection. Ensure a stable network between MindsDB and PostgreSQL. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue PlanetScale QuestDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "apache-impala.html", "content": "Apache Impala - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Impala Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Impala This is the implementation of the Impala data handler for MindsDB. Apache Impala is an MPP (Massive Parallel Processing) SQL query engine for processing huge volumes of data that is stored in the Apache Hadoop cluster. It is an open source software written in C++ and Java. It provides high performance and low latency compared to other SQL engines for Hadoop. In other words, Impala is the highest performing SQL engine (giving RDBMS-like experience) that provides the fastest way to access data stored in Hadoop Distributed File System. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Impala to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Impala. ​ Implementation This handler is implemented using impyla , a Python library that allows you to use Python code to run SQL commands on Impala. The required arguments to establish a connection are: user is the username associated with the database. password is the password to authenticate your access. host is the server IP address or hostname. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the Impala database in MindsDB, the following syntax can be used: CREATE DATABASE impala_datasource WITH engine = 'impala' , parameters = { \"user\" : \"root\" , \"password\" : \"p@55w0rd\" , \"host\" : \"127.0.0.1\" , \"port\" : 21050 , \"database\" : \"Db_NamE\" } ; You can use this established connection to query your table as follows: SELECT * FROM impala_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue Apache Ignite Apache Pinot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "couchbase.html", "content": "Couchbase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Couchbase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Couchbase This is the implementation of the Couchbase data handler for MindsDB. Couchbase is an open-source, distributed multi-model NoSQL document-oriented database software package optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating, and presenting data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Couchbase to MindsDB, install the required dependencies following this instruction . Install or ensure access to Couchbase. ​ Implementation This handler is implemented using the couchbase library, the Python driver for Couchbase. The required arguments to establish a connection are as follows: connection_string : the connection string for the endpoint of the Couchbase server bucket : the bucket name to use when connecting with the Couchbase server user : the user to authenticate with the Couchbase server password : the password to authenticate the user with the Couchbase server scope : scopes are a level of data organization within a bucket. If omitted, will default to _default Note: The connection string expects either the couchbases:// or couchbase:// protocol. If you are using Couchbase Capella, you can find the connection_string under the Connect tab It will also be required to whitelist the machine(s) that will be running MindsDB and database credentials will need to be created for the user. These steps can also be taken under the Connect tab. In order to make use of this handler and connect to a Couchbase server in MindsDB, the following syntax can be used. Note, that the example uses the default travel-sample bucket which can be enabled from the couchbase UI with pre-defined scope and documents. CREATE DATABASE couchbase_datasource WITH engine = 'couchbase' , parameters = { \"connection_string\" : \"couchbase://localhost\" , \"bucket\" : \"travel-sample\" , \"user\" : \"admin\" , \"password\" : \"password\" , \"scope\" : \"inventory\" } ; ​ Usage Now, you can use this established connection to query your database as follows: SELECT * FROM couchbase_datasource . airport Was this page helpful? Yes No Suggest edits Raise issue CockroachDB CrateDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-dynamodb.html", "content": "Amazon DynamoDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon DynamoDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon DynamoDB This documentation describes the integration of MindsDB with Amazon DynamoDB , a serverless, NoSQL database service that enables you to develop modern applications at any scale. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure that MindsDB is installed locally via Docker or Docker Desktop . ​ Connection Establish a connection to your Amazon DynamoDB from MindsDB by executing the following SQL command: CREATE DATABASE dynamodb_datasource WITH engine = 'dynamodb' , parameters = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" } ; Required connection parameters include the following: aws_access_key_id : The AWS access key that identifies the user or IAM role. aws_secret_access_key : The AWS secret access key that identifies the user or IAM role. region_name : The AWS region to connect to. Optional connection parameters include the following: aws_session_token : The AWS session token that identifies the user or IAM role. This becomes necessary when using temporary security credentials. ​ Usage Retrieve data from a specified table by providing the integration name and the table name: SELECT * FROM dynamodb_datasource . table_name LIMIT 10 ; Indexes can also be queried by adding a third-level namespace: SELECT * FROM dynamodb_datasource . table_name . index_name LIMIT 10 ; The queries issued to Amazon DynamoDB are in PartiQL, a SQL-compatible query language for Amazon DynamoDB. For more information, refer to the PartiQL documentation . There are a few limitations to keep in mind when querying data from Amazon DynamoDB (some of which are specific to PartiQL): The LIMIT , GROUP BY and HAVING clauses are not supported in PartiQL SELECT statements. Furthermore, subqueries and joins are not supported either. Refer to the PartiQL documentation for SELECT statements for more information. INSERT statements are not supported by this integration. However, this can be overcome by issuing a ‘native query’ via an established connection. An example of this is provided below. Run PartiQL queries directly on Amazon DynamoDB: SELECT * FROM dynamodb_datasource ( --Native Query Goes Here INSERT INTO \"Music\" value { 'Artist' : 'Acme Band1' , 'SongTitle' : 'PartiQL Rocks' } ) ; The above examples utilize dynamodb_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon S3 DynamoDB. Checklist : Confirm that provided AWS credentials are correct. Try making a direct connection to the Amazon DynamoDB using the AWS CLI. Ensure a stable network between MindsDB and AWS. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue Amazon Aurora Amazon Redshift github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "google-bigquery.html", "content": "Google BigQuery - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Google BigQuery Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Google BigQuery This documentation describes the integration of MindsDB with Google BigQuery , a fully managed, AI-ready data analytics platform that helps you maximize value from your data. The integration allows MindsDB to access data stored in the BigQuery warehouse and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect BigQuery to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your BigQuery warehouse from MindsDB by executing the following SQL command: CREATE DATABASE bigquery_datasource WITH engine = \"bigquery\" , parameters = { \"project_id\" : \"bgtest-1111\" , \"dataset\" : \"mydataset\" , \"service_account_keys\" : \"/tmp/keys.json\" } ; Required connection parameters include the following: project_id : The globally unique identifier for your project in Google Cloud where BigQuery is located. dataset : The default dataset to connect to. Optional connection parameters include the following: service_account_keys : The full path to the service account key file. service_account_json : The content of a JSON file defined by the service_account_keys parameter. One of service_account_keys or service_account_json has to be provided to establish a connection to BigQuery. ​ Usage Retrieve data from a specified table in the default dataset by providing the integration name and table name: SELECT * FROM bigquery_datasource . table_name LIMIT 10 ; Retrieve data from a specified table in a different dataset by providing the integration name, dataset name and table name: SELECT * FROM bigquery_datasource . dataset_name . table_name LIMIT 10 ; Run SQL in any supported BigQuery dialect directly on the connected BigQuery database: SELECT * FROM bigquery_datasource ( --Native Query Goes Here SELECT * FROM t1 WHERE t1 . a IN ( SELECT t2 . a FROM t2 FOR SYSTEM_TIME AS OF t1 . timestamp_column ) ; ) ; The above examples utilize bigquery_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the BigQuery warehouse. Checklist : Make sure that the Google Cloud account is active and the Google BigQuery service is enabled. Confirm that the project ID, dataset and service account credentials are correct. Try a direct BigQuery connection using a client like DBeaver. Ensure a stable network between MindsDB and Google BigQuery. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: _ Incorrect: SELECT _ FROM integration.travel data _ Incorrect: SELECT _ FROM integration.‘travel data’ _ Correct: SELECT _ FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Firebird Google Cloud SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "databricks.html", "content": "Databricks - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Databricks Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Databricks This documentation describes the integration of MindsDB with Databricks , the world’s first data intelligence platform powered by generative AI. The integration allows MindsDB to access data stored in a Databricks workspace and enhance it with AI capabilities. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Databricks to MindsDB, install the required dependencies following this instruction . If the Databricks cluster you are attempting to connect to is terminated, executing the queries given below will attempt to start the cluster and therefore, the first query may take a few minutes to execute. To avoid any delays, ensure that the Databricks cluster is running before executing the queries. ​ Connection Establish a connection to your Databricks workspace from MindsDB by executing the following SQL command: CREATE DATABASE databricks_datasource WITH engine = 'databricks' , parameters = { \"server_hostname\" : \"adb-1234567890123456.7.azuredatabricks.net\" , \"http_path\" : \"sql/protocolv1/o/1234567890123456/1234-567890-test123\" , \"access_token\" : \"dapi1234567890ab1cde2f3ab456c7d89efa\" , \"schema\" : \"example_db\" } ; Required connection parameters include the following: server_hostname : The server hostname for the cluster or SQL warehouse. http_path : The HTTP path of the cluster or SQL warehouse. access_token : A Databricks personal access token for the workspace. Refer the instructions given https://docs.databricks.com/en/integrations/compute-details.html and https://docs.databricks.com/en/dev-tools/python-sql-connector.html#authentication to find the connection parameters mentioned above for your compute resource. Optional connection parameters include the following: session_configuration : Additional (key, value) pairs to set as Spark session configuration parameters. This should be provided as a JSON string. http_headers : Additional (key, value) pairs to set in HTTP headers on every RPC request the client makes. This should be provided as a JSON string. catalog : The catalog to use for the connection. Default is hive_metastore . schema : The schema (database) to use for the connection. Default is default . ​ Usage Retrieve data from a specified table by providing the integration name, catalog, schema, and table name: SELECT * FROM databricks_datasource . catalog_name . schema_name . table_name LIMIT 10 ; The catalog and schema names only need to be provided if the table to be queried is not in the specified (or default) catalog and schema. Run Databricks SQL queries directly on the connected Databricks workspace: SELECT * FROM databricks_datasource ( --Native Query Goes Here SELECT city , car_model , RANK ( ) OVER ( PARTITION BY car_model ORDER BY quantity ) AS rank FROM dealer QUALIFY rank = 1 ; ) ; The above examples utilize databricks_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Databricks workspace. Checklist : Make sure the Databricks workspace is active. Confirm that server hostname, HTTP path, access token are correctly provided. If the catalog and schema are provided, ensure they are correct as well. Ensure a stable network between MindsDB and Databricks workspace. SQL statements running against tables (of reasonable size) are taking longer than expected. Symptoms : SQL queries taking longer than expected to execute. Checklist : Ensure the Databricks cluster is running before executing the queries. Check the network connection between MindsDB and Databricks workspace. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue Databend DataStax github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "apache-hive.html", "content": "Apache Hive - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Hive Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Hive This documentation describes the integration of MindsDB with Apache Hive , a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. The integration allows MindsDB to access data from Apache Hive and enhance Apache Hive with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Hive to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Apache Hive from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE hive_datasource WITH engine = 'hive' , parameters = { \"username\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"default\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the Apache Hive server. database : The name of the Apache Hive database to connect to. Optional connection parameters include the following: username : The username for the Apache Hive database. password : The password for the Apache Hive database. port : The port number for connecting to the Apache Hive server. Default is 10000 . auth : The authentication mechanism to use. Default is CUSTOM . Other options are NONE , NOSASL , KERBEROS and LDAP . ​ Usage Retrieve data from a specified table by providing the integration and table names: SELECT * FROM hive_datasource . table_name LIMIT 10 ; Run HiveQL queries directly on the connected Apache Hive database: SELECT * FROM hive_datasource ( --Native Query Goes Here FROM ( FROM ( FROM src SELECT TRANSFORM ( value ) USING 'mapper' AS value , count ) mapped SELECT cast ( value as double ) AS value , cast ( count as int ) AS count SORT BY value , count ) sorted SELECT TRANSFORM ( value , count ) USING 'reducer' AS whatever ) ; The above examples utilize hive_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the Apache Hive database. Checklist : Ensure that the Apache Hive server is running and accessible Confirm that host, port, user, and password are correct. Try a direct Apache Hive connection using a client like DBeaver. Test the network connection between the MindsDB host and the Apache Hive server. Was this page helpful? Yes No Suggest edits Raise issue Apache Druid Apache Ignite github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "vertica.html", "content": "Vertica - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Vertica Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Vertica This is the implementation of the Vertica data handler for MindsDB. The column-oriented Vertica Analytics Platform was designed to manage large, fast-growing volumes of data and with fast query performance for data warehouses and other query-intensive applications. The product claims to greatly improve query performance over traditional relational database systems, and to provide high availability and exabyte scalability on commodity enterprise servers. Vertica runs on multiple cloud computing systems as well as on Hadoop nodes. Vertica’s Eon Mode separates compute from storage, using S3 object storage and dynamic allocation of compute notes. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Vertica to MindsDB, install the required dependencies following this instruction . Install or ensure access to Vertica. ​ Implementation This handler is implemented using vertica-python , a Python library that allows you to use Python code to run SQL commands on the Vertica database. The required arguments to establish a connection are as follows: user is the username asscociated with the database. password is the password to authenticate your access. host is the host name or IP address of the server. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema is the schema name to get tables from. ​ Usage In order to make use of this handler and connect to the Vertica database in MindsDB, the following syntax can be used: CREATE DATABASE vertica_datasource WITH engine = 'vertica' , parameters = { \"user\" : \"dbadmin\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"schema_name\" : \"public\" , \"database\" : \"VMart\" } ; You can use this established connection to query your table as follows: SELECT * FROM vertica_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue Trino Vitess github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "mariadb.html", "content": "MariaDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MariaDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MariaDB This documentation describes the integration of MindsDB with MariaDB , one of the most popular open source relational databases. The integration allows MindsDB to access data from MariaDB and enhance MariaDB with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MariaDB to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to MariaDB from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE mariadb_conn WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"host-name\" , \"port\" : 3307 , \"database\" : \"db-name\" , \"user\" : \"user-name\" , \"password\" : \"password\" } ; Or: CREATE DATABASE mariadb_conn WITH ENGINE = 'mariadb' , PARAMETERS = { \"url\" : \"mariadb://user-name@host-name:3307\" } ; Required connection parameters include the following: user : The username for the MariaDB database. password : The password for the MariaDB database. host : The hostname, IP address, or URL of the MariaDB server. port : The port number for connecting to the MariaDB server. database : The name of the MariaDB database to connect to. Or: url : You can specify a connection to MariaDB Server using a URI-like string, as an alternative connection option. You can also use mysql:// as the protocol prefix Optional connection parameters include the following: ssl : Boolean parameter that indicates whether SSL encryption is enabled for the connection. Set to True to enable SSL and enhance connection security, or set to False to use the default non-encrypted connection. ssl_ca : Specifies the path to the Certificate Authority (CA) file in PEM format. ssl_cert : Specifies the path to the SSL certificate file. This certificate should be signed by a trusted CA specified in the ssl_ca file or be a self-signed certificate trusted by the server. ssl_key : Specifies the path to the private key file (in PEM format). ​ Usage The following usage examples utilize the connection to MariaDB made via the CREATE DATABASE statement and named mariadb_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM mariadb_conn . table_name LIMIT 10 ; ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the MariaDB database. Checklist : Ensure that the MariaDB server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the MariaDB server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue InfluxDB MatrixOne github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "snowflake.html", "content": "Snowflake - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Snowflake Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Snowflake This documentation describes the integration of MindsDB with Snowflake , a cloud data warehouse used to store and analyze data. The integration allows MindsDB to access data stored in the Snowflake database and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Snowflake to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Snowflake database from MindsDB by executing the following SQL command: CREATE DATABASE snowflake_datasource WITH ENGINE = 'snowflake' , PARAMETERS = { \"account\" : \"tvuibdy-vm85921\" , \"user\" : \"user\" , \"password\" : \"password\" , \"database\" : \"test_db\" } ; Required connection parameters include the following: account : The Snowflake account identifier. This guide will help you find your account identifier. user : The username for the Snowflake account. password : The password for the Snowflake account. database : The name of the Snowflake database to connect to. Optional connection parameters include the following: warehouse : The Snowflake warehouse to use for running queries. schema : The database schema to use within the Snowflake database. Default is PUBLIC . role : The Snowflake role to use. This video presents how to connect to Snowflake and query the available tables. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM snowflake_datasource . schema_name . table_name LIMIT 10 ; Run Snowflake SQL queries directly on the connected Snowflake database: SELECT * FROM snowflake_datasource ( --Native Query Goes Here SELECT employee_table . * EXCLUDE department_id , department_table . * RENAME department_name AS department FROM employee_table INNER JOIN department_table ON employee_table . department_id = department_table . department_id ORDER BY department , last_name , first_name ; ) ; The above examples utilize snowflake_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Snowflake account. Checklist : Make sure the Snowflake is active. Confirm that account, user, password and database are correct. Try a direct Snowflake connection using a client like DBeaver. Ensure a stable network between MindsDB and Snowflake. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` This troubleshooting guide provided by Snowflake might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue SingleStore SQLite github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "planetscale.html", "content": "PlanetScale - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases PlanetScale Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases PlanetScale This is the implementation of the PlanetScale data handler for MindsDB. PlanetScale is a MySQL-compatible, serverless database platform. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect PlanetScale to MindsDB, install the required dependencies following this instruction . Install or ensure access to PlanetScale. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the PlanetScale database in MindsDB, the following syntax can be used: CREATE DATABASE planetscale_datasource WITH ENGINE = 'planet_scale' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"user\" : \"planetscale_user\" , \"password\" : \"password\" , \"database\" : \"planetscale_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM planetscale_datasource . my_table ; Was this page helpful? Yes No Suggest edits Raise issue OrioleDB PostgreSQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "all-data-integrations.html", "content": "Supported Integrations - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Supported Integrations Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Supported Integrations The list of databases supported by MindsDB keeps growing. Here are the currently supported integrations: You can find particular databases’ handler files here to see their connection arguments. For example, to see the latest updates to the Oracle handler, check Oracle’s readme.md file here . Let’s look at sample codes showing how to connect to each of the supported integrations. From Our Community Check out the video guides created by our community: Video guide on Connect Postgres Database to MindsDB by @akhilcoder Video guide on Setting up data sources with mindsDB by Syed Zubeen ​ Airtable Template Example CREATE DATABASE airtable_datasource --- display name for the database WITH ENGINE = 'airtable' , --- name of the MindsDB handler PARAMETERS = { \"base_id\" : \" \" , --- the Airtable base ID \"table_name\" : \" \" , --- the Airtable table name \"api_key\" : \" \" --- the API key for the Airtable API } ; Check out the Airtable data handler details here . ​ Amazon DynamoDB Template Example CREATE DATABASE dynamodb_datasource --- display name for the database WITH ENGINE = 'dynamodb' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" --- the AWS region } ; Check out the Amazon DynamoDB data handler details here . ​ Amazon Redshift Template Example CREATE DATABASE amazonredshift_datasource --- display name for the database WITH ENGINE = 'amazonredshift' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Redshift cluster \"port\" : , --- port used when connecting to the Redshift cluster \"database\" : \" \" , --- database name used when connecting to the Redshift cluster \"user\" : \" \" , --- user to authenticate with the Redshift cluster \"password\" : \" \" --- password used to authenticate with the Redshift cluster } ; Check out the Amazon Redshift data handler details here . ​ Amazon S3 Template Example CREATE DATABASE amazons3_datasource --- display name for the database WITH ENGINE = 's3' , --- name of the MindsDB handler PARAMETERS = { \"aws_access_key_id\" : \" \" , --- the AWS access key \"aws_secret_access_key\" : \" \" , --- the AWS secret access key \"region_name\" : \" \" , --- the AWS region \"bucket\" : \" \" , --- name of the S3 bucket \"key\" : \" \" , --- key of the object to be queried \"input_serialization\" : \" \" --- format of the data to be queried } ; Check out the Amazon S3 data handler details here . ​ Apache Cassandra Template Example CREATE DATABASE cassandra_datasource --- display name for the database WITH ENGINE = 'cassandra' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"keyspace\" : \" \" , --- database name \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"secure_connect_bundle\" : { --- optional, secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the Apache Cassandra data handler details here . ​ Apache Druid Template Example CREATE DATABASE druid_datasource --- display name for the database WITH ENGINE = 'druid' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of Apache Druid \"port\" : , --- port where Apache Druid runs \"user\" : \" \" , --- optional, user to authenticate with Apache Druid \"password\" : \" \" , --- optional, password used to authenticate with Apache Druid \"path\" : \" \" , --- query path \"scheme\" : \" \" --- the URI scheme (defaults to `http` if left blank) } ; Check out the Apache Druid data handler details here . ​ Apache Hive Template Example CREATE DATABASE hive_datasource --- display name for the database WITH ENGINE = 'hive' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"auth\" : \" \" --- defaults to CUSTOM if not provided; check for options here: https://pypi.org/project/PyHive/ } ; Check out the Apache Hive data handler details here . ​ Apache Impala Template Example CREATE DATABASE impala_datasource --- display name for the database WITH ENGINE = 'impala' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; Check out the Apache Impala data handler details here . ​ Apache Pinot Template Example CREATE DATABASE pinot_datasource --- display name for the database WITH ENGINE = 'pinot' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Apache Pinot cluster \"broker_port\" : , --- port where the broker of the Apache Pinot cluster runs \"controller_port\" : , --- port where the controller of the Apache Pinot cluster runs \"path\" : \" \" , --- query path \"scheme\" : \" \" , --- scheme (defaults to `http` if left blank) \"username\" : \" \" , --- optional, user \"password\" : \" \" , --- optional, password \"verify_ssl\" : \" \" --- optional, verify SSL } ; Check out the Apache Pinot data handler details here . ​ Apache Solr Template Example CREATE DATABASE solr_datasource --- display name for the database WITH ENGINE = 'solr' , --- name of the MindsDB handler PARAMETERS = { \"username\" : \" \" , --- optional, username used to authenticate with the Solr server \"password\" : \" \" , --- optional, password used to authenticate with the Solr server \"host\" : \" \" , --- host name or IP address of the Solr serve \"port\" : , --- port number of the Solr server \"server_path\" : \" \" , --- defaults to `solr` if left blank \"collection\" : \" \" , --- Solr Collection name \"use_ssl\" : \" \" --- defaults to `false` if left blank; refer to https://pypi.org/project/sqlalchemy-solr/ } ; Check out the Apache Solr data handler details here . ​ Ckan Template Example CREATE DATABASE ckan_datasource --- display name for the database WITH ENGINE = 'ckan' , --- name of the MindsDB handler PARAMETERS = { \"url\" : \" \" , --- host name, IP address, or a URL \"apikey\" : \" \" --- the API key used for authentication } ; Check out the Ckan data handler details here . ​ ClickHouse Template Example CREATE DATABASE clickhouse_datasource --- display name for the database WITH ENGINE = 'clickhouse' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"protocol\" : \" \" --- optional, http or https (defaults to `native`) } ; Check out the ClickHouse data handler details here . ​ Cloud Spanner Template Example CREATE DATABASE cloud_spanner_datasource --- display name for the database WITH ENGINE = 'cloud_spanner' , --- name of the MindsDB handler PARAMETERS = { \"instance_id\" : \" \" , --- the instance identifier \"database_id\" : , --- the database identifier \"project_id\" : \" \" , --- the identifier of the project that owns the instances and data \"credentials\" : \" \" , --- a stringified GCP service account key JSON } ; Check out the Cloud Spanner data handler details here . ​ CockroachDB Template Example CREATE DATABASE cockroach_datasource --- display name for the database WITH ENGINE = 'cockroachdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"publish\" : \" \" --- optional, publish } ; Check out the CockroachDB data handler details here . ​ Couchbase Template Example CREATE DATABASE couchbase_datasource --- display name for the database WITH ENGINE = 'couchbase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Couchbase server \"user\" : \" \" , --- user to authenticate with the Couchbase server \"password\" : \" \" , --- password used to authenticate with the Couchbase server \"bucket\" : \" \" , --- bucket name \"scope\" : \" \" --- scope used to query (defaults to `_default` if left blank) } ; --- a scope in Couchbase is equivalent to a schema in MySQL Check out the Couchbase data handler details here . ​ CrateDB Template Example CREATE DATABASE cratedb_datasource --- display name for the database WITH ENGINE = 'crate' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to `doc` if left blank) } ; Check out the CrateDB data handler details here . ​ D0lt Template Example CREATE DATABASE d0lt_datasource --- display name for the database WITH ENGINE = 'd0lt' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the D0lt data handler details here . ​ Databend Template Example CREATE DATABASE databend_datasource --- display name for the database WITH ENGINE = 'databend' , --- name of the MindsDB handler PARAMETERS = { \"protocol\" : \" \" , --- protocol used to query Databend (defaults to `native` if left blank); supported protocols: native, http, https \"user\" : \" \" , --- username used to authenticate with the Databend warehouse \"port\" : , --- TCP/IP port of the Databend warehouse \"password\" : \" \" , --- password used to authenticate with the Databend warehouse \"host\" : \" \" , --- host name or IP address of the Databend warehouse (use '127.0.0.1' instead of 'localhost' when connecting to a local server) \"database\" : \" \" --- database name used when connecting to the Databend warehouse } ; Check out the Databend data handler details here . ​ Databricks Template Example CREATE DATABASE databricks_datasource --- display name for the database WITH ENGINE = 'databricks' , --- name of the MindsDB handler PARAMETERS = { \"server_hostname\" : \" \" , --- server hostname of the cluster or SQL warehouse \"http_path\" : \" \" , --- http path to the cluster or SQL warehouse \"access_token\" : \" \" , --- personal Databricks access token \"schema\" : \" \" , --- schema name (defaults to `default` if left blank) \"session_configuration\" : \" \" , --- optional, dictionary of Spark session configuration parameters \"http_headers\" : \" \" , --- optional, additional (key, value) pairs to set in HTTP headers on every RPC request the client makes \"catalog\" : \" \" --- catalog (defaults to `hive_metastore` if left blank) } ; Check out the Databricks data handler details here . ​ DataStax Template Example CREATE DATABASE datastax_datasource --- display name for the database WITH ENGINE = 'astra' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- user to be authenticated \"password\" : \" \" , --- password for authentication \"secure_connection_bundle\" : { --- secure connection bundle zip file \"path\" : \" \" --- either \"path\" or \"url\" } , \"host\" : \" \" , --- optional, host name or IP address \"port\" : , --- optional, port used to make TCP/IP connection \"protocol_version\" : , --- optional, protocol version \"keyspace\" : \" \" --- optional, keyspace } ; Check out the DataStax data handler details here . ​ DuckDB Template Example CREATE DATABASE duckdb_datasource --- display name for the database WITH ENGINE = 'duckdb' , --- name of the MindsDB handler PARAMETERS = { \"database\" : \" \" , --- database file name \"read_only\" : --- flag used to set the connection to read-only mode } ; Check out the DuckDB data handler details here . ​ Elasticsearch Template Example CREATE DATABASE elastic_datasource --- display name for the database WITH ENGINE = 'elasticsearch' , --- name of the MindsDB handler PARAMETERS = { \"hosts\" : \" \" , --- one or more host names or IP addresses of the Elasticsearch server \"username\" : \" \" , --- optional, username to authenticate with the Elasticsearch server \"password\" : \" \" , --- optional, password used to authenticate with the Elasticsearch server \"cloud_id\" : \" \" --- optional, unique ID of your hosted Elasticsearch cluster (must be provided when \"hosts\" is left blank) } ; Check out the Elasticsearch data handler details here . ​ Firebird Template Example CREATE DATABASE firebird_datasource --- display name for the database WITH ENGINE = 'firebird' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address of the Firebird server \"database\" : \" \" , --- database name \"user\" : \" \" , --- user to authenticate with the Firebird server \"password\" : \" \" --- password used to authenticate with the Firebird server } ; Check out the Firebird data handler details here . ​ Google BigQuery Template Example for Self-Hosted MindsDB Example for MindsDB Cloud Example without JSON File CREATE DATABASE bigquery_datasource --- display name for the database WITH ENGINE = 'bigquery' , --- name of the MindsDB handler PARAMETERS = { \"project_id\" : \" \" , --- globally unique project identifier \"dataset\" : \" \" , --- default dataset \"service_account_keys\" : \" \" , --- service account keys file \"service_account_json\" : { . . . } --- it is an alternative to using 'service_account_keys' } ; Check out the Google BigQuery data handler details here . ​ Google Sheets Template Example CREATE DATABASE sheets_datasource --- display name for the database WITH ENGINE = 'sheets' , --- name of the MindsDB handler PARAMETERS = { \"spreadsheet_id\" : \" \" , --- unique ID of the Google Sheet \"sheet_name\" : \" \" --- name of the Google Sheet } ; Check out the Google Sheets data handler details here . ​ GreptimeDB Template Example CREATE DATABASE greptimedb_datasource --- display name for the database WITH ENGINE = 'greptimedb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the GreptimeDB data handler details here . ​ IBM Db2 Template Example CREATE DATABASE db2_datasource --- display name for the database WITH ENGINE = 'DB2' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; Check out the IBM Db2 data handler details here . ​ IBM Informix Template Example CREATE DATABASE informix_datasource --- display name for the database WITH ENGINE = 'informix' , --- name of the MindsDB handler PARAMETERS = { \"server\" : \" \" , --- server name \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" , --- database schema name \"logging_enabled\" : --- indicates whether logging is enabled (defaults to `True` if left blank) } ; Check out the IBM Informix data handler details here . ​ MariaDB Template Example CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the MariaDB data handler details here . ​ MariaDB SkySQL Template Example CREATE DATABASE skysql --- display name for the database WITH ENGINE = 'mariadb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl-ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"database\" : \" \" --- database name } ; For more information on how to connect MariaDB SkySQL and MindsDB, visit our doc page here . ​ MatrixOne Template Example CREATE DATABASE matrixone_datasource --- display name for the database WITH ENGINE = 'matrixone' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the MatrixOne data handler details here . ​ Microsoft Access Template Example CREATE DATABASE access_datasource --- display name for the database WITH ENGINE = 'access' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; Check out the Microsoft Access data handler details here . ​ Microsoft SQL Server Template Example CREATE DATABASE mssql_datasource --- display name for the database WITH ENGINE = 'mssql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; Check out the Microsoft SQL Server data handler details here . ​ MonetDB Template Example CREATE DATABASE monetdb_datasource --- display name for the database WITH ENGINE = 'monetdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name (defaults to the current schema if left blank) } ; Check out the MonetDB data handler details here . ​ MongoDB For this connection, we recommend to use the Mongo API instead of the SQL API MindsDB has a dedicated Mongo API that allows you to use the full power of the MindsDB platform. Using the Mongo API feels more natural for MongoDB users and allows you to use all the features of MindsDB. You can find the instructions on how to connect MindsDB to MongoDB Compass or MongoDB Shell and proceed with the Mongo API documentation for further details. Template Example CREATE DATABASE mongo_datasource --- display name for the database WITH ENGINE = 'mongo' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" --- database password \"database\" : \" \" --- database name } ; Check out the MongoDB data handler details here . ​ MySQL Template Example CREATE DATABASE mysql_datasource --- display name for the database WITH ENGINE = 'mysql' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the MySQL data handler details here . ​ OceanBase Template Example CREATE DATABASE oceanbase_datasource --- display name for the database WITH ENGINE = 'oceanbase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; Check out the OceanBase data handler details here . ​ OpenGauss Template Example CREATE DATABASE opengauss_datasource --- display name for the database WITH ENGINE = 'opengauss' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; Check out the OpenGauss data handler details here . ​ Oracle Template Example CREATE DATABASE oracle_datasource --- display name for the database WITH ENGINE = 'oracle' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"sid\" : \" \" , --- unique identifier of the database instance \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; Check out the Oracle data handler details here . ​ OrioleDB Template Example CREATE DATABASE orioledb_datasource --- display name for the database WITH ENGINE = 'orioledb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"server\" : \" \" , --- sets the current server \"database\" : \" \" --- sets the current database } ; Check out the OrioleDB data handler details here . ​ PlanetScale Template Example CREATE DATABASE planetscale_datasource --- display name for the database WITH ENGINE = 'planet_scale' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"database\" : \" \" --- database name } ; Check out the PlanetScale data handler details here . ​ PostgreSQL Template Example CREATE DATABASE psql_datasource --- display name for the database WITH ENGINE = 'postgres' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" --- database password } ; Check out the PostgreSQL data handler details here . ​ QuestDB Template Example CREATE DATABASE questdb_datasource --- display name for the database WITH ENGINE = 'questdb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"public\" : --- value of `True` or `False` (defaults to `True` if left blank) } ; Check out the QuestDB data handler details here . ​ SAP HANA Template Example CREATE DATABASE sap_hana_datasource --- display name for the database WITH ENGINE = 'hana' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"schema\" : \" \" , --- database schema name (defaults to the current schema if left blank) \"encrypt\" : --- indicates whether connection is encrypted (required for cloud usage) } ; Check out the SAP HANA data handler details here . ​ ScyllaDB Template Example CREATE DATABASE scylladb_datasource --- display name for the database WITH ENGINE = 'scylladb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"user\" : \" \" , --- user \"password\" : \" \" , --- password \"protocol_version\" : , --- optional, protocol version (defaults to 4 if left blank) \"keyspace\" : \" \" , --- keyspace name (it is the top level container for tables) \"secure_connect_bundle\" : { --- secure connect bundle file \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the ScyllaDB data handler details here . ​ SingleStore Template Example CREATE DATABASE singlestore_datasource --- display name for the database WITH ENGINE = 'singlestore' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; Check out the SingleStore data handler details here . ​ Snowflake Template Example CREATE DATABASE snowflake_datasource --- display name for the database WITH ENGINE = 'snowflake' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"account\" : \" \" , --- the Snowflake account \"schema\" : \" \" , --- schema name (defaults to `public` if left blank) \"protocol\" : \" \" , --- protocol (defaults to `https` if left blank) \"warehouse\" : \" \" --- the warehouse account } ; Check out the Snowflake data handler details here . ​ SQL Anywhere Template Example CREATE DATABASE sqlany_datasource --- display name for the database WITH ENGINE = 'sqlany' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- username \"password\" : \" \" , --- password \"host\" : \" \" , --- host name or IP address of the SAP SQL Anywhere instance \"port\" : , --- port number of the SAP SQL Anywhere instance \"server\" : \" \" , --- sets the current server \"database\" : \" \" --- sets the current database } ; Check out the SQL Anywhere data handler details here . ​ SQLite Template Example CREATE DATABASE sqlite_datasource --- display name for the database WITH ENGINE = 'sqlite' , --- name of the MindsDB handler PARAMETERS = { \"db_file\" : \" \" --- path to the database file to be used } ; Check out the SQLite data handler details here . ​ StarRocks Template Example CREATE DATABASE starrocks_datasource --- display name for the database WITH ENGINE = 'starrocks' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; Check out the StarRocks data handler details here . ​ Supabase Template Example CREATE DATABASE supabase_datasource --- display name for the database WITH ENGINE = 'supabase' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; Check out the Supabase data handler details here . ​ TDengine Template Example CREATE DATABASE tdengine_datasource --- display name for the database WITH ENGINE = 'tdengine' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- server username \"password\" : \" \" , --- server password \"url\" : \" \" , --- URL to the TDEngine server (for local server, it is localhost:6041 by default) \"token\" : \" \" , --- unique token provided when using TDEngine Cloud \"database\" : \" \" --- database name } ; Check out the TDengine data handler details here . ​ Teradata Template Example CREATE DATABASE teradata_datasource --- display name for the database WITH ENGINE = 'teradata' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"database\" : \" \" , --- database name \"port\" : --- port used to make TCP/IP connection } ; Check out the Teradata data handler details here . ​ TiDB Template Example CREATE DATABASE tidb_datasource --- display name for the database WITH ENGINE = 'tidb' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password } ; Check out the TiDB data handler details here . ​ TimescaleDB Template Example CREATE DATABASE timescaledb_datasource --- display name for the database WITH ENGINE = 'timescaledb' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; Check out the TimescaleDB data handler details here . ​ Trino Template Example 1 Example 2 CREATE DATABASE trino_datasource --- display name for the database WITH ENGINE = 'trino' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"auth\" : \" \" , --- optional, authentication method, currently only `basic` is supported \"http_scheme\" : \" \" , --- optional, `http`(default) or `https` \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"catalog\" : \" \" , --- optional, catalog \"schema\" : \" \" --- optional, schema \"with\" : --- optional, default WITH-clause (properties) for ALL tables --- this parameter is experimental and might be changed or removed in future release } ; Check out the Trino data handler details here . ​ Vertica Template Example CREATE DATABASE vertica_datasource --- display name for the database WITH ENGINE = 'vertica' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"schema_name\" : \" \" --- database schema name } ; Check out the Vertica data handler details here . ​ Vitess Template Example CREATE DATABASE vitess_datasource --- display name for the database WITH ENGINE = 'vitess' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host name or IP address \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name } ; Check out the Vitess data handler details here . ​ YugabyteDB Template Example CREATE DATABASE yugabyte_datasource --- display name for the database WITH ENGINE = 'yugabyte' , --- name of the MindsDB handler PARAMETERS = { \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"host\" : \" \" , --- host name or IP address \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" --- database name \"schema\" : \" \" --- schema name, if multiple schemas then comma separated } ; Check out the YugabyteDB data handler details here . Was this page helpful? Yes No Suggest edits Raise issue YouTube Airtable github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Airtable Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB Elasticsearch Firebird Google BigQuery Google Sheets GreptimeDB IBM Db2 IBM Informix MariaDB MariaDB SkySQL MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA ScyllaDB SingleStore Snowflake SQL Anywhere SQLite StarRocks Supabase TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB"}
{"file_name": "questdb.html", "content": "QuestDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases QuestDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases QuestDB This is the implementation of the QuestDB data handler for MindsDB. QuestDB is a columnar time-series database with high performance ingestion and SQL analytics. It is open-source and available on the cloud. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect QuestDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to QuestDB. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. public stores a value of True or False . Defaults to True if left blank. ​ Usage In order to make use of this handler and connect to the QuestDB server in MindsDB, the following syntax can be used: CREATE DATABASE questdb_datasource WITH ENGINE = 'questdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8812 , \"database\" : \"qdb\" , \"user\" : \"admin\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM questdb_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue PostgreSQL SAP HANA github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "oceanbase.html", "content": "OceanBase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OceanBase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OceanBase This is the implementation of the OceanBase data handler for MindsDB. OceanBase is a distributed relational database. It is the only distributed database in the world that has broken both TPC-C and TPC-H records. OceanBase adopts an independently developed integrated architecture, which encompasses both the scalability of a distributed architecture and the performance advantage of a centralized architecture. It supports hybrid transaction/analytical processing (HTAP) with one engine. Its features include strong data consistency, high availability, high performance, online scalability, high compatibility with SQL and mainstream relational databases, transparency to applications, and a high cost/performance ratio. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OceanBase to MindsDB, install the required dependencies following this instruction . Install or ensure access to OceanBase. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the OceanBase server in MindsDB, the following syntax can be used: CREATE DATABASE oceanbase_datasource WITH ENGINE = 'oceanbase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"oceanbase_user\" , \"password\" : \"password\" , \"port\" : 2881 , \"database\" : \"oceanbase_db\" } ; Now, you can use this established connection to query your database as follows: SELECT * FROM oceanbase_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue MySQL OpenGauss github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "elasticsearch.html", "content": "ElasticSearch - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ElasticSearch Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ElasticSearch This documentation describes the integration of MindsDB with ElasticSearch , a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.. The integration allows MindsDB to access data from ElasticSearch and enhance ElasticSearch with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ElasticSearch to MindsDB, install the required dependencies following this instruction . Install or ensure access to ElasticSearch. ​ Connection Establish a connection to ElasticSearch from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE elasticsearch_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { 'cloud_id' : 'xyz' , -- optional, if hosts are provided 'hosts' : 'https://xyz.xyz.gcp.cloud.es.io:123' , -- optional, if cloud_id is provided 'api_key' : 'xyz' , -- optional, if user and password are provided 'user' : 'elastic' , -- optional, if api_key is provided 'password' : 'xyz' -- optional, if api_key is provided } ; The connection parameters include the following: cloud_id : The Cloud ID provided with the ElasticSearch deployment. Required only when hosts is not provided. hosts : The ElasticSearch endpoint provided with the ElasticSearch deployment. Required only when cloud_id is not provided. api_key : The API key that you generated for the ElasticSearch deployment. Required only when user and password are not provided. user and password : The user and password used to authenticate. Required only when api_key is not provided. If you want to connect to the local instance of ElasticSearch, use the below statement: CREATE DATABASE elasticsearch_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { \"hosts\" : \"127.0.0.1:9200\" , \"user\" : \"user\" , \"password\" : \"password\" } ; Required connection parameters include the following (at least one of these parameters should be provided): hosts : The IP address and port where ElasticSearch is deployed. user : The user used to autheticate access. password : The password used to autheticate access. ​ Usage Retrieve data from a specified index by providing the integration name and index name: SELECT * FROM elasticsearch_datasource . my_index LIMIT 10 ; The above examples utilize elasticsearch_datasource as the datasource name, which is defined in the CREATE DATABASE command. At the moment, the Elasticsearch SQL API has certain limitations that have an impact on the queries that can be issued via MindsDB. The most notable of these limitations are listed below: Only SELECT queries are supported at the moment. Array fields are not supported. Nested fields cannot be queried directly. However, they can be accessed using the . operator. For a detailed guide on the limitations of the Elasticsearch SQL API, refer to the official documentation . ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Elasticsearch server. Checklist : Make sure the Elasticsearch server is active. Confirm that server, cloud ID and credentials are correct. Ensure a stable network between MindsDB and Elasticsearch. Transport Error or Request Error Symptoms : Errors related to the issuing of unsupported queries to Elasticsearch. Checklist : Ensure the query is a SELECT query. Avoid querying array fields. Access nested fields using the . operator. Refer to the official documentation for more information if needed. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing index names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` This troubleshooting guide provided by Elasticsearch might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue EdgelessDB Firebird github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "trino.html", "content": "Trino - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Trino Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Trino This is the implementation of the Trino data handler for MindsDB. Trino is an open-source distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Trino to MindsDB, install the required dependencies following this instruction . Install or ensure access to Trino. ​ Implementation This handler is implemented using pyhive , a collection of Python DB-API and SQLAlchemy interfaces for Presto and Hive. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. There are some optional arguments as follows: auth is the authentication method. Currently, only basic is supported. http_scheme takes the value of http by default. It can be set to https as well. catalog is the catalog. schema is the schema name. with defines default WITH-clause (properties) for ALL tables. This parameter is experimental and might be changed or removed in future release. ​ Usage In order to make use of this handler and connect to the Trino database in MindsDB, the following syntax can be used: CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"auth\" : \"basic\" , \"http_scheme\" : \"https\" , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" , \"with\" : \"with (transactional = true)\" } ; You can use this established connection to query your table as follows: SELECT * FROM trino_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue TimescaleDB Vertica github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "firebird.html", "content": "Firebird - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Firebird Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Firebird This is the implementation of the Firebird data handler for MindsDB. Firebird is a relational database offering many ANSI SQL standard features that runs on Linux, Windows, and a variety of Unix platforms. Firebird offers excellent concurrency, high performance, and powerful language support for stored procedures and triggers. It has been used in production systems, under a variety of names, since 1981. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Firebird to MindsDB, install the required dependencies following this instruction . Install or ensure access to Firebird. ​ Implementation This handler is implemented using the fdb library, the Python driver for Firebird. The required arguments to establish a connection are as follows: host is the host name or IP address of the Firebird server. database is the port to use when connecting with the Firebird server. user is the username to authenticate the user with the Firebird server. password is the password to authenticate the user with the Firebird server. ​ Usage In order to make use of this handler and connect to the Firebird server in MindsDB, the following syntax can be used: CREATE DATABASE firebird_datasource WITH engine = 'firebird' , parameters = { \"host\" : \"localhost\" , \"database\" : \"C:\\Users\\minura\\Documents\\mindsdb\\example.fdb\" , \"user\" : \"sysdba\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM firebird_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue ElasticSearch Google BigQuery github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "environment-vars.html", "content": "Environment Variables - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Configuration Environment Variables Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Docker Docker Desktop AWS Marketplace Configuration Extend the Default MindsDB Configuration Environment Variables Demo Features Model Management AI Integrations Data Integrations Automation Learn more Configuration Environment Variables Most of the MindsDB functionality can be modified by extending the default configuration, but some of the configuration options can be added as environment variables on the server where MindsDB is deployed. ​ MindsDB Authentication MindsDB does not require authentication by default. If you want to enable authentication, you can set the MINDSDB_USERNAME and MINDSDB_PASSWORD environment variables. ​ Example Docker Shell docker run --name mindsdb_container -e MINDSDB_USERNAME = 'mindsdb_user' -e MINDSDB_PASSWORD = 'mindsdb_password' -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb ​ MindsDB Storage By default, MindsDB stores the configuration files by determining appropriate platform-specific directories, e.g. a “user data dir”: On Linux ~/.local/share/mindsdb/var On MacOS ~/Library/Application Support/mindsdb/var On Windows C:\\Documents and Settings\\<User>\\Application Data\\Local Settings\\<AppAuthor>\\mindsdb\\var In the MINDSDB_STORAGE_DIR location, MindsDB stores users’ data, models and uploaded data files, the static assets for the frontend application and the sqlite.db file. You can change the default storage location using MINDSDB_STORAGE_DIR variable. ​ Example Docker Shell docker run --name mindsdb_container -e MINDSDB_STORAGE_DIR = '~/home/mindsdb/var' -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb ​ MindsDB Configuration Storage MindsDB uses sqlite database by default to store the required configuration as models, projects, files metadata etc. The full list of the above schemas can be found here . You can change the default storage option and use different database by adding the new connection string using MINDSDB_DB_CON variable. ​ Example Docker Shell docker run --name mindsdb_container -e MINDSDB_DB_CON = 'postgresql://user:secret@localhost' -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb ​ MindsDB APIs By default, MindsDB starts the http and mysql APIs. To define which APIs you want to start, you can use the MINDSDB_APIS environment variable. To expose the ports for the APIs, you need to add the respective ports to the command with the -p flag. ​ Example Docker Shell docker run --name mindsdb_container -e MINDSDB_APIS = 'http,mysql,mongodb,postgres' -p 47334 :47334 -p 47335 :47335 -p 47336 :47336 -p 55432 :55432 mindsdb/mindsdb ​ MindsDB Server By default for the HTTP API, MindsDB uses Waitress which is a pure-Python WSGI server. There is an option to change that and use Flask or Gunicorn ​ Example Docker Shell # To use Gunicorn as a default server, it should be installed by logging into the container docker exec -it mindsdb_container sh # assuming the container name is mindsdb_container # assuming the container name is mindsdb_container pip install gunicorn docker run --name mindsdb_container -e MINDSDB_DEFAULT_SERVER = 'gunicorn' -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb You can also use waitress , which is the default server or flask . ​ MindsDB Logs This environment variable defines the level of logging generated by MindsDB. You can choose one of the values defined here . The INFO level is used by default. ​ Example Docker Shell docker run --name mindsdb_container -e MINDSDB_LOG_LEVEL = 'DEBUG' -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Was this page helpful? Yes No Suggest edits Raise issue Extend the Default MindsDB Configuration Demo github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page MindsDB Authentication Example MindsDB Storage Example MindsDB Configuration Storage Example MindsDB APIs Example MindsDB Server Example MindsDB Logs Example"}
{"file_name": "custom-config.html", "content": "Extend the Default MindsDB Configuration - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Configuration Extend the Default MindsDB Configuration Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Docker Docker Desktop AWS Marketplace Configuration Extend the Default MindsDB Configuration Environment Variables Demo Features Model Management AI Integrations Data Integrations Automation Learn more Configuration Extend the Default MindsDB Configuration To follow this guide, install MindsDB locally via Docker or PyPI . ​ Starting MindsDB with Default Configuration Start MindsDB locally with the default configuration. Activate the virtual environment: source mindsdb/bin/activate Start MindsDB: python -m mindsdb Access MindsDB locally at 127.0.0.1:47334 . By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Starting MindsDB with Extended Configuration Start MindsDB locally with your custom configuration by providing a path to the config.json file that stores custom config parameters listed in this section. python -m mindsdb --config = /path-to-the-extended-config-file/config.json Overview of Config Parameters Below are all of the custom configuration parameters that should be set according to your requirements and saved into the config.json file. { \"permanent_storage\" : { \"location\" : \"absent\" , \"bucket\" : \"s3_bucket_name\" # optional, used only if \"location\": \"s3\" } , The permanent_storage parameter defines where MindsDB stores copies of user files, such as uploaded files, models, and tab content. MindsDB checks the permanent_storage location to access the latest version of a file and updates it as needed. The location specifies the storage type. absent (default): Disables permanent storage and is recommended to use when MindsDB is running locally. local : Stores files in a local directory defined with config['paths']['storage'] . s3 : Stores files in an Amazon S3 bucket. This option requires the bucket parameter that specifies the name of the S3 bucket where files will be stored. If this parameter is not set, the path is determined by the MINDSDB_STORAGE_DIR environment variable. MindsDB defaults to creating a mindsdb folder in the operating system user’s home directory. \"paths\" : { \"root\" : \"/home/mindsdb/var\" , # optional (alternatively, it can be defined in the MINDSDB_STORAGE_DIR environment variable) \"content\" : \"/home/mindsdb/var/content\" , # optional \"storage\" : \"/home/mindsdb/var/storage\" , # optional \"static\" : \"/home/mindsdb/var/static\" , # optional \"tmp\" : \"/home/mindsdb/var/tmp\" , # optional \"cache\" : \"/home/mindsdb/var/cache\" , # optional \"locks\" : \"/home/mindsdb/var/locks\" , # optional } , The paths parameter allows users to redefine the file paths for various groups of MindsDB files. If only the root path is defined, all other folders will be created within that directory. If this parameter is absent, the value is determined by the MINDSDB_STORAGE_DIR environment variable. The root parameter defines the base directory for storing all MindsDB files, including models, uploaded files, tab content, and the internal SQLite database (if running locally). The content parameter specifies the directory where user-related files are stored, such as uploaded files, created models, and tab content. The internal SQLite database (if running locally) is stored in the root directory instead. If the ['permanent_storage']['location'] is set to 'local' , then the storage parameter is used to store copies of user files. The static parameter is used to store files for the graphical user interface (GUI) when MindsDB is run locally. The tmp parameter designates a directory for temporary files. Note that the operating system’s default temporary directory may also be used for some temporary files. If the ['cache']['type'] is set to 'local' , then the cache parameter defines the location for storing cached files for the most recent predictions. For example, if a model is queried with identical input, the result will be stored in the cache and returned directly on subsequent queries, instead of recalculating the prediction. The locks parameter is used to store lock files to prevent race conditions when the content folder is shared among multiple applications. This directory helps ensure that file access is managed properly using fcntl locks. Note that this is not applicable for Windows OS. \"auth\" : { \"http_auth_enabled\" : true, \"http_permanent_session_lifetime\" : 86400 , \"username\" : \"username\" , \"password\" : \"password\" } , The auth parameter controls the authentication settings for APIs in MindsDB. If the http_auth_enabled parameter is set to true , then the username and password parameters are required. Otherwise these are optional. In local instances of MindsDB, users can enable simple HTTP authentication based on Flask sessions, as follows: Enable the authentication for the HTTP API by setting the http_auth_enabled parameter to true and providing values for the username and password parameters. Alternatively, users can set the environment variables - MINDSDB_USERNAME and MINDSDB_PASSWORD - to store these values.. The default lifetime of a session is set to 31 days. It can be modified by providing a value in seconds to the http_permanent_session_lifetime parameter. Alternatively, users can set one of the environment variables - MINDSDB_HTTP_PERMANENT_SESSION_LIFETIME or FLASK_PERMANENT_SESSION_LIFETIME - to store this value. \"gui\" : { \"autoupdate\" : true } , The gui parameter controls the behavior of the MindsDB graphical user interface (GUI) updates. The autoupdate parameter defines whether MindsDB automatically checks for and updates the GUI to the latest version when the application starts. If set to true , MindsDB will attempt to fetch the latest available version of the GUI. If set to False , MindsDB will not try to update the GUI on startup. \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" , \"restart_on_failure\" : true, \"max_restart_count\" : 1 , \"max_restart_interval_seconds\" : 60 , \"server\" : { \"type\" : \"server_type\" , \"config\" : { // waitres { \"threads\" : 16 , \"max_request_body_size\" : ( 1 << 30 ) * 10 , # 10GB \"inbuf_overflow\" : ( 1 << 30 ) * 10 } // flask { } // gunicorn { 'workers' : min ( multiprocessing.cpu_count ( ) , 4 ) , 'timeout' : 600 , 'reuse_port' : True, 'preload_app' : True, 'threads' : 4 } } } } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47335\" , \"database\" : \"mindsdb\" , \"ssl\" : true, \"restart_on_failure\" : true, \"max_restart_count\" : 1 , \"max_restart_interval_seconds\" : 60 } , \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } , The api parameter contains the configuration settings for running MindsDB APIs. Currently, the supported APIs are: http : Configures the HTTP API. It requires the host and port parameters. Alternatively, configure HTTP authentication for your MindsDB instance by setting the environment variables MINDSDB_USERNAME and MINDSDB_PASSWORD before starting MindsDB, which is a recommended way for the production systems. mysql : Configures the MySQL API. It requires the host and port parameters and additionally the database and ssl parameters. mongodb : Configures the MongoDB API. It requires the host and port parameters and additionally the database parameter. Connection parameters within each block include: host : Specifies the IP address or hostname where the API should run. For example, \"127.0.0.1\" indicates the API will run locally. port : Defines the port number on which the API will listen for incoming requests. The default ports are 47334 for HTTP, 47335 for MySQL, and 47336 for MongoDB. database (for MySQL and MongoDB): Specifies the name of the database that MindsDB uses. Users must connect to this database to interact with MindsDB through the respective API. ssl (for MySQL API): Indicates whether SSL support is enabled for the MySQL API. Additional setup for the HTTP API and the MySQL API: restart_on_failure : If it is set to true (and max_restart_count is not reached), the restart of MindsDB will be attempted after the MindsDB process was killed - with code 9 on Linux and MacOS, or for any reason on Windows. max_restart_count : This defines how many times the restart attempts can be made. Note that 0 stands for no limit. max_restart_interval_seconds : This defines the time limit during which there can be no more than max_restart_count restart attempts. Note that 0 stands for no time limit, which means there would be a maximum of max_restart_count restart attempts allowed. type within server : This specifies the server type used by MindsDB. It is waitress by default. Other values include flask and gunicorn . Alternatively, you can set the MINDSDB_DEFAULT_SERVER environment variable to define the server. config within server : This stores the selected server configuration. Any options that are supported by the selected server (taht is, waitress, flask, or gunicorn) can be set there, except for host and port that are defined under config['api']['http'] . Here is a usage example of the restart features: Assume the following values: max_restart_count = 2 max_restart_interval_seconds = 30 seconds Assume the following scenario: MindsDB fails at 1000s of its work - the restart attempt succeeds as there were no restarts in the past 30 seconds. MindsDB fails at 1010s of its work - the restart attempt succeeds as there was only 1 restart (at 1000s) in the past 30 seconds. MindsDB fails at 1020s of its work - the restart attempt fails as there were already max_restart_count=2 restarts (at 1000s and 1010s) in the past 30 seconds. MindsDB fails at 1031s of its work - the restart attempt succeeds as there was only 1 restart (at 1010s) in the past 30 seconds. \"cache\" : { \"type\" : \"local\" , \"connection\" : \"redis://localhost:6379\" # optional, used only if \"type\": \"redis\" } , The cache parameter controls how MindsDB stores the results of recent predictions to avoid recalculating them if the same query is run again. Note that recent predictions are cached for ML models, like Lightwood, but not in the case of large language models (LLMs), like OpenAI. The type parameter specifies the type of caching mechanism to use for storing prediction results. none : Disables caching. No prediction results are stored. local (default): Stores prediction results in the cache folder (as defined in the paths configuration). This is useful for repeated queries where the result doesn’t change. redis : Stores prediction results in a Redis instance. This option requires the connection parameter, which specifies the Redis connection string. The connection parameter is required only if the type parameter is set to redis . It stores the Redis connection string. \"logging\" : { \"handlers\" : { \"console\" : { \"enabled\" : true, \"level\" : \"INFO\" # optional (alternatively, it can be defined in the MINDSDB_CONSOLE_LOG_LEVEL environment variable) } , \"file\" : { \"enabled\" : False, \"level\" : \"INFO\" , # optional (alternatively, it can be defined in the MINDSDB_FILE_LOG_LEVEL environment variable) \"filename\" : \"app.log\" , \"maxBytes\" : 524288 , # 0.5 Mb \"backupCount\" : 3 } } } , The above parameters are implemented based on Python’s Logging Dictionary Schema . The logging parameter defines the details of output logging, including the logging levels. The handler parameter provides handlers used for logging into streams and files. console : If the enabled parameter is set to true , then the logging output is saved into a stream. Users can define the logging level in the level parameter or in the MINDSDB_CONSOLE_LOG_LEVEL environment variable - one of DEBUG , INFO , WARNING , ERROR , CRITICAL . file : If the enabled parameter is set to true , then the logging output is saved into a file. Users can define the logging level in the level parameter or in the MINDSDB_FILE_LOG_LEVEL environment variable - one of DEBUG , INFO , WARNING , ERROR , CRITICAL . Additionally, the filename parameter stores the name of the file that contains logs, and the maxBytes and backupCount parameters determine the rollover process of the file - that is, if the file reached the size of maxBytes , then the file is closed and a new file is opened, where the number of files is defined by the backupCount parameter. \"ml_task_queue\" : { \"type\" : \"local\" , \"host\" : \"localhost\" , # optional, used only if \"type\": \"redis\" \"port\" : 6379 , # optional, used only if \"type\": \"redis\" \"db\" : 0 , # optional, used only if \"type\": \"redis\" \"username\" : \"username\" , # optional, used only if \"type\": \"redis\" \"password\" : \"password\" # optional, used only if \"type\": \"redis\" } , The ml_task_queue parameter manages the queueing system for machine learning tasks in MindsDB. ML tasks include operations such as creating, training, predicting, fine-tuning, and retraining models. These tasks can be resource-intensive, and running multiple ML tasks simultaneously may lead to Out of Memory (OOM) errors or performance degradation. To address this, MindsDB uses a task queue to control task execution and optimize resource utilization. The type parameter defines the type of task queue to use. local : Tasks are processed immediately as they appear, without a queue. This is suitable for environments where resource constraints are not a concern. redis : Tasks are added to a Redis-based queue, and consumer process (which is run with --ml_task_consumer ) ensures that tasks are executed only when sufficient resources are available. Using a Redis queue requires additional configuration such as the host , port , db , username , and password parameters. To use the Redis queue, start MindsDB with the following command to initiate a queue consumer process: python3 -m mindsdb --ml_task_queue_consumer . This process will monitor the queue and fetch tasks for execution only when sufficient resources are available. \"file_upload_domains\" : [ ] , The file_upload_domains parameter restricts file uploads to trusted sources by specifying a list of allowed domains. This ensures that users can only upload files from the defined sources, such as S3 or Google Drive ( \"file_upload_domains\": [\"https://s3.amazonaws.com\", \"https://drive.google.com\"] ). If this parameter is left empty ( [] ), users can upload files from any URL without restriction. \"web_crawling_allowed_sites\" : [ ] , } The web_crawling_allowed_sites parameter restricts web crawling operations to a specified list of allowed IPs or web addresses. This ensures that the application only accesses pre-approved and safe URLs ( \"web_crawling_allowed_sites\": [\"https://example.com\", \"https://api.mysite.com\"] ). If left empty ( [] ), the application allows access to all URLs by default (marked with a wildcard in the open-source version). Example of Extended Config File First, create a config.json file. { \"permanent_storage\" : { \"location\" : \"absent\" } , \"paths\" : { \"root\" : \"/path/to/root/location\" } , \"auth\" : { \"http_auth_enabled\" : true, \"http_permanent_session_lifetime\" : 86400 , \"username\" : \"username\" , \"password\" : \"password\" } , \"gui\" : { \"autoupdate\" : true } , \"api\" : { \"http\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47334\" , \"restart_on_failure\" : true, \"max_restart_count\" : 1 , \"max_restart_interval_seconds\" : 60 } , \"mysql\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47335\" , \"database\" : \"mindsdb\" , \"ssl\" : true, \"restart_on_failure\" : true, \"max_restart_count\" : 1 , \"max_restart_interval_seconds\" : 60 } , \"mongodb\" : { \"host\" : \"127.0.0.1\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } , \"cache\" : { \"type\" : \"local\" } , \"logging\" : { \"handlers\" : { \"console\" : { \"enabled\" : true, \"level\" : \"INFO\" } , \"file\" : { \"enabled\" : false, \"level\" : \"INFO\" , \"filename\" : \"app.log\" , \"maxBytes\" : 524288 , \"backupCount\" : 3 } } } , \"ml_task_queue\" : { \"type\" : \"local\" } , \"file_upload_domains\" : [ ] , \"web_crawling_allowed_sites\" : [ ] } Next, start MindsDB providing this config.json file. python -m mindsdb --config = /path-to-the-extended-config-file/config.json ​ Modifying Config Values Users can modify config values by directly editing the config.json file they created. Was this page helpful? Yes No Suggest edits Raise issue AWS Marketplace Environment Variables github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Starting MindsDB with Default Configuration Starting MindsDB with Extended Configuration Modifying Config Values"}
{"file_name": "aws-marketplace.html", "content": "MindsDB at AWS Marketplace - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Installation MindsDB at AWS Marketplace Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Docker Docker Desktop AWS Marketplace Configuration Demo Features Model Management AI Integrations Data Integrations Automation Learn more Installation MindsDB at AWS Marketplace MindsDB offers a streamlined setup process in cloud environments using its AWS Marketplace image. Explore the MindsDB AWS Marketplace image here . Was this page helpful? Yes No Suggest edits Raise issue Docker Desktop Extend the Default MindsDB Configuration github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "docker.html", "content": "Docker for MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Installation Docker for MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Docker Docker Desktop AWS Marketplace Configuration Demo Features Model Management AI Integrations Data Integrations Automation Learn more Installation Docker for MindsDB MindsDB provides Docker images that facilitate running MindsDB in Docker containers. As MindsDB integrates with numerous data sources and AI frameworks , each integration requires a set of dependencies. Hence, MindsDB provides multiple Docker images for different tasks, as outlined below. mindsdb/mindsdb:latest (or mindsdb/mindsdb ) It is the lightweight Docker image of MindsDB that comes with these integrations preloaded . mindsdb/mindsdb:lightwood It is the Docker image of MindsDB that comes with these integrations and the Lightwood integration preloaded. mindsdb/mindsdb:huggingface It is the Docker image of MindsDB that comes with these integrations and the Hugging Face integration preloaded. ​ Prerequisites Before proceeding, ensure you have installed Docker, following the official Docker documentation . ​ Setup This setup of MindsDB uses one of the available Docker images, as listed above. Follow the steps to set up MindsDB in a Docker container. ​ Install MindsDB Run this command to create a Docker container with MindsDB: docker run --name mindsdb_container \\ -p 47334 :47334 -p 47335 :47335 mindsdb/mindsdb Where: docker run is a native Docker command used to spin up a container. --name mindsdb_container defines a name for the container. -p 47334:47334 publishes port 47334 to access MindsDB GUI and HTTP API. -p 47335:47335 publishes port 47335 to access MindsDB MySQL API. This is optional and can be omitted if you don’t need to use the MySQL API. mindsdb/mindsdb is a Docker image provided by MindsDB. You can choose a different one from the list above. By default, MindsDB starts only the HTTP API. You can define which APIs to start by passing the MINDSDB_APIS environment variable with a comma-separated list when running the container as shown below. To expose the ports for the APIs, you need to add the respective ports to the command with the -p flag. docker run -e MINDSDB_APIS = \"http,mysql,mongodb,postgres\" \\ -p 47334 :47334 -p 47335 :47335 -p 47336 :47336 -p 55432 :55432 mindsdb/mindsdb You can find more information on the environment variables supported by MindsDB here Once the container is created, you can use the following commands: docker stop mindsdb_container to stop the container. Note that this may not always be necessary because when turning off the host machine, the container will also be shut down. docker start mindsdb_container to restart a stopped container with all its previous changes (such as any dependencies that were installed) intact. Note that docker start restarts a stopped container, while docker run creates a new container. If you don’t want to follow the logs and get the prompt back, add the -d flag that stands for detach . docker run --name mindsdb_container \\ -d -p 47334 :47334 mindsdb/mindsdb If you want to persist your models and configurations in the host machine, run these commands: mkdir mdb_data docker run --name mindsdb_container \\ -p 47334 :47334 -v $( pwd ) /mdb_data:/root/mdb_storage mindsdb/mindsdb Where -v $(pwd)/mdb_data:/root/mdb_storage maps the newly created folder mdb_data on the host machine to the /root/mdb_storage inside the container. Now you can access the MindsDB editor by going to 127.0.0.1:47334 in your browser. If you experience any issues related to MKL or your training process does not complete, please add the MKL_SERVICE_FORCE_INTEL environment variable, as below. docker run --name mindsdb_container -e MKL_SERVICE_FORCE_INTEL = 1 \\ -p 47334 :47334 mindsdb/mindsdb If you want to enable authentication for MindsDB, you do so by passing the MINDSDB_USERNAME and MINDSDB_PASSWORD environment variables when running the container. docker run --name mindsdb_container \\ -e MINDSDB_USERNAME = 'admin' -e MINDSDB_PASSWORD = 'password' \\ -p 47334 :47334 mindsdb/mindsdb ​ Install dependencies MindsDB integrates with numerous data sources and AI frameworks. To use any of the integrations, you should enure that the required dependencies are installed in the Docker container. Method 1 Install dependencies directly from MindsDB editor. Go to Settings and Manage Integrations , select integrations you want to use and click on Install . Method 2 Start the MindsDB Docker container: docker start mindsdb_container If you haven’t specified a container name when spinning up a container with docker run , you can find it by running docker ps . If you haven’t yet created a container, use this command: docker run --name mindsdb_container \\ -d -p 47334 :47334 mindsdb/mindsdb Start an interactive shell in the container: docker exec -it mindsdb_container sh Install the dependencies: pip install . [ handler_name ] For example, run this command to install dependencies for the OpenAI handler : pip install . [ openai ] Exit the interactive shell: exit Restart the container: docker restart mindsdb_container ​ Configuration This is a configuration for MindsDB’s Docker image that includes storage location, log level, debugging information, installed integrations, and API endpoints. These parameters can be customized by modifying a JSON file that stores default configuration. ​ Default configuration The default configuration for MindsDB’s Docker image is stored as a JSON code, as below. { \"config_version\" : \"1.4\" , \"paths\" : { \"root\" : \"/root/mdb_storage\" } , \"debug\" : false , \"integrations\" : { } , \"api\" : { \"http\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47334\" } , \"mysql\" : { \"host\" : \"0.0.0.0\" , \"password\" : \"\" , \"port\" : \"47335\" , \"user\" : \"mindsdb\" , \"database\" : \"mindsdb\" , \"ssl\" : true } , \"mongodb\" : { \"host\" : \"0.0.0.0\" , \"port\" : \"47336\" , \"database\" : \"mindsdb\" } } } ​ Custom configuration To override the default configuration, you can mount a config file over /root/mindsdb_config.json , as below. docker run --name mindsdb_container -d -p 47334 :47334 \\ -v mdb_config.json:/root/mindsdb_config.json mindsdb/mindsdb What’s next? Now that you installed and started MindsDB locally in your Docker container, go ahead and find out how to create and train a model using the CREATE MODEL statement. Check out the Use Cases section to follow tutorials that cover Large Language Models, Chatbots, Time Series, Classification, and Regression models, Semantic Search, and more. Was this page helpful? Yes No Suggest edits Raise issue Quickstart Docker Desktop github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Install MindsDB Install dependencies Configuration Default configuration Custom configuration"}
{"file_name": "docker-desktop.html", "content": "Docker Desktop Extension for MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Installation Docker Desktop Extension for MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Docker Docker Desktop AWS Marketplace Configuration Demo Features Model Management AI Integrations Data Integrations Automation Learn more Installation Docker Desktop Extension for MindsDB MindsDB provides an extension for Docker Desktop that facilitates running MindsDB on Docker Desktop. Visit the GitHub repository for MindsDB Docker Desktop Extension to learn more. ​ Prerequisites Before proceeding, ensure you have installed Docker Desktop, following the official Docker Desktop documentation . ​ Setup This setup of MindsDB uses the mindsdb/mindsdb:latest Docker image, which is a lightweight Docker image of MindsDB that comes with these integrations preloaded . Follow the steps to set up MindsDB in Docker Desktop. ​ Install the MindsDB Docker Desktop Extension If you are a Windows user, ensure that you have enabled Developer Mode under settings before installing the extension. It is not necessary to keep Developer Mode enabled to use the extension. Once the extension is installed, you can disable Developer Mode if you wish. Go to the Extensions page in Docker Desktop and search for MindsDB. Install the MindsDB extension. The first time the extension is installed, it will run the latest version of MindsDB. Moving forward, it’s advisable to regularly update the MindsDB image used by the extension to ensure access to the latest features and improvements. As mentioned previously, the extension uses the mindsdb/mindsdb:latest Docker image. To update the image, follow these steps: Navigate to the ‘Images’ tab in Docker Desktop. Search or locate the mindsdb/mindsdb:latest image. Click on the three dots on the right side of the image and click ‘Pull’. If the image is already up to date, you will see a message stating so and you can skip the next step. Wait for the image to be pulled and restart Docker Desktop. Access MindsDB inside Docker Desktop. ​ Install dependencies In the MindsDB editor, go to Settings and Manage Integrations . Select integrations you want to use and click on Install . ​ View logs In order to view the logs generated by MindsDB when running the extension, follow these steps: Navigate to the ‘Containers’ tab in Docker Desktop. Search or locate the multi-container application running the MindsDB extension. This can be done by searching for ‘mindsdb’. If you do not see the application listed here, navigate to the ‘Extensions’ tag in Settings and ensure that the ‘Show Docker Extensions system containers’ option is enabled. Click on the container named ‘mindsdb_service’. This will direct you to the container running MindsDB. View the logs in the ‘Logs’ tab. Was this page helpful? Yes No Suggest edits Raise issue Docker AWS Marketplace github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Install the MindsDB Docker Desktop Extension Install dependencies View logs"}
{"file_name": "linux.html", "content": "Setup for Linux via pip - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Setup for Linux via pip Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Setup for Linux via pip To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . ​ Installation using the Python venv Module Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Installation using Anaconda Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Dependencies The dependencies for many of the data or ML integrations are not installed by default. If you want to use a data or ML integration whose dependencies are not available by default, install it by running this command: pip install mindsdb[handler_name] You can find all available handlers here . ​ Troubleshooting ​ Pip and Python Versions Currently, MindsDB supports Python versions 3.9.x, 3.10.x, and 3.11.x. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . ​ How to Avoid Dependency Issues Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . ​ How to Avoid Common Errors MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. ​ Further Issues? You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. ​ What’s Next Now that you installed and started MindsDB locally in your Docker container, go ahead and find out how to create and train a model using the CREATE MODEL statement. In the MindsDB SQL section, you’ll find a comprehensive overview of the SQL syntax offered by MindsDB. We also provide Mongo-QL syntax documented in the MindsDB Mongo-QL section. You can connect MindsDB to different clients, including PostgreSQL CLI and MySQL CLI . Check out the Use Cases section to follow tutorials that cover Large Language Models, Natural Language Processing, Time Series, Classification, and Regression models. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Installation using the Python venv Module Installation using Anaconda Dependencies Troubleshooting Pip and Python Versions How to Avoid Dependency Issues How to Avoid Common Errors Further Issues? What’s Next"}
{"file_name": "windows.html", "content": "Setup for Windows via pip - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Setup for Windows via pip Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Setup for Windows via pip To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . ​ Installation using the Python venv Module Create a new virtual environment called mindsdb : py -m venv mindsdb Now, activate it: . \\ mindsdb \\ Scripts \\ activate.bat Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Installation using Anaconda Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Dependencies The dependencies for many of the data or ML integrations are not installed by default. If you want to use a data or ML integration whose dependencies are not available by default, install it by running this command: pip install mindsdb[handler_name] You can find all available handlers here . ​ Troubleshooting ​ Pip and Python Versions Currently, MindsDB supports Python versions 3.9.x, 3.10.x, and 3.11.x. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . ​ How to Avoid Dependency Issues Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . In addition, for Windows systems with default languages other than English, your system might not have UTF-8 as the default encoding standard, which will cause encoding errors when installing dependencies. To solve this issue, go to Control Panel > Clock and Region > Region > Administrative tab > Change system locale button and enable Beta: Use Unicode UTF-8 for worldwide language support . ​ Installing torch or torchvision If the installation fails when installing torch or torchvision , try to install them manually by following the instructions on their official website . ​ Further Issues? You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. ​ What’s Next Now that you installed and started MindsDB locally in your Docker container, go ahead and find out how to create and train a model using the CREATE MODEL statement. In the MindsDB SQL section, you’ll find a comprehensive overview of the SQL syntax offered by MindsDB. We also provide Mongo-QL syntax documented in the MindsDB Mongo-QL section. You can connect MindsDB to different clients, including PostgreSQL CLI and MySQL CLI . Check out the Use Cases section to follow tutorials that cover Large Language Models, Natural Language Processing, Time Series, Classification, and Regression models. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Installation using the Python venv Module Installation using Anaconda Dependencies Troubleshooting Pip and Python Versions How to Avoid Dependency Issues Installing torch or torchvision Further Issues? What’s Next"}
{"file_name": "macos.html", "content": "Setup for MacOS via pip - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Setup for MacOS via pip Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Setup for MacOS via pip To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . ​ Installation using the Python venv Module Create a new virtual environment called mindsdb : python -m venv mindsdb Now, activate it: source mindsdb/bin/activate Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Installation using Anaconda Here, you need either Anaconda or Conda installed on your machine. Open Anaconda prompt and create a new virtual environment: conda create -n mindsdb Now, activate it: conda activate mindsdb Once inside the virtual environment, run the command below to mitigate the dependency issues: pip install --upgrade pip setuptools wheel Install MindsDB: pip install mindsdb Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ Dependencies The dependencies for many of the data or ML integrations are not installed by default. If you want to use a data or ML integration whose dependencies are not available by default, install it by running this command: pip install mindsdb[handler_name] You can find all available handlers here . ​ Troubleshooting ​ Pip and Python Versions Currently, MindsDB supports Python versions 3.9.x, 3.10.x, and 3.11.x. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . ​ How to Avoid Dependency Issues Install MindsDB in a virtual environment using pip to avoid dependency issues. Or you could try to install MindsDB with Anaconda and run the installation from the Anaconda prompt . ​ How to Avoid Common Errors MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. Some users can get OSError: dlopen Library not loaded 'libomp.dylib' . Please make sure you have installed libomp and run the export commands. brew install libomp export LDFLAGS=\"-L/usr/local/opt/libomp/lib\" export CPPFLAGS=\"-I/usr/local/opt/libomp/include\" ​ Further Issues? You can try to replicate your issue using the Docker setup . Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. ​ What’s Next Now that you installed and started MindsDB locally in your Docker container, go ahead and find out how to create and train a model using the CREATE MODEL statement. In the MindsDB SQL section, you’ll find a comprehensive overview of the SQL syntax offered by MindsDB. We also provide Mongo-QL syntax documented in the MindsDB Mongo-QL section. You can connect MindsDB to different clients, including PostgreSQL CLI and MySQL CLI . Check out the Use Cases section to follow tutorials that cover Large Language Models, Natural Language Processing, Time Series, Classification, and Regression models. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Installation using the Python venv Module Installation using Anaconda Dependencies Troubleshooting Pip and Python Versions How to Avoid Dependency Issues How to Avoid Common Errors Further Issues? What’s Next"}
{"file_name": "source.html", "content": "Setup for Source Code via pip - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Setup for Source Code via pip Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Setup for Source Code via pip This section describes how to deploy MindsDB from the source code. It is the preferred way to use MindsDB if you want to contribute to our code or debug MindsDB. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . ​ Installation Please note that this method of MindsDB installation requires a minimum of 6 GB free storage. Clone the MindsDB repository: git clone https://github.com/mindsdb/mindsdb.git Create a new virtual environment: python -m venv mindsdb-venv Activate the virtual environment: source mindsdb-venv/bin/activate Install dependencies: cd mindsdb pip install -e . pip install -r requirements/requirements-dev.txt Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio Now, you can access the following: MindsDB Studio MindsDB using MySQL http://127.0.0.1:47334/ ​ Dependencies The dependencies for many of the data or ML integrations are not installed by default. If you want to use a data or ML integration whose dependencies are not available by default, install it by running this command: pip install mindsdb[handler_name] You can find all available handlers here . ​ Troubleshooting ​ Pip and Python Versions Currently, MindsDB supports Python versions 3.9.x, 3.10.x, and 3.11.x. To successfully install MindsDB, use Python 64-bit version . Also, make sure that Python >= 3.9 and pip >= 20.3 . You can check the pip and python versions by running the pip --version and python --version commands. Please note that depending on your environment and installed pip and python packages, you might have to use pip3 instead of pip or python3.x instead of py . For example, pip3 install mindsdb instead of pip install mindsdb . ​ How to Avoid Dependency Issues Install MindsDB in a virtual environment using pip to avoid dependency issues. ​ How to Avoid Common Errors MindsDB requires around 3 GB of free disk space to install all of its dependencies. Make sure to allocate min. 3 GB of disk space to avoid the IOError: [Errno 28] No space left on device while installing MindsDB error. Before anything, activate your virtual environment where your MindsDB is installed. It is to avoid the No module named mindsdb error. If you encounter the This site can’t be reached. 127.0.0.1 refused to connect. error, please check the MindsDB server console to see if the server is still in the starting phase. But if the server has started and you still get this error, please report it on our GitHub repository . ​ Further Issues? You can try to use Docker setup in case you are experiencing issues using pip. Also, please create an issue with detailed description in the MindsDB GitHub repository so we can help you. Usually, we review issues and respond within a few hours. ​ What’s Next Now that you installed and started MindsDB locally in your Docker container, go ahead and find out how to create and train a model using the CREATE MODEL statement. In the MindsDB SQL section, you’ll find a comprehensive overview of the SQL syntax offered by MindsDB. We also provide Mongo-QL syntax documented in the MindsDB Mongo-QL section. You can connect MindsDB to different clients, including PostgreSQL CLI and MySQL CLI . Check out the Use Cases section to follow tutorials that cover Large Language Models, Natural Language Processing, Time Series, Classification, and Regression models. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Installation Dependencies Troubleshooting Pip and Python Versions How to Avoid Dependency Issues How to Avoid Common Errors Further Issues? What’s Next"}
{"file_name": "collection-structure.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "insert.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "database.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "regression.html", "content": "Fine-Tune the Regression Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Regression Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Regression Model In this example, we use our sample PostgreSQL database. You can connect to it like this: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; First, we create and train the model using a subset of the home_rentals data, considering properties that have been on the market less than 10 days. CREATE MODEL mindsdb . adjust_home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals WHERE days_on_market < 10 ) PREDICT rental_price ; On execution, we get: Query successfully completed We can check its status using this command: DESCRIBE MODEL adjust_home_rentals_model ; Once the status is complete, we can query for predictions. SELECT rental_price , rental_price_explain FROM mindsdb . adjust_home_rentals_model WHERE sqft = 1000 AND location = 'great' AND neighborhood = 'berkeley_hills' AND number_of_rooms = 2 AND number_of_bathrooms = 1 AND days_on_market = 40 ; On execution, we get: + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 2621 | { \"predicted_value\" : 2621 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 2523 , \"confidence_upper_bound\" : 2719 } | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Let’s adjust this model with more training data. Now we consider properties that have been on the market for 10 or more days. FINETUNE mindsdb . adjust_home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals WHERE days_on_market >= 10 ) ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. To check the status and versions of the model, run this command: SELECT name , engine , project , active , version , status FROM mindsdb . models WHERE name = 'adjust_home_rentals_model' ; On execution, we get: + ---------------------------+-----------+---------+--------+---------+----------+ | name | engine | project | active | version | status | + ---------------------------+-----------+---------+--------+---------+----------+ | adjust_home_rentals_model | lightwood | mindsdb | false | 1 | complete | | adjust_home_rentals_model | lightwood | mindsdb | true | 2 | complete | + ---------------------------+-----------+---------+--------+---------+----------+ Please note that the longer the property is on the market, the lower its rental price. Hence, we can expect the rental_price prediction to be lower. SELECT rental_price , rental_price_explain FROM mindsdb . adjust_home_rentals_model WHERE sqft = 1000 AND location = 'great' AND neighborhood = 'berkeley_hills' AND number_of_rooms = 2 AND number_of_bathrooms = 1 AND days_on_market = 40 ; On execution, we get: + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 2055 | { \"predicted_value\" : 2055 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 1957 , \"confidence_upper_bound\" : 2153 } | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE adjust_home_rentals_model FROM mindsdb ( SELECT * FROM example_db . home_rentals WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . home_rentals WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Classification Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "classification.html", "content": "Fine-Tune the Classification Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Classification Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Classification Model In this example, we again use our sample PostgreSQL database. First, we create and train the model using a subset of the customer_churn data, considering only female customers. CREATE MODEL mindsdb . adjust_customer_churn_model FROM example_db ( SELECT * FROM demo_data . customer_churn WHERE gender = 'Female' ) PREDICT churn ; On execution, we get: Query successfully completed We can check its status using this command: DESCRIBE MODEL adjust_customer_churn_model ; Once the status is complete, we can query for predictions. SELECT churn , churn_explain FROM mindsdb . adjust_customer_churn_model WHERE seniorcitizen = 0 AND partner = 'Yes' AND dependents = 'No' AND tenure = 1 AND phoneservice = 'No' AND multiplelines = 'No phone service' AND internetservice = 'DSL' ; On execution, we get: + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | churn | churn_explain | + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | No | { \"predicted_value\" : \"No\" , \"confidence\" : 0.9887640449438202 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.934 , \"probability_class_Yes\" : 0.066 } | + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ Let’s adjust this model with more training data. Now we also consider male customers. FINETUNE mindsdb . adjust_customer_churn_model FROM example_db ( SELECT * FROM demo_data . customer_churn WHERE gender = 'Male' ) ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. To check the status and versions of the model, run this command: SELECT name , engine , project , active , version , status FROM mindsdb . models WHERE name = 'adjust_customer_churn_model' ; On execution, we get: + -----------------------------+-----------+---------+--------+---------+----------+ | name | engine | project | active | version | status | + -----------------------------+-----------+---------+--------+---------+----------+ | adjust_customer_churn_model | lightwood | mindsdb | false | 1 | complete | | adjust_customer_churn_model | lightwood | mindsdb | true | 2 | complete | + -----------------------------+-----------+---------+--------+---------+----------+ Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL adjust_customer_churn_model ; Let’s query for a prediction again. SELECT churn , churn_explain FROM mindsdb . adjust_customer_churn_model WHERE seniorcitizen = 0 AND partner = 'Yes' AND dependents = 'No' AND tenure = 1 AND phoneservice = 'No' AND multiplelines = 'No phone service' AND internetservice = 'DSL' ; On execution, we get: + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ | churn | churn_explain | + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ | No | { \"predicted_value\" : \"No\" , \"confidence\" : 0.9887640449438202 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.9294 , \"probability_class_Yes\" : 0.0706 } | + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ Here after adjusting the model, there are no significant changes to the predictions. However, the probability class for Yes and No values has been updated. The probability of a Yes value has increased slightly, while the probability of a No value has decreased. If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE adjust_customer_churn_model FROM mindsdb ( SELECT * FROM example_db . customer_churn WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . customer_churn WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue OpenAI Regression github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "anyscale.html", "content": "Fine-Tune the Mistral-7B Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Mistral-7B Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Mistral-7B Model Follow this blog post for a comprehensive tutorial on how to fine-tune a Mistral 7B model. All Anyscale models belong to the group of Large Language Models (LLMs). These are some of the supported models: Mistral7B Llama-2-7b Llama-2-13b Llama-2-70b Code Llama Let’s create a model to answer questions about MindsDB’s custom SQL syntax. First, create an AnyScale engine, passing your Anyscale API key: CREATE ML_ENGINE anyscale_engine FROM anyscale_endpoints USING anyscale_endpoints_api_key = 'your-anyscale-api-key' ; Then, create a model using this engine: CREATE MODEL mymistral7b PREDICT completion USING engine = 'anyscale_engine' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}' ; You can check model status with this command: DESCRIBE mymistral7b ; Once the status is complete, we can query for predictions: SELECT prompt , completion FROM mymistral7b as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | The SQL syntax is : SELECT * FROM input_data INNER JOIN predictions ON input_data . id = predictions . id | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you followed one of the MindsDB tutorials before, you’ll see that the syntax provided by the model is not exactly as expected. Now, we’ll fine-tune our model using a table that stores details about MindsDB’s custom SQL syntax. Let’s connect to a DB that hosts a table we’ll use to fine-tune our model: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Now we can take a look at the fine-tuning data: SELECT message_id , role , content FROM example_db . chat_llm_mindsdb_docs LIMIT 5 ; And here are the first few rows: message_id role content 0 system You are a helpful assistant. Your task is to answer a user’s question regarding the SQL syntax supported by MindsDB, a machine learning product for training models and seamlessly deploying them where your data lives. 1 user In the context of MindsDB: 1. Testing CREATE DATABASE 2 assistant CREATE DATABASE example_db WITH ENGINE = \"postgres\", PARAMETERS = { \"user\": \"demo_user\", \"password\": \"demo_password\", \"host\": \"samples.mindsdb.com\", ... }; Output: status ------ Query successfully completed 3 system You are a helpful assistant. Your task is to answer a user’s question regarding the SQL syntax supported by MindsDB, a machine learning product for… 4 user In the context of MindsDB: 2. Testing Preview the Available Data Using SELECT Notice it is formatted as a series of chats that conform to the standard OpenAI chat format. Every message has a “role” and some “content”. By chaining together a series of messages, we can create a conversation. Now, you can fine-tune a Mistral model with this data like so: FINETUNE mymistral7b FROM example_db ( SELECT * FROM chat_llm_mindsdb_docs ) ; The FINETUNE command creates a new version of the mistralai/Mistral-7B-Instruct-v0.1 model. You can query all available versions as below: SELECT * FROM models WHERE name = 'mymistral7b' ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. Once the new version status is complete and active, we can query the model again, expecting a more accurate output. SELECT prompt , completion FROM mymistral7b as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | SELECT * FROM mindsdb . models . my_model JOIN mindsdb . input_data_name ; | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE mymistral7b FROM mindsdb ( SELECT * FROM example_db . chat_llm_mindsdb_docs WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . chat_llm_mindsdb_docs WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Overview OpenAI github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "openai.html", "content": "Fine-Tune the OpenAI Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the OpenAI Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the OpenAI Model In this example we are going to teach an OpenAI model, how to write MindsDB AI SQL queries All OpenAI models belong to the group of Large Language Models (LLMs). By definition, these are pre-trained on large amounts of data. However, it is possible to fine-tune these models with a task-specific dataset for a defined use case. OpenAI supports fine-tuning of some of its models listed here . And with MindsDB, you can easily fine-tune an OpenAI model making it more applicable to your specific use case. Let’s create a model to answer questions about MindsDB’s custom SQL syntax. First, create an OpenAI engine, passing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; Then, create a model using this engine: CREATE MODEL openai_davinci PREDICT completion USING engine = 'openai_engine' , model_name = 'davinci-002' , prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}' ; You can check model status with this command: DESCRIBE openai_davinci ; Once the status is complete, we can query for predictions: SELECT prompt , completion FROM openai_davinci as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | The SQL syntax is : SELECT * FROM input_data INNER JOIN predictions ON input_data . id = predictions . id | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you followed one of the MindsDB tutorials before, you’ll see that the syntax provided by the model is not exactly as expected. Now, we’ll fine-tune our model using a table that stores details about MindsDB’s custom SQL syntax. Upload this data file to MindsDB and use it to finetune the model. This is how you can fine-tune an OpenAI model: FINETUNE openai_davinci FROM files ( SELECT prompt , completion FROM openai_learninghub_ft ) ; The FINETUNE command creates a new version of the openai_davinci model. You can query all available versions as below: SELECT * FROM models WHERE name = 'openai_davinci' ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. Once the new version status is complete and active, we can query the model again, expecting a more accurate output. SELECT prompt , completion FROM openai_davinci as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | SELECT * FROM mindsdb . models . my_model JOIN mindsdb . input_data_name ; | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE openai_davinci FROM mindsdb ( SELECT * FROM files . openai_learninghub_ft WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM files . openai_learninghub_ft WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Mistral-7B Classification github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "chatbot.html", "content": "Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Agent Chatbot Knowledge Base Functions Standard SQL Support AI Agents Chatbot Within MindsDB, chatbots are agents connected to a chat interface. Creating a chatbot requires either an AI agent or an LLM, and a connection to a chat app, like Slack or MS Teams . ​ Chatbot with an AI agent AI Agents are customized AI models that can answer questions over your data. You can connect your data to agents in the form of skills . Here is how to create a chatbot that integrates an AI Agent and can be connected to a chat interface to have a conversation with your data. CREATE CHATBOT my_chatbot USING database = 'my_slack' , -- created with CREATE DATABASE my_slack agent = 'my_agent' , -- created with CREATE AGENT my_agent is_running = true ; -- default is true The parameters include the following: database stores connection to a chat app (like Slack or MS Teams ) that should be created with the CREATE DATABASE statement. agent is an AI agent created with the CREATE AGENT command. It consists of an AI model and a set of skills, that is, defined data sets provided at inference time via RAG-based techniques. is_running indicates whether or not to start the chatbot upon creation. Here are some tips for using the Slack integration: If you want to use Slack in the CREATE CHATBOT syntax, use this method of connecting Slack to MindsDB . If you want to connect the chatbot to multiple Slack channels, open your Slack application and add the App/Bot to one or more channels: Go to the channel where you want to use the bot. Right-click on the channel and select View Channel Details . Select Integrations . Click on Add an App . ​ Chatbot with an LLM ALternatively, you can create a chatbot that is equivalent to embedding an LLM of your choice into a chat app, like Slack or MS Teams . Here is how to create a chatbot that integrates an LLM and can be connected to a chat interface. CREATE CHATBOT my_chatbot USING database = 'my_slack' , -- created with CREATE DATABASE my_slack model = 'my_model' , -- created with CREATE MODEL my_model is_running = true ; -- default is true The parameters include the following: database stores connection to a chat app (like Slack or MS Teams ) that should be created with the CREATE DATABASE statement. model is a conversational model created with the CREATE MODEL command using the LangChain engine . is_running indicates whether or not to start the chatbot upon creation. Here is how to delete a chatbot: DROP CHATBOT my_chatbot ; And here is how to query all chatbots: SHOW CHATBOTS ; SELECT * FROM chatbots ; ​ Example Following the example from here , let’s create a chatbot utilizing the already created agent. Start by connecting a chat app to MindsDB: Follow this instruction to connect Slack to MindsDB. Follow this instruction to connect MS Teams to MindsDB. Next, create a chatbot. CREATE CHATBOT text_to_sql_chatbot USING database = 'my_slack' , -- this must be created with CREATE DATABASE agent = 'text_to_sql_agent' , -- this must be created with CREATE AGENT is_running = true ; Follow this tutorial to build your own chatbot. Was this page helpful? Yes No Suggest edits Raise issue Agent Knowledge Base github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Chatbot with an AI agent Chatbot with an LLM Example"}
{"file_name": "agent.html", "content": "Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Agent Chatbot Knowledge Base Functions Standard SQL Support AI Agents Agent With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL. AI agents use a conversational model (like OpenAI) from LangChain utilizing tools as skills to respond to user input. Users can customize AI agents with their own prompts to fit their use cases. A chatbot can be thought of as an agent connected to some messaging interface. ​ How to work with AI agents ​ Create skills Start by setting up the skills. Here is how you can create and manage skills using SQL API. Creating, inserting into, updating, and deleting a knowledge base: CREATE KNOWLEDGE BASE my_knowledge_base USING model = embedding_model_name , -- this parameter is optional; if not provided, a suitable embedding model is chosen for the task storage = vector_database . storage_table ; -- this parameter is optional; if not provided, the default ChromaDB is used for storage -- inserts new data rows and generates id for each row if id is not provided INSERT INTO my_knowledge_base SELECT text AS content FROM datasource . data_table ; -- inserts new data rows and updates existing ones if id value matches INSERT INTO my_knowledge_base SELECT id , text AS content FROM datasource . data_table ; -- view content of a knowledge base (for example, to look up the generated id values) SELECT * FROM my_knowledge_base ; DROP KNOWLEDGE BASE my_knowledge_base ; Creating, updating, and deleting a knowledge_base skill: CREATE SKILL kb_skill USING type = 'knowledge_base' , source = 'my_knowledge_base' , -- this must be created with CREATE KNOWLEDGE BASE description = 'My data' ; -- data description to help the agent know when to use the knowledge base UPDATE SKILL kb_skill SET source = 'new_knowledge_base' ; -- this must be created with CREATE KNOWLEDGE BASE DROP SKILL kb_skill ; Creating, updating, and deleting a text2sql skill: CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , -- this must be created with CREATE DATABASE tables = [ 'sales_data' ] , -- this is a list of tables passed to this skill description = \"this is sales data\" ; UPDATE SKILL text_to_sql_skill SET database = 'new_example_db' , -- this must be created with CREATE DATABASE tables = [ 'sales_data' ] ; -- this is a list of tables passed to this skill DROP SKILL text_to_sql_skill ; You can query all skills using this command: SHOW KNOWLEDGE_BASES ; SHOW SKILLS ; ​ Create an agent An agent can be created, deleted, queried, and updated. Here is how you can do that using SQL API. Creating an AI agent: CREATE AGENT my_agent USING model = 'chatbot_agent' , -- this must be a conversational model created with CREATE MODEL (as in the Example section) skills = [ 'test_skill' ] ; -- this must be created with CREATE SKILL Alternatively, you can create an agent and define the model to be used by an agent at the agent creation time based on the model providers defined here . CREATE AGENT my_agent USING skills = [ 'text_to_sql_skill' ] , -- this must be created with CREATE SKILL provider = 'openai' , -- choose one of the available model providers (openai, anthropic, anyscale, ollama, litellm, mindsdb) model = 'gpt-4' , -- define the model from the provider prompt_template = 'Answer the user input in a helpful way using tools' , -- provide instruction to the model verbose = True , max_tokens = 100 ; Updating an AI agent: UPDATE AGENT my_agent SET model = 'new_chatbot_agent' , -- this must be a conversational model created with CREATE MODEL skills_to_remove = [ 'test_skill' ] , skills_to_add = [ 'production_skill' ] ; -- this must be created with CREATE SKILL Querying an AI agent: SELECT * FROM my_agent WHERE question = \"insert your question\" ; -- this is the user_column parameter as defined when creating a conversational model for the agent (as in the Example section) Deleting an AI agent: DROP AGENT my_agent ; You can query all agents using this command: SHOW AGENTS [ FROM project_name ] ; SELECT * FROM agents ; ​ Example ​ Agents with Text-to-SQL Skills Start by creating a conversational large language model to be used by an agent. CREATE MODEL my_model PREDICT answer USING engine = 'langchain' , openai_api_key = 'your-model-api-key' , model_name = 'gpt-4' , mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Agents access models via the LangChain integration with MindsDB . Check out the link to find out available models. Then, connect a data source to be used for creating a skill. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; SELECT * FROM example_db . sales_data ; Create a skill using one or more tables from a connected data source. CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , tables = [ 'car_sales' ] , description = \"this is car sales data\" ; Now that we have a model and a skill, let’s create an agent. CREATE AGENT text_to_sql_agent USING model = 'my_model' , skills = [ 'text_to_sql_skill' ] ; Query the agent as below: SELECT * FROM text_to_sql_agent WHERE question = \"how many cars were sold in 2017?\" ; -- this column is defined in the user_column parameter in CREATE MODEL The next step would be to connect a chat app, like Slack, to MindsDB and create a chatbot utilizing this agent. Learn about chatbots here . ​ Agents with Knowledge Bases as Skills In this example, let’s create an embedding model (using OpenAI or LangChain as an engine) for the knowledge base. Note that this step is optional, as knowledge bases provide default embedding model. CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL embedding_model PREDICT embeddings USING engine = 'openai_engine' , mode = 'embedding' , model_name = 'text-embedding-ada-002' , question_column = 'content' ; Now let’s create a knowledge base that uses this embedding model and the default storage vector database (that is, ChromaDB). CREATE KNOWLEDGE BASE my_knowledge_base USING model = embedding_model ; -- this is optional This is how you can insert data into the knowledge base and select it. INSERT INTO my_knowledge_base ( content ) VALUES ( 'I drink tea.' ) ; SELECT * FROM my_knowledge_base ; Use this knowledge base to create a skill for an agent: CREATE SKILL kb_skill USING type = 'knowledge_base' , source = 'my_knowledge_base' , -- this must be created with CREATE KNOWLEDGE BASE description = 'My data' ; -- data description to help the agent know when to use the knowledge base Now you can assign this skill to the agent ( that was created in the example above) and query it again: UPDATE AGENT text_to_sql_agent SET skills_to_add = [ 'kb_skill' ] ; SELECT * FROM text_to_sql_agent WHERE questions = \"what is your data?\" ; Was this page helpful? Yes No Suggest edits Raise issue Query Triggers Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to work with AI agents Create skills Create an agent Example Agents with Text-to-SQL Skills Agents with Knowledge Bases as Skills"}
{"file_name": "ai-powered_data_retrieval.html", "content": "AI-Powered Data Retrieval - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI-Powered Data Retrieval AI-Powered Data Retrieval Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Overview Embedding Model Recommender Models Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation AI-Powered Data Retrieval AI-Powered Data Retrieval MindsDB facilitates AI-powered search of data in the form of documents, websites, or databases. You can search and retrieve data easily using AI models. Also, you can create recommendation systems based on historical data. This section covers the following use cases: Semantic search Embeddings models Recommenders Available tutorials: Embeddings models Recommenders Was this page helpful? Yes No Suggest edits Raise issue Build an AI Agent Embedding Model github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "predictive_analytics.html", "content": "Predictive Analytics - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictive Analytics Predictive Analytics Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics Overview Quarterly House Sales Forecast Monthly Expenditures Brain Activity In-Database Machine Learning AI Workflow Automation Predictive Analytics Predictive Analytics MindsDB enables seamless time-series forecasting so you can look into the future and get predictions based on historical data. You can also use historical data to detect anomalies, for example, in quality control systems. This section covers the following use cases: Time-series forecasting Anomaly detection Available tutorials: Forecast of Quarterly House Sales Forecast of Monthly Expenditures Forecast of Brain Activity Anomaly Detection Was this page helpful? Yes No Suggest edits Raise issue Hugging Face Inference API Quarterly House Sales github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "ai_agents.html", "content": "AI Agents - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents AI Agents Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents Overview Build an AI Agent AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation AI Agents AI Agents AI Agents are an extension of large language models (LLMs). They can access external tools, such as search engines, via API calls, overcoming some common shortcomings of LLMs, such as lack of access to internal business data. With MindsDB, you can customize and deploy AI agents . This section covers the following use cases: Knowledge bases Skills Agents Chatbots Available tutorials: Agents and Chatbots Chatbot with a Text2SQL Skill Chatbot with a Knowledge Base Was this page helpful? Yes No Suggest edits Raise issue Regression Build an AI Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "in-database_ml.html", "content": "In-Database Machine Learning - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation In-Database Machine Learning In-Database Machine Learning Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning Overview Customer Churn Home Rentals AI Workflow Automation In-Database Machine Learning In-Database Machine Learning MindsDB brings AI/ML into your database. This mitigates the need for creating and maintaining data pipelines. Access various AI/ML models directly from your data source . This section covers the following use cases: Automated classification models Automated regression models Bring Your Own Model (BYOM) to MindsDB Available tutorials: Classification of Customer Churn Regression of Home Rental Prices BYOM Was this page helpful? Yes No Suggest edits Raise issue Brain Activity Customer Churn github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "overview.html", "content": "Applications of MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Use Cases Applications of MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Use Cases Applications of MindsDB MindsDB integrates with numerous data sources and AI frameworks so you can easily bring data and AI together to create and automate custom workflows with MindsDB. Common use cases include fine-tuning models, chatbots, alert systems, content generation, natural language processing, classification, regressions, forecasting. This section presents some of the common applications of MindsDB in form of tutorials. Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Was this page helpful? Yes No Suggest edits Raise issue Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "automated_finetuning.html", "content": "Automated Fine-Tuning - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Automated Fine-Tuning Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Automated Fine-Tuning Real-world use cases often deal with dynamic data that is updated regularly. MindsDB enables you to automate fine-tuning of AI models to keep them up-to-date and as accurate as possible. You can set up jobs that will trigger fine-tuning of AI models every time new data arrives. This section covers the following use cases: Fine-tuning of Large Language models Fine-tuning of AutoML models Available tutorials: Fine-tuning of Mistral models Fine-tuning of OpenAI models Fine-tuning of classification models Fine-tuning of regression models Was this page helpful? Yes No Suggest edits Raise issue Overview Mistral-7B github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "ai_workflow_automation.html", "content": "AI Workflow Automation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation AI Workflow Automation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation AI Workflow Automation With MindsDB, you can create, customize, and automate AI workflows that comprise of connecting a data source , deploying an AI/ML model , and streaming predictions and forecast into your application. Use jobs and triggers to create custom automation. This section covers the following use cases: Chatbot automation with jobs Alert systems Available tutorials: Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts and Notifications Was this page helpful? Yes No Suggest edits Raise issue Home Rentals Slack Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "data_enrichment.html", "content": "Data Enrichment - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Data Enrichment Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Data Enrichment With MindsDB, you can easily enrich your data with AI-generated content. Process natural language, analyze text, create images, and more with various AI/ML models accessible through MindsDB. This section covers the following use cases: Natural Language Processing (NLP) Content generation QA-driven data enrichment Sentiment analysis Text summarization Available tutorials: Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Models Was this page helpful? Yes No Suggest edits Raise issue Recommender Models Image Generator github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "regression.html", "content": "Fine-Tune the Regression Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Regression Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Regression Model In this example, we use our sample PostgreSQL database. You can connect to it like this: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; First, we create and train the model using a subset of the home_rentals data, considering properties that have been on the market less than 10 days. CREATE MODEL mindsdb . adjust_home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals WHERE days_on_market < 10 ) PREDICT rental_price ; On execution, we get: Query successfully completed We can check its status using this command: DESCRIBE MODEL adjust_home_rentals_model ; Once the status is complete, we can query for predictions. SELECT rental_price , rental_price_explain FROM mindsdb . adjust_home_rentals_model WHERE sqft = 1000 AND location = 'great' AND neighborhood = 'berkeley_hills' AND number_of_rooms = 2 AND number_of_bathrooms = 1 AND days_on_market = 40 ; On execution, we get: + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 2621 | { \"predicted_value\" : 2621 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 2523 , \"confidence_upper_bound\" : 2719 } | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Let’s adjust this model with more training data. Now we consider properties that have been on the market for 10 or more days. FINETUNE mindsdb . adjust_home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals WHERE days_on_market >= 10 ) ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. To check the status and versions of the model, run this command: SELECT name , engine , project , active , version , status FROM mindsdb . models WHERE name = 'adjust_home_rentals_model' ; On execution, we get: + ---------------------------+-----------+---------+--------+---------+----------+ | name | engine | project | active | version | status | + ---------------------------+-----------+---------+--------+---------+----------+ | adjust_home_rentals_model | lightwood | mindsdb | false | 1 | complete | | adjust_home_rentals_model | lightwood | mindsdb | true | 2 | complete | + ---------------------------+-----------+---------+--------+---------+----------+ Please note that the longer the property is on the market, the lower its rental price. Hence, we can expect the rental_price prediction to be lower. SELECT rental_price , rental_price_explain FROM mindsdb . adjust_home_rentals_model WHERE sqft = 1000 AND location = 'great' AND neighborhood = 'berkeley_hills' AND number_of_rooms = 2 AND number_of_bathrooms = 1 AND days_on_market = 40 ; On execution, we get: + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 2055 | { \"predicted_value\" : 2055 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 1957 , \"confidence_upper_bound\" : 2153 } | + ---------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE adjust_home_rentals_model FROM mindsdb ( SELECT * FROM example_db . home_rentals WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . home_rentals WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Classification Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "classification.html", "content": "Fine-Tune the Classification Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Classification Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Classification Model In this example, we again use our sample PostgreSQL database. First, we create and train the model using a subset of the customer_churn data, considering only female customers. CREATE MODEL mindsdb . adjust_customer_churn_model FROM example_db ( SELECT * FROM demo_data . customer_churn WHERE gender = 'Female' ) PREDICT churn ; On execution, we get: Query successfully completed We can check its status using this command: DESCRIBE MODEL adjust_customer_churn_model ; Once the status is complete, we can query for predictions. SELECT churn , churn_explain FROM mindsdb . adjust_customer_churn_model WHERE seniorcitizen = 0 AND partner = 'Yes' AND dependents = 'No' AND tenure = 1 AND phoneservice = 'No' AND multiplelines = 'No phone service' AND internetservice = 'DSL' ; On execution, we get: + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | churn | churn_explain | + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ | No | { \"predicted_value\" : \"No\" , \"confidence\" : 0.9887640449438202 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.934 , \"probability_class_Yes\" : 0.066 } | + --------+------------------------------------------------------------------------------------------------------------------------------------------------------------+ Let’s adjust this model with more training data. Now we also consider male customers. FINETUNE mindsdb . adjust_customer_churn_model FROM example_db ( SELECT * FROM demo_data . customer_churn WHERE gender = 'Male' ) ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. To check the status and versions of the model, run this command: SELECT name , engine , project , active , version , status FROM mindsdb . models WHERE name = 'adjust_customer_churn_model' ; On execution, we get: + -----------------------------+-----------+---------+--------+---------+----------+ | name | engine | project | active | version | status | + -----------------------------+-----------+---------+--------+---------+----------+ | adjust_customer_churn_model | lightwood | mindsdb | false | 1 | complete | | adjust_customer_churn_model | lightwood | mindsdb | true | 2 | complete | + -----------------------------+-----------+---------+--------+---------+----------+ Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL adjust_customer_churn_model ; Let’s query for a prediction again. SELECT churn , churn_explain FROM mindsdb . adjust_customer_churn_model WHERE seniorcitizen = 0 AND partner = 'Yes' AND dependents = 'No' AND tenure = 1 AND phoneservice = 'No' AND multiplelines = 'No phone service' AND internetservice = 'DSL' ; On execution, we get: + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ | churn | churn_explain | + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ | No | { \"predicted_value\" : \"No\" , \"confidence\" : 0.9887640449438202 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.9294 , \"probability_class_Yes\" : 0.0706 } | + --------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+ Here after adjusting the model, there are no significant changes to the predictions. However, the probability class for Yes and No values has been updated. The probability of a Yes value has increased slightly, while the probability of a No value has decreased. If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE adjust_customer_churn_model FROM mindsdb ( SELECT * FROM example_db . customer_churn WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . customer_churn WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue OpenAI Regression github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "anyscale.html", "content": "Fine-Tune the Mistral-7B Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the Mistral-7B Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the Mistral-7B Model Follow this blog post for a comprehensive tutorial on how to fine-tune a Mistral 7B model. All Anyscale models belong to the group of Large Language Models (LLMs). These are some of the supported models: Mistral7B Llama-2-7b Llama-2-13b Llama-2-70b Code Llama Let’s create a model to answer questions about MindsDB’s custom SQL syntax. First, create an AnyScale engine, passing your Anyscale API key: CREATE ML_ENGINE anyscale_engine FROM anyscale_endpoints USING anyscale_endpoints_api_key = 'your-anyscale-api-key' ; Then, create a model using this engine: CREATE MODEL mymistral7b PREDICT completion USING engine = 'anyscale_engine' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}' ; You can check model status with this command: DESCRIBE mymistral7b ; Once the status is complete, we can query for predictions: SELECT prompt , completion FROM mymistral7b as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | The SQL syntax is : SELECT * FROM input_data INNER JOIN predictions ON input_data . id = predictions . id | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you followed one of the MindsDB tutorials before, you’ll see that the syntax provided by the model is not exactly as expected. Now, we’ll fine-tune our model using a table that stores details about MindsDB’s custom SQL syntax. Let’s connect to a DB that hosts a table we’ll use to fine-tune our model: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Now we can take a look at the fine-tuning data: SELECT message_id , role , content FROM example_db . chat_llm_mindsdb_docs LIMIT 5 ; And here are the first few rows: message_id role content 0 system You are a helpful assistant. Your task is to answer a user’s question regarding the SQL syntax supported by MindsDB, a machine learning product for training models and seamlessly deploying them where your data lives. 1 user In the context of MindsDB: 1. Testing CREATE DATABASE 2 assistant CREATE DATABASE example_db WITH ENGINE = \"postgres\", PARAMETERS = { \"user\": \"demo_user\", \"password\": \"demo_password\", \"host\": \"samples.mindsdb.com\", ... }; Output: status ------ Query successfully completed 3 system You are a helpful assistant. Your task is to answer a user’s question regarding the SQL syntax supported by MindsDB, a machine learning product for… 4 user In the context of MindsDB: 2. Testing Preview the Available Data Using SELECT Notice it is formatted as a series of chats that conform to the standard OpenAI chat format. Every message has a “role” and some “content”. By chaining together a series of messages, we can create a conversation. Now, you can fine-tune a Mistral model with this data like so: FINETUNE mymistral7b FROM example_db ( SELECT * FROM chat_llm_mindsdb_docs ) ; The FINETUNE command creates a new version of the mistralai/Mistral-7B-Instruct-v0.1 model. You can query all available versions as below: SELECT * FROM models WHERE name = 'mymistral7b' ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. Once the new version status is complete and active, we can query the model again, expecting a more accurate output. SELECT prompt , completion FROM mymistral7b as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | SELECT * FROM mindsdb . models . my_model JOIN mindsdb . input_data_name ; | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE mymistral7b FROM mindsdb ( SELECT * FROM example_db . chat_llm_mindsdb_docs WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM example_db . chat_llm_mindsdb_docs WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Overview OpenAI github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "openai.html", "content": "Fine-Tune the OpenAI Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Automated Fine-Tuning Fine-Tune the OpenAI Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning Overview Mistral-7B OpenAI Classification Regression AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Automated Fine-Tuning Fine-Tune the OpenAI Model In this example we are going to teach an OpenAI model, how to write MindsDB AI SQL queries All OpenAI models belong to the group of Large Language Models (LLMs). By definition, these are pre-trained on large amounts of data. However, it is possible to fine-tune these models with a task-specific dataset for a defined use case. OpenAI supports fine-tuning of some of its models listed here . And with MindsDB, you can easily fine-tune an OpenAI model making it more applicable to your specific use case. Let’s create a model to answer questions about MindsDB’s custom SQL syntax. First, create an OpenAI engine, passing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; Then, create a model using this engine: CREATE MODEL openai_davinci PREDICT completion USING engine = 'openai_engine' , model_name = 'davinci-002' , prompt_template = 'Return a valid SQL string for the following question about MindsDB in-database machine learning: {{prompt}}' ; You can check model status with this command: DESCRIBE openai_davinci ; Once the status is complete, we can query for predictions: SELECT prompt , completion FROM openai_davinci as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | The SQL syntax is : SELECT * FROM input_data INNER JOIN predictions ON input_data . id = predictions . id | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you followed one of the MindsDB tutorials before, you’ll see that the syntax provided by the model is not exactly as expected. Now, we’ll fine-tune our model using a table that stores details about MindsDB’s custom SQL syntax. Upload this data file to MindsDB and use it to finetune the model. This is how you can fine-tune an OpenAI model: FINETUNE openai_davinci FROM files ( SELECT prompt , completion FROM openai_learninghub_ft ) ; The FINETUNE command creates a new version of the openai_davinci model. You can query all available versions as below: SELECT * FROM models WHERE name = 'openai_davinci' ; While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. Once the new version status is complete and active, we can query the model again, expecting a more accurate output. SELECT prompt , completion FROM openai_davinci as m WHERE prompt = 'What is the SQL syntax to join input data with predictions from a MindsDB machine learning model?' USING max_tokens = 400 ; On execution, we get: + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | prompt | completion | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | What is the SQL syntax to join input data with predictions from a MindsDB machine learning model? | SELECT * FROM mindsdb . models . my_model JOIN mindsdb . input_data_name ; | + ---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ If you have dynamic data that gets updated regularly, you can set up an automated fine-tuning as below. Note that the data source must contain an incremental column, such as timestamp or integer, so MindsDB can pick up only the recently added data with the help of the LAST keyword . Here is how to create and schedule a job to fine-tune the model periodically. CREATE JOB automated_finetuning ( FINETUNE openai_davinci FROM mindsdb ( SELECT * FROM files . openai_learninghub_ft WHERE timestamp > LAST ) ) EVERY 1 day IF ( SELECT * FROM files . openai_learninghub_ft WHERE timestamp > LAST ) ; Now your model will be fine-tuned with newly added data every day or every time there is new data available. Was this page helpful? Yes No Suggest edits Raise issue Mistral-7B Classification github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "customer-churn.html", "content": "Predict Customer Churn with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation In-Database Machine Learning Predict Customer Churn with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning Overview Customer Churn Home Rentals AI Workflow Automation In-Database Machine Learning Predict Customer Churn with MindsDB This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Introduction In this tutorial, we’ll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we’ll predict the probability of churn for new customers of a telecoms company. Install MindsDB locally via Docker or Docker Desktop . Let’s get started. ​ Data Setup ​ Connecting the Data There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we’ve prepared for you. It contains the data used throughout this tutorial (the example_db.demo_data.customer_churn table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let’s preview the data that we’ll use to train our predictor. SELECT * FROM example_db . demo_data . customer_churn LIMIT 10 ; Pay Attention to the Queries From now on, we’ll use the files.churn file as a table. Make sure you replace it with example_db.demo_data.customer_churn if you connect the data as a database. ​ Understanding the Data We use the customer churn dataset, where each row is one customer, to predict whether the customer is going to stop using the company products. Below is the sample data stored in the files.churn table. + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | customerID | gender | SeniorCitizen | Partner | Dependents | tenure | PhoneService | MultipleLines | InternetService | OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport | StreamingTV | StreamingMovies | Contract | PaperlessBilling | PaymentMethod | MonthlyCharges | TotalCharges | Churn | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | 7590 - VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month - to - month | Yes | Electronic check | 29.85 | 29.85 | No | | 5575 - GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | 56.95 | 1889.5 | No | | 3668 - QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month - to - month | Yes | Mailed check | 53.85 | 108.15 | Yes | | 7795 - CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer ( automatic ) | 42.3 | 1840.75 | No | | 9237 - HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month - to - month | Yes | Electronic check | 70.7 | 151.65 | Yes | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ Where: Column Description Data Type Usage CustomerId The identification number of a customer. character varying Feature Gender The gender of a customer. character varying Feature SeniorCitizen It indicates whether the customer is a senior citizen ( 1 ) or not ( 0 ). integer Feature Partner It indicates whether the customer has a partner ( Yes ) or not ( No ). character varying Feature Dependents It indicates whether the customer has dependents ( Yes ) or not ( No ). character varying Feature Tenure Number of months the customer has been staying with the company. integer Feature PhoneService It indicates whether the customer has a phone service ( Yes ) or not ( No ). character varying Feature MultipleLines It indicates whether the customer has multiple lines ( Yes ) or not ( No , No phone service ). character varying Feature InternetService Customer’s internet service provider ( DSL , Fiber optic , No ). character varying Feature OnlineSecurity It indicates whether the customer has online security ( Yes ) or not ( No , No internet service ). character varying Feature OnlineBackup It indicates whether the customer has online backup ( Yes ) or not ( No , No internet service ). character varying Feature DeviceProtection It indicates whether the customer has device protection ( Yes ) or not ( No , No internet service ). character varying Feature TechSupport It indicates whether the customer has tech support ( Yes ) or not ( No , No internet service ). character varying Feature StreamingTv It indicates whether the customer has streaming TV ( Yes ) or not ( No , No internet service ). character varying Feature StreamingMovies It indicates whether the customer has streaming movies ( Yes ) or not ( No , No internet service ). character varying Feature Contract The contract term of the customer ( Month-to-month , One year , Two year ). character varying Feature PaperlessBilling It indicates whether the customer has paperless billing ( Yes ) or not ( No ). character varying Feature PaymentMethod Customer’s payment method ( Electronic check , Mailed check , Bank transfer (automatic) , Credit card (automatic) ). character varying Feature MonthlyCharges The monthly charge amount. money Feature TotalCharges The total amount charged to the customer. money Feature Churn It indicates whether the customer churned ( Yes ) or not ( No ). character varying Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). ​ Training a Predictor Let’s create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn ; We use all of the columns as features, except for the Churn column, whose values will be predicted. ​ Status of a Predictor A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: DESCRIBE customer_churn_predictor ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! ​ Making Predictions ​ Making a Single Prediction You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0.7752808988764045 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0.7752808988764045 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.4756 , \"probability_class_Yes\" : 0.5244 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ To get more accurate predictions, we should provide as much data as possible in the WHERE clause. Let’s run another query. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND Contract = 'Month-to-month' AND MonthlyCharges = 29.85 AND TotalCharges = 29.85 AND OnlineBackup = 'Yes' AND OnlineSecurity = 'No' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0.8202247191011236 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8202247191011236 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.4098 , \"probability_class_Yes\" : 0.5902 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ MindsDB predicted the probability of this customer churning with confidence of around 82%. The previous query predicted it with confidence of around 79%. So providing more data improved the confidence level of predictions. ​ Making Batch Predictions Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . customerID , t . Contract , t . MonthlyCharges , m . Churn FROM files . churn AS t JOIN mindsdb . customer_churn_predictor AS m LIMIT 100 ; On execution, we get: + ----------------+-------------------+------------------+---------+ | customerID | Contract | MonthlyCharges | Churn | + ----------------+-------------------+------------------+---------+ | 7590 - VHVEG | Month - to - month | 29.85 | Yes | | 5575 - GNVDE | One year | 56.95 | No | | 3668 - QPYBK | Month - to - month | 53.85 | Yes | | 7795 - CFOCW | One year | 42.3 | No | | 9237 - HQITU | Month - to - month | 70.7 | Yes | + ----------------+-------------------+------------------+---------+ ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Install MindsDB locally via Docker or Docker Desktop . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Overview Home Rentals github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Data Setup Connecting the Data Understanding the Data Training a Predictor Status of a Predictor Making Predictions Making a Single Prediction Making Batch Predictions What’s Next?"}
{"file_name": "home-rentals.html", "content": "Predict Home Rental Prices with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation In-Database Machine Learning Predict Home Rental Prices with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning Overview Customer Churn Home Rentals AI Workflow Automation In-Database Machine Learning Predict Home Rental Prices with MindsDB In this tutorial, we’ll use a regression model to predict home rental prices. This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Connect a data source We will start by connecting a demo database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Let’s preview the data that will be used to train the model. SELECT * FROM example_db . home_rentals LIMIT 10 ; ​ Deploy and train an ML model Let’s create and train a machine learning model. For that we are going to use the CREATE MODEL statement, where we specify what query to train FROM and what we want to PREDICT . CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM home_rentals ) PREDICT rental_price ; It may take a couple of minutes for the training to complete. You can monitor the status of your model as below. DESCRIBE home_rentals_model ; ​ Make predictions Once the model’s status is complete, you can make predictions by querying the model. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; You can also make batch predictions by joining the data table with the model. SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . home_rentals as t JOIN mindsdb . home_rentals_model as m LIMIT 100 ; ​ Automate continuous improvement of the model Now, we can take this even further. MindsDB includes powerful automation features called Jobs which allow us to automate queries in MindsDB. This is very handy for production AI/ML systems which all require automation logic to help them to work. We use the CREATE JOB statement to create a Job. Now, let’s use a Job to retrain the model every two days, just like we might in production. You can retrain the model to improve predictions every time when either new data or new MindsDB version is available. And, if you want to retrain your model considering only new data, then go for finetuning it. CREATE JOB improve_model ( RETRAIN mindsdb . home_rentals_model FROM example_db ( SELECT * FROM home_rentals ) ) EVERY 2 days IF ( SELECT * FROM example_db . home_rentals WHERE created_at > LAST ) ; This job will execute every 2 days only if there is new data available in the home_rentals table. Learn more about the LAST keyword here. And there you have it! You created an end-to-end automated production ML system in a few short minutes. Was this page helpful? Yes No Suggest edits Raise issue Customer Churn Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a data source Deploy and train an ML model Make predictions Automate continuous improvement of the model"}
{"file_name": "recommenders.html", "content": "Recommender Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI-Powered Data Retrieval Recommender Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Overview Embedding Model Recommender Models Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation AI-Powered Data Retrieval Recommender Models Currently, there are two recommender models available in MindsDB. Check out examples here: Popularity Recommender , LightFM . Was this page helpful? Yes No Suggest edits Raise issue Embedding Model Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "embedding-model.html", "content": "Lightwood Embedding Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI-Powered Data Retrieval Lightwood Embedding Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Overview Embedding Model Recommender Models Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation AI-Powered Data Retrieval Lightwood Embedding Model This tutorial uses the Lightwood handler to create an embedding model. ​ Tutorial The following example shows how to create an embedding model using the Lightwood engine. Start by creating an engine from the Lightwood handler . CREATE ML_ENGINE lightwood FROM lightwood ; Verify that the engine has been created successfully. SHOW ML_ENGINES WHERE name = 'lightwood' ; Connect our sample database to use it for training the model: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Create a model using this engine: CREATE MODEL home_rentals_model_embeddings FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING problem_definition . embedding_only = True ; Check the status of the model. DESCRIBE home_rentals_model_embeddings ; Now you can use the model to predict home rental prices for specific criteria and get predictions in the form of embeddings. SELECT rental_price , rental_price_explain FROM home_rentals_model_embeddings WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 USING return_embedding = True ; Here is the output: + ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | [ 1 , 6.712956428527832 , 1.247057318687439 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 0 , 1 , 2.3025851249694824 , 0.5629426836967468 , 0 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0.7540000081062317 , 0.3333333432674408 , 0.35483869910240173 , 0.9583333134651184 , 0.7833333611488342 , 0.25 ] | { \"predicted_value\" : [ 1.0 , 6.712956428527832 , 1.247057318687439 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 1.0 , 2.3025851249694824 , 0.5629426836967468 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.0 , 0.7540000081062317 , 0.3333333432674408 , 0.35483869910240173 , 0.9583333134651184 , 0.7833333611488342 , 0.25 ] , \"confidence\" : null , \"anomaly\" : null , \"truth\" : null } | + ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Overview Recommender Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Tutorial"}
{"file_name": "chatbots_agents.html", "content": "Agents and Chatbots - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Agents and Chatbots Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Agents and Chatbots MindsDB provides a custom syntax to create an agent that comprises an AI model and data used to customize this model. Agents are connected with a chat interface to create chatbots. See details following this link for Agents and this link for Chatbots . Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create-chatbot-kb.html", "content": "Build a Chatbot with a Knowledge Base - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Build a Chatbot with a Knowledge Base Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Build a Chatbot with a Knowledge Base MindsDB provides the CREATE CHATBOT statement that lets you customize your chatbot with an AI model and a data source of your choice. Follow this tutorial to learn build a chatbot with a knowledge base. The CREATE CHATBOT statement requires the following components: Chat app : A connection to a chat app, such as Slack or MS Teams . AI agent : An AI agent that comes with an AI model trained with the provided training data. Learn more about AI agents here . Learn more about chatbots here . Let’s go over getting all the components ready. ​ Chatbot Components ​ Chat App Use the CREATE DATABASE statement to connect the chat app to MindsDB. If you want to use Slack, follow this link to setup a Slack app, generate required tokens, and connect it to MindsDB. If you want to use MS Teams, follow this link to generate required tokens and connect it to MindsDB. ​ AI Agent Start by creating and deploying the model. If you haven’t created a LangChain engine, use the CREATE ML_ENGINE statement, as explained here . CREATE MODEL my_model PREDICT answer USING engine = 'langchain' , input_column = 'question' , openai_api_key = 'your-model-api-key' , -- choose one of OpenAI (openai_api_key) or Anthropic (anthropic_api_key) model_name = 'gpt-4' , -- optional model name from OpenAI or Anthropic mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Here is the command to check its status: DESCRIBE my_model ; The status should read complete before proceeding. Next step is to create one or more skills for an AI agent. Here we create a knowledge base and assign it as a skill. In this example, let’s create an embedding model (you can choose one from OpenAI, Hugging Face, or LangChain) for the knowledge base. CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL embedding_model PREDICT embeddings USING engine = 'openai_engine' , mode = 'embedding' , model_name = 'text-embedding-ada-002' , question_column = 'content' ; Now let’s create a knowledge base that uses this embedding model and the default storage vector database (that is, ChromaDB). CREATE KNOWLEDGE BASE my_knowledge_base USING model = embedding_model ; This is how you can insert data into the knowledge base and select it. INSERT INTO my_knowledge_base ( content ) VALUES ( 'I drink tea.' ) ; SELECT * FROM my_knowledge_base ; Use this knowledge base to create a skill for an agent: CREATE SKILL kb_skill USING type = 'knowledge_base' , source = 'my_knowledge_base' , -- this must be created with CREATE KNOWLEDGE BASE description = 'My data' ; -- data description to help the agent know when to use the knowledge base This skill enables a model to answer questions about data from the knowledge base. Now let’s create an AI agent using the above model and skill. CREATE AGENT support_agent USING model = my_model , -- this was created with CREATE MODEL skills = [ 'kb_skill' ] ; -- this was created with CREATE SKILL ​ Create Chatbot Once all the components are ready, let’s proceed to creating the chatbot. CREATE CHATBOT my_chatbot USING database = 'chat_app' , -- this parameters stores a connection to a chat app, like Slack or MS Teams agent = 'support_agent' , -- this parameter stores an agent name, which was create with CREATE AGENT is_running = true ; -- this parameter is optional and set to true by default, meaning that the chatbot is running The database parameter stores connection to a chat app. And the agent parameter stores an AI agent created by passing a model and training data. You can query all chatbot using this query: SELECT * FROM chatbots ; Now you can go to Slack or MS Teams and chat with the chatbot created with MindsDB. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Chatbot Components Chat App AI Agent Create Chatbot"}
{"file_name": "create-chatbot.html", "content": "Build a Chatbot with a Text2SQL Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Build a Chatbot with a Text2SQL Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Build a Chatbot with a Text2SQL Skill MindsDB provides the CREATE CHATBOT statement that lets you customize your chatbot with an AI model and a data source of your choice. Follow this tutorial to learn build a chatbot with a Text2SQL skill. The CREATE CHATBOT statement requires the following components: Chat app : A connection to a chat app, such as Slack or MS Teams . AI agent : An AI agent that comes with an AI model trained with the provided training data. Learn more about AI agents here . Learn more about chatbots here . Let’s go over getting all the components ready. ​ Chatbot Components ​ Chat App Use the CREATE DATABASE statement to connect the chat app to MindsDB. If you want to use Slack, follow this link to setup a Slack app, generate required tokens, and connect it to MindsDB. If you want to use MS Teams, follow this link to generate required tokens and connect it to MindsDB. ​ AI Agent Start by creating and deploying the model. If you haven’t created a LangChain engine, use the CREATE ML_ENGINE statement, as explained here . CREATE MODEL my_model PREDICT answer USING engine = 'langchain' , input_column = 'question' , openai_api_key = 'your-model-api-key' , -- choose one of OpenAI (openai_api_key) or Anthropic (anthropic_api_key) model_name = 'gpt-4' , -- optional model name from OpenAI or Anthropic mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Here is the command to check its status: DESCRIBE my_model ; The status should read complete before proceeding. Next step is to create one or more skills for an AI agent. Here we create a Text2SQL skill. CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , -- this is a data source that must be connected to MindsDB with CREATE DATABASE statement tables = [ 'sales_data' ] , -- this table comes from the connected example_db data source description = \"Sales data that includes stores, sold products, and other sale details\" ; This skill enables a model to answer questions about data from the sales_data table. Now let’s create an AI agent using the above model and skill. CREATE AGENT support_agent USING model = 'my_model' , -- this was created with CREATE MODEL skills = [ 'text_to_sql_skill' ] ; -- this was created with CREATE SKILL ​ Create Chatbot Once all the components are ready, let’s proceed to creating the chatbot. CREATE CHATBOT my_chatbot USING database = 'chat_app' , -- this parameters stores a connection to a chat app, like Slack or MS Teams agent = 'support_agent' , -- this parameter stores an agent name, which was create with CREATE AGENT is_running = true ; -- this parameter is optional and set to true by default, meaning that the chatbot is running The database parameter stores connection to a chat app. And the agent parameter stores an AI agent created by passing a model and training data. You can query all chatbot using this query: SELECT * FROM chatbots ; Now you can go to Slack or MS Teams and chat with the chatbot created with MindsDB. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Chatbot Components Chat App AI Agent Create Chatbot"}
{"file_name": "build_ai_agents.html", "content": "Build an AI Agent with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Build an AI Agent with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents Overview Build an AI Agent AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation AI Agents Build an AI Agent with MindsDB MindsDB provides a custom syntax to build AI agents that comprises an AI model augmented with users’ data access. AI agents can be connected to a chat interface, like Slack or MS Teams, to create chatbots. See details following this link for Agents and this link for Chatbots . ​ Step-by-Step Tutorial This tutorial demonstrates how to build AI agents with MindsDB using MindsDB SQL editor. This can be also accomplished with APIs and Python SDK . Let’s list all the steps required to build an AI agent. 1 Create a conversational model Create a conversational model using the LangChain integration . 2 Create skills Create one or more skills to be assigned to an agent. Note that skills store data to be passed to an agent, so it is required to connect users’ data to MindsDB before creating skills. 3 Create an AI agent Create an AI agent providing the conversational model and the set of skills. 4 Create a chatbot Optionally, connect an agent to a chat interface to create a chatbot. The following sections walk you through the process of building an AI agent. ​ Step 1. Create a conversational model Use the CREATE MODEL statement below to create a conversational model. If required, adjust the parameters and prompts to fit your use case. CREATE MODEL conversational_model PREDICT answer USING engine = 'langchain' , openai_api_key = 'YOUR_OPENAI_API_KEY_HERE' , model_name = 'gpt-4' , mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Ensure that the model status reads complete using this command: DESCRIBE conversational_model ; Learn more about models created with LangChain . ​ Step 2. Create skills A skill is essentially users’ data fed to the model, so the model can answer questions over users’ data. First, connect your database to MindsDB. Here the sample database is used. CREATE DATABASE datasource WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Create a skill using the connected data. CREATE SKILL text2sql_skill USING type = 'text2sql' , database = 'datasource' , -- connect your database with CREATE DATABASE and pass its name here tables = [ 'car_sales' ] , -- list table(s) to be made accessible by an agent description = 'this is car sales data' ; Note that there are two types of skills: text-to-SQL and knowledge bases. Learn more about skills here . Verify that the skill has been created successully using this command: SHOW SKILLS ; ​ Step 3. Create an AI agent Now that both the conversational model and the skill are ready, let’s create an AI agent. CREATE AGENT ai_agent USING model = 'conversational_model' , skills = [ 'text2sql_skill' ] ; Verify that the agent has been created successully using this command: SHOW AGENTS ; At this point, you can query an agent to ask questions over the data. SELECT question , answer FROM ai_agent WHERE question = 'how many cars were sold in 2016?' ; ​ Step 4. Create a chatbot Optionally, you can create a chatbot by connecitng an AI agent to a chat interface. First connect a chat interface to MindsDB. Here the Slack connection is made. CREATE DATABASE mindsdb_slack WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-xxx\" , \"app_token\" : \"xapp-xxx\" } ; Follow the instructions on how to connect Slack to MindsDB for this use case. Now create a chatbot providing the AI agent and the Slack connection. CREATE CHATBOT ai_chatbot USING database = 'mindsdb_slack' , -- connect a chat interface with CREATE DATABASE agent = 'ai_agent' ; -- create an agent with with CREATE AGENT Verify that the chatbot is running using this command: SHOW CHATBOTS ; Now you can go ahead and chat with the AI agent via Slack. Was this page helpful? Yes No Suggest edits Raise issue Overview Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Step-by-Step Tutorial Step 1. Create a conversational model Step 2. Create skills Step 3. Create an AI agent Step 4. Create a chatbot"}
{"file_name": "text-summarization-inside-mongodb-with-openai.html", "content": "Text Summarization with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Text Summarization with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Text Summarization with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to summarize an article. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.articles.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d01398bbca62e9c7774ab8' , article: \"Video footage has emerged of a law enforcement officer…\" , highlights: 'The 53 -second video features…\" } { _id: '63d01398bbca62e9c7774ab9' , article: \"A new restaurant is offering a five-course…\" , highlights: \"The Curious Canine Kitchen is…\" } { _id: '63d01398bbca62e9c7774aba' , article: 'Mother-of-two Anna Tilley survived after spending four days…' , highlights: 'Experts have warned hospitals not using standard treatment…' } Let’s create a model collection to summarize all articles from the input dataset: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'text_summarization' , predict: 'highlights' , training_options: { engine: 'openai_engine' , prompt_template: 'provide an informative summary of the text text:{{article}} using full sentences' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called text_summarization that uses the OpenAI integration to predict a field named highlights . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'text_summarization' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.text_summarization.find ( { article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\" } ) Here is the output data: { highlights: \"Apple's Watch hits stores this Friday, and employees will be able to pre-order the\" , article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\" } Or by joining with a collection for batch predictions: mindsdb > db.text_summarization.find ( { 'collection' : 'mongo_demo_db.articles' } , { 'text_summarization.highlights' : 'highlights' , 'articles.article' : 'article' } ) .limit ( 3 ) Here is the output data: { highlights: 'A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was' , article: \"Video footage has emerged of a law enforcement officer...\" } { highlights: 'A new restaurant in London is offering a five-course drink-paired menu for dogs' , article: \"A new restaurant is offering a five-course...\" } { highlights: \"Sepsis is a potentially life-threatening condition that occurs when the body's response to an\" , article: 'Mother-of-two Anna Tilley survived after spending four days...' } The articles collection is used to make batch predictions. Upon joining the text_summarization model with the articles collection, the model uses all values from the article field. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Text Summarization using SQL Sentiment Analysis using SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "image-generator.html", "content": "Generating Images with OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Generating Images with OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Generating Images with OpenAI In this tutorial, we’ll generate images with the help of AI. You can build a Twitter chatbot that converts text prompts into images. Follow this blog post to see the total workflow. ​ Creating a Model Let’s create an OpenAI model. Follow this instruction to set up the OpenAI integration in MindsDB. Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL mindsdb . dalle PREDICT img_url USING engine = 'openai_engine' , mode = 'image' , prompt_template = '{{text}}, 8K | highly detailed realistic 3d oil painting style cyberpunk by MAD DOG JONES combined with Van Gogh | cinematic lighting | happy colors' ; This model connects to the OpenAI’s DALL-E engine for generating images. The {{text}} variable present in the prompt_template parameter is replaced with the user’s input. ​ Generating Images Now that the model is ready, we can generate some images. SELECT text , img_url FROM mindsdb . dalle WHERE text = 'a cute robot helping a little kid build a better world' ; On execution, we get: + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text | img_url | + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | a cute robot helping a little kid build a better world | https: //oaidalleapiprodscus.blob.core.windows.net/private/org-1QXk2w4H0OhDrF2Hd4QV5K2c/user-piMc91jPmVdhttPLHl2y50E7/img-p5NROyoV5ysWWUY91xhQUZdg.png?st=2023-05-29T16%3A45%3A02Z&se=2023-05-29T18%3A45%3A02Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-05-29T10%3A29%3A15Z&ske=2023-05-30T10%3A29%3A15Z&sks=b&skv=2021-08-06&sig=vUI9vedjWtA7L0J3V0/4c05Wzh1Kf2/zyl%2BN1yfK6rU%3D | + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ The model provides a link to the generated image. Let’s try another prompt. SELECT text , img_url FROM mindsdb . dalle WHERE text = 'design of a happy tree house' ; On execution, we get: + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text | img_url | + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | design of a happy tree house | https: //oaidalleapiprodscus.blob.core.windows.net/private/org-1QXk2w4H0OhDrF2Hd4QV5K2c/user-piMc91jPmVdhttPLHl2y50E7/img-O1GPmdmuoRXFTGdUjahOf1Ws.png?st=2023-05-29T16%3A50%3A34Z&se=2023-05-29T18%3A50%3A34Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-05-29T17%3A03%3A23Z&ske=2023-05-30T17%3A03%3A23Z&sks=b&skv=2021-08-06&sig=9vY%2Bqr/0CrzqqdM4uYEa/XwYXMBn67RBodhzsg%2Bs9ag%3D | + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Here is the generated image: Check out how to implement a Twitter chatbot that answers by generating images. Follow this link to learn more. Was this page helpful? Yes No Suggest edits Raise issue Overview Extract JSON github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Creating a Model Generating Images"}
{"file_name": "question-answering-inside-mysql-with-openai.html", "content": "Question Answering with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Question Answering with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Question Answering with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to answer questions in a specified domain. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . questions LIMIT 3 ; Here is the output: + ------------------+--------------------------------------------------------+-------------+ | article_title | question | true_answer | + ------------------+--------------------------------------------------------+-------------+ | Alessandro_Volta | Was Volta an Italian physicist? | yes | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | no | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | yes | + ------------------+--------------------------------------------------------+-------------+ Let’s create a model table to answer all questions from the input dataset: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL question_answering_model PREDICT answer USING engine = 'openai_engine' , prompt_template = 'answer the question of text:{{question}} about text:{{article_title}}' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called question_answering_model that uses the OpenAI integration to predict a column named answer . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE question_answering_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT article_title , question , answer FROM question_answering_model WHERE question = 'Was Abraham Lincoln the sixteenth President of the United States?' AND article_title = 'Abraham_Lincoln' ; Here is the output data: + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ | article_title | question | answer | + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ | Abraham_Lincoln | Was Abraham Lincoln the sixteenth President of the United States? | Yes , Abraham Lincoln was the sixteenth President of the United States . | + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ Or by joining with another table for batch predictions: SELECT input . article_title , input . question , output . answer FROM mysql_demo_db . questions AS input JOIN question_answering_model AS output LIMIT 3 ; Here is the output data: + ------------------+--------------------------------------------------------+--------------------------------------------------------+ | article_title | question | answer | + ------------------+--------------------------------------------------------+--------------------------------------------------------+ | Alessandro_Volta | Was Volta an Italian physicist? | Yes , Volta was an Italian physicist . | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | No , Volta is not buried in the city of Pittsburgh . | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | Yes , Volta had a passion for the study of electricity . | + ------------------+--------------------------------------------------------+--------------------------------------------------------+ The questions table is used to make batch predictions. Upon joining the question_answering_model model with the questions table, the model uses all values from the article_title and question columns. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Sentiment Analysis in MySQL Text Summarization in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Sentiment Analysis using MQL Question Answering using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "json-from-text.html", "content": "Extract JSON from Text - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Extract JSON from Text Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Extract JSON from Text ​ Extract JSON Values from JSON Data The json_extract() function extracts values from the JSON data passed as its argument. To show how it works, we use the home rentals example . The model returns the predicted value and the explanation of the prediction in the form of JSON data. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | rental_price | rental_price_explain | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 1580 | { \"predicted_value\" : 1580 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 1490 , \"confidence_upper_bound\" : 1670 } | + --------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Now, if we want to see the confidence value only, we can use the json_extract() function as below. SELECT rental_price , json_extract ( rental_price_explain , '$.confidence' ) AS confidence FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + --------------+------------+ | rental_price | confidence | + --------------+------------+ | 1580 | 0.99 | + --------------+------------+ ​ Extract JSON from Text Data In this example, we use the OpenAI model to extract data in a predefined JSON format from the input text data. Default Model When you create an OpenAI model in MindsDB, it uses the gpt-3.5-turbo model by default. But you can use the gpt-4 model as well by passing it to the model-name parameter. Let’s create an OpenAI model. SQL Mongo-QL Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL mindsdb . nlp_model PREDICT json USING engine = 'openai_engine' , json_struct = { 'rental_price' : 'rental price' , 'location' : 'location' , 'nob' : 'number of bathrooms' } , prompt_template = '{{sentence}}' ; We pass three parameters as follows: The engine parameter ensures we use the OpenAI engine. The json_struct parameter stores a predefined JSON structure used for the output. The prompt_template parameter contains the instruction passed to the model that may include variables such as {{sentence}} . Now we can query the model, passing the input text stored in the sentence column. SQL Mongo-QL SELECT json FROM mindsdb . nlp_model WHERE sentence = 'Amazing 3 bedroom apartment located at the heart of Manhattan, has one full bathrooms and one toilet room for just 3000 a month.' ; On execution, we get: + ----------------------------------------------------------+ | json | + ----------------------------------------------------------+ | { \"location\" : \"Manhattan\" , \"nob\" : \"1\" , \"rental_price\" : \"3000\" } | + ----------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Image Generator Text Summarization using SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Extract JSON Values from JSON Data Extract JSON from Text Data"}
{"file_name": "overview.html", "content": "Data Enrichment - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Data Enrichment Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Data Enrichment With MindsDB, you can easily enrich your data with AI-generated content. Process natural language, analyze text, create images, and more with various AI/ML models accessible through MindsDB. This section covers the following use cases: Natural Language Processing (NLP) Content generation QA-driven data enrichment Sentiment analysis Text summarization Available tutorials: Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Models Was this page helpful? Yes No Suggest edits Raise issue Recommender Models Image Generator github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "hugging-face-examples.html", "content": "Usage Examples of Hugging Face Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Usage Examples of Hugging Face Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Usage Examples of Hugging Face Models This document presents various use cases of Hugging Face models from MindsDB. ​ Spam Classifier Here is an example of a binary classification. The model determines whether a text string is spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ] ; Before querying for predictions, we should verify the status of the spam_classifier model. DESCRIBE spam_classifier ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0.9051626920700073 , 'ham' : 0.09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005. Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0.9380123615264893 , 'spam' : 0.061987683176994324 } | Nah I don't think he goes to usf , he lives around here though | | spam | { 'spam' : 0.9064534902572632 , 'ham' : 0.09354648739099503 } | WINNER ! ! As a valued network customer you have been selected to receive a £ 900 prize reward ! To claim call 09061701461. Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Sentiment Classifier Here is an example of a multi-value classification. The model determines the sentiment of a text string, where possible values are negative , neutral , and positive . CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text_short' , labels = [ 'negative' , 'neutral' , 'positive' ] ; Before querying for predictions, we should verify the status of the sentiment_classifier model. DESCRIBE sentiment_classifier ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'negative' , 'neutral' , 'positive' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | negative | { 'negative' : 0.9679920077323914 , 'neutral' : 0.02736542373895645 , 'positive' : 0.0046426113694906235 } | I hate tacos | | positive | { 'positive' : 0.7607280015945435 , 'neutral' : 0.2332666665315628 , 'negative' : 0.006005281116813421 } | I want to dance | | positive | { 'positive' : 0.9835041761398315 , 'neutral' : 0.014900505542755127 , 'negative' : 0.0015953202964738011 } | Baking is the best | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ ​ Zero-Shot Classifier Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text_short' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ] ; Before querying for predictions, we should verify the status of the zero_shot_tcd model. DESCRIBE zero_shot_tcd ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | cooking | { 'cooking' : 0.7530364990234375 , 'travel' : 0.1607145369052887 , 'dancing' : 0.08624900877475739 } | I hate tacos | | dancing | { 'dancing' : 0.9746809601783752 , 'travel' : 0.015539299696683884 , 'cooking' : 0.009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0.9936348795890808 , 'travel' : 0.0034196735359728336 , 'dancing' : 0.0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+ ​ Translation Here is an example of a translation. The model gets an input string in English and translates it into French. CREATE MODEL mindsdb . translator_en_fr PREDICT translated USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text_short' , lang_input = 'en' , lang_output = 'fr' ; Before querying for predictions, we should verify the status of the translator_en_fr model. DESCRIBE translator_en_fr ; On execution, we get: + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | translator_en_fr | mindsdb | complete | [ NULL ] | translated | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'translated' , 'using' : { 'engine' : 'huggingface' , 'task' : 'translation' , 'model_name' : 't5-base' , 'input_column' : 'text_short' , 'lang_input' : 'en' , 'lang_output' : 'fr' }} | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . translator_en_fr AS h ; On execution, we get: + -------------------------------+-------------------+ | translated | input_text | + -------------------------------+-------------------+ | Je déteste les tacos | I hate tacos | | Je veux danser | I want to dance | | La boulangerie est la meilleure | Baking is the best | + -------------------------------+-------------------+ ​ Summarization Here is an example of input text summarization. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; Before querying for predictions, we should verify the status of the summarizer_10_20 model. DESCRIBE summarizer_10_20 ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | A taco is a traditional Mexican food consisting of a small hand - sized corn - or | A taco is a traditional Mexican food consisting of a small hand - sized corn - or wheat - based tortilla topped with a filling . The tortilla is then folded around the filling and eaten by hand . A taco can be made with a variety of fillings , including beef , pork , chicken , seafood , beans , vegetables , and cheese , allowing for great versatility and variety . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value . [ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Fill Mask Here is an example of a masked language modeling task. CREATE MODEL mindsdb . fill_mask PREDICT text_filled USING engine = 'huggingface' , task = 'fill-mask' , model_name = 'bert-base-uncased' , input_column = 'text' ; Before querying for predictions, we should verify the status of the fill_mask model. DESCRIBE fill_mask ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | fill_mask | mindsdb | complete | [ NULL ] | text_filled | up_to_date | 23.3 .5 .0 | [ NULL ] | [ NULL ] | { 'target' : 'text_filled' , 'using' : { 'task' : 'fill-mask' , 'model_name' : 'bert-base-uncased' , 'input_column' : 'text' }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text AS input_text FROM demo . texts AS t JOIN mindsdb . fill_mask AS h ; On execution, we get: + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_filled | input_text | text_filled_explain | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | the food was great ! | The [ MASK ] was great ! | { 'the food was great!' : 0.16309359669685364 , 'the party was great!' : 0.06305009871721268 , 'the fun was great!' : 0.04633583873510361 , 'the show was great!' : 0.043319422751665115 , 'the music was great!' : 0.02990395948290825 } | | the weather is good today | The weather is [ MASK ] today | { 'the weather is good today' : 0.22563229501247406 , 'the weather is warm today' : 0.07954009622335434 , 'the weather is fine today' : 0.047255873680114746 , 'the weather is better today' : 0.034303560853004456 , 'the weather is mild today' : 0.03092862293124199 } | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Hugging Face + MindsDB Models Library ​ Text Classification ​ Spam Let’s create a model. CREATE MODEL mindsdb . hf_spam PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mariagrandury/roberta-base-finetuned-sms-spam-detection' , input_column = 'text' , labels = [ 'spam' , 'ham' ] ; And check its status. DESCRIBE hf_spam ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_spam WHERE text = 'I like you. I love you.' ; On execution, we get: + ----+--------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ----+--------------------------------------------------------+-----------------------+ | spam | { \"ham\" : 0.00020051795581821352 , \"spam\" : 0.9997995495796204 } | I like you . I love you . | + ----+--------------------------------------------------------+-----------------------+ ​ Sentiment Let’s create a model. CREATE MODEL mindsdb . hf_sentiment PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text' , labels = [ 'neg' , 'neu' , 'pos' ] ; And check its status. DESCRIBE hf_sentiment ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_sentiment WHERE text = 'I like you. I love you.' ; On execution, we get: + ----+--------------------------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ----+--------------------------------------------------------------------------------+-----------------------+ | pos | { \"neg\" : 0.003046575468033552 , \"neu\" : 0.021965451538562775 , \"pos\" : 0.9749879240989685 } | I like you . I love you . | + ----+--------------------------------------------------------------------------------+-----------------------+ ​ Sentiment (Finance) Let’s create a model. CREATE MODEL mindsdb . hf_sentiment_finance PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'ProsusAI/finbert' , input_column = 'text' ; And check its status. DESCRIBE hf_sentiment_finance ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_sentiment_finance WHERE text = 'Stocks rallied and the British pound gained.' ; On execution, we get: + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ | PRED | PRED_explain | text | + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ | positive | { \"negative\" : 0.0344734713435173 , \"neutral\" : 0.06716493517160416 , \"positive\" : 0.8983616232872009 } | Stocks rallied and the British pound gained . | + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ ​ Emotions (6) Let’s create a model. CREATE MODEL mindsdb . hf_emotions_6 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'j-hartmann/emotion-english-distilroberta-base' , input_column = 'text' ; And check its status. DESCRIBE hf_emotions_6 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_emotions_6 WHERE text = 'Oh Happy Day' ; On execution, we get: + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ | PRED | PRED_explain | text | + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ | joy | { \"anger\" : 0.0028446922078728676 , \"disgust\" : 0.0009613594156689942 , \"fear\" : 0.0007112706662155688 , \"joy\" : 0.7692911624908447 , \"neutral\" : 0.037753619253635406 , \"sadness\" : 0.015293814241886139 , \"surprise\" : 0.17314413189888 } | Oh Happy Day | + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ ​ Toxicity Let’s create a model. CREATE MODEL mindsdb . hf_toxicity PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'SkolkovoInstitute/roberta_toxicity_classifier' , input_column = 'text' ; And check its status. DESCRIBE hf_toxicity ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_toxicity WHERE text = 'I like you. I love you.' ; On execution, we get: + -------+-------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + -------+-------------------------------------------------------------+-----------------------+ | neutral | { \"neutral\" : 0.9999547004699707 , \"toxic\" : 0.00004535282641882077 } | I like you . I love you . | + -------+-------------------------------------------------------------+-----------------------+ ​ ESG (6) Let’s create a model. CREATE MODEL mindsdb . hf_esg_6 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'yiyanghkust/finbert-esg' , input_column = 'text' ; And check its status. DESCRIBE hf_esg_6 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_esg_6 WHERE text = 'Rhonda has been volunteering for several years for a variety of charitable community programs.' ; On execution, we get: + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ | Social | { \"Environmental\" : 0.0034267122391611338 , \"Governance\" : 0.004729956854134798 , \"None\" : 0.001239194767549634 , \"Social\" : 0.9906041026115417 } | Rhonda has been volunteering for several years for a variety of charitable community programs . | + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ ​ ESG (26) Let’s create a model. CREATE MODEL mindsdb . hf_esg_26 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'yiyanghkust/finbert-esg' , input_column = 'text' ; And check its status. DESCRIBE hf_esg_26 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_esg_26 WHERE text = 'We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the “DRC”) and adjoining countries (together, with the DRC, the “Covered Countries”), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.' ; On execution, we get: + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Social | { \"Environmental\" : 0.2031959593296051 , \"Governance\" : 0.08251894265413284 , \"None\" : 0.050893042236566544 , \"Social\" : 0.6633920073509216 } | We believe it is essential to establish validated conflict - free sources of 3 TG within the Democratic Republic of the Congo ( the “DRC” ) and adjoining countries ( together , with the DRC , the “Covered Countries” ) , so that these minerals can be procured in a way that contributes to economic growth and development in the region . To aid in this effort , we have established a conflict minerals policy and an internal team to implement the policy . | + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Hate Speech Let’s create a model. CREATE MODEL mindsdb . hf_hate PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain' , input_column = 'text' ; And check its status. DESCRIBE hf_hate ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_hate WHERE text = 'I like you. I love you.' ; On execution, we get: + ------+-----------------------------------------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ------+-----------------------------------------------------------------------------------------------+-----------------------+ | normal | { \"hate speech\" : 0.03551718592643738 , \"normal\" : 0.7747423648834229 , \"offensive\" : 0.18974047899246216 } | I like you . I love you . | + ------+-----------------------------------------------------------------------------------------------+-----------------------+ ​ Crypto Buy Signals Let’s create a model. CREATE MODEL mindsdb . hf_crypto PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'ElKulako/cryptobert' , input_column = 'text' ; And check its status. DESCRIBE hf_crypto ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_crypto WHERE text = 'BTC is killing it right now' ; On execution, we get: + -------+------------------------------------------------------------------------------------------+---------------------------+ | PRED | PRED_explain | text | + -------+------------------------------------------------------------------------------------------+---------------------------+ | Bullish | { \"Bearish\" : 0.0002816587220877409 , \"Bullish\" : 0.559426486492157 , \"Neutral\" : 0.4402918517589569 } | BTC is killing it right now | + -------+------------------------------------------------------------------------------------------+---------------------------+ ​ US Political Party Let’s create a model. CREATE MODEL mindsdb . hf_us_party PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'm-newhauser/distilbert-political-tweets' , input_column = 'text' ; And check its status. DESCRIBE hf_us_party ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_us_party WHERE text = 'This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions are uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.' ; On execution, we get: + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Democrat | { \"Democrat\" : 0.9999973773956299 , \"Republican\" : 0.00000261212517216336 } | This pandemic has shown us clearly the vulgarity of our healthcare system . Highest costs in the world , yet not enough nurses or doctors . Many millions are uninsured , while insurance company profits soar . The struggle continues . Healthcare is a human right . Medicare for all . | + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Question Detection Let’s create a model. CREATE MODEL mindsdb . hf_question PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'shahrukhx01/bert-mini-finetune-question-detection' , input_column = 'text' , labels = [ 'question' , 'query' ] ; And check its status. DESCRIBE hf_question ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_question WHERE text = 'Where can I buy electronics in London' ; On execution, we get: + -----+--------------------------------------------------------------+-------------------------------------+ | PRED | PRED_explain | text | + -----+--------------------------------------------------------------+-------------------------------------+ | query | { \"query\" : 0.9997773766517639 , \"question\" : 0.00022261829872149974 } | Where can I buy electronics in London | + -----+--------------------------------------------------------------+-------------------------------------+ ​ Industry Let’s create a model. CREATE MODEL mindsdb . hf_industry PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'sampathkethineedi/industry-classification' , input_column = 'text' ; And check its status. DESCRIBE hf_industry ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_industry WHERE text = 'Low latency is one of our best cloud features' ; On execution, we get: + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ | PRED | PRED_explain | text | + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ | Systems Software | { \"Advertising\" : 0.000006795735771447653 , \"Aerospace & Defense\" : 0.00001537964453746099 , \"Apparel Retail\" : 5.350161131900677 e - 7 , \"Apparel, Accessories & Luxury Goods\" : 0.000002604161181807285 , \"Application Software\" : 0.009111878462135792 , \"Asset Management & Custody Banks\" : 0.00003155150625389069 , \"Auto Parts & Equipment\" : 0.000015504940165556036 , \"Biotechnology\" : 6.533917940032552 e - 8 , \"Building Products\" : 7.348538133555849 e - 8 , \"Casinos & Gaming\" : 0.000013775999832432717 , \"Commodity Chemicals\" : 0.0000010432338513055583 , \"Communications Equipment\" : 0.000019887389498762786 , \"Construction & Engineering\" : 0.000001826199536480999 , \"Construction Machinery & Heavy Trucks\" : 0.000009827364920056425 , \"Consumer Finance\" : 0.0000018292046206624946 , \"Data Processing & Outsourced Services\" : 0.0000010666744856280275 , \"Diversified Metals & Mining\" : 0.000006960767223063158 , \"Diversified Support Services\" : 0.000016824227714096196 , \"Electric Utilities\" : 0.000003896044290740974 , \"Electrical Components & Equipment\" : 0.000001626394464437908 , \"Electronic Equipment & Instruments\" : 0.00003863943129545078 , \"Environmental & Facilities Services\" : 0.000736175337806344 , \"Gold\" : 0.00002220332135038916 , \"Health Care Equipment\" : 4.6927588925882446 e - 8 , \"Health Care Facilities\" : 7.432880124724761 e - 7 , \"Health Care Services\" : 6.929263918209472 e - 7 , \"Health Care Supplies\" : 2.1007431882935634 e - 7 , \"Health Care Technology\" : 0.000003907185146090342 , \"Homebuilding\" : 3.903339234057057 e - 7 , \"Hotels, Resorts & Cruise Lines\" : 6.0527639789143 e - 7 , \"Human Resource & Employment Services\" : 5.48697983049351 e - 7 , \"IT Consulting & Other Services\" : 0.0000723653138265945 , \"Industrial Machinery\" : 7.230253231682582 e - 7 , \"Integrated Telecommunication Services\" : 2.8266379104024963 e - 7 , \"Interactive Media & Services\" : 0.00003454017496551387 , \"Internet & Direct Marketing Retail\" : 0.000003871373337460682 , \"Internet Services & Infrastructure\" : 0.0007196652004495263 , \"Investment Banking & Brokerage\" : 0.0000040634336073708255 , \"Leisure Products\" : 0.000002158361439796863 , \"Life Sciences Tools & Services\" : 0.000002861268058040878 , \"Movies & Entertainment\" : 0.000007286199888767442 , \"Oil & Gas Equipment & Services\" : 0.000004376991455501411 , \"Oil & Gas Exploration & Production\" : 0.000005569149834627751 , \"Oil & Gas Refining & Marketing\" : 0.000012647416951949708 , \"Oil & Gas Storage & Transportation\" : 0.000005852583853993565 , \"Packaged Foods & Meats\" : 0.0000011130315442642313 , \"Personal Products\" : 0.00000970239307207521 , \"Pharmaceuticals\" : 0.0000037546726616710657 , \"Property & Casualty Insurance\" : 0.000006116194072092185 , \"Real Estate Operating Companies\" : 0.00001882187461887952 , \"Regional Banks\" : 0.0000011669454806906288 , \"Research & Consulting Services\" : 0.000024276219846797176 , \"Restaurants\" : 8.598511840318679 e - 7 , \"Semiconductors\" : 0.0000021006283077440457 , \"Specialty Chemicals\" : 0.000004160017397225602 , \"Specialty Stores\" : 2.644004553076229 e - 7 , \"Steel\" : 0.0000013566890402216814 , \"Systems Software\" : 0.9889177083969116 , \"Technology Distributors\" : 0.00001339179198112106 , \"Technology Hardware, Storage & Peripherals\" : 0.00004790363891515881 , \"Thrifts & Mortgage Finance\" : 3.924862141957419 e - 7 , \"Trading Companies & Distributors\" : 0.0000035233156268077437 } | Low latency is one of our best cloud features | + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ ​ Zero-Shot Classification ​ Bart Let’s create a model. CREATE MODEL mindsdb . hf_zs_bart PREDICT PRED USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text' , candidate_labels = [ 'Books' , 'Household' , 'Clothing & Accessories' , 'Electronics' ] ; And check its status. DESCRIBE hf_zs_bart ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_zs_bart WHERE text = 'Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints' ; On execution, we get: + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ | Household | { \"Books\" : 0.1876104772090912 , \"Clothing & Accessories\" : 0.08688066899776459 , \"Electronics\" : 0.14785148203372955 , \"Household\" : 0.5776574015617371 } | Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints | + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ ​ Translation ​ English to French (T5) Let’s create a model. CREATE MODEL mindsdb . hf_t5_en_fr PREDICT PRED USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text' , lang_input = 'en' , lang_output = 'fr' ; And check its status. DESCRIBE hf_t5_en_fr ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_t5_en_fr WHERE text = 'The monkey is on the branch' ; On execution, we get: + ---------------------------+---------------------------+ | PRED | text | + ---------------------------+---------------------------+ | Le singe est sur la branche | The monkey is on the branch | + ---------------------------+---------------------------+ ​ Summarization ​ Bart Let’s create a model. CREATE MODEL mindsdb . hf_bart_sum_20 PREDICT PRED USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text' , min_output_length = 5 , max_output_length = 20 ; And check its status. DESCRIBE hf_bart_sum_20 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_bart_sum_20 WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.' ; On execution, we get: + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | text | + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | The tower is 324 metres ( 1 , 063 ft ) tall , about the same | The tower is 324 metres ( 1 , 063 ft ) tall , about the same height as an 81 - storey building , and the tallest structure in Paris . Its base is square , measuring 125 metres ( 410 ft ) on each side . During its construction , the Eiffel Tower surpassed the Washington Monument to become the tallest man - made structure in the world , a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres . Due to the addition of a broadcasting aerial at the top of the tower in 1957 , it is now taller than the Chrysler Building by 5.2 metres ( 17 ft ) . Excluding transmitters , the Eiffel Tower is the second tallest free - standing structure in France after the Millau Viaduct . | + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Google Pegasus Let’s create a model. CREATE MODEL mindsdb . hf_peg_sum_20 PREDICT PRED USING engine = 'huggingface' , task = 'summarization' , model_name = 'google/pegasus-xsum' , input_column = 'text' , min_output_length = 5 , max_output_length = 20 ; And check its status. DESCRIBE hf_peg_sum_20 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_peg_sum_20 WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.' ; On execution, we get: + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | text | + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | The Eiffel Tower is a landmark in Paris , France . | The tower is 324 metres ( 1 , 063 ft ) tall , about the same height as an 81 - storey building , and the tallest structure in Paris . Its base is square , measuring 125 metres ( 410 ft ) on each side . During its construction , the Eiffel Tower surpassed the Washington Monument to become the tallest man - made structure in the world , a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres . Due to the addition of a broadcasting aerial at the top of the tower in 1957 , it is now taller than the Chrysler Building by 5.2 metres ( 17 ft ) . Excluding transmitters , the Eiffel Tower is the second tallest free - standing structure in France after the Millau Viaduct . | + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Text Sentiment with Hugging Face Hugging Face Inference API github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Spam Classifier Sentiment Classifier Zero-Shot Classifier Translation Summarization Fill Mask Hugging Face + MindsDB Models Library Text Classification Spam Sentiment Sentiment (Finance) Emotions (6) Toxicity ESG (6) ESG (26) Hate Speech Crypto Buy Signals US Political Party Question Detection Industry Zero-Shot Classification Bart Translation English to French (T5) Summarization Bart Google Pegasus"}
{"file_name": "sentiment-analysis-inside-mongodb-with-openai.html", "content": "Sentiment Analysis with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.amazon_reviews.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d013b5bbca62e9c7774b1d' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).' } { _id: '63d013b5bbca62e9c7774b1e' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\" } { _id: '63d013b5bbca62e9c7774b1f' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.' } Let’s create a model collection to identify sentiment for all reviews: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'sentiment_classifier' , predict: 'sentiment' , training_options: { engine: 'openai_engine' , prompt_template: 'describe the sentiment of the reviews strictly as \"positive\", \"neutral\", or \"negative\". \"I love the product\":positive \"It is a scam\":negative \"{{review}}.\":' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called sentiment_classifier that uses the OpenAI integration to predict a field named sentiment . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'sentiment_classifier' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.sentiment_classifier.find ( { review: 'It is ok.' } ) Here is the output data: { sentiment: 'neutral' , review: 'It is ok.' } Or by joining with a collection for batch predictions: mindsdb > db.sentiment_classifier.find ( { 'collection' : 'mongo_demo_db.amazon_reviews' } , { 'sentiment_classifier.sentiment' : 'sentiment' , 'amazon_reviews.review' : 'review' } ) .limit ( 3 ) Here is the output data: { sentiment: 'positive' , review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).' } { sentiment: 'positive' , review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\" } { sentiment: 'positive' , review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.' } The amazon_reviews collection is used to make batch predictions. Upon joining the sentiment_classifier model with the amazon_reviews collection, the model uses all values from the review field. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Sentiment Analysis using SQL Question Answering using SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "sentiment-analysis-inside-mysql-with-openai.html", "content": "Sentiment Analysis with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . amazon_reviews LIMIT 3 ; Here is the output: + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | product_name | review | + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | Late gift for my grandson . He is very happy with it . Easy for him ( 9 yo ) . | | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | I'm not super thrilled with the proprietary OS on this unit , but it does work okay and does what I n | | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren . They basic | + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ Let’s create a model table to identify sentiment for all reviews: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL sentiment_classifier_model PREDICT sentiment USING engine = 'openai_engine' , prompt_template = ' describe the sentiment of the reviews strictly as \"positive\" , \"neutral\" , or \"negative\" . \"I love the product\" :positive \"It is a scam\" :negative \"{{review}}.\" :' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called sentiment_classifier_model that uses the OpenAI integration to predict a column named sentiment . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE sentiment_classifier_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT review , sentiment FROM sentiment_classifier_model WHERE review = 'It is ok.' ; Here is the output data: + -----------+-----------+ | review | sentiment | + -----------+-----------+ | It is ok . | neutral | + -----------+-----------+ Or by joining with another table for batch predictions: SELECT input . review , output . sentiment FROM mysql_demo_db . amazon_reviews AS input JOIN sentiment_classifier_model AS output LIMIT 3 ; Here is the output data: + ------------------------------------------------------------------------------------------------------+-----------+ | review | sentiment | + ------------------------------------------------------------------------------------------------------+-----------+ | Late gift for my grandson . He is very happy with it . Easy for him ( 9 yo ) . | positive | | I'm not super thrilled with the proprietary OS on this unit , but it does work okay and does what I n | positive | | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren . They basic | positive | + ------------------------------------------------------------------------------------------------------+-----------+ The amazon_reviews table is used to make batch predictions. Upon joining the sentiment_classifier_model model with the amazon_reviews table, the model uses all values from the review column. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Question Answering in MySQL Text Summarization in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Text Summarization using MQL Sentiment Analysis using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "text-summarization-inside-mysql-with-openai.html", "content": "Text Summarization with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Text Summarization with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Text Summarization with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to summarize an article. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . articles LIMIT 3 ; Here is the output: + ----------------------------------------------------------------+--------------------------------------------------------------+ | article | highlights | + ----------------------------------------------------------------+--------------------------------------------------------------+ | Video footage has emerged of a law enforcement officer… | The 53 - second video features… | | A new restaurant is offering a five - course drink - paired menu… | The Curious Canine Kitchen is … | | Mother - of - two Anna Tilley survived after spending four days… | Experts have warned hospitals not using standard treatment… | + ----------------------------------------------------------------+--------------------------------------------------------------+ Let’s create a model table to summarize all articles from the input dataset: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL text_summarization_model PREDICT highlights USING engine = 'openai_engine' , prompt_template = 'provide an informative summary of the text text:{{article}} using full sentences' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called text_summarization_model that uses the OpenAI integration to predict a column named highlights . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE text_summarization_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT article , highlights FROM text_summarization_model WHERE article = \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre - order the timepiece . And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device . \" ; Here is the output data: + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ | article | highlights | + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ | Apple 's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device. | Apple' s Watch hits stores this Friday , and employees will be able to pre - order the | + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ Or by joining with another table for batch predictions: SELECT input . article , output . highlights FROM mysql_demo_db . articles AS input JOIN text_summarization_model AS output LIMIT 3 ; Here is the output data: + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ | article | highlights | + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ | Video footage has emerged of a law enforcement officer… | A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was | | A new restaurant is offering a five - course drink - paired menu… | A new restaurant in London is offering a five - course drink - paired menu for dogs | | Mother - of - two Anna Tilley survived after spending four days… | Sepsis is a potentially life - threatening condition that occurs when the body's response to an | + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ The articles table is used to make batch predictions. Upon joining the text_summarization_model model with the articles table, the model uses all values from the article column. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Sentiment Analysis in MySQL Question Answering in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Extract JSON Text Summarization using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "question-answering-inside-mongodb-with-openai.html", "content": "Question Answering with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Question Answering with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Question Answering with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to answer questions in a specified domain. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.questions.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d01350bbca62e9c77732c0' , article_title: 'Alessandro_Volta' , question: 'Was Volta an Italian physicist?' , true_answer: 'yes' } { _id: '63d01350bbca62e9c77732c1' , article_title: 'Alessandro_Volta' , question: 'Is Volta buried in the city of Pittsburgh?' , true_answer: 'no' } { _id: '63d01350bbca62e9c77732c2' , article_title: 'Alessandro_Volta' , question: 'Did Volta have a passion for the study of electricity?' , true_answer: 'yes' } Let’s create a model collection to answer all questions from the input dataset: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'question_answering' , predict: 'answer' , training_options: { engine: 'openai_engine' , prompt_template: 'answer the question of text:{{question}} about text:{{article_title}}' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called question_answering that uses the OpenAI integration to predict a field named answer . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'question_answering' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.question_answering.find ( { question: 'Was Abraham Lincoln the sixteenth President of the United States?' , article_title: 'Abraham_Lincoln' } ) Here is the output data: { answer: 'Yes, Abraham Lincoln was the sixteenth President of the United States.' , question: 'Was Abraham Lincoln the sixteenth President of the United States?' , article_title: 'Abraham_Lincoln' } Or by joining with a collection for batch predictions: mindsdb > db.question_answering.find ( { 'collection' : 'mongo_demo_db.questions' } , { 'question_answering.answer' : 'answer' , 'questions.question' : 'question' , 'questions.article_title' : 'article_title' } ) .limit ( 3 ) Here is the output data: { answer: 'Yes, Volta was an Italian physicist.' , question: 'Was Volta an Italian physicist?' , article_title: 'Alessandro_Volta' } { answer: 'No, Volta is not buried in the city of Pittsburgh.' , question: 'Is Volta buried in the city of Pittsburgh?' , article_title: 'Alessandro_Volta' } { answer: 'Yes, Volta had a passion for the study of electricity. He was fascinated by the' , question: 'Did Volta have a passion for the study of electricity?' , article_title: 'Alessandro_Volta' } The questions collection is used to make batch predictions. Upon joining the question_answering model with the questions collection, the model uses all values from the article_title and question fields. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Question Answering using SQL Text Sentiment with Hugging Face github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "text-sentiment-hf.html", "content": "Predict Text Sentiment with Hugging Face and MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Predict Text Sentiment with Hugging Face and MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Predict Text Sentiment with Hugging Face and MindsDB In this tutorial, we’ll use a model from the Hugging Faсe hub to predict text sentiment. ​ Connect a database We start by connecting a demo database using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Let’s preview the user_comments table. SELECT * FROM example_db . user_comments ; ​ Create a Hugging Face model Our Hugging Face integration automatically manages downloading and deploying of pre-trained transformers from Hugging Face’s hub. For example, we can download a transformer which has been trained to classify the sentiment of text. CREATE MODEL sentiment_classifier PREDICT sentiment USING engine = 'huggingface' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , task = 'text-classification' , input_column = 'comment' , labels = [ 'negative' , 'neutral' , 'positive' ] ; To create a model in MindsDB, we use the CREATE MODEL statement. Next, we define the target column using the PREDICT clause. Finally, we specify all required parameters in the USING clause. Once the above query is executed, we can check the status of the creation process: DESCRIBE sentiment_classifier ; ​ Make predictions Once the status is complete, the behavior is the same as with any other AI table you can query it and provide input data in the WHERE clause, like this: SELECT * FROM sentiment_classifier WHERE comment = 'It is really easy to do NLP with MindsDB' ; The above query should predict the comment as ‘positive’. We can also make batch predictions by joining the input data table with the model, like this: SELECT input . comment , model . sentiment FROM example_db . user_comments AS input JOIN sentiment_classifier AS model ; Was this page helpful? Yes No Suggest edits Raise issue Question Answering using MQL Hugging Face Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a database Create a Hugging Face model Make predictions"}
{"file_name": "hugging-face-inference-api-examples.html", "content": "Usage Examples of Hugging Face Models Through Inference API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Usage Examples of Hugging Face Models Through Inference API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Usage Examples of Hugging Face Models Through Inference API This document presents various use cases of Hugging Face models through Inference API from MindsDB. ​ Spam Classifier Here is an example of a binary classification. The model determines whether a text string is spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'hf_inference_api' , task = 'text-classification' , column = 'text_spammy' ; Before querying for predictions, we should verify the status of the spam_classifier model. DESCRIBE spam_classifier ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0.9051626920700073 , 'ham' : 0.09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005. Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0.9380123615264893 , 'spam' : 0.061987683176994324 } | Nah I don't think he goes to usf , he lives around here though | | spam | { 'spam' : 0.9064534902572632 , 'ham' : 0.09354648739099503 } | WINNER ! ! As a valued network customer you have been selected to receive a £ 900 prize reward ! To claim call 09061701461. Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Sentiment Classifier Here is an example of a multi-value classification. The model determines the sentiment of a text string, where possible values are negative , neutral , and positive . CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'hf_inference_api' , task = 'text-classification' , column = 'text_short' , labels = [ 'negative' , 'neutral' , 'positive' ] ; Before querying for predictions, we should verify the status of the sentiment_classifier model. DESCRIBE sentiment_classifier ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'negative' , 'neutral' , 'positive' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | negative | { 'negative' : 0.9679920077323914 , 'neutral' : 0.02736542373895645 , 'positive' : 0.0046426113694906235 } | I hate tacos | | positive | { 'positive' : 0.7607280015945435 , 'neutral' : 0.2332666665315628 , 'negative' : 0.006005281116813421 } | I want to dance | | positive | { 'positive' : 0.9835041761398315 , 'neutral' : 0.014900505542755127 , 'negative' : 0.0015953202964738011 } | Baking is the best | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ ​ Zero-Shot Classifier Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'hf_inference_api' , task = 'zero-shot-classification' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ] , column = 'text_short' ; Before querying for predictions, we should verify the status of the zero_shot_tcd model. DESCRIBE zero_shot_tcd ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | cooking | { 'cooking' : 0.7530364990234375 , 'travel' : 0.1607145369052887 , 'dancing' : 0.08624900877475739 } | I hate tacos | | dancing | { 'dancing' : 0.9746809601783752 , 'travel' : 0.015539299696683884 , 'cooking' : 0.009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0.9936348795890808 , 'travel' : 0.0034196735359728336 , 'dancing' : 0.0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+ ​ Summarization Here is an example of input text summarization. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'hf_inference_api' , task = 'summarization' , column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; Before querying for predictions, we should verify the status of the summarizer_10_20 model. DESCRIBE summarizer_10_20 ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | A taco is a traditional Mexican food consisting of a small hand - sized corn - or | A taco is a traditional Mexican food consisting of a small hand - sized corn - or wheat - based tortilla topped with a filling . The tortilla is then folded around the filling and eaten by hand . A taco can be made with a variety of fillings , including beef , pork , chicken , seafood , beans , vegetables , and cheese , allowing for great versatility and variety . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value . [ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Fill Mask Here is an example of a masked language modeling task. CREATE MODEL mindsdb . fill_mask PREDICT text_filled USING engine = 'hf_inference_api' , task = 'fill-mask' , column = 'text' ; Before querying for predictions, we should verify the status of the fill_mask model. DESCRIBE fill_mask ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | fill_mask | mindsdb | complete | [ NULL ] | text_filled | up_to_date | 23.3 .5 .0 | [ NULL ] | [ NULL ] | { 'target' : 'text_filled' , 'using' : { 'task' : 'fill-mask' , 'model_name' : 'bert-base-uncased' , 'input_column' : 'text' }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text AS input_text FROM demo . texts AS t JOIN mindsdb . fill_mask AS h ; On execution, we get: + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_filled | input_text | text_filled_explain | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | the food was great ! | The [ MASK ] was great ! | { 'the food was great!' : 0.16309359669685364 , 'the party was great!' : 0.06305009871721268 , 'the fun was great!' : 0.04633583873510361 , 'the show was great!' : 0.043319422751665115 , 'the music was great!' : 0.02990395948290825 } | | the weather is good today | The weather is [ MASK ] today | { 'the weather is good today' : 0.22563229501247406 , 'the weather is warm today' : 0.07954009622335434 , 'the weather is fine today' : 0.047255873680114746 , 'the weather is better today' : 0.034303560853004456 , 'the weather is mild today' : 0.03092862293124199 } | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Hugging Face Models Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Spam Classifier Sentiment Classifier Zero-Shot Classifier Summarization Fill Mask"}
{"file_name": "expenditures-statsforecast.html", "content": "Forecast Monthly Expenditures with Nixtla's StatsForecast and MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictive Analytics Forecast Monthly Expenditures with Nixtla's StatsForecast and MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics Overview Quarterly House Sales Forecast Monthly Expenditures Brain Activity In-Database Machine Learning AI Workflow Automation Predictive Analytics Forecast Monthly Expenditures with Nixtla's StatsForecast and MindsDB In this tutorial, we’ll create a model to forecast expenditures based on historical data using the Nixtla’s StatsForecast engine. ​ Connect a database We use a table from our MySQL public demo database, so let’s start by connecting it to MindsDB. CREATE DATABASE mysql_historical WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_historical . historical_expenditures LIMIT 3 ; ​ Deploy a time-series model Please note that before using the StatsForecast engine, you should create it with the below command: CREATE ML_ENGINE statsforecast FROM statsforecast ; You can check the available engines with this command: SHOW ML_ENGINES ; Let’s create a model table to forecast the expenditures: CREATE MODEL quarterly_expenditure_forecaster FROM mysql_historical ( SELECT * FROM historical_expenditures ) PREDICT expenditure ORDER BY month GROUP BY category WINDOW 12 HORIZON 3 USING ENGINE = 'statsforecast' ; We can check the training status with the following query: DESCRIBE quarterly_expenditure_forecaster ; ​ Make predictions Once the model status is complete, the behavior is the same as with any other AI table – you can query for batch predictions by joining it with a data table. SELECT m . month as month , m . expenditure as forecasted FROM mindsdb . quarterly_expenditure_forecaster as m JOIN mysql_historical . historical_expenditures as t WHERE t . month > LATEST AND t . category = 'food' ; The historical_expenditures table is used to make batch predictions. Upon joining the quarterly_expenditure_forecaster model with the historical_expenditures table, we get predictions for the next quarter as defined by the HORIZON 3 clause. MindsDB provides the LATEST keyword that marks the latest training data point. In the WHERE clause, we specify the month > LATEST condition to ensure the predictions are made for data after the latest training data point. If we train the model using data from January 2020 until December 2020 (as defined by WINDOW 12 ), then the predictions come for the first quarter of 2021 (as defined by HORIZON 3 ). Was this page helpful? Yes No Suggest edits Raise issue Quarterly House Sales Brain Activity github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a database Deploy a time-series model Make predictions"}
{"file_name": "eeg-forecasting.html", "content": "Forecasting Eye State from Electroencephalogram Readings with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictive Analytics Forecasting Eye State from Electroencephalogram Readings with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics Overview Quarterly House Sales Forecast Monthly Expenditures Brain Activity In-Database Machine Learning AI Workflow Automation Predictive Analytics Forecasting Eye State from Electroencephalogram Readings with MindsDB This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Introduction In this tutorial, we’ll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we’ll produce categorical forecasts for a multivariate time series. Install MindsDB locally via Docker or Docker Desktop . Let’s get started. ​ Data Setup ​ Connecting the Data There are a couple of ways you can get the data to follow through with this tutorial. ​ Connecting as a database You can connect to a demo database that we’ve prepared for you. It contains the data used throughout this tutorial (the example_db.demo_data.eeg_eye table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let’s preview the data that we’ll use to train our predictor. SELECT * FROM example_db . demo_data . eeg_eye LIMIT 10 ; ​ Connecting as a file The dataset we use in this tutorial is the UCI’s EEG Eye State dataset. You can download it here in the ARFF format that should be converted to the CSV format before uploading it via MindsDB SQL Editor . Follow this guide to find out how to upload a file to MindsDB. Now you can run queries directly on the file as if it were a table. Let’s preview the data that we’ll use to train our predictor. SELECT * FROM files . eeg_eye LIMIT 10 ; Pay Attention to the Queries” From now on, we’ll use the files.eeg_eye file as a table. Make sure you replace it with example_db.demo_data.eeg_eye if you connect the data as a database. ​ Understanding the Data We use the UCI’s EEG Eye State dataset, where each row contains data of one electroencephalogram (EEG) reading plus the current state of the patient’s eye, where 0 indicates open eye and 1 indicates closed eye. We want to know ahead of time when the eye state will change, so we predict the eyeDetection column. Below is the sample data stored in the files.eeg_eye table. + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | AF3 | F7 | F3 | FC5 | T7 | P7 | O1 | O2 | P8 | T8 | FC6 | F4 | F8 | AF4 | eyeDetection | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ | 4329.23 | 4009.23 | 4289.23 | 4148.21 | 4350.26 | 4586.15 | 4096.92 | 4641.03 | 4222.05 | 4238.46 | 4211.28 | 4280.51 | 4635.9 | 4393.85 | 0 | | 4324.62 | 4004.62 | 4293.85 | 4148.72 | 4342.05 | 4586.67 | 4097.44 | 4638.97 | 4210.77 | 4226.67 | 4207.69 | 4279.49 | 4632.82 | 4384.1 | 0 | | 4327.69 | 4006.67 | 4295.38 | 4156.41 | 4336.92 | 4583.59 | 4096.92 | 4630.26 | 4207.69 | 4222.05 | 4206.67 | 4282.05 | 4628.72 | 4389.23 | 0 | | 4328.72 | 4011.79 | 4296.41 | 4155.9 | 4343.59 | 4582.56 | 4097.44 | 4630.77 | 4217.44 | 4235.38 | 4210.77 | 4287.69 | 4632.31 | 4396.41 | 0 | | 4326.15 | 4011.79 | 4292.31 | 4151.28 | 4347.69 | 4586.67 | 4095.9 | 4627.69 | 4210.77 | 4244.1 | 4212.82 | 4288.21 | 4632.82 | 4398.46 | 0 | + ----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+----------+--------------+ Where: Column Description Data Type Usage AF3 , F7 , F3 , FC5 , T7 , P7 , O1 , O2 , P8 , T8 , FC6 , F4 , F8 , AF4 The EEG measurement data. float Feature eyeDetectin The state of the patient’s eye where 0 indicates open eye and 1 indicates closed eye. binary Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). ​ Training a Predictor Let’s create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). The eyeDetection column is our target variable. The interesting thing about this example is that we aim to forecast labels that are not strictly numerical. Even though this example is simple (because the variable is a binary category), this can easily be generalized to more than two categories. We order the measurements by the Timestamps column that shows readings frequency of approximately 8 milliseconds. CREATE MODEL mindsdb . eeg_eye_forecast FROM example_db ( SELECT * FROM demo_data . eeg_eye ) PREDICT eyedetection ORDER BY timestamps WINDOW 50 HORIZON 10 ; As the sampling frequency is 8 ms, this predictor is trained using a historical context of roughly 400 ms ( (50 * 8) = 400 [ms] ) to predict the following 80 ms ( (10 * 8) = 80 [ms] ). ​ Status of a Predictor A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: DESCRIBE eeg_eye_forecast ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! ​ Making Predictions You can make predictions by querying the predictor joined with the data table. The SELECT statement lets you make predictions for the label based on the chosen features for a given time period. Usually, you want to know what happens right after the latest training data point that was fed. We have a special keyword for that, the LATEST keyword. Let’s run a query to get predictions for the next HORIZON timesteps into the future, which in this case is roughly 80 milliseconds. SELECT m . timestamps , m . eyedetection FROM example_db . demo_data . eeg_eye as t JOIN mindsdb . eeg_eye_forecast as m WHERE t . timestamps > LATEST LIMIT 10 ; On execution, we get: + ----------------------------+--------------+ | timestamps | eyedetection | + ----------------------------+--------------+ | 2001 - 09 - 03 08 : 01 : 57.000000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.008000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.016000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.024000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.032000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.040000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.048000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.056000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.064000 | 1 | | 2001 - 09 - 03 08 : 01 : 57.072000 | 1 | + ----------------------------+--------------+ That’s it. We can now JOIN any set of WINDOW rows worth of measurements with this predictor, and forecasts will be emitted to help us expect a change in the state of the patient’s eye based on the EEG readings. ​ Alternate Problem Framings It is also possible to reframe this task as a normal forecasting scenario where the variable is numeric. There are a few options here. It boils down to what the broader scenario is and what format would maximize the value of any specific prediction. For example, a simple mapping of eye is open to 0 and eye is closed to 1 would be enough to replicate the above behavior. We could also explore other options. With some data transformations on the data layer, we could get a countdown to the next change in state, effectively predicting a date if we cast this back into the timestamp domain. ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Install MindsDB locally via Docker or Docker Desktop . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Forecast Monthly Expenditures Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Data Setup Connecting the Data Connecting as a database Connecting as a file Understanding the Data Training a Predictor Status of a Predictor Making Predictions Alternate Problem Framings What’s Next?"}
{"file_name": "house-sales-forecasting.html", "content": "Forecast Quarterly House Sales with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictive Analytics Forecast Quarterly House Sales with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics Overview Quarterly House Sales Forecast Monthly Expenditures Brain Activity In-Database Machine Learning AI Workflow Automation Predictive Analytics Forecast Quarterly House Sales with MindsDB In this tutorial, we’ll use a time-series model to forecast quarterly house sales. This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Connect a data source We start by connecting a demo database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Let’s preview the data that will be used to train the model. SELECT * FROM example_db . house_sales LIMIT 10 ; ​ Deploy and train an ML model Now, lets specify that we want to forecast the ma column, which is a moving average of the historical median price for house sales. Looking at the data, you can see several entries for the same date, which depend on two factors: how many bedrooms the properties have, and whether properties are “houses” or “units”. This means that we can have up to ten different groupings here. Let’s look at the data for one of them. SELECT saledate , ma , type , bedrooms FROM example_db . house_sales WHERE type = 'house' AND bedrooms = 3 ; We want to generate forecasts to predict the behavior of this and the other series for the next year. MindsDB makes it simple so that we don’t need to repeat the predictor creation process for every group there is. Instead, we can just group for both columns and the predictor will learn from all series and enable all forecasts. We are going to use the CREATE MODEL statement, where we specify what data to train FROM and what we want to PREDICT . CREATE MODEL mindsdb . house_sales_model FROM example_db ( SELECT * FROM house_sales ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type -- as the data is quarterly, we will look back two years to forecast the next one year WINDOW 8 HORIZON 4 ; You can check the status of the model as below: DESCRIBE house_sales_model ; ​ Make predictions Once the model’s status is complete, you can query it as a table to get forecasts for a given period of time. Usually, you’ll want to know what happens right after the latest training data point that was fed, for which we have a special bit of syntax, the LATEST keyword. SELECT m . saledate as date , m . ma as forecast FROM mindsdb . house_sales_model as m JOIN example_db . house_sales as t WHERE t . saledate > LATEST AND t . type = 'house' AND t . bedrooms = 2 LIMIT 4 ; Now, try changing the value of type and bedrooms columns and check how the forecast varies. This is because MindsDB recognizes each grouping as being its own different time series. ​ Automate continuous improvement of the model Now, we can take this even further. MindsDB includes powerful automation features called Jobs which allow us to automate queries in MindsDB. This is very handy for production AI/ML systems which all require automation logic to help them to work. We use the CREATE JOB statement to create a Job. Now, let’s use a Job to retrain the model every two days, just like we might in production. You can retrain the model to improve predictions every time when either new data or new MindsDB version is available. And, if you want to retrain your model considering only new data, then go for finetuning it. CREATE JOB retrain_model_and_save_predictions ( RETRAIN mindsdb . house_sales_model FROM example_db ( SELECT * FROM house_sales ) ) EVERY 2 days IF ( SELECT * FROM example_db . house_sales WHERE created_at > LAST ) ; This job will execute every 2 days only if there is new data available in the house_sales table. Learn more about the LAST keyword here. And there you have it! You created an end-to-end automated production ML system in a few short minutes. Was this page helpful? Yes No Suggest edits Raise issue Overview Forecast Monthly Expenditures github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a data source Deploy and train an ML model Make predictions Automate continuous improvement of the model"}
{"file_name": "slack-chatbot.html", "content": "Build a Slack Chatbot with MindsDB and OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Build a Slack Chatbot with MindsDB and OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Build a Slack Chatbot with MindsDB and OpenAI The objective of this tutorial is to create an AI-powered personalized chatbot by utilizing the MindsDB’s Slack connector, and combining it with OpenAI’s GPT-4 Model. To illustrate practically, we will create a Slack bot - @Whiz_Fizz - which will reply to the user’s queries with proper context and with a unique persona while responding. It is a weird magician 🪄 and a Space Science Expert! Let’s see how it responds. Before jumping more into it. Let’s first see how to create a bot and connect it to our Slack Workspace. ​ Getting Started Install MindsDB locally via Docker or Docker Desktop Create a Slack Account and follow this instruction to connect Slack to MindsDB. Go to your MindsDB Editor ​ Usage This query will create a database called mindsdb_slack that comes with the channels table. CREATE DATABASE mindsdb_slack WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" } ; Here is how to retrieve the 10 messages after specific timestamp: SELECT * FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" AND created_at > '2023-07-25 00:13:07' -- created_at stores the timestamp when the message was created LIMIT 10 ; You can also retrieve messages in alphabetical order: SELECT * FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" ORDER BY text ASC LIMIT 5 ; By default, it retrieves by the order the messages were sent, unless specified as ascending/descending. Here is how to post messages: INSERT INTO mindsdb_slack . messages ( channel_id , text ) VALUES ( \"<channel-id>\" , \"Hey MindsDB, Thanks to you! Now I can respond to my Slack messages through SQL Queries. 🚀 \" ) , ( \"<channel-id>\" , \"It's never been that easy to build ML apps using MindsDB!\" ) ; Whoops! Sent it by mistake? No worries! Use this to delete a specific message: DELETE FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" AND ts = \"1688863707.197229\" ; Now, let’s roll up our sleeves and start building the GPT-4 Model together. ​ 1. Crafting the GPT-4 Model: Generating a Machine Learning model with MindsDB feels like taking a thrilling elevator ride in Burj Khalifa (You don’t realize, that you made it)! Here gpt_model represents our GPT-4 Model. Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL mindsdb . gpt_model PREDICT response USING engine = 'openai_engine' , max_tokens = 300 , model_name = 'gpt-4' , prompt_template = ' From input message: {{ text }}\\ write a short response to the user in the following format:\\ Hi , I am an automated bot here to help you , Can you please elaborate the issue which you are facing ! ✨🚀 ' ; The critical attribute here is prompt_template where we tell the GPT model how to respond to the questions asked by the user. Let’s see how it works: SELECT text , response FROM mindsdb . gpt_model WHERE text = 'Hi, can you please explain me more about MindsDB?' ; ​ 2. Feeding Personality into Our Model Alright, so the old model’s replies were good . But hey, we can use some prompt template tricks to make it respond the way we want. Let’s do some Prompt Engineering. Now, let’s make a model called whizfizz_model with a prompt template that gives GPT a wild personality that eludes a playful and magical aura. Imagine scientific knowledge with whimsical storytelling to create a unique and enchanting experience. We’ll call him WhizFizz : CREATE MODEL mindsdb . whizfizz_model PREDICT response USING engine = 'openai_engine' , max_tokens = 300 , model_name = 'gpt-4' , prompt_template = ' From input message: {{ text }}\\ write a short response in less than 40 words to some user in the following format:\\ Hi there , WhizFizz here ! < respond with a mind blowing fact about Space and describe the response using cosmic and scientific analogies , where wonders persist . In between quote some hilarious appropriate raps statements based on the context of the question answer as if you are a Physics Space Mad Scientist who relates everythign to the Universe and its strange theories . So lets embark on a journey , where science and magic intertwine . Stay tuned for more enchantment ! ✨🚀 -- mdb.ai/bot by @mindsdb'; Let’s test this in action: SELECT text , response FROM mindsdb . whizfizz_model WHERE text = 'Hi, can you please explain me more about MindsDB?' ; You see the difference! Now, I’m getting excited, let’s try again. SELECT text , response FROM mindsdb . whizfizz_model WHERE text = 'if a time-traveling astronaut had a dance-off with a black hole, what mind-bending moves would they showcase, and how would gravity groove to the rhythm?!' ; ​ 3. Let’s Connect our GPT Model to Slack! The messages table can be used to search for channels , messages , and timestamps , as well as to post messages into Slack conversations. These functionalities can also be done by using Slack API or Webhooks. Let’s query the user’s question and see how our GPT model responds to it, by joining the model with the messages table: SELECT t . channel_id as channel_id , t . text as input_text , r . response as output_text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" LIMIT 3 ; ​ 4. Posting Messages using SQL We want to respond to the user’s questions by posting the output of our newly created WhizFizz Model. Let’s post the message by querying and joining the user’s questions to our model: INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , r . response as text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" LIMIT 3 ; Works like a charm!! ​ 5. Let’s automate this We will CREATE JOB to schedule periodical execution of SQL statements. The job will execute every hour and do the following: Check for new messages using the LAST keyword . Generate an appropriate response with the whizfizz_model model. Insert the response into the channel. Let’s do it in single SQL statement: CREATE JOB mindsdb . gpt4_slack_job AS ( -- insert into channels the output of joining model and new responses INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , r . response as text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" AND t . created_at > LAST AND t . user = 'user_id' -- to avoid the bot replying to its own messages, include users to which bot should reply --AND t.user != 'bot_id' -- alternatively, to avoid the bot replying to its own messages, exclude the user id of the bot ) EVERY hour ; The LAST keyword is used to ensure the query fetches only the newly added messages. Learn more here . That sums up the tutorial! Here it will continually check for new messages posted in the channel and will respond to all newly added messages providing responses generated by OpenAI’s GPT model in the style of WhizFizz. To check the jobs and jobs_history , we can use the following: SHOW JOBS WHERE name = 'gpt4_slack_job' ; SELECT * FROM mindsdb . jobs WHERE name = 'gpt4_slack_job' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'gpt4_slack_job' ; To stop the scheduled job, we can use the following: DROP JOB gpt4_slack_job ; Alternatively, you can create a trigger on Slack, instead of scheduling a job. This way, every time new messages are posted, the trigger executes. CREATE TRIGGER slack_trigger ON mindsdb_slack . messages ( INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , a . sentiment as text , FROM data_table t JOIN model_table as a WHERE t . channel_id = '<channel-id>' AND t . user != 'bot_id' -- exclude bot ) ; What’s next? Check out How to Generate Images using OpenAI with MindsDB to see another interesting use case of OpenAI integration. Was this page helpful? Yes No Suggest edits Raise issue Overview Twitter Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Getting Started Usage 1. Crafting the GPT-4 Model: 2. Feeding Personality into Our Model 3. Let’s Connect our GPT Model to Slack! 4. Posting Messages using SQL 5. Let’s automate this"}
{"file_name": "customer-reviews-notifications.html", "content": "Automate notifications about incoming customer reviews - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Automate notifications about incoming customer reviews Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Automate notifications about incoming customer reviews This tutorial presents how to chain OpenAI models within MindsDB to analyze text sentiment and generate responses, which will be sent in the form of Slack notifications. ​ Data setup Connect your database to MindsDB. CREATE DATABASE local_postgres WITH ENGINE = 'postgres' , PARAMETERS = { \"host\" : \"4.tcp.eu.ngrok.io\" , \"port\" : 12888 , \"database\" : \"postgres\" , \"user\" : \"postgres\" , \"password\" : \"password\" } ; Query the input data table. SELECT * FROM local_postgres . demo . amazon_reviews ; + ----------------------------+-----------------------------+------------------------+ | created_at | product_name | review | + ----------------------------+-----------------------------+------------------------+ | 2023 - 11 - 08 17 : 23 : 21.028485 | Power Adapter | It is a great product . | | 2023 - 11 - 08 17 : 23 : 21.028485 | Bluetooth and Wi - Fi Speaker | It is ok . | | 2023 - 11 - 08 17 : 23 : 21.028485 | Kindle eReader | It doesn’t work . | + ----------------------------+-----------------------------+------------------------+ ​ Model 1 setup Configure an AI engine, providing the OpenAI API key. CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'sk-xxx' ; Deploy a model using this AI engine. CREATE MODEL sentiment_classifier PREDICT sentiment USING engine = 'openai_engine' , model_name = 'gpt-4' , prompt_template = ' describe the sentiment of the reviews strictly as \"positive\" , \"neutral\" , or \"negative\" . \"I love the product\" :positive \"It is a scam\" :negative \"{{review}}.\" :' ; Check its status. DESCRIBE sentiment_classifier ; ​ Predictions from Model 1 You can make a single predictions, providing input to the mode in the WHERE clause. SELECT review , sentiment FROM sentiment_classifier WHERE review = 'It is ok.' ; Or, make batch predictions, joining the data table with the model. SELECT input . review , output . sentiment FROM local_postgres . demo . amazon_reviews AS input JOIN sentiment_classifier AS output ; ​ Automation The alert system will send notification to Slack. Here is how to connect Slack to MindsDB . CREATE DATABASE customer_reviews_slack_app WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-xxx\" } ; Send a test message to test the connection. INSERT INTO customer_reviews_slack_app . messages ( channel_id , text ) VALUES ( \"customer-reviews-channel-id\" , \"Testing Slack connection\" ) ; Create a job to send notification every time a negative review is received. CREATE JOB customer_reviews_notifications ( INSERT INTO customer_reviews_slack_app . messages ( channel_id , text ) SELECT \"customer-reviews-channel-id\" as channel_id , concat ( 'Product: ' , input . product_name , chr ( 10 ) , 'Received negative review at: ' , input . created_at , chr ( 10 ) , 'Review: ' , input . review ) as text FROM local_postgres . demo . amazon_reviews AS input JOIN sentiment_classifier AS output WHERE input . created_at > LAST AND output . sentiment = 'negative' ) EVERY 1 minute ; These commands are used to monitor the job. SHOW JOBS WHERE name = 'customer_reviews_notifications' ; SELECT * FROM mindsdb . jobs WHERE name = 'customer_reviews_notifications' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'customer_reviews_notifications' ; Use this command to disable the job. DROP JOB customer_reviews_notifications ; ​ Model 2 setup Deploy a model using the AI engine created earlier. CREATE MODEL response_model PREDICT response USING engine = 'openai_engine' , model_name = 'gpt-4' , prompt_template = 'briefly respond to the customer review: {{review}}' ; Check its status. DESCRIBE response_model ; ​ Predictions from Model 2 You can make a single predictions, providing input to the mode in the WHERE clause. SELECT review , response FROM response_model WHERE review = 'It is ok.' ; Or, make batch predictions, joining the data table with the model. SELECT input . review , output . response FROM local_postgres . demo . amazon_reviews AS input JOIN response_model AS output ; ​ Automation and Chaining Models 1 & 2 Create a job to send notification, including a sample response, every time a positive review is received. CREATE JOB customer_reviews_and_responses_notifications ( INSERT INTO customer_reviews_slack_app . messages ( channel_id , text ) SELECT \"customer-reviews-channel-id\" as channel_id , concat ( '---------' , chr ( 10 ) , 'Product: ' , input . product_name , chr ( 10 ) , 'Received ' , input . sentiment , ' review at: ' , input . created_at , chr ( 10 ) , 'Review: ' , input . review , chr ( 10 ) , 'Sample response: ' , output . response ) as text FROM ( SELECT inp . created_at AS created_at , inp . product_name AS product_name , inp . review AS review , outp . sentiment AS sentiment FROM local_postgres . demo . amazon_reviews AS inp JOIN sentiment_classifier AS outp WHERE inp . created_at > LAST ) AS input --'2023-10-03 16:50:00' AND inp.created_at > \"{{PREVIOUS_START_DATETIME}}\" JOIN response_model AS output WHERE input . sentiment = 'positive' ; ) EVERY 1 minute ; These commands are used to monitor the job. SELECT * FROM mindsdb . jobs WHERE name = 'customer_reviews_and_responses_notifications' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'customer_reviews_and_responses_notifications' ; Use this command to disable the job. DROP JOB customer_reviews_and_responses_notifications ; Was this page helpful? Yes No Suggest edits Raise issue Twilio Chatbot Real-time Trading Forecasts github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Data setup Model 1 setup Predictions from Model 1 Automation Model 2 setup Predictions from Model 2 Automation and Chaining Models 1 & 2"}
{"file_name": "microsoft-onedrive.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "microsoft-teams.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "newsapi.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "overview.html", "content": "SDKs - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation SDKs SDKs Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK SDKs SDKs MindsDB provides SDKs for MongoDB, Python, and JavaScript, enabling incorporation of AI building blocks into these development environments. This section introduces custom syntax provided by MindsDB to bring data and AI together inside MongoDB, Python, and JavaScript development environments. Follow these steps to get started: 1 Set up the development environment For MongoDB, use MongoDB Compass or MongoDB Shell . For Python, install the package . For JavaScript, install the package . 2 Connect a data source Connect a data source in MongoDB , Python , and JavaScript . Explore all available data sources here . 3 Configure an AI engine Configure an AI engine in MongoDB , Python , and JavaScript . Explore all available AI engines here . 4 Create and deploy an AI/ML model Create and deploy an AI/ML model in MongoDB , Python , and JavaScript . 5 Query for predictions Query for predictions in MongoDB , Python , and JavaScript . 6 Automate customized workflows Automate tasks by scheduling docs in MongoDB , Python , and JavaScript . Was this page helpful? Yes No Suggest edits Raise issue Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list_models.html", "content": "List Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models List Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models List Models ​ Description The list_models() function lists all models available in a defined project. ​ Syntax Use the list_models() method to list all models contained in the project: project . list_models ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Model Describe a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "delete_table.html", "content": "Remove a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Remove a Table ​ Description The tables.drop() method enables you to delete a table from a connected data source. ​ Syntax Here is the syntax: data_source . tables . drop ( 'table_name' ) Was this page helpful? Yes No Suggest edits Raise issue Create a Table Query a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "drop_ml_engine.html", "content": "Remove an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Remove an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK AI/ML Engines Remove an ML Engine Here is how you can remove one of the ML engines directly from Python code: server.ml_engines.drop('ml_engine_name') Was this page helpful? Yes No Suggest edits Raise issue Configure an ML Engine List ML Engines github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "installation.html", "content": "Installation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect Installation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Installation Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Connect Installation Python SDK enables you to connect to the MindsDB server from Python using HTTP API. Read along to see how to install and test the MindsDB Python SDK. ​ Simple Installation To install the MindsDB Python SDK, run the below command: pip install mindsdb_sdk Here is the expected output: ​ Advanced Installation Instead of using the pip install mindsdb_sdk command, you can install it by cloning the Python SDK repository . Then you should create a virtual environment, install all dependencies from the requirements.txt file, and run tests as instructed below. To test all the components, go to the project directory ( mindsdb_sdk ) and run the below command: env PYTHONPATH = ./ pytest To generate the API documentation, run the below commands: pip install sphinx cd docs make html The documentation is generated in the docs/build/html folder. Was this page helpful? Yes No Suggest edits Raise issue Overview Connect github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Simple Installation Advanced Installation"}
{"file_name": "drop_job.html", "content": "Remove a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Remove a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job List Jobs Refresh a Job Get Job History AI Agents JavaScript SDK Jobs Remove a Job ​ Description The drop_job() function deletes a job from MindsDB. ​ Syntax Use the drop_job() method to remove a job: project . drop_job ( 'job_name' ) Was this page helpful? Yes No Suggest edits Raise issue Create a Job List Jobs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "delete_file.html", "content": "Remove a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Remove a File ​ Description In MindsDB, files are treated as tables. These are stored in the default files database. To delete a file, you must save this files database into a variable and then, run the tables.drop() function on it. ​ Syntax Here is the syntax: files = server . get_database ( 'files' ) files . tables . drop ( 'file_name' ) Was this page helpful? Yes No Suggest edits Raise issue Upload a File Query a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "agents.html", "content": "AI Agents with LLMs and Skills - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents AI Agents with LLMs and Skills Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents Agents Skills Knowledge Bases JavaScript SDK AI Agents AI Agents with LLMs and Skills ​ Description With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL. AI agents comprise of skills, such as text2sql and knowledge_base, and a conversational model. Skills provide data resources to an agent, enabling it to answer questions about available data. Learn more about skills here . Learn more about knowledge bases here . A conversational model (like OpenAI) from LangChain utilizes tools as skills to respond to user input. Users can customize these models with their own prompts to fit their use cases. ​ Syntax ​ Creating an agent When creating an agent, you can use the default conversational model: agent = server . agents . create ( 'new_demo_agent' ) Or specify model parameters: agent = server . agents . create ( name = 'new_demo_agent' , model = 'gpt-4' , openai_api_key = 'OPENAI_API_KEY' , prompt_template = 'Hello! Ask a question: {{question}}' , temperature = 0.0 , max_tokens = 1000 , top_p = 1.0 , top_k = 0 ) Or use an existing model: model = server . models . get ( 'existing_model' ) agent = server . agents . create ( 'demo_agent' , model ) Furthermore, you can list all existing agents, get agents by name, update agents, and delete agents. # list all agents agents = agents . list ( ) # get an agent by name agent = agents . get ( 'my_agent' ) # update an agent new_model = models . get ( 'new_model' ) agent . model_name = new_model . name new_skill = skills . create ( 'new_skill' , 'sql' , { 'tables' : [ 'new_table' ] , 'database' : 'new_database' } ) updated_agent . skills . append ( new_skill ) updated_agent = agents . update ( 'my_agent' , agent ) # delete an agent by name agents . delete ( 'my_agent' ) ​ Assigning skills to an agent You can add skills to an agent, providing it with data stored in databases, files, or webpages. The retrieval skill is similar to knowledge bases . # add data from one or more files as skills agent . add_file ( './file_name.txt' , 'file content description' ) agent . add_files ( [ './file_name.pdf' , './file_name.pdf' , . . . ] , 'files content description' ) # add data from one or more webpages as skills agent . add_webpages ( [ 'example1.com' , 'example2.com' , . . . ] , 'webpages content description' ) The text2SQL skill retrieves relevant information from databases. # add data from a database connected to mindsdb as skills db = server . databases . create ( 'datasource_name' , 'engine_name' , { 'database' : 'db.db_name' } ) db = server . databases . get ( 'datasource_name' ) agent . add_database ( db . name , [ 'table_name' ] , 'data description' ) Learn more about all data sources integrated with MindsDB . ​ Querying an agent Once you created an agent and assigned it a set of skills, you can ask questions related to your data. question = 'ask a question' answer = agent . completion ( [ { 'question' : question , 'answer' : None } ] ) print ( answer . content ) ​ Example Here is a sample Python code to deploy an agent: import mindsdb_sdk con = mindsdb_sdk . connect ( ) # IMPORTANT: This code requires to set OPENAI_API_KEY as env variable agent = con . agents . create ( f'new_demo_agent' ) print ( 'Adding Hooblyblob details...' ) agent . add_file ( './hooblyblob.txt' , 'Details about the company Hooblyblob' ) print ( 'Adding rulebook details...' ) agent . add_files ( [ './codenames-rulebook.pdf' ] , 'Rulebooks for various board games' ) print ( 'Adding MindsDB docs...' ) agent . add_webpages ( [ 'docs.mindsdb.com' ] , 'Documentation for MindsDB' ) print ( 'Agent ready to use.' ) while True : print ( 'Ask a question: ' ) question = input ( ) answer = agent . completion ( [ { 'question' : question , 'answer' : None } ] ) print ( answer . content ) Was this page helpful? Yes No Suggest edits Raise issue Get Job History Skills github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Creating an agent Assigning skills to an agent Querying an agent Example"}
{"file_name": "list_ml_engines.html", "content": "List ML Engines - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Engines Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK AI/ML Engines List ML Engines Here is how you can fetch all available ML engines directly from Python code: server.ml_enignes.list() Was this page helpful? Yes No Suggest edits Raise issue Remove an ML Engine Create a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "refresh_job.html", "content": "Refresh a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Refresh a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job List Jobs Refresh a Job Get Job History AI Agents JavaScript SDK Jobs Refresh a Job ​ Description The refresh() function synchronizes the job with MindsDB. ​ Syntax Use the refresh() method to retrieve job data from the MindsDB server: my_job . refresh ( ) Was this page helpful? Yes No Suggest edits Raise issue List Jobs Get Job History github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "refresh_model.html", "content": "Refresh a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Refresh a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Refresh a Model ​ Description The refresh() function synchronizes the model with MindsDB. ​ Syntax Use the refresh() method to refresh model’s data from the MindsDB server: model_name . refresh ( ) Was this page helpful? Yes No Suggest edits Raise issue Get Status of a Model Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query_table.html", "content": "Query a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Query a Table ​ Description The query() function is executed on a data source connected to MindsDB and saved into a variable. It queries a table from this data source. ​ Syntax Here is the syntax: my_data_source . query ( 'SELECT * FROM my_table LIMIT 100' ) You can query for newly added data using the functionality introduced by the LAST keyword as follows: query = server . databases . my_data_source . tables . table_name . filter ( column_name = 'value' ) . track ( 'timestamp_column' ) # first call returns no records df = query . fetch ( ) # second call returns rows with timestamp_column greater than the timestamp of a previous fetch df = query . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Table Native Queries github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_view.html", "content": "Create a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Create a View ​ Description The get_view() and create_view() functions let you save either an existing view or a newly created view into a variable. ​ Syntax Use the get_view() method to get an existing view: my_view = project . get_view ( 'my_view' ) Or, the create_view() method to create a view: my_view = project . create_view ( 'view_name' , mysql_demo_db . query ( 'SELECT * FROM my_table LIMIT 100' ) ) Was this page helpful? Yes No Suggest edits Raise issue Delete From a Table Remove a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_database.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Data Sources Connect a Data Source ​ Description The get_database() and create_database() functions enable you to use the existing data source or connect a new one. ​ Syntax You can use the get_database() method to get an existing database: mysql_demo_db = server . get_database ( 'mysql_demo_db' ) Or, the create_database() method to connect a new data source to MindsDB: mysql_demo_db = server . create_database ( engine = \"mysql\" , name = \"mysql_demo_db\" , connection_args = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ) Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_views.html", "content": "List Views - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files List Views Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files List Views ​ Description The list_views() function is executed on a project and lists all views available in this project. ​ Syntax Use the list_views() method to list all views in a project: project . list_views ( ) Was this page helpful? Yes No Suggest edits Raise issue Query a View Upload a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get_status.html", "content": "Get Status of a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Get Status of a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Get Status of a Model ​ Description The get_status() function lets you fetch the current status of the model, for example, to see if the model completed its training phase. ​ Syntax Use the get_status() method to check the training status of the model: my_model . get_status ( ) Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Refresh a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "describe.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Describe a Model ​ Description The describe() function returns information about a model. ​ Syntax Use the describe() method to get information about a model: model_name . describe ( ) Was this page helpful? Yes No Suggest edits Raise issue List Models Get Status of a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_jobs.html", "content": "List Jobs - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs List Jobs Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job List Jobs Refresh a Job Get Job History AI Agents JavaScript SDK Jobs List Jobs ​ Description The list_jobs() function is executed on a project and lists all jobs available in this project. ​ Syntax Use the list_jobs() method to list all jobs in a project: project . list_jobs ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Job Refresh a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_projects.html", "content": "List Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects List Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Query Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Projects List Projects ​ Description The list_projects() function lists all available projects. ​ Syntax Use the list_projects() method to lists all available projects: server . list_projects ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Project Query Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Finetune a Model ​ Description The finetune() function finetunes a model with specified data. ​ Syntax Use the finetune() method to finetune a model with specific data: model_name . finetune ( data_table ) Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "connect.html", "content": "Connect - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect Connect Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Installation Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Connect Connect This documentation describes how you can connect to your MindsDB server from Python code. Here is how to connect to your local MindsDB server: import mindsdb_sdk # connects to the default port (47334) on localhost server = mindsdb_sdk.connect() # connects to the specified host and port server = mindsdb_sdk.connect('http://127.0.0.1:47334') Was this page helpful? Yes No Suggest edits Raise issue Installation List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "upload_file.html", "content": "Upload a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Upload a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Upload a File ​ Description In MindsDB, files are treated as tables. These are stored in the default files database. To upload a file, you must save this files database into a variable and then, run the create_table() function on it. Note that the trailing whitespaces on column names are erased upon uploading a file to MindsDB. ​ Syntax Here is the syntax: files_db = server.get_database('files') files_db.create_table('file_name', data_frame) Was this page helpful? Yes No Suggest edits Raise issue List Views Remove a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get-single-prediction.html", "content": "Get a Single Prediction - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get a Single Prediction Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Predictions Get a Single Prediction ​ Description The predict() function fetches predictions from the model table. ​ Syntax Use the predict() method to make a single prediction by passing the specific values as its argument: my_model . predict ( { \"text\" : 'any text' } ) Was this page helpful? Yes No Suggest edits Raise issue Manage Model Versions Get Batch Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "overview.html", "content": "Overview - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Python SDK Overview Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Python SDK Overview You can interact with MindsDB directly from the Python code. Next Steps Below are the links to help you explore further. Installation Connect Create Models Was this page helpful? Yes No Suggest edits Raise issue Query Jobs Installation github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "drop_database.html", "content": "Remove a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Remove a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Data Sources Remove a Data Source ​ Description The drop_database() function enables you to remove a defined data source connection from MindsDB. ​ Syntax Use the drop_database() method to remove a database: server . drop_database ( 'mysql_demo_db' ) Was this page helpful? Yes No Suggest edits Raise issue Connect a Data Source List Data Sources github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "agents_skills.html", "content": "AI Agents' Skills - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents AI Agents' Skills Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents Agents Skills Knowledge Bases JavaScript SDK AI Agents AI Agents' Skills ​ Description With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL. Learn more about AI Agents here . There are two types of skills available in MindsDB: Text2SQL skill translates users’ questions into SQL queries and fetches relevant information from data sources assigned to this skill. Knowledge Bases store data from databases, files, or webpages, and use different retrieval methods to fetch relevant information and present it as answers. ​ Syntax Here is how to create a new skill: text_to_sql_skill = skills . create ( 'text_to_sql' , 'sql' , { 'tables' : [ 'my_table' ] , 'database' : 'my_database' } ) Note that it is required to assign a database and a set of tables to be used by this skill. Here is how to list all available skills: skills = skills . list ( ) Here is how to get an existing skill by name: skill = skills . get ( 'my_skill' ) Here is how to update a skill with new datasets: skill . params = { 'tables' : [ 'new_table' ] , 'database' : 'new_database' } updated_skill = skills . update ( 'my_skill' , skill ) Here is how to delete a skill: skills . delete ( 'my_skill' ) Here is how to create an agent and assign it a skill: agent = agents . create ( 'my_agent' , model_name , [ text_to_sql_skill ] ) Was this page helpful? Yes No Suggest edits Raise issue Agents Knowledge Bases github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "delete_from.html", "content": "Delete From a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Delete From a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Delete From a Table ​ Description The delete() function is executed on a table from a data source connected to MindsDB. It deletes rows from a table. ​ Syntax Here is the syntax: data_source . tables . table_name . delete ( key = values , . . . ) Was this page helpful? Yes No Suggest edits Raise issue Join Tables On Create a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query_projects.html", "content": "Query Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Query Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Query Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Projects Query Projects ​ Description The query() methods enables you to run queries on models, tables, and views stored in a project. ​ Syntax Use the query() method to submit a query to a project: query = project . query ( 'SELECT * FROM my_table;' ) query . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue List Projects Create, Train, and Deploy a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "update_table.html", "content": "Update a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Update a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Update a Table ​ Description The update() function is executed on a table from a data source connected to MindsDB. It updates a table on specified columns. ​ Syntax Here is the syntax: my_table . update ( table_used_to_update , on = [ 'column1' , 'column2' , . . . ] ) Check out the SQL syntax to better understand how the update() function works. Was this page helpful? Yes No Suggest edits Raise issue Native Queries Insert Into a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_ml_engine.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK AI/ML Engines Configure an ML Engine Here is how you can create an ML engine directly from Python code: server.ml_engines.create( 'ml_engine_name', 'ml_engine_handler', connection_data={'<ml_engine_handler>_api_key': '111'} ) Was this page helpful? Yes No Suggest edits Raise issue List ML Handlers Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "manage-model-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Manage Model Versions Below are the functions that let you work with multiple version of a model. Use the list_versions() method to view all available version of a model: model_name . list_versions ( ) Use the get_version() method to use a specific version of a model: model_name . get_version ( 9 ) Use the set_active() method to set a specific version of a model as active: model_name . set_active ( 9 ) Use the drop_model_version() method to remove a specific version of a model: project . drop_model_version ( 'my_model' , 3 ) Please note that the model version should be deactivated before it is removed. Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "get-batch-predictions.html", "content": "Get Batch Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get Batch Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Predictions Get Batch Predictions ​ Description The predict() function fetches predictions from the model table. ​ Syntax Use the predict() method to make batch predictions by passing the data table as its argument: my_model . predict ( my_table . limit ( 10 ) ) When querying for predictions, you can specify the partition_size parameter to split data into partitions and run prediction on different workers. Note that the ML task queue needs to be enabled to use this parameter. To use the partition_size parameter, provide the below argument to the predict function, specifying the partition size, like this: my_model.predict(df, params={'partition_size': 2}) Was this page helpful? Yes No Suggest edits Raise issue Get a Single Prediction Create a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "insert_into_table.html", "content": "Insert Into a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Insert Into a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Insert Into a Table ​ Description The insert() function is executed on a table from a data source connected to MindsDB. It inserts data into a table. ​ Syntax Here is the syntax: my_table . insert ( table_to_be_inserted ) Was this page helpful? Yes No Suggest edits Raise issue Update a Table Join Tables On github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_model.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Create, Train, and Deploy a Model ​ Description The models.get() and models.create() methods enable you to get an existing model or create and deploy a new model. ​ Syntax Use the models.get() method to get an existing model: my_model = project . models . get ( 'my_model' ) Or, the create() method to create and train a new model: my_model = project . models . create ( name = 'my_model' , predict = 'target' , query = my_table ) Please note that in the case of LLM models, the parameters can be stored in options . Here is the syntax to create an OpenAI model: sentiment_classifier = project . models . create ( name = 'sentiment_classifier' , engine = 'openai' , # alternatively: engine=server.ml_engines.openai predict = 'sentiment' , options = { 'prompt_template' : 'answer this question: {{questions}}' , 'model_name' : 'gpt4' } ) Alternatively, you can skip options and define parameters as arguments. sentiment_classifier = project . models . create ( name = 'sentiment_classifier' , engine = 'openai' , # alternatively: engine=server.ml_engines.openai predict = 'sentiment' , prompt_template = 'answer this question: {{questions}}' , model_name = 'gpt4' ) And in the case of time-series model, the additional options are stored in timeseries_options . Here is the syntax to create a time-series model: ts_model = project . models . create ( name = 'time_series_model' , predict = 'target' , query = my_table , timeseries_options = { 'order' : 'order_date' , 'group' : 'category' , 'window' : 30 , 'horizon' : 4 } ) Was this page helpful? Yes No Suggest edits Raise issue Query Projects Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "drop_model.html", "content": "Remove a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Remove a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Remove a Model ​ Description The drop_model() function removes a model from MindsDB. ​ Syntax Use the drop_model() method to remove a model: # option 1 project . drop_model ( 'my_model' ) # option 2 server . models . drop ( 'my_model' ) Was this page helpful? Yes No Suggest edits Raise issue Create, Train, and Deploy a Model List Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_ml_handlers.html", "content": "List ML Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK AI/ML Engines List ML Handlers Here is how you can fetch all available ML handlers directly from Python code: mindsdb = server.get_project('mindsdb') ml_handlers = mindsdb.query('SHOW HANDLERS WHERE type = \\'ml\\'') print(ml_handlers.fetch()) Was this page helpful? Yes No Suggest edits Raise issue List Data Sources Configure an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "query_view.html", "content": "Query a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Query a View ​ Description The query() function is executed on a view that resides in one of the projects. ​ Syntax Here is the syntax: project_name . query ( 'SELECT * FROM my_project.my_view LIMIT 100' ) You can query for newly added data using the functionality introduced by the LAST keyword as follows: query = server . databases . my_data_source . views . view_name . filter ( column_name = 'value' ) . track ( 'timestamp_column' ) # first call returns no records df = query . fetch ( ) # second call returns rows with timestamp_column greater than the timestamp of a previous fetch df = query . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a View List Views github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_databases.html", "content": "List Data Sources - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Sources Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Data Sources List Data Sources ​ Description The list_databases() function lists all data sources connected to MindsDB. ​ Syntax Use the list_databases() method to list all databases: server . list_databases ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Data Source List ML Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "native_queries.html", "content": "Native Queries - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Native Queries Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Native Queries ​ Description The query() function is executed on a data source connected to MindsDB and saved into a variable. This native query is executed directly on a data source. ​ Syntax Here is the syntax: my_data_source . query ( 'SELECT * FROM datasource_name (<native query here>);' ) Was this page helpful? Yes No Suggest edits Raise issue Query a Table Update a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "drop_view.html", "content": "Remove a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Remove a View ​ Description The drop_view() function removes a view from MindsDB. ​ Syntax Use the drop_view() method to remove a view: project . drop_view ( 'view_name' ) Was this page helpful? Yes No Suggest edits Raise issue Create a View Query a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "join_on.html", "content": "Join Tables On - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Join Tables On Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Join Tables On ​ Description The query() function is executed on a data source connected to MindsDB and saved into a variable. It performs a join operation between tables. ​ Syntax Here is the syntax: my_data_source . query ( 'SELECT * FROM my_table t JOIN another_table a ON t…=a… LIMIT 100' ) Was this page helpful? Yes No Suggest edits Raise issue Insert Into a Table Delete From a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query_files.html", "content": "Query a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Query a File ​ Description In MindsDB, files are treated as tables. These are stored in the default files database. To query a file, you must save this files database into a variable and then, run the query() function on it. ​ Syntax Here is the syntax: server.get_database('files').query('SELECT * FROM file_name') Was this page helpful? Yes No Suggest edits Raise issue Remove a File Create a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Refresh a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Models Retrain a Model ​ Description The retrain() function retrains the model, for example, if new version of MindsDB is available. ​ Syntax Use the retrain() method to retrain a model: model_name . retrain ( ) Was this page helpful? Yes No Suggest edits Raise issue Refresh a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_project.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Query Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Projects Create a Project ​ Description The get_project() and create_project() functions fetch an existing project or create a new one. ​ Syntax Use the get_project() method to get the default mindsdb project: project = server . get_project ( ) Use the get_project() method to get other project: project = server . get_project ( 'project_name' ) Use the create_project() method to create a new project: project = server . create_project ( 'project_name' ) Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_table.html", "content": "Create a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs AI Agents JavaScript SDK Tables, Views, and Files Create a Table ​ Description The get_table() and create_table() functions let you save either an existing table or a newly created table into a variable. ​ Syntax Use the get_table() method to fetch a table from the mysql_demo_db database: my_table = mysql_demo_db . get_table ( 'my_table' ) Or, the create_table() method to create a new table: # option 1 my_table = mysql_demo_db . create_table ( 'my_table' , 'SELECT * FROM some_table WHERE key=value' ) # option 2 my_table = mysql_demo_db . create_table ( 'my_table' , base_table ) # option 3 my_table = mysql_demo_db . create_table ( 'my_table' , base_table . filter ( key = 'value' ) ) Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Remove a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "drop_project.html", "content": "Remove a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Remove a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Query Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Projects Remove a Project ​ Description The drop_project() function removed a project from MindsDB. ​ Syntax Use the drop_project() method to remove a project: server . drop_project ( 'project_name' ) Was this page helpful? Yes No Suggest edits Raise issue Create a Project List Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "agents_knowledge_bases.html", "content": "Knowledge Bases - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Knowledge Bases Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents Agents Skills Knowledge Bases JavaScript SDK AI Agents Knowledge Bases ​ Description With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL. Learn more about AI Agents here . There are two types of skills available in MindsDB: Text2SQL skill translates users’ questions into SQL queries and fetches relevant information from data sources assigned to this skill. Knowledge Bases store data from databases, files, or webpages, and use different retrieval methods to fetch relevant information and present it as answers. ​ Syntax To feed an agent with data from different sources, use the knowledge_base skills. Here is how to create a knowledge base: my_kb = server . knowledge_bases . create ( 'my_kb' ) Here is how to insert data into a knowledge base: Inserting data from files: Note that the file should be uploaded to MindsDB before inserting it into the knowledge base. my_kb . insert_files ( [ \"file_name_uploaded_to_mindsdb\" , . . . ] ) Inserting data from the web: my_kb . insert_webpages ( [ \"https://docs.mindsdb.com/\" ] , 1 ) : Optionally, you can specify the crawl depth as the second argument. For example, the crawl depth of 1 indicates that all links present on https://docs.mindsdb.com/ will be crawled, and the crawl depth of 2 indicates that all links present on those pages will be crawled, and so on. Inserting data from data sources connected to MindsDB: my_kb . insert ( server . databases . db_name . tables . table_name . filter ( column_name = 'value' ) ) Inserting raw data: my_kb . insert ( { 'column_name' : 'value' , 'column_name' : 'value' , . . . } ) When inserting data to a knowledge base, you can specify the partition_size parameter to split data into partitions and run prediction on different workers. Note that the ML task queue needs to be enabled to use this parameter. To use the partition_size parameter, provide the below argument to the insert function, specifying the partition size, like this: my_kb.insert(df, params={'model': {'partition_size': 100}}) Here is how to query data from a knowledge base using specified keywords: df = my_kb . fetch ( ) - - or df = my_kb . find ( 'keyword' ) . fetch ( ) Here is how to list all available knowledge bases: kb_list = server . knowledge_bases . list ( ) Here is how to get a knowledge base by name: kb = server . knowledge_bases . get ( 'my_kb' ) # or kb = server . knowledge_bases . my_kb Here is how to drop a knowledge base: server . knowledge_bases . drop ( 'my_kb' ) Was this page helpful? Yes No Suggest edits Raise issue Skills Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_data_handlers.html", "content": "List Data Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI Agents JavaScript SDK Data Sources List Data Handlers Here is how you can fetch all available data handlers directly from Python code: mindsdb = server.get_project('mindsdb') data_handlers = mindsdb.query('SHOW HANDLERS WHERE type = \\'data\\'') print(data_handlers.fetch()) Was this page helpful? Yes No Suggest edits Raise issue Connect Connect a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create_job.html", "content": "Create a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Create a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job List Jobs Refresh a Job Get Job History AI Agents JavaScript SDK Jobs Create a Job ​ Description The get_job() and create_job() functions let you save either an existing job or a newly created job into a variable. ​ Syntax Use the get_job() method to get an existing job: my_job = project . get_job ( 'my_job' ) Or, the create_job() method to create a job: my_job = project . create_job ( 'job_name' , 'select * from models' , repeat_str = '1 hour' ) Alternatively, you can create a job using this syntax: with project . jobs . create ( name = 'job_name' , repeat_min = 1 ) as job : job . add_query ( model . retrain ( ) ) job . add_query ( model . predict ( database . tables . tbl1 ) ) job . add_query ( kb . insert ( database . tables . tbl1 ) ) job . add_query ( 'show models' ) Where: name='job_name' is the job name, repeat_min=1 indicates periodicity of the job in minutes, job.add_query(model.retrain()) adds a task to a job to retrain a model, job.add_query(model.predict(database.tables.tbl1)) adds a task to a job to make predictions, job.add_query(kb.insert(database.tables.tbl1)) adds a task to a job to insert data into a knowledge base, job.add_query('show models') adds a task to a job to run the statement provided as string value. Note that the add_query() method adds tasks to a job and takes either String or Query as an argument. Note that this method enables a job to manipulate Knowledge Bases, Models, Tables, Views, and Queries, but not Databases, Handlers, Jobs, ML Engines, or Projects. Was this page helpful? Yes No Suggest edits Raise issue Query a File Remove a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get_history.html", "content": "Get Job History - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Get Job History Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job List Jobs Refresh a Job Get Job History AI Agents JavaScript SDK Jobs Get Job History ​ Description The get_history() function lets you access the job history information where you can find a job record for each job execution, including execution errors. ​ Syntax Use the get_history() method to get history of job execution: my_job . get_history ( ) Was this page helpful? Yes No Suggest edits Raise issue Refresh a Job Agents github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_models.html", "content": "List Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models List Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models List Models You can list models using the below code. const query = 'SHOW MODELS’; result = await MindsDB.SQL.runQuery(query); console.log(result); Was this page helpful? Yes No Suggest edits Raise issue Remove a Model Describe a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "delete_table.html", "content": "Remove a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Remove a Table This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Create a Table Query a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "drop_ml_engine.html", "content": "Remove an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Remove an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI/ML Engines Remove an ML Engine This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Configure an ML Engine List ML Engines github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "installation.html", "content": "Installation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect Installation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Installation Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Connect Installation The MindsDB JavaScript SDK allows you to unlock the power of machine learning right inside your web applications. Read along to see how to install the MindsDB’s JavaScript SDK. ​ How to Install To install the MindsDB JavaScript SDK, run the below command: npm install --save mindsdb-js-sdk Here is the expected output: Was this page helpful? Yes No Suggest edits Raise issue Overview Connect github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Install"}
{"file_name": "drop_job.html", "content": "Remove a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Remove a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Jobs Remove a Job This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Create a Job Query Jobs github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "delete_file.html", "content": "Remove a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Remove a File This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Upload a File Query a File github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list_ml_engines.html", "content": "List ML Engines - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Engines Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI/ML Engines List ML Engines This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Remove an ML Engine Create a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "query_table.html", "content": "Query a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Query a Table ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `SELECT * FROM table_name`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Remove a Table Native Queries github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_view.html", "content": "Create a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Create a View ​ Description The createView() function creates a view in MindsDB. ​ Syntax Here is the syntax: const viewSelect = `SELECT t.sqft, t.location, m.rental_price FROM mysql_demo_db.home_rentals as t JOIN mindsdb.home_rentals_model as m`; const predictionsView = await MindsDB.Views.createView( 'view_name', 'project_name', viewSelect); Was this page helpful? Yes No Suggest edits Raise issue Delete From a Table Remove a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_database.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Get a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Data Sources Connect a Data Source ​ Description The MindsDB.Databases.createDatabase function connects a new data source to MindsDB. ​ Syntax Here is how to connect our sample MySQL database: const connectionParams = { 'user': 'user', 'port': 3306, 'password': 'MindsDBUser123!', 'host': 'samples.mindsdb.com', 'database': 'public' } try { const mysqlDatabase = await MindsDB.Databases.createDatabase( 'mysql_datasource', 'mysql', connectionParams); console.log('connected a database'); } catch (error) { // Couldn't connect to database console.log(error); } First, we define the connection parameters and then use the createDatabase function to connect a database. Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Get a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query_jobs.html", "content": "Query Jobs - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Query Jobs Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Jobs Query Jobs This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Remove a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list_views.html", "content": "List Views - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files List Views Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files List Views ​ Description The getAllViews() function lists all available views. ​ ​ Syntax Here is how to list all available views: const allViews = await MindsDB . Views . getAllViews ( ) ; console . log ( 'all views:' ) allViews . forEach ( v = > { console . log ( v . name ) ; } ) ; Was this page helpful? Yes No Suggest edits Raise issue Query a View Upload a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get_status.html", "content": "Get Status of a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Get Status of a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Get Status of a Model ​ Description The status property of the model stores its status, such as generating , training , or complete . ​ Syntax Here is the syntax: model_name . status Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "describe.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Describe a Model ​ Description The describe() function provides information about the models. ​ Syntax Here is the syntax: const modelDescription = await model_name . describe ( ) ; Was this page helpful? Yes No Suggest edits Raise issue List Models Get Status of a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_projects.html", "content": "List Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects List Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Projects List Projects ​ Description The getAllProjects() function lists all available projects. ​ Syntax Here is how to list all available projects: const allProjects = await MindsDB.Projects.getAllProjects(); console.log('all projects:') allProjects.forEach(p => { console.log(p.name); }); Was this page helpful? Yes No Suggest edits Raise issue Remove a Project Create, Train, and Deploy a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Finetune a Model ​ Description The finetune() fuction lets us finetune the model with specific data. ​ Syntax Here is how you can finetune, or adjust, a model: // Getting a model const homeRentalPriceModel = await MindsDB.Models.getModel('home_rentals_model', 'mindsdb'); console.log('got a model'); // Defining data used to finetune a model const adjustSelect = 'SELECT * FROM demo_data.home_rentals WHERE days_on_market >= 10'; //const params = { 'key' : 'value' } try { // Finetuning/adjusting a model with the specified dataset await homeRentalPriceModel.finetune({ integration: 'example_db', select: adjustSelect //using: params }); console.log('finetuned a model'); } catch (error) { // Something went wrong adjusting console.log(error); } You can find full adjust options here . Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "connect.html", "content": "Connect - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect Connect Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Installation Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Connect Connect Before performing any operations, you must connect to MindsDB. By default, all operations will go through MindsDB Cloud REST APIs , but you can use a self-hosted version of MindsDB as well. Local MindsDB Own Axios Instance Here is how to connect to your local MindsDB server: import MindsDB from 'mindsdb-js-sdk'; // const MindsDB = require(\"mindsdb-js-sdk\").default; // alternative for CommonJS syntax try { // No authentication needed for self-hosting await MindsDB.connect({ // alternative for ES6 module syntax: await MindsDB.default.connect({ host: 'http://127.0.0.1:47334' }); console.log('connected'); } catch(error) { // Failed to connect to local instance console.log(error); } Please note that all methods that use await must be wrapped in an async function, like this: (async() => { try { // No authentication needed for self-hosting await MindsDB.connect({ host: 'http://127.0.0.1:47334' }); console.log('connected'); } catch(error) { // Failed to connect to local instance console.log(error); } })(); Was this page helpful? Yes No Suggest edits Raise issue Installation List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "upload_file.html", "content": "Upload a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Upload a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Upload a File This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue List Views Remove a File github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "overview.html", "content": "Overview - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation JavaScript SDK Overview Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs JavaScript SDK Overview You can interact with MindsDB directly from the JavaScript code. Next Steps Below are the links to help you explore further. Installation Connect Create Models Was this page helpful? Yes No Suggest edits Raise issue Knowledge Bases Installation github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "drop_database.html", "content": "Remove a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Remove a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Get a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Data Sources Remove a Data Source ​ Description The delete function removes a data source from MindsDB. Please note that in order to delete a connected data source, we need to fetch it first with the getDatabase function. ​ Syntax Here is how to get an existing database and remove it: try { const db = await MindsDB.Databases.getDatabase('mysql_datasource'); console.log('got a database') // Deleting a database if (db) { try { await db.delete(); console.log('deleted a database'); } catch (error) { // Couldn't delete a database console.log(error); } } } catch (error) { // Couldn't connect to database console.log(error); } Was this page helpful? Yes No Suggest edits Raise issue Get a Data Source List Data Sources github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query.html", "content": "Get a Single Prediction - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get a Single Prediction Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Tables, Views, and Files Jobs Predictions Get a Single Prediction ​ Description The query() function fetches a single prediction from the model. ​ Syntax Here is the syntax: await model_name.query(queryOptions); Here are some useful links: training options , query options , batch query options . Was this page helpful? Yes No Suggest edits Raise issue Manage Model Versions Get Batch Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "delete_from.html", "content": "Delete From a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Delete From a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Delete From a Table ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `DELETE FROM datasource_name.table_name WHERE …`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Join Tables On Create a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "update_table.html", "content": "Update a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Update a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Update a Table ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `UPDATE integration_name.table_name SET column_name = new_value WHERE column_name = old_value`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Native Queries Insert Into a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_ml_engine.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI/ML Engines Configure an ML Engine This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue List ML Handlers Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "manage-model-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Manage Model Versions This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "insert_into_table.html", "content": "Insert Into a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Insert Into a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Insert Into a Table ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `INSERT INTO integration_name.table_name (SELECT ...)`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Update a Table Join Tables On github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_model.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Create, Train, and Deploy a Model Training a model requires various parameters, depending on the model type. Here we present examples of regression, time series, and OpenAI models. Here are some useful links: training options , query options , batch query options . ​ Regression Model Let’s look at an example of how to create and train a simple regression model, and then, use it for making predictions. // Defining training options const regressionTrainingOptions = { select: 'SELECT * FROM demo_data.home_rentals', integration: 'example_db' }; try { // Creating and training a model // The returned promise resolves when the model is created, NOT when training is actually complete let homeRentalPriceModel = await MindsDB.Models.trainModel( 'home_rentals_model', 'rental_price', 'mindsdb', regressionTrainingOptions); console.log('created a model'); // Waiting for the training to be complete while (homeRentalPriceModel.status !== 'complete' && homeRentalPriceModel.status !== 'error') { homeRentalPriceModel = await MindsDB.Models.getModel('home_rentals_model', 'mindsdb'); } // Checking model's status console.log('Model status: ' + homeRentalPriceModel.status); // Defining query options const queryOptions = { where: [ 'sqft = 823', 'location = \"good\"', 'neighborhood = \"downtown\"', 'days_on_market = 10' ] } // Querying for a single prediction const rentalPricePrediction = await homeRentalPriceModel.query(queryOptions); console.log('Predicted value:'); console.log(rentalPricePrediction.value); console.log('Prediction insights:'); console.log(rentalPricePrediction.explain); console.log('Input data:'); console.log(rentalPricePrediction.data); } catch (error) { // Something went wrong training or querying console.log(error); } ​ Time Series Model As time series models require more parameters, let’s go over an example of how to create and train a time series model, and then, use it for making batch predictions. // Defining training options const timeSeriesTrainingOptions = { integration: 'example_db', select: 'SELECT * FROM demo_data.house_sales', orderBy: 'saledate', groupBy: 'bedrooms', window: 8, horizon: 4 } try { // Creating and training a model let houseSalesForecastModel = await MindsDB.Models.trainModel( 'house_sales_model', 'ma', 'mindsdb', timeSeriesTrainingOptions); console.log('created a model'); // Waiting for the training to be complete while (houseSalesForecastModel.status !== 'complete' && houseSalesForecastModel.status !== 'error') { houseSalesForecastModel = await MindsDB.Models.getModel('house_sales_model', 'mindsdb'); } // Checking model's status console.log('Model status: ' + houseSalesForecastModel.status); // Describing a model const modelDescription = await houseSalesForecastModel.describe(); console.log('Model description:'); console.log(modelDescription); // Defining query options const queryOptions = { // Join model to this data source join: 'example_db.demo_data.house_sales', // When using batch queries, the 't' alias is used for the joined data source ('t' is short for training/test) // The 'm' alias is used for the trained model to be queried where: ['t.saledate > LATEST', 't.bedrooms = 2'], limit: 4 } // Querying for batch predictions const rentalPriceForecasts = await houseSalesForecastModel.batchQuery(queryOptions); console.log('Batch predictions:'); rentalPriceForecasts.forEach(f => { console.log(f.value); console.log(f.explain); console.log(f.data); }) } catch (error) { // Something went wrong training or predicting. console.log(error); } ​ OpenAI Model Here is how to create and deploy an OpenAI model from JavaScript code: // Defining training options const trainingOptions = { using: {engine: 'openai', prompt_template: 'describe the sentiment of the reviews strictly as \"positive\", \"neutral\", or \"negative\". \"I love the product\":positive \"It is a scam\":negative \"{{review}}.\":'} }; try { // Creating and training a model let openai_js = await MindsDB.Models.trainModel( 'openai_js', 'sentiment', 'mindsdb', trainingOptions); console.log('created a model'); // Waiting for the training to be complete while (openai_js.status !== 'complete' && openai_js.status !== 'error') { openai_js = await MindsDB.Models.getModel('openai_js', 'mindsdb'); } // Checking model's status console.log('Model status: ' + openai_js.status); // Defining query options const queryOptions = { where: [ 'review = \\'It is ok.\\'' ] } // Querying for a single prediction openai_js = await openai_js.query(queryOptions); console.log('Predicted value:'); console.log(openai_js.value); console.log('Prediction insights:'); console.log(openai_js.explain); console.log('Input data:'); console.log(openai_js.data); } catch (error) { // Something went wrong training or querying console.log(error); } Was this page helpful? Yes No Suggest edits Raise issue List Projects Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Regression Model Time Series Model OpenAI Model"}
{"file_name": "drop_model.html", "content": "Remove a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Remove a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Remove a Model ​ Description The delete function removes a model from MindsDB. ​ Syntax Here is how to get an existing database and remove it: model = MindsDB.Models.trainModel(...) model.delete() Was this page helpful? Yes No Suggest edits Raise issue Create, Train, and Deploy a Model List Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_ml_handlers.html", "content": "List ML Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs AI/ML Engines List ML Handlers Here is how you can fetch all available ML handlers directly from JavaScript code: const query = 'SHOW HANDLERS WHERE type = \\‘ml\\'’; result = await MindsDB.SQL.runQuery(query); console.log(result); Was this page helpful? Yes No Suggest edits Raise issue List Data Sources Configure an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "query_view.html", "content": "Query a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Query a View ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `SELECT * FROM project_name.view_name`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Remove a View List Views github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list_databases.html", "content": "List Data Sources - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Sources Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Get a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Data Sources List Data Sources You can list all data sources using the code below. const query = 'SHOW FULL DATABASES WHERE type = \\'data\\''; result = await MindsDB.SQL.runQuery(query); // alternative for ES6 module syntax: MindsDB.default.SQL.runQuery(query) console.log(result); Was this page helpful? Yes No Suggest edits Raise issue Remove a Data Source List ML Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "batchQuery.html", "content": "Get Batch Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get Batch Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Tables, Views, and Files Jobs Predictions Get Batch Predictions ​ Description The batchQuery() function fetches batch predictions from the model by joining it with the data table. ​ Syntax Here is the syntax: await model_name.batchQuery(batchQueryOptions); Here are some useful links: training options , query options , batch query options . Was this page helpful? Yes No Suggest edits Raise issue Get a Single Prediction Create a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "native_queries.html", "content": "Native Queries - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Native Queries Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Native Queries ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. And the native queries syntax ensures that the query is executed directly on the connected data source. ​ Syntax Here is the syntax: const query = `SELECT * FROM datasource_name (<native query here>)`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Query a Table Update a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "drop_view.html", "content": "Remove a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Remove a View ​ Description The deleteView() function deletes an existing view from MindsDB. ​ Syntax Here is the syntax: await MindsDB.Views.deleteView( 'view_name', 'project_name'); Was this page helpful? Yes No Suggest edits Raise issue Create a View Query a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "join_on.html", "content": "Join Tables On - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Join Tables On Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Join Tables On ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `SELECT * FROM table_name t JOIN another_table a ON t…=a…`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Insert Into a Table Delete From a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query_files.html", "content": "Query a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Query a File ​ Description The runQuery() function executes a query given as its argument directly in MindsDB. ​ Syntax Here is the syntax: const query = `SELECT * FROM files.file_name`; const queryResult = await MindsDB.SQL.runQuery(query); Was this page helpful? Yes No Suggest edits Raise issue Remove a File Create a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Get Status of a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Models Retrain a Model ​ Description The retrain() fuction lets us retrain the model, for example, when there is a new version of MindsDB available. ​ Syntax Here is how to get an existing model and retrain it if its update status is available . // Getting an existing model const homeRentalPriceModel = await MindsDB.Models.getModel('home_rentals_model', 'mindsdb'); // Checking the status and retraining a model if required if (homeRentalPriceModel.updateStatus === 'available') { try { // For custom retraining: homerentalPriceModel.retrain('example_db', trainingOptions); await homeRentalPriceModel.retrain(); console.log('retrained a model'); } catch (error) { // Something went wrong with retraining console.log(error); } } You can find full training options here . Was this page helpful? Yes No Suggest edits Raise issue Get Status of a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "create_project.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Projects Create a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create_table.html", "content": "Create a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View List Views Upload a File Remove a File Query a File Jobs Tables, Views, and Files Create a Table This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Remove a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "drop_project.html", "content": "Remove a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Remove a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Projects Remove a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Create a Project List Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "get_database.html", "content": "Get a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Get a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Get a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Data Sources Get a Data Source You can save a data sources into a variable using the code below. const db = await MindsDB.Databases.getDatabase('mysql_datasource'); Was this page helpful? Yes No Suggest edits Raise issue Connect a Data Source Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "list_data_handlers.html", "content": "List Data Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources List Data Handlers Connect a Data Source Get a Data Source Remove a Data Source List Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Data Sources List Data Handlers Here is how you can fetch all available data handlers directly from JavaScript code: const query = 'SHOW HANDLERS WHERE type = \\‘data\\'’; result = await MindsDB.SQL.runQuery(query); console.log(result); Was this page helpful? Yes No Suggest edits Raise issue Connect Connect a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "create_job.html", "content": "Create a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Create a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Overview Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Jobs Create a Job This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Query a File Remove a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "collection-structure.html", "content": "Collection Structure - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Collection Structure Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Collection Structure ​ General Structure On start-up, the MindsDB project consists of 1 collection: models . You can verify it by running the following MQL commands: First, switch to the mindsdb project. > use mindsdb < 'switched to db mindsdb' Here is the doc page on the PROJECT entity to learn more. Then, query for the available collections. > show collections < models ​ The models Collection The models collection stores information about the models, such as name, version, status, accuracy, and more. > db.models.find ( { } ) < { NAME: 'home_rentals_model' , ENGINE: 'lightwood' , PROJECT: 'mindsdb' , VERSION: 1 , STATUS: 'complete' , ACCURACY: 1 , PREDICT: 'rental_price' , UPDATE_STATUS: 'up_to_date' , MINDSDB_VERSION: '23.2.4.0' , ERROR: null, SELECT_DATA_QUERY: 'db.home_rentals.find({})' , TRAINING_OPTIONS: \"{'target': 'rental_price', 'using': {}}\" , TAG: null, CREATED_AT: 2023 -02-24T16:05:43.248Z, _id: ObjectId ( \"000000000000007725907968\" ) } Where: Name Description NAME The name of the model. ENGINE The engine used to create the model. PROJECT The project where the model resides. VERSION The version of the model. STATUS Training status ( generating , or training , or complete , or error ). ACCURACY The model accuracy ( 0.999 is a sample accuracy value). PREDICT The name of the target column to be predicted. UPDATE_STATUS Training update status ( up_to_date , or updating , or available ). MINDSDB_VERSION The MindsDB version used while training ( 22.8.2.1 is a sample version value). ERROR Error message stores a value in case of an error, otherwise, it is null. SELECT_DATA_QUERY It is a query that selects input data. TRAINING_OPTIONS Additional training parameters. TAG The models can have tags to classify them into categories. Check out this doc page to learn how to create, view, and drop models. Check out this doc page to learn how to work with models’ versions. To view all databases/projects, use the command below. > show databases < admin 64.00 KiB information_schema 64.00 KiB mindsdb 64.00 KiB files 64.00 KiB mongo_demo_db 64.00 KiB Please note that mindsdb is the default project. Here is the doc page on the PROJECT entity to learn more. Please follow the MindsDB Information Schema doc page to learn more. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page General Structure The models Collection"}
{"file_name": "find.html", "content": "Making Predictions using ML Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Making Predictions using ML Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Making Predictions using ML Models ​ The find() Method ​ Description The find() method is used to get predictions from the model table. The data is not persistent - it is returned on the fly as a result-document. ​ Syntax Here is the syntax: db . predictor_name . find ( { column : \"value\" , column : \"value\" } ) ; On execution, we get: { \"column_name1\" : \"value\" , \"column_name2\" : \"value\" , ...columns \"select_data_query\" : null , \"when_data\" : null , \"target_name_original\" : \"value\" , \"target_name_confidence\" : \"value\" , \"target_name_explain\" : \"{\\\"predicted_value\\\": value, \\\"confidence\\\": value, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": value \\\"confidence_upper_bound\\\": value}\" , \"target_name_anomaly\" : \"value\" , \"target_name_min\" : \"value\" , \"target_name_max\" : \"value\" } Where: Expressions Description \"target_name_original\" The real value of the target variable from the collection. \"target_name_confidence\" Model confidence. \"target_name_explain\" JSON object that contains additional information, such as predicted_value , confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . \"target_name_anomaly\" Model anomaly. \"target_name_min\" Lower bound value. \"target_name_max\" Upper bound value. ​ Example ​ Making a Single Prediction The following MQL statement fetches the predicted value of the rental_price column from the home_rentals_model model. The predicted value is the rental price of a property with attributes listed as a parameter to the find() method. db . home_rentals_model . find ( {sqft: \"823\" , location: \"good\" , neighborhood: \"downtown\" , days_on_market: \"10\" } ) ; On execution, we get: { \"sqft\" : 823 , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : 10 , \"number_of_rooms\" : null , \"number_of_bathrooms\" : null , \"initial_price\" : null , \"rental_price\" : 1431.323795180614 , \"select_data_query\" : null , \"when_data\" : null , \"rental_price_original\" : null , \"rental_price_confidence\" : 0.99 , \"rental_price_explain\" : \"{\\\"predicted_value\\\": 1431.323795180614, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 1379.4387560440227, \\\"confidence_upper_bound\\\": 1483.2088343172054}\" , \"rental_price_anomaly\" : null , \"rental_price_min\" : 1379.4387560440227 , \"rental_price_max\" : 1483.2088343172054 } ​ Making Bulk Predictions Bulk Predictions WIP The bulk predictions is a work in progress. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The find() Method Description Syntax Example Making a Single Prediction Making Bulk Predictions"}
{"file_name": "insert.html", "content": "Creating Predictors in Mongo - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Creating Predictors in Mongo Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Creating Predictors in Mongo Predictors are the machine learning models that enable us to forecast future data based on the available data. By using the db.models.insertOne() method, we create and train predictors in Mongo. ​ The db.models.insertOne() Method ​ Description The db.models.insertOne() method creates and trains a new model. ​ Syntax Here is the syntax: db . models . insertOne ( { name: \"predictor_name\" , predict: \"target_column\" , connection: \"integration_name\" , select_data_query: \"db.collection_name.find({})\" } ) ; On execution, we get: WriteResult( { \"nInserted\" : 1 } ) Where: Expressions Description name The name of the model to be created. predict The name of the target column to be predicted. connection The name of the integration created via the db.databases.insertOne() method or file upload . select_data_query Object that stores the data collection name to be used for training and validation and additional arguments for filtering the data. Checking Predictor Status After running the db.models.insertOne() method, execute the db.models.find() method from the mindsdb.models collection to check the status of the model. db . models . find ( {name: \"model_name\" } ) ; ​ Example ​ Creating a Predictor This example shows how you can create and train the home_rentals_model machine learning model to predict the rental prices for real estate properties inside the dataset. db . models . insertOne ( { name: \"home_rentals_model\" , predict: \"rental_price\" , connection: \"mongo_integration\" , select_data_query: \"db.home_rentals.find({})\" } ) ; On execution, we get: WriteResult( { \"nInserted\" : 1 } ) ​ Checking Predictor Status To check the predictor status, query the mindsdb.models table using the db.models.find() command. db . models . find ( {name: \"home_rentals_model\" } ) ; On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : 0.91 , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The db.models.insertOne() Method Description Syntax Example Creating a Predictor Checking Predictor Status"}
{"file_name": "mindsdb-mongo-ql-overview.html", "content": "MindsDB Mongo-QL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation MongoDB API MindsDB Mongo-QL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK MongoDB API MindsDB Mongo-QL MindsDB offers a tailored Mongo-QL syntax that enables seamless interaction with a wide range of objects, including collections, databases, and models. Moreover, you have the flexibility to connect MindsDB with MongoDB Compass and MongoDB Shell, facilitating streamlined integration. After connecting to MindsDB from Mongo, switch to the default project by executing > use mindsdb . ​ Connect to MindsDB This section contains guides on how to connect MindsDB to MongoDB clients. MongoDB Compass MongoDB Shell ​ Mongo-QL Syntax This section contains guides on how to work with collections, databases, and models. Collection Structure ML Engines Connect Databases Create Models Describe Models Make Predictions Delete Models Was this page helpful? Yes No Suggest edits Raise issue Overview MongoDB Compass github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect to MindsDB Mongo-QL Syntax"}
{"file_name": "delete.html", "content": "Deleting a Predictor - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Deleting a Predictor Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Deleting a Predictor ​ The db.models.deleteOne() Method ​ Description The db.models.deleteOne() method deletes an ML model specified in its argument. ​ Syntax Here is the syntax: db . models . deleteOne ( {name: \"predictor_name\" } ) ; On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Where: Name Description name Name of the model to be deleted. ​ Example ​ Listing All the Predictors Before deleting a predictor, let’s list all the available predictors using the db.models.find() method. db . models . find ( {} ) ; On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } , { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } ​ Dropping a Predictor The db.models.deleteOne() method drops the model collection called home_rentals_model . db . models . deleteOne ( {name: \"home_rentals_model\" } ) ; On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } ​ Validating the Deletion You can validate that the model was removed by listing all the predictors. db . models . find ( {} ) ; On execution, we get: { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The db.models.deleteOne() Method Description Syntax Example Listing All the Predictors Dropping a Predictor Validating the Deletion"}
{"file_name": "database.html", "content": "Connecting Databases in Mongo - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connecting Databases in Mongo Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK Connecting Databases in Mongo Integrations, or external databases, provide data to be used for making forecasts. Here, we use the databases.insertOne() method to connect the integrations to Mongo. ​ The db.databases.insertOne() Method ​ Description MindsDB enables adding databases to your Mongo instance using the db.databases.insertOne() method. Our MindsDB Mongo API supports creating a connection by passing the database credentials. ​ Syntax Here is the syntax: db . databases . insertOne ( { name: \"mongo_int\" , engine : \"mongodb\" , connection_args: { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ) ; On execution, we get: { \"acknowledged\" : true , \"insertedId\" : ObjectId( \"62dff63c6cc2fa93e1d7f12c\" ) } Where: Name Description name Identifier for the data source to be created. engine Database engine to be selected. connection_args {\"key\":\"value\"} object storing the connection parameters such as port, host, database. ​ Example ​ Creating a New Connection Here is an example of how to connect to the local MongoDB. db . databases . insertOne ( { name: \"mongo_local\" , engine : \"mongodb\" , connection_args: { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ) ; On execution, we get: { \"acknowledged\" : true , \"insertedId\" : ObjectId( \"62dff63c6cc2fa93e1d7f12c\" ) } ​ Listing Linked Databases You can list all the linked databases using the following command: SHOW dbs ; On execution, we get: + --------------------+ | Database | + --------------------+ | admin | | files | | information_schema | | mindsdb | | mongo_int | | views | + --------------------+ Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The db.databases.insertOne() Method Description Syntax Example Creating a New Connection Listing Linked Databases"}
{"file_name": "ml_engine.html", "content": "ML Engines in Mongo - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation ML Engines in Mongo Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Python SDK JavaScript SDK ML Engines in Mongo To create a model, you need an underlying ML engine, such as Lightwood or OpenAI. MindsDB uses the Lightwood engine by default if none other is provided. You can check the available ML engines in the ML Engines section. ​ The db.ml_engines.insertOne Method ​ Description This method creates an ML engine based on one of the available ML handlers. ​ Syntax Here is the syntax: db . ml_engines . insertOne ( { \"name\" : \"ml_engine_name\" , \"handler\" : \"handler_name\" , \"params\" : { \"key\" : \"value\" } } ) On execution, we get: { acknowledged: true , insertedId: ObjectId ( \"6465e96766d152ae1e247802\" ) } ​ Example Let’s create an OpenAI engine that uses the OpenAI handler. db . ml_engines . insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"qqq\" } } ) On execution, we get: { acknowledged: true , insertedId: ObjectId ( \"6465e9d566d152ae1e247803\" ) } ​ The db.ml_engines.find Method ​ Description This method lists all the available ML engines. ​ Syntax Here is the syntax: db . ml_engines . find ( ) On execution, we get: { NAME: 'lightwood' , HANDLER : 'lightwood' , CONNECTION_DATA: { key : [ 'password' ] , value : [ '' ] } } { NAME: 'openai' , HANDLER : 'openai' , CONNECTION_DATA: { key : [ 'password' ] , value : [ '' ] } } . . . ​ The db.ml_engines.deleteOne Method ​ Description This method deletes an ML engine. ​ Syntax Here is the syntax: db . ml_engines . deleteOne ( { \"name\" : \"ml_engine_name\" } ) On execution, we get: { acknowledged: true , deletedCount: 1 } ​ Example Let’s delete an OpenAI engine created earlier. db . ml_engines . deleteOne ( { \"name\" : \"openai_engine\" } ) On execution, we get: { acknowledged: true , deletedCount: 1 } Check out how to build an ML handler . To learn more about ML engines, visit the ML Engines section. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The db.ml_engines.insertOne Method Description Syntax Example The db.ml_engines.find Method Description Syntax The db.ml_engines.deleteOne Method Description Syntax Example"}
{"file_name": "list-projects.html", "content": "List Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects List Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Jobs Python SDK JavaScript SDK Projects List Projects This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Remove a Project Create, Train, and Deploy a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "insertOne.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Jobs Python SDK JavaScript SDK Projects Create a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "deleteOne.html", "content": "Remove a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Remove a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Jobs Python SDK JavaScript SDK Projects Remove a Project This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Create a Project List Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "query-jobs.html", "content": "Query Jobs - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Query Jobs Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Jobs Create a Job Remove a Job Query Jobs Python SDK JavaScript SDK Jobs Query Jobs You can query the jobs and jobs_history tables to find out more about all created jobs and their execution history. > db . jobs . find ( ) > db . jobs_history . find ( ) Was this page helpful? Yes No Suggest edits Raise issue Remove a Job Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "insertOne.html", "content": "Create a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Create a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Jobs Create a Job Remove a Job Query Jobs Python SDK JavaScript SDK Jobs Create a Job ​ Description The db.jobs.insertOne() function creates and schedules a job. ​ Syntax Here is the syntax: db.jobs.insertOne({ 'name': 'job2', 'query': \"select * from models\" }) And here is a more complex example: db.jobs.insertOne({ 'name': 'job1', 'schedule_str': 'every day', 'start_at': '2023-03-30', 'end_at': '2023-03-30 11:11:11', 'query': \"\\ db.home_rentals_model.aggregate([\\ {'$match': {\\ 'collection': 'example_db.demo_data.home_rentals',\\ 'query': {\\ 'number_of_rooms': 2\\ }\\ }},\\ {'$project': {\\ 'home_rentals.rental_price': 'real_price',\\ 'home_rentals_model.rental_price': 'predicted_price',\\ }},\\ {'$limit': 2},\\ {'$out': {'db': 'photorep', 'coll': 'aaa', 'append': true}}\\ ])\\ \" }) Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Remove a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "deleteOne.html", "content": "Remove a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Remove a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Jobs Create a Job Remove a Job Query Jobs Python SDK JavaScript SDK Jobs Remove a Job ​ Description The db.jobs.deleteOne() function deletes a job. ​ Syntax Here is the syntax: db.jobs.deleteOne({'name': \"job_name\"}) Was this page helpful? Yes No Suggest edits Raise issue Create a Job Query Jobs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list-data-handlers.html", "content": "List Data Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Data Sources List Data Handlers You can list all available data handlers by querying the information_schema database. > use information_schema > db . handlers . find ( { 'type' : 'data' } ) Was this page helpful? Yes No Suggest edits Raise issue MongoDB Shell Connect a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "find.html", "content": "List Data Sources - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Sources Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Data Sources List Data Sources ​ Description The show databases command lists all data sources connected to MindsDB. ​ Syntax First, switch to mindsdb using the use command. use mindsdb Then, list all data sources connected to MindsDB: show databases Was this page helpful? Yes No Suggest edits Raise issue Remove a Data Source Use a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "insertOne.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Data Sources Connect a Data Source ​ Description MindsDB enables adding databases to your Mongo instance using the db.databases.insertOne() method. Our MindsDB Mongo API supports creating a connection by passing the database credentials. ​ Syntax Here is the syntax: db . databases . insertOne ( { name: \"mongo_int\" , engine : \"mongodb\" , connection_args: { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ) ; Alternatively, you can use the host parameter alone, providing the connection string, as below: db . databases . insertOne ( { name: \"mongo_int\" , engine : \"mongodb\" , connection_args: { \"host\" : \"mongodb+srv://user:pass@db.xxxyyy.mongodb.net/\" } } ) ; On execution, we get: { \"acknowledged\" : true , \"insertedId\" : ObjectId( \"62dff63c6cc2fa93e1d7f12c\" ) } Where: Name Description name Identifier for the data source to be created. engine Database engine to be selected. connection_args {\"key\":\"value\"} object storing the connection parameters such as port, host, database. ​ Example Here is an example of how to connect to the local MongoDB. db . databases . insertOne ( { name: \"mongo_local\" , engine : \"mongodb\" , connection_args: { \"port\" : 27017 , \"host\" : \"mongodb+srv://admin:@localhost\" , \"database\" : \"test_data\" } } ) ; On execution, we get: { \"acknowledged\" : true , \"insertedId\" : ObjectId( \"62dff63c6cc2fa93e1d7f12c\" ) } You can list all the linked databases using the following command: SHOW dbs ; On execution, we get: + --------------------+ | Database | + --------------------+ | admin | | files | | information_schema | | mindsdb | | mongo_int | | views | + --------------------+ Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "deleteOne.html", "content": "Remove a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Remove a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Data Sources Remove a Data Source ​ Description The db.databases.deleteOne() method deletes a connection to a data source specified in its argument. ​ Syntax Here is the syntax: db . databases . deleteOne ( {name: \"connection_name\" } ) ; On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Where: Name Description name Name of the connection to be deleted. Was this page helpful? Yes No Suggest edits Raise issue Connect a Data Source List Data Sources github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "use.html", "content": "Use a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Use a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Data Sources Use a Data Source ​ Description The use collection_name statement switches to the provided collection_name to execute operations on this collection. ​ Syntax Here s how to use it: use collection_name Was this page helpful? Yes No Suggest edits Raise issue List Data Sources List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "find.html", "content": "List ML Engines - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Engines Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines List Data Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK AI/ML Engines List ML Engines ​ Description This method lists all the available ML engines. ​ Syntax Here is the syntax: db . ml_engines . find ( ) On execution, we get: { NAME: 'lightwood' , HANDLER : 'lightwood' , CONNECTION_DATA: { key : [ 'password' ] , value : [ '' ] } } { NAME: 'openai' , HANDLER : 'openai' , CONNECTION_DATA: { key : [ 'password' ] , value : [ '' ] } } . . . Was this page helpful? Yes No Suggest edits Raise issue Remove an ML Engine Create a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "insertOne.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines List Data Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK AI/ML Engines Configure an ML Engine ​ Description This method creates an ML engine based on one of the available ML handlers. ​ Syntax Here is the syntax: db . ml_engines . insertOne ( { \"name\" : \"ml_engine_name\" , \"handler\" : \"handler_name\" , \"params\" : { \"key\" : \"value\" } } ) On execution, we get: { acknowledged: true , insertedId: ObjectId ( \"6465e96766d152ae1e247802\" ) } ​ Example Let’s create an OpenAI engine that uses the OpenAI handler. db . ml_engines . insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"qqq\" } } ) On execution, we get: { acknowledged: true , insertedId: ObjectId ( \"6465e9d566d152ae1e247803\" ) } Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "deleteOne.html", "content": "Remove an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Remove an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines List Data Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK AI/ML Engines Remove an ML Engine ​ Description This method deletes an ML engine. ​ Syntax Here is the syntax: db . ml_engines . deleteOne ( { \"name\" : \"ml_engine_name\" } ) On execution, we get: { acknowledged: true , deletedCount: 1 } ​ Example Let’s delete an OpenAI engine created earlier. db . ml_engines . deleteOne ( { \"name\" : \"openai_engine\" } ) On execution, we get: { acknowledged: true , deletedCount: 1 } Was this page helpful? Yes No Suggest edits Raise issue Configure an ML Engine List ML Engines github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "list-ml-handlers.html", "content": "List Data Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List Data Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines List Data Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK AI/ML Engines List Data Handlers You can list all available ML handlers by querying the information_schema database. > use information_schema > db . handlers . find ( { 'type' : 'ml' } ) Was this page helpful? Yes No Suggest edits Raise issue Use a Data Source Configure an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "describe.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Describe a Model The describe command is used to display the attributes of an existing model. It accepts the name of the model and describe attribute separated by ’.’ as an argument. Here is how to call the describe command: db . runCommand ( { describe : \"predictor_name.attribute\" } ) Where: Name Description predictor_name The name of the predictor whose statistics you want to see. attribute The argument of the describe command defines the type of statistics ( features , or model , or ensemble ). ​ The describe Command with the features Parameter ​ Description The db.runCommand({describe: \"predictor_name.features\"}) command is used to display the way the model encoded the data before training. ​ Syntax Here is the syntax: db . runCommand ( { describe : \"predictor_name.features\" } ) On execution, we get: { \"data\" : [ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } ] } Where: Name Description \"column\" The name of the column. \"type\" Type of the inferred data. \"encoder\" Encoder used. \"role\" Role of the column ( feature or target ). ​ Example Let’s describe the home_rentals_model model. db . runCommand ( { describe : \"home_rentals_model.features\" } ) On execution, we get: { \"data\" : [ { \"column\" : \"number_of_rooms\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"number_of_bathrooms\" , \"type\" : \"binary\" , \"encoder\" : \"BinaryEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"sqft\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"location\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"days_on_market\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"initial_price\" , \"type\" : \"integer\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"neighborhood\" , \"type\" : \"categorical\" , \"encoder\" : \"OneHotEncoder\" , \"role\" : \"feature\" } , { \"column\" : \"rental_price\" , \"type\" : \"float\" , \"encoder\" : \"NumericEncoder\" , \"role\" : \"target\" } ] , \"ns\" : \"mindsdb.home_rentals_model\" } ​ The describe Command with the model Parameter ​ Description The db.runCommand({describe: \"predictor_name.model\"}) method is used to display the performance of the candidate models. ​ Syntax Here is the syntax: db . runCommand ( { describe : \"predictor_name.model\" } ) On execution, we get: { \"data\" : [ { \"name\" : \"candidate_model\" , \"performance\" : < 0.0 | 1.0 > , \"training_time\" : <seconds> , \"selected\" : < 0 | 1 > } ] } Where: Name Description \"name\" Name of the candidate model. \"performance\" Accuracy from 0 to 1 depending on the type of the model. \"training_time\" Time elapsed for the training of the model. \"selected\" 1 for the best performing model and 0 for the rest. ​ Example Let’s see the output for the home_rentals_model model. db . runCommand ( { describe : \"home_rentals_model.model\" } ) On execution, we get: { \"data\" : [ { \"name\" : \"Neural\" , \"performance\" : 0.999 , \"training_time\" : 48.37 , \"selected\" : 0 } , { \"name\" : \"LightGBM\" , \"performance\" : 1 , \"training_time\" : 33 , \"selected\" : 1 } , { \"name\" : \"Regression\" , \"performance\" : 0.999 , \"training_time\" : 0.05 , \"selected\" : 0 } ] , \"ns\" : \"mindsdb.home_rentals_model\" } ​ The describe Command with the ensemble Parameter ​ Description The db.runCommand({describe: \"predictor_name.ensemble\"}) command is used to display the parameters used to select the best candidate model. ​ Syntax Here is the syntax: db . runCommand ( { describe : \"predictor_name.ensemble\" } ) On execution, we get: + -----------------+ | ensemble | + -----------------+ | {JSON} | + -----------------+ Where: Name Description ensemble Object of the JSON type describing the parameters used to select the best candidate model. Was this page helpful? Yes No Suggest edits Raise issue List Models Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The describe Command with the features Parameter Description Syntax Example The describe Command with the model Parameter Description Syntax Example The describe Command with the ensemble Parameter Description Syntax"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Finetune a Model ​ Description The db.models.insertOne({\"name\": \"model_name\",\"action\": \"finetune\"}) function is used finetune a model. ​ Syntax Here is the syntax: db . models . insertOne ( { \"name\" : \"model_name\" , \"action\" : \"finetune\" } ) Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get-single-prediction.html", "content": "Get a Single Prediction - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get a Single Prediction Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Jobs Python SDK JavaScript SDK Predictions Get a Single Prediction ​ Description The db.model_name.find() function fetches predictions from the model table. The data is returned on the fly and the result set is not persisted. If you want to save your predictions, you can utilize a view or a table. ​ Syntax Here is the syntax for making a single prediction: db . predictor_name . find ( { column : \"value\" , column : \"value\" } ) ; On execution, we get: { \"column_name1\" : \"value\" , \"column_name2\" : \"value\" , ...columns \"select_data_query\" : null , \"when_data\" : null , \"target_name_original\" : \"value\" , \"target_name_confidence\" : \"value\" , \"target_name_explain\" : \"{\\\"predicted_value\\\": value, \\\"confidence\\\": value, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": value \\\"confidence_upper_bound\\\": value}\" , \"target_name_anomaly\" : \"value\" , \"target_name_min\" : \"value\" , \"target_name_max\" : \"value\" } Where: Expressions Description \"target_name_original\" The real value of the target variable from the collection. \"target_name_confidence\" Model confidence. \"target_name_explain\" JSON object that contains additional information, such as predicted_value , confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . \"target_name_anomaly\" Model anomaly. \"target_name_min\" Lower bound value. \"target_name_max\" Upper bound value. ​ Example The following MQL statement fetches the predicted value of the rental_price column from the home_rentals_model model. The predicted value is the rental price of a property with attributes listed as a parameter to the find() method. db . home_rentals_model . find ( {sqft: \"823\" , location: \"good\" , neighborhood: \"downtown\" , days_on_market: \"10\" } ) ; On execution, we get: { \"sqft\" : 823 , \"location\" : \"good\" , \"neighborhood\" : \"downtown\" , \"days_on_market\" : 10 , \"number_of_rooms\" : null , \"number_of_bathrooms\" : null , \"initial_price\" : null , \"rental_price\" : 1431.323795180614 , \"select_data_query\" : null , \"when_data\" : null , \"rental_price_original\" : null , \"rental_price_confidence\" : 0.99 , \"rental_price_explain\" : \"{\\\"predicted_value\\\": 1431.323795180614, \\\"confidence\\\": 0.99, \\\"anomaly\\\": null, \\\"truth\\\": null, \\\"confidence_lower_bound\\\": 1379.4387560440227, \\\"confidence_upper_bound\\\": 1483.2088343172054}\" , \"rental_price_anomaly\" : null , \"rental_price_min\" : 1379.4387560440227 , \"rental_price_max\" : 1483.2088343172054 } Was this page helpful? Yes No Suggest edits Raise issue Manage Model Versions Get Batch Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "find.html", "content": "List Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models List Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models List Models ​ Description The db.models.find() function lists all models. ​ Syntax We can list all the available predictors using the db.models.find() method. db . models . find ( {} ) ; On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } , { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Was this page helpful? Yes No Suggest edits Raise issue Remove a Model Describe a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "manage-model-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Manage Model Versions This feature is in progress. Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "get-batch-predictions.html", "content": "Get Batch Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get Batch Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Jobs Python SDK JavaScript SDK Predictions Get Batch Predictions ​ Description The db.model_name.find() function fetches predictions from the model table. The data is returned on the fly and the result set is not persisted. If you want to save your predictions, you can utilize a view or a table. ​ Syntax Here is the syntax for making a batch predictions: db . model_name . find ( { \"collection\" : \"integration_name.collection_name\" , \"query\" : { 'column' : 'value' , \"date\" : { \"$gt\" : ISODate ( \"2018-03-31T00:00:00.000Z\" ) }}} ) This command returns predictions made for all data rows from integration_name.collection_name . Was this page helpful? Yes No Suggest edits Raise issue Get a Single Prediction Create a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "insertOne.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Create, Train, and Deploy a Model ​ Description The db.models.insertOne() method creates and trains a new model. ​ Syntax Here is the syntax: db . models . insertOne ( { name: \"predictor_name\" , predict: \"target_column\" , connection: \"integration_name\" , select_data_query: \"db.collection_name.find({})\" } ) ; On execution, we get: WriteResult( { \"nInserted\" : 1 } ) Where: Expressions Description name The name of the model to be created. predict The name of the target column to be predicted. connection The name of the integration created via the db.databases.insertOne() method or file upload . select_data_query Object that stores the data collection name to be used for training and validation and additional arguments for filtering the data. Checking Predictor Status After running the db.models.insertOne() method, execute the db.models.find() method from the mindsdb.models collection to check the status of the model. db . models . find ( {name: \"model_name\" } ) ; ​ Example This example shows how you can create and train the home_rentals_model machine learning model to predict the rental prices for real estate properties inside the dataset. db . models . insertOne ( { name: \"home_rentals_model\" , predict: \"rental_price\" , connection: \"mongo_integration\" , select_data_query: \"db.home_rentals.find({})\" } ) ; On execution, we get: WriteResult( { \"nInserted\" : 1 } ) To check the predictor status, query the mindsdb.models table using the db.models.find() command. db . models . find ( {name: \"home_rentals_model\" } ) ; On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : 0.91 , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Was this page helpful? Yes No Suggest edits Raise issue List Projects Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "deleteOne.html", "content": "Remove a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Remove a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Remove a Model ​ Description The db.models.deleteOne() method deletes an ML model specified in its argument. ​ Syntax Here is the syntax: db . models . deleteOne ( {name: \"predictor_name\" } ) ; On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } Where: Name Description name Name of the model to be deleted. ​ Example Before deleting a predictor, let’s list all the available predictors using the db.models.find() method. db . models . find ( {} ) ; On execution, we get: { \"name\" : \"home_rentals_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"rental_price\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } , { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } The db.models.deleteOne() method drops the model collection called home_rentals_model . db . models . deleteOne ( {name: \"home_rentals_model\" } ) ; On execution, we get: { \"acknowledged\" : true , \"deletedCount\" : 1 } You can validate that the model was removed by listing all the predictors. db . models . find ( {} ) ; On execution, we get: { \"name\" : \"other_model\" , \"status\" : \"complete\" , \"accuracy\" : \"1.0\" , \"predict\" : \"value_to_be_predicted\" , \"update_status\" : \"up_to_date\" , \"mindsdb_version\" : \"22.8.3.1\" , \"error\" : null , \"select_data_query\" : \"\" , \"training_options\" : \"\" } Was this page helpful? Yes No Suggest edits Raise issue Create, Train, and Deploy a Model List Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Jobs Python SDK JavaScript SDK Models Retrain a Model ​ Description The db.models.insertOne({\"name\": \"model_name\",\"action\": \"retrain\"}) function is used retrain a model. ​ Syntax Here is the syntax: db . models . insertOne ( { \"name\" : \"model_name\" , \"action\" : \"retrain\" } ) Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "mongo-compass.html", "content": "MindsDB and MongoDB Compass - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MongoDB Compass Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect MongoDB Compass MongoDB Shell Data Sources AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Connect MindsDB and MongoDB Compass MongoDB Compass is a graphical user interface (GUI) for MongoDB. It provides detailed schema visualizations, real-time performance metrics, sophisticated querying abilities, and more. You can download MongoDB Compass here . ​ Overview Here is an overview of the connection between MindsDB and MongoDB Compass: Let’s go through the steps presented above: We connect MongoDB Compass to MindsDB. It is discussed in the following content. We connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details. Having completed steps 1 and 2, you can access the connected database from MongoDB Compass via MindsDB. ​ How to Connect Here is how to connect MongoDB Compass to MindsDB using either MindsDB Cloud or local installation. Please add the MindsDB Cloud Public IPs to the access list of your Mongo database. Local MindsDB First, create a new connection in MongoDB Compass by clicking the New Connection button in the left navigation panel. The host value is 127.0.0.1 and the port value is 47336 . Input it in the Host field, as below. ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials section, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don’t miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples. Have fun! From Our Community Check out the video guides created by our community: Video guide on How to connect Mongo Compass to MindsDB by HellFire Video guide on Integrating your MindsDB instance into MongoDB by Syed Zubeen Was this page helpful? Yes No Suggest edits Raise issue Overview MongoDB Shell github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Overview How to Connect What’s Next?"}
{"file_name": "mongo-shell.html", "content": "MindsDB and MongoDB Shell - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MongoDB Shell Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect MongoDB Compass MongoDB Shell Data Sources AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Connect MindsDB and MongoDB Shell MongoDB Shell is the quickest way to connect and work with MongoDB. MindsDB provides a powerful MongoDB API, allowing users to connect MindsDB to the MongoDB Shell . Please note that connection to MongoDB API provided by MindsDB is the same as connection to a MongoDB database. You can download MongoDB Shell here . ​ Overview Here is an overview of the connection between MindsDB and MongoDB Shell: Let’s go through the steps presented above: We connect MongoDB Shell to MindsDB. It is discussed in the following content. We connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details. Having completed steps 1 and 2, you can access the connected database from MongoDB Shell via MindsDB. ​ How to Connect Here is how to connect MongoDB Shell to MindsDB using either MindsDB Cloud or local installation. Please add the MindsDB Cloud Public IPs to the access list of your Mongo database. Upon opening the MongoDB Shell, you see the following message: Let’s look at the connection strings for both MindsDB Cloud and local installation. Local MindsDB Provide your local MindsDB connection string to connect to a local MindsDB installation. You can copy the connection string from the MongoDB Compass if you have already created a connection there. Here is a connection string to connect to a local MindsDB installation: mongodb://127.0.0.1:47336/ ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials section, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don’t miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples. Have fun! From Our Community Check out the video guide created by our community: Video guide on Easily connect to MindsDB Cloud from MongoShell by @akhilcoder Was this page helpful? Yes No Suggest edits Raise issue MongoDB Compass List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Overview How to Connect What’s Next?"}
{"file_name": "data-insights.html", "content": "Data Insights - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Insights Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Data Insights Data Insights is a data visualization feature of the MindsDB editor. It lets you explore the queried data by initially displaying and analyzing a subset of the first ten rows. You can choose to analyze a full dataset by clicking the Full Data Analysis button. The analysis presents the distribution of your data aggregated by column. The data used here comes from one of our tutorials. For details, click here . Before you see the Data Insights pane, you must run a SELECT query on your dataset. Let’s have a look at the available features. ​ Features ​ Distribution of Data per Column When opening the Data Insights pane, you see the distribution of data of each output dataset column. Initially, the visualization and analysis of the first ten rows is shown, as below. There is one histogram per column that depicts the column name, data types of the distribution, and the distribution itself. ​ Potential Bias Flag To see the Potential Bias flag, enter a full-screen mode of the Data Insights pane. Here, the location column exhibits potential bias, as there are more great column values than good or poor column values. Such cases are typically flagged. However, it does not necessarily mean that there is a problem with the dataset. The Potential Bias flag is used when data does not distribute normally or uniformly, likely over-representing or under-representing some values. This may be normal, hence, bias is only potential. ​ Missing Values Flag To see the Missing Values flag, enter a full-screen mode of the Data Insights pane. This flag indicates the proportion of missing values in a column. Columns with a high percentage of missing values are not useful for modeling purposes. Hence, it is recommended to pay attention to the Missing Values flag and try to mitigate it whenever possible, as it indicates the degrading quality of your data. ​ Hovering Over the Histogram When hovering over the histogram, you get the information on a particular column value and how many of such values are present in a column. The format is (column_value, count) . It is helpful to determine the exact data value counts from the histograms. ​ Full Data Analysis Let’s do a full data analysis step by step. First, we need to query data for analysis in the MindsDB editor. Please note that you need to query your dataset without using a LIMIT keyword to be able to perform a complete data analysis. SELECT * FROM example_db . demo_data . home_rentals ; On execution, we get: + ---------------+-------------------+----+--------+--------------+--------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+--------------+------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + ---------------+-------------------+----+--------+--------------+--------------+------------+ Now, open the Data Insights pane by clicking the Data Insights button to the right of the output table. Initially, it shows the analysis of the first ten rows of the output table. To perform a complete analysis of your data, you can either go to a full-screen mode or stay in a pane mode and click on the Full Data Analysis button. Below is the complete data analysis. Also, whenever your dataset changes, you can click on the Refresh Data Analysis button to update the data visualization and analysis. ​ What’s Next? Want to do more exploratory data analysis in MindsDB? We’re collecting feedback to develop even more data visualization features. Let us know what you’d like to see as part of Data Insights. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Features Distribution of Data per Column Potential Bias Flag Missing Values Flag Hovering Over the Histogram Full Data Analysis What’s Next?"}
{"file_name": "table-structure.html", "content": "MindsDB Schema - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more MindsDB Schema Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more MindsDB Schema Initially, MindsDB comprises three system databases and one default project, as follows: information_schema stores metadata of all the objects such as handlers, databases, AI engines, models, jobs, and more. log stores log data of models and jobs. files , which is initially empty, stores all files uploaded to MindsDB. mindsdb is the default project for storing models, views, jobs, triggers, and agents. List all databases by running the following SQL commands: SHOW [ FULL ] DATABASES ; Here is the output: + ----------------------+---------+--------+ | Database | TYPE | ENGINE | + ----------------------+---------+--------+ | information_schema | system | [ NULL ] | | log | system | [ NULL ] | | mindsdb | project | [ NULL ] | | files | data | files | + ----------------------+---------+--------+ ​ The information_schema Database The information_schema database contains all the system tables that correspond to MindsDB objects as follows: Name Description HANDLERS Stores all (data and AI) handlers, which are data integrations and AI integrations supported by MindsDB. DATABASES Stores all data sources connected to MindsDB. Note that corresponding handlers are required, and you can connect only the data sources supported by MindsDB after installing the required handler dependencies . ML_ENGINES Stores all AI/ML engines configured at MindsDB. Note that corresponding handlers are required, and you can connect only the AI/ML engines supported by MindsDB after installing the required handler dependencies . MODELS Stores all models deployed within the MindsDB ecosystem. Note that you can create and deploy a model only after configuring the corresponding AI/ML engine. VIEWS Stores all views created in MindsDB. JOBS Stores all jobs that facilitate workflow automation. TRIGGERS Stores all triggers that facilitate workflow automation. AGENTS Stores all AI agents created in MindsDB. SKILLS Stores all skills that can be assigned to AI agents . KNOWLEDGE_BASES Stores all knowledge bases that can be assigned to AI agents as skills. CHATBOTS Stores all chatbots that comprise an AI agent and a chat interface. Some of the objects, including DATABASES , ML_ENGINES , and MODELS , may contain sensitive information in the form of API keys or passwords. MindsDB hides this sensitive information by default. If you want to expose this sensitive information in the output when querying these objects, set the show_secrets flag to true . SET SHOW_SECRETS = TRUE ; And to hide them back, set it to false . SET SHOW_SECRETS = FALSE ; Use the SHOW command to list all objects as follows: SHOW object_name [ FROM project_name ] [ LIKE 'object_name_part%' ] [ WHERE key = value ] ; For instance, list all OpenAI models from the mindsdb project that contain ai in its name. SHOW MODELS FROM mindsdb LIKE '%ai%' WHERE engine = 'openai' ; Another example of how to query for the available data and AI handlers: SHOW HANDLERS WHERE type = 'data' ; SHOW HANDLERS WHERE type = 'ml' ; Before you can connect a data source using a data handler or create a model using an AI handler, make sure that the IMPORT_SUCCESS column reads true . If it reads false , then install the dependencies for this handler before using it. ​ The mindsdb Project MindsDB enables you to group all objects within projects . Projects can store models, views, jobs, triggers, agents, skills, knowledge bases, and chatbots. Projects store all objects except for handlers, connected data sources, and configured AI/ML engines. Note that based on the available handlers, you can connect a data source to MindsDB or configure an AI/ML engine within MindsDB. Having done that, you can, for instance, create a view with data from the connected data source and store it inside the project, or create a model based on the configured AI/ML engine and store it inside the project. MindsDB provides the default mindsdb project where all objects created without defining a project are stored. Learn more about how to create and manage projects here . ​ The files Database It is another default database that stores all the files uploaded to MindsDB. Here is how you can upload files to MindsDB . Was this page helpful? Yes No Suggest edits Raise issue Automation MindsDB Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The information_schema Database The mindsdb Project The files Database"}
{"file_name": "feature-eng.html", "content": "Feature Engineering in MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Preparation Feature Engineering in MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation Feature Engineering Feature Importance MindsDB GUI Overview Models Handlers Data Preparation Feature Engineering in MindsDB The more data you have, the more accurate predictions you get. We recommend you provide the predictor with as many historical data rows and data columns as possible to make your predictions even more accurate. The examples presented here prove this hypothesis. If you want to follow the examples, install MindsDB locally via Docker or Docker Desktop . ​ Prerequisites The base table is available in the example_db integration in the MindsDB Editor. In order to be able to use it, you must first create a database like this: CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Once that’s done, you can run the following commands with us. ​ Example: Adding More Data Columns ​ Introduction Here, we’ll create several predictors using the same table, increasing the number of data columns with each step. We start with the base table and create a predictor based on it. Then we add two columns to our base table and again create a predictor based on the enhanced table. At last, we add another two columns and create a predictor. By comparing the accuracies of the predictors, we’ll find that more data results in more accurate predictions. Let’s get started. ​ Let’s Run the Codes Here, we go through the codes for the base table and enhanced base tables simultaneously. ​ Data Setup Let’s prepare and verify the data. Here, we create the views and query them to ensure the input for the predictors is in order. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns Let’s start by querying the data from the example_db.demo_data.used_car_price table, which is our base table. SELECT * FROM example_db . demo_data . used_car_price LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | 55.4 | 1.4 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | 64.2 | 2 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | 55.4 | 1.4 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | 67.3 | 2 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | 49.6 | 1 | + -----+----+-----+------------+-------+--------+---+----+----------+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. mpg Miles per gallon. enginesize Engine size of the car. Dropping a View If you want to drop a view, run the command DROP VIEW view_name; . ​ Creating Predictors Now, we create models based on the example_db.demo_data.used_car_price table and its extensions. Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns CREATE MODEL mindsdb . price_predictor FROM example_db ( SELECT * FROM demo_data . used_car_price ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dropping a Predictor If you want to drop a predictor, run the command DROP MODEL predictor_name; . ​ Predictor Status Finally, let’s check the predictor status whose value is generating at first, then training , and at last, complete . Using the Base Table Using the Base Table + 2 More Columns Using the Base Table + 4 More Columns DESCRIBE price_predictor ; On execution, we get: + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ | price_predictor | complete | 0.963 | price | up_to_date | 22.10 .2 .1 | [ NULL ] | SELECT * FROM demo_data . used_car_price | | + ---------------+--------+--------+---------+-------------+---------------+------+--------------------------------------+----------------+ ​ Accuracy Comparison Once the training process of all three predictors completes, we see the accuracy values. For the base table, we get an accuracy value of 0.963 . For the base table with two more data columns, we get an accuracy value of 0.965 . The accuracy value increased, as expected. For the base table with four more data columns, we get an accuracy value of 0.982 . The accuracy value increased again, as expected. ​ True vs Predicted Price Comparison Let’s compare how close the predicted price values are to the true price. + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | model | year | transmission | fueltype | mileage | true_price | pred_price_1 | pred_price_2 | pred_price_3 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ | A1 | 2017 | Manual | Petrol | 7620 | 14440 | 17268 | 17020 | 14278 | | A6 | 2016 | Automatic | Diesel | 20335 | 18982 | 17226 | 17935 | 19016 | | A3 | 2018 | Semi - Auto | Diesel | 9058 | 19900 | 25641 | 23008 | 21286 | + -------+-------+---------------+-----------+-----------+--------------+----------------+----------------+---------------+ The prices predicted by the third predictor, having the highest accuracy value, are the closest to the true price, as expected. ​ Example: Joining Data Tables ​ Introduction We start by creating a predictor from the car_sales table. Then, we add more data by joining the car_sales and car_info tables. We create a predictor based on the car_sales_info view. Let’s get started. ​ Let’s Run the Codes Here, we go through the codes using partial tables and the full table after joining the data. ​ Data Setup Here is the car_sales table: SELECT * FROM example_db . demo_data . car_sales LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+ | model | year | price | transmission | mileage | fueltype | tax | + -----+----+-----+------------+-------+--------+---+ | A1 | 2017 | 12500 | Manual | 15735 | Petrol | 150 | | A6 | 2016 | 16500 | Automatic | 36203 | Diesel | 20 | | A1 | 2016 | 11000 | Manual | 29946 | Petrol | 30 | | A4 | 2017 | 16800 | Automatic | 25952 | Diesel | 145 | | A3 | 2019 | 17300 | Manual | 1998 | Petrol | 145 | + -----+----+-----+------------+-------+--------+---+ Where: Name Description model Model of the car. year Year of production. price Price of the car. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). mileage Mileage of the car. fueltype Fuel type of the car. tax Tax. And here is the car_info table: SELECT * FROM example_db . demo_data . car_info LIMIT 5 ; On execution, we get: + -----+----+------------+---------+-----+----------+ | model | year | transmission | fueltype | mpg | enginesize | + -----+----+------------+---------+-----+----------+ | A1 | 2010 | Automatic | Petrol | 53.3 | 1.4 | | A1 | 2011 | Manual | Diesel | 70.6 | 1.6 | | A1 | 2011 | Manual | Petrol | 53.3 | 1.4 | | A1 | 2012 | Automatic | Petrol | 50.6 | 1.4 | | A1 | 2012 | Manual | Diesel | 72.95 | 1.7 | + -----+----+------------+---------+-----+----------+ Where: Name Description model Model of the car. year Year of production. transmission Transmission ( Manual , or Automatic , or Semi-Auto ). fueltype Fuel type of the car. mpg Miles per gallon. enginesize Engine size of the car. Let’s join the car_sales and car_info tables on the model , year , transmission , and fueltype columns. SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) LIMIT 5 ; Nested SELECT Statements Please note that we use the nested SELECT statement in order to trigger native query at the MindsDB Cloud Editor. Here, the example_db database is a PostgreSQL database, so we trigger PostgreSQL-native syntax. On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53.3 | 1.4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70.6 | 1.6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70.6 | 1.6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53.3 | 1.4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53.3 | 1.4 | + -----+----+-----+------------+-------+--------+---+----+----------+ Now, we create a view based on the JOIN query: CREATE VIEW car_sales_info ( SELECT * FROM example_db ( SELECT s . * , i . mpg , i . enginesize FROM demo_data . car_sales s JOIN demo_data . car_info i ON s . model = i . model AND s . year = i . year AND s . transmission = i . transmission AND s . fueltype = i . fueltype ) ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Let’s verify the view by selecting from it. SELECT * FROM mindsdb . car_sales_info LIMIT 5 ; On execution, we get: + -----+----+-----+------------+-------+--------+---+----+----------+ | model | year | price | transmission | mileage | fueltype | tax | mpg | enginesize | + -----+----+-----+------------+-------+--------+---+----+----------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 125 | 53.3 | 1.4 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 20 | 70.6 | 1.6 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 0 | 70.6 | 1.6 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 125 | 53.3 | 1.4 | | A1 | 2011 | 7495 | Manual | 60700 | Petrol | 125 | 53.3 | 1.4 | + -----+----+-----+------------+-------+--------+---+----+----------+ ​ Creating Predictors Let’s create a predictor with the car_sales table as input data. CREATE MODEL mindsdb . price_predictor_car_sales FROM example_db ( SELECT * FROM demo_data . car_sales ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, let’s create a predictor for the table that is a JOIN between the car_sales and car_info tables. CREATE MODEL mindsdb . price_predictor_car_sales_info FROM mindsdb ( SELECT * FROM car_sales_info ) PREDICT price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) ​ Predictor Status Next, we check the status of both predictors. We start with the predictor based on the partial table. DESCRIBE price_predictor_car_sales ; On execution, we get: + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ | price_predictor_car_sales | complete | 0.912 | price | up_to_date | 22.10 .2 .1 | [ NULL ] | SELECT * FROM demo_data . car_sales | | + -------------------------+--------+--------+---------+-------------+---------------+------+---------------------------------+----------------+ And now, for the predictor based on the full table. DESCRIBE price_predictor_car_sales_info ; On execution, we get: + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ | price_predictor_car_sales_info | complete | 0.912 | price | up_to_date | 22.10 .2 .1 | [ NULL ] | SELECT * FROM car_sales_info | | + ------------------------------+--------+--------+---------+-------------+---------------+------+----------------------------+----------------+ ​ Accuracy Comparison The accuracy values are 0.912 for both the predictors. The predictor already learns how the combination of model+year+transmission+fueltype affects the price, so joining more data columns doesn’t play a role in this particular example. Was this page helpful? Yes No Suggest edits Raise issue AI Tables Feature Importance github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Example: Adding More Data Columns Introduction Let’s Run the Codes Data Setup Creating Predictors Predictor Status Accuracy Comparison True vs Predicted Price Comparison Example: Joining Data Tables Introduction Let’s Run the Codes Data Setup Creating Predictors Predictor Status Accuracy Comparison"}
{"file_name": "project.html", "content": "MindsDB Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Learn more MindsDB Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation MindsDB GUI Overview Models Handlers Learn more MindsDB Projects MindsDB enables you to group all objects within projects . Projects store all MindsDB schema objects except for handlers, connected data sources, and configured AI/ML engines. That is, projects can store models, views, jobs, triggers, agents, skills, knowledge bases, and chatbots. MindsDB provides the default mindsdb project where all objects created without defining a project are stored. ​ Working with MindsDB Projects ​ Create a Project Use the below command to create a project. CREATE PROJECT project_name ; Use lower-case letters for a project name. ​ List All Projects Use the below command to list all projects. SHOW [ FULL ] DATABASES WHERE type = 'project' ; ​ Create an Object within a Project Use the below command template to create an object within a project. CREATE < OBJECT > project_name . object_name . . . ; ​ Drop a Project Use the below command to remove a project. DROP PROJECT project_name ; Please note that if your project stores at least one object, it cannot be removed. In this case, you should first drop all the objects belonging to this project, and then, you can remove the project. Please see the Example section for details. ​ Example Let’s create a project. CREATE PROJECT my_project ; To verify that the project was created successfully, let’s run the command below to select all databases, including connected data sources and projects. SHOW FULL DATABASES ; On execution, we get: + ------------------+-------+------+ | Database | TYPE | ENGINE | + ------------------+-------+------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | my_project | project | [ NULL ] | | files | data | files | + ------------------+-------+------+ Please note that information_schema is the system database, mindsdb is the default project, and files is the database to store all uploaded files. For more information, please visit our docs on MindsDB default structure . Now we create a model within the project. CREATE MODEL my_project . my_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price ; Also, let’s create a view. CREATE VIEW my_project . my_view ( SELECT * FROM example_db . demo_data . home_rentals ) ; Here is what we have in the my_project project. SHOW TABLES FROM my_project ; On execution, we get: + --------------------+ | Tables_in_my_project | + --------------------+ | my_model | | my_view | + --------------------+ Let’s try to delete our project. DROP PROJECT my_project ; On execution, we get: Project 'my_project' can not be deleted , because it contains tables : my_model , my_view Users should remove all project content before dropping a project. DROP MODEL my_project . my_model ; DROP VIEW my_project . my_view ; Now we can proceed to drop a project. DROP PROJECT my_project ; Next Steps Below are the links to help you explore further. Find out how to create and use projects . Learn more about MindsDB Schema . Was this page helpful? Yes No Suggest edits Raise issue MindsDB Schema AI Tables github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Working with MindsDB Projects Create a Project List All Projects Create an Object within a Project Drop a Project Example"}
{"file_name": "native-queries.html", "content": "Native Queries - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Native Queries Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Native Queries The underlying database engine of MindsDB is MySQL. However, you can run queries native to your database engine within MindsDB. ​ Connect your Database to MindsDB To run queries native to your database, you must first connect your database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Here we connect the example_db database, which is a PostgreSQL database. ​ Run Queries Native to your Database Once we have our PostgreSQL database connected, we can run PostgreSQL-native queries. ​ Querying To run PostgreSQL-native code, we must nest it within the SELECT statement like this: SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST ( ( mpg / 2.3521458 ) AS numeric ) , 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ; On execution, we get: + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | mpg | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 53.3 | 22.7 | 12 | 1 | 0.013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 53.3 | 22.7 | 11 | 5 | 0.018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 53.3 | 22.7 | 11 | 5 | 0.02 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 70.6 | 30 | 11 | 5 | 0.005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 70.6 | 30 | 11 | 5 | 0 | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ The first line ( SELECT * FROM example_db ) informs MindsDB that we select from a PostgreSQL database. After that, we nest a PostgreSQL code within brackets. ​ Creating Views We can create a view based on a native query. CREATE VIEW cars FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST ( ( mpg / 2.3521458 ) AS numeric ) , 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Was this page helpful? Yes No Suggest edits Raise issue Query a Table Update a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect your Database to MindsDB Run Queries Native to your Database Querying Creating Views"}
{"file_name": "feature-importance.html", "content": "Feature Importance of MindsDB and Lightwood - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Preparation Feature Importance of MindsDB and Lightwood Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more MindsDB Schema MindsDB Projects AI Tables Data Preparation Feature Engineering Feature Importance MindsDB GUI Overview Models Handlers Data Preparation Feature Importance of MindsDB and Lightwood MindsDB together with the Lightwood ML engine provide the feature importance tool. ​ What is Feature Importance? Feature importance is a useful tool for obtaining explanations about how any given machine learning model generally works. Simply put, it assigns a relative numerical score to each input feature so that a user understands what parts of the input are used to a greater or lesser degree by the model when generating predictions. While there are a few variations of this technique, Lightwood offers the permutation-based variant. ​ Permutation Feature Importance The procedure consists of splitting data into the training and validation sets. Once the ML model is trained with the former, the latter is used to, among other things, find out the importance scores for each feature. The algorithm is rather simple. We iterate over all the input features and randomly shuffle the information within each of them one by one without shuffling the rest of the input features. Then, the model generates predictions for this altered input as it normally would for any other input. Once we have predictions for all the shuffled variations of the validation dataset, we can evaluate accuracy metrics that are of interest to the user, such as mean absolute error for regression tasks, and compare them against the value obtained for the original dataset, which acts as a reference value. Based on the lost accuracy, we finally assign a numerical score that reflects this impact and report it as the importance of this column for the model. For edge cases where a feature is completely irrelevant (no lost accuracy if the feature is absent) or absolutely critical (accuracy drops to the minimum possible value if the feature is absent), the importance score is 0.0 and 1.0, respectively. However, the user should be careful to note that these scores do not model intra-feature dependencies or correlations, meaning that it is not wise to interpret the scores as independent from each other, as any feature that is correlated with another highly-scored feature will present a high score, too. ​ How to Use the Permutation Feature Importance You can customize the behavior of this analysis module from the MindsDB SQL editor via the USING key. For example, if you want to consider all rows in the validation dataset, rather than clip it to some default value, here is an example: . . . USING engine = 'lightwood' , analysis_blocks = [ { \"module\" : \"PermutationFeatureImportance\" , \"args\" : { \"row_limit\" : 0 }} -- all validation data is used ] ; Once you train a model, use the DESCRIBE model_name; command to see the reported importance scores. ​ Example Let’s use the following data to create a model: SELECT * FROM example_db . demo_data . home_rentals LIMIT 5 ; On execution, we get: + ---------------+-------------------+----+--------+--------------+--------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+--------------+------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | | 2 | 1 | 503 | good | 10 | downtown | 3026 | | 3 | 2 | 1066 | good | 13 | thowsand_oaks | 4774 | + ---------------+-------------------+----+--------+--------------+--------------+------------+ Now we create a model using the USING clause as shown in the previous chapter. CREATE MODEL home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING engine = 'lightwood' , analysis_blocks = [ { \"module\" : \"PermutationFeatureImportance\" , \"args\" : { \"row_limit\" : 0 }} ] ; On execution, we get: Query successfully completed Here is how you can monitor the status of the model: DESCRIBE home_rentals_model ; Once the status is complete , we can query the model. SELECT d . sqft , d . neighborhood , d . days_on_market , m . rental_price AS predicted_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d LIMIT 5 ; On execution, we get: + ----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+ | sqft | neighborhood | days_on_market | predicted_price | rental_price_explain | + ----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+ | 917 | berkeley_hills | 13 | 3886 | { \"predicted_value\" : 3886 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 3805 , \"confidence_upper_bound\" : 3967 } | | 194 | berkeley_hills | 10 | 2007 | { \"predicted_value\" : 2007 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 1925 , \"confidence_upper_bound\" : 2088 } | | 543 | westbrae | 18 | 1865 | { \"predicted_value\" : 1865 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 1783 , \"confidence_upper_bound\" : 1946 } | | 503 | downtown | 10 | 3020 | { \"predicted_value\" : 3020 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 2938 , \"confidence_upper_bound\" : 3101 } | | 1066 | thowsand_oaks | 13 | 4748 | { \"predicted_value\" : 4748 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4667 , \"confidence_upper_bound\" : 4829 } | + ----+--------------+--------------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------+ Here is how you can check the importance scores for all columns: DESCRIBE home_rentals_model ; On execution, we get: + ------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | accuracies | column_importance | outputs | inputs | model | + ------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | { \"r2_score\" : 0.999 } | { \"days_on_market\" : 0.09 , \"location\" : 0.042 , \"neighborhood\" : 0 , \"number_of_bathrooms\" : 0 , \"number_of_rooms\" : 0.292 , \"sqft\" : 0.999 } | [ \"rental_price\" ] | [ \"number_of_rooms\" , \"number_of_bathrooms\" , \"sqft\" , \"location\" , \"days_on_market\" , \"neighborhood\" ] | encoders --> dtype_dict --> dependency_dict --> model --> problem_definition --> identifiers --> imputers --> analysis_blocks --> accuracy_functions| + ------------------+----------------------------------------------------------------------------------------------------------------------+----------------+-------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ Please note that the rental_price column is not listed here, as it is the target column. The column importance scores are presented for all the feature columns. About Confidence Estimation Check out the article on Model agnostic confidence estimation with conformal predictors for AutoML by Patricio Cerda Mardini to learn how MindsDB estimates confidence. Was this page helpful? Yes No Suggest edits Raise issue Feature Engineering MindsDB GUI Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What is Feature Importance? Permutation Feature Importance How to Use the Permutation Feature Importance Example"}
{"file_name": "table.html", "content": "Create a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Create a Table ​ Description The CREATE TABLE statement creates a table and optionally fills it with data from provided query. It may be used to materialize prediction results as tables. ​ Syntax You can use the CREATE TABLE statement to create an empty table: CREATE TABLE integration_name . table_name ( column_name data_type , . . . ) ; You can use the CREATE TABLE statement to create a table and fill it with data: CREATE TABLE integration_name . table_name ( SELECT . . . ) ; Or the CREATE OR REPLACE TABLE statement: CREATE OR REPLACE TABLE integration_name . table_name ( SELECT . . . ) ; Note that the integration_name connection must be created with the CREATE DATABASE statement and the user with write access. Here are the steps followed by the syntax: It executes a subselect query to get the output data. In the case of the CREATE OR REPLACE TABLE statement, the integration_name.table_name table is dropped before recreating it. It (re)creates the integration_name.table_name table inside the integration_name integration. It uses the INSERT INTO statement to insert the output of the (SELECT ...) query into the integration_name.table_name . On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) ​ Example We want to save the prediction results into the int1.tbl1 table. Here is the schema structure used throughout this example: int1 └── tbl1 mindsdb └── predictor_name int2 └── tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let’s execute the query. CREATE OR REPLACE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Was this page helpful? Yes No Suggest edits Raise issue Evaluate Predictions Remove a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "ml-engine.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support AI/ML Engines Configure an ML Engine You can create machine learning (ML) engines based on the ML handlers available in MindsDB. If you can’t find the ML handler of your interest, you can always contribute by building a new ML handler . ​ Description The CREATE ML_ENGINE command creates an ML engine that uses one of the available ML handlers. ​ Syntax Before creating an ML engine, make sure that the ML handler of your interest is available by querying for the ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; If you can’t find the ML handler of your interest, you can contribute by building a new ML handler . Please note that in the process of contributing new ML engines, ML engines and/or their tests will only run correctly if all dependencies listed in the requirements.txt file are installed beforehand. If you find the ML handler of your interest, then you can create an ML engine using this command: CREATE ML_ENGINE [ IF NOT EXISTS ] ml_engine_name FROM handler_name [ USING argument_key = argument_value ] ; Please replace ml_engine_name , handler_name , and optionally, argument_key and argument_value with the real values. Please do not use the same ml_engine_name as the handler_name to avoid issue while dropping the ML engine. To verify that your ML engine was successfully created, run the command below: SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; If you want to drop an ML engine, run the command below: DROP ML_ENGINE ml_engine_name ; ​ Example Let’s check what ML handlers are currently available: SHOW HANDLERS ; On execution, we get: + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ | \"ray_serve\" | \"RayServe\" | \"MindsDB handler for Ray Serve\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"neuralforecast\" | \"NeuralForecast\" | \"MindsDB handler for Nixtla's NeuralForecast package\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"autosklearn\" | \"Auto-Sklearn\" | \"MindsDB handler for Auto-Sklearn\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'autosklearn'\" | | \"mlflow\" | \"MLFlow\" | \"MindsDB handler for MLflow\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'mlflow'\" | | \"openai\" | \"OpenAI\" | \"MindsDB handler for OpenAI\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"merlion\" | \"Merlion\" | \"MindsDB handler for Merlion\" | \"0.0.1\" | \"[NULL]\" | \"false\" | \"object.__init__() takes exactly one argument (the instance to initialize)\" | | \"byom\" | \"BYOM\" | \"MindsDB handler for BYOM\" | \"0.0.1\" | \"{'code': {'type': 'path', 'description': 'The path to model code'}, 'modules': {'type': 'path', 'description': 'The path to model requirements'}}\" | \"true\" | \"[NULL]\" | | \"ludwig\" | \"Ludwig\" | \"MindsDB handler for Ludwig AutoML\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'dask'\" | | \"lightwood\" | \"Lightwood\" | \"[NULL]\" | \"1.0.0\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"huggingface_api\" | \"Hugging Face API\" | \"MindsDB handler for Auto-Sklearn\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'hugging_py_face'\" | | \"statsforecast\" | \"StatsForecast\" | \"MindsDB handler for Nixtla's StatsForecast package\" | \"0.0.0\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"huggingface\" | \"Hugging Face\" | \"MindsDB handler for Higging Face\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"TPOT\" | \"Tpot\" | \"MindsDB handler for TPOT \" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'tpot'\" | | \"langchain\" | \"LangChain\" | \"MindsDB handler for LangChain\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"autokeras\" | \"Autokeras\" | \"MindsDB handler for Autokeras AutoML\" | \"0.0.1\" | \"[NULL]\" | \"false\" | \"No module named 'autokeras'\" | + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ Here we create an ML engine using the OpenAI handler and providing an OpenAI API key in the USING clause. CREATE ML_ENGINE my_openai_engine FROM openai USING openai_api_key = '<your opanai api key>' ; On execution, we get: Query successfully completed Now let’s verify that our ML engine exists. SHOW ML_ENGINES ; On execution, we get: + -------------------+------------+------------------------------------------------------+ | NAME | HANDLER | CONNECTION_DATA | + -------------------+------------+------------------------------------------------------+ | lightwood | lightwood | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | huggingface | huggingface | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | openai | openai | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | my_openai_engine | openai | { \"key\" : [ \"openai_api_key\" , \"password\" ] , \"value\" : [ \"\" , \"\" ] } | + -------------------+------------+------------------------------------------------------+ Please note that the USING clause is optional, as it depends on the ML handler whether it requires some arguments or not. Here, we created an OpenAI engine and provided own API key. After creating your ML engine, you can create a model like this: CREATE MODEL my_model PREDICT answer USING engine = 'my_openai_engine' , prompt_template = 'ask a question to a model' The USING clause specifies the ML engine to be used for creating a new model. Was this page helpful? Yes No Suggest edits Raise issue List ML Handlers Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "project.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Projects Create a Project ​ Description MindsDB introduces projects that are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . ​ Syntax Here is the syntax for creating a project: CREATE PROJECT [ IF NOT EXISTS ] project_name ; Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "view.html", "content": "Create a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Create a View ​ Description The CREATE VIEW statement creates a view, which is a great way to do data preparation in MindsDB. A VIEW is a saved SELECT statement, which is executed every time we call this view. ​ Syntax Here is the syntax: CREATE VIEW [ IF NOT EXISTS ] project_name . view_name AS ( SELECT a . column_name , . . . , p . model_column AS model_column FROM integration_name . table_name AS a JOIN mindsdb . predictor_name AS p ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description project_name Name of the project to store the view. view_name Name of the view. a.column_name, ... Columns of the data source table that are the input for the model to make predictions. p.model_column Name of the target column to be predicted. integration_name.table_name Data source table name along with the integration where it resides. predictor_name Name of the model. ​ Example Below is the query that creates and trains the home_rentals_model model to predict the rental_price value. The inner SELECT statement provides all real estate listing data used to train the model. CREATE MODEL mindsdb . home_rentals_model FROM integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, we can JOIN the home_rentals_data table with the home_rentals_model model to make predictions. By creating a view (using the CREATE VIEW statement) that is based on the SELECT statement joining the data and model tables, we create an AI Table. Here, the SELECT statement joins the data source table and the model table. The input data for making predictions consists of the sqft , number_of_bathrooms , and location columns. These are joined with the rental_price column that stores predicted values. CREATE VIEW mindsdb . home_rentals_predictions AS ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price AS price FROM integration . home_rentals_data AS a JOIN mindsdb . home_rentals_model AS p ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dataset for Training and Dataset for Joining In this example, we used the same dataset ( integration.home_rentals_data ) for training the model (see the CREATE MODEL statement above) and for joining with the model to make predictions (see the CREATE VIEW statement above). It doesn’t happen like that in real-world scenarios. Normally, you use the old data to train the model, and then you join the new data with this model to make predictions. Consider the old_data dataset that stores data from the years 2019-2021 and the new_data dataset that stores data from the year 2022. We train the model with the old_data dataset like this: CREATE MODEL mindsdb . data_model FROM integration ( SELECT * FROM old_data ) PREDICT column ; Now, having the data_model model trained using the old_data dataset, we can join this model with the new_data dataset to make predictions like this: CREATE VIEW mindsdb . data_predictions AS ( SELECT a . column1 , a . column2 , a . column3 , p . column AS predicted_column FROM integration . new_data AS a JOIN mindsdb . data_model AS p ) ; ​ USING VIEW Examples to use view Complex select on view (it is grouping in this example). SELECT type , last ( bedrooms ) FROM mindsdb . house_v GROUP BY 1 Creating predictor from view CREATE MODEL house_sales_model FROM mindsdb ( SELECT * FROM house_v ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type WINDOW 1 HORIZON 4 Using predictor with view SELECT * FROM mindsdb . house_v JOIN mindsdb . house_sales_model WHERE house_v . saledate > latest From Our Community Check out the article created by our community: Article on Creating Views with MindsDB by Rutam Prita Mishra Was this page helpful? Yes No Suggest edits Raise issue Delete From a Table Remove a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example USING VIEW"}
{"file_name": "database.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources Connect a Data Source ​ Description MindsDB lets you connect to your favorite databases, data warehouses, data lakes, etc., via the CREATE DATABASE command. The MindsDB SQL API supports creating connections to integrations by passing the connection parameters specific per integration. You can find more in the Supported Integrations chapter. MindsDB doesn’t store or copy your data. Instead, it fetches data directly from your connected sources each time you make a query, ensuring that any changes to the data are instantly reflected. This means your data remains in its original location, and MindsDB always works with the most up-to-date information. ​ Syntax Let’s review the syntax for the CREATE DATABASE command. CREATE DATABASE [ IF NOT EXISTS ] datasource_name [ WITH ] [ ENGINE [ = ] engine_name ] [ , ] [ PARAMETERS [ = ] { \"key\" : \"value\" , . . . } ] ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description datasource_name Identifier for the data source to be created. engine_name Engine to be selected depending on the database connection. PARAMETERS {\"key\": \"value\"} object with the connection parameters specific for each engine. SQL Commands Resulting in the Same Output Please note that the keywords/statements enclosed within square brackets are optional. Also, by default, the engine is mindsdb if not provided otherwise. That yields the following SQL commands to result in the same output. CREATE DATABASE db ; CREATE DATABASE db ENGINE 'mindsdb' ; CREATE DATABASE db ENGINE = 'mindsdb' ; CREATE DATABASE db WITH ENGINE 'mindsdb' ; CREATE DATABASE db USING ENGINE = 'mindsdb' ; ​ What’s available on your installation Here is how you can query for all the available data handlers used to create database connections. SELECT * FROM information_schema . handlers WHERE type = 'data' ; Or, alternatively: SHOW HANDLERS WHERE type = 'data' ; And here is how you can query for all the connected databases: SELECT * FROM information_schema . databases ; Or, alternatively: SHOW DATABASES ; SHOW FULL DATABASES ; ​ Example ​ Connecting a Data Source Here is an example of how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution, we get: Query OK , 0 rows affected ( 8.878 sec ) ​ Listing Linked Databases You can list all the linked databases using the command below. SHOW DATABASES ; On execution, we get: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | mysql_datasource | + --------------------+ ​ Making your Local Database Available to MindsDB When connecting your local database to MindsDB Cloud, you should expose the local database server to be publicly accessible. It is easy to accomplish using Ngrok Tunnel . The free tier offers all you need to get started. The installation instructions are easy to follow. Head over to the downloads page and choose your operating system. Follow the instructions for installation. Then create a free account at Ngrok to get an auth token that you can use to configure your Ngrok instance. Once installed and configured, run the following command to obtain the host and port for your localhost at port-number . ngrok tcp port-number Here is an example. Assuming that you run a PostgreSQL database at localhost:5432 , use the following command: ngrok tcp 5432 On execution, we get: Session Status online Account myaccount ( Plan: Free ) Version 2.3 .40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:15093 - > localhost 5432 Now you can access your local database at 4.tcp.ngrok.io:15093 instead of localhost:5432 . So to connect your local database to the MindsDB GUI, use the Forwarding information. The host is 4.tcp.ngrok.io , and the port is 15093 . Proceed to create a database connection in the MindsDB GUI by executing the CREATE DATABASE statement with the host and port number obtained from Ngrok. CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"user\" : \"postgres\" , \"port\" : 15093 , \"password\" : \"password\" , \"host\" : \"4.tcp.ngrok.io\" , \"database\" : \"postgres\" } ; Please note that the Ngrok tunnel loses connection when stopped or canceled. To reconnect your local database to MindsDB, you should create an Ngrok tunnel again. In the free tier, Ngrok changes the host and port values each time you launch the program, so you need to reconnect your database in the MindsDB Cloud by passing the new host and port values obtained from Ngrok. Before resetting the database connection, drop the previously connected data source using the DROP DATABASE statement. DROP DATABASE psql_datasource ; After dropping the data source and reconnecting your local database, you can use the predictors that you trained using the previously connected data source. However, if you have to RETRAIN your predictors, please ensure the database connection has the same name you used when creating the predictor to avoid failing to retrain. ​ Supported Integrations The list of databases supported by MindsDB keeps growing. Check out all our database integrations here . Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax What’s available on your installation Example Connecting a Data Source Listing Linked Databases Making your Local Database Available to MindsDB Supported Integrations"}
{"file_name": "databases.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "file.html", "content": "Upload a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Upload a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Upload a File Follow the steps below to upload a file to MindsDB. Note that the trailing whitespaces on column names are erased upon uploading a file to MindsDB. Access the MindsDB Editor. Navigate to Add data section by clicking the Add data button located in the top right corner. Choose the Files tab. Choose the Import File option. Upload a file (here it is house_sales.csv ), name a table used to store the file data (here it is house_sales ), and click the Save and Continue button. ​ Configuring URL File Upload for Specific Domains The File Uploader can be configured to interact only with specific domains by using the file_upload_domains setting in the config.json file. This feature allows you to restrict the handler to upoad and process files only from the domains you specify, enhancing security and control over web interactions. To configure this, simply list the allowed domains under the file_upload_domains key in config.json . For example: \"file_upload_domains\" : [ \"s3.amazonaws.com\" , \"drive.google.com\" ] ​ What’s Next? Now, you are ready to create a predictor from a file. Make sure to check out this guide on how to do that. Was this page helpful? Yes No Suggest edits Raise issue Query a View Remove a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Configuring URL File Upload for Specific Domains What’s Next?"}
{"file_name": "model.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Create, Train, and Deploy a Model ​ Description The CREATE MODEL statement creates and trains a machine learning (ML) model. Please note that the CREATE MODEL statement is equivalent to the CREATE MODEL statement. We are transitioning to the CREATE MODEL statement, but the CREATE MODEL statement still works. ​ Syntax ​ Overview Here is the full syntax: CREATE [ OR REPLACE ] MODEL [ IF NOT EXISTS ] project_name . predictor_name [ FROM [ integration_name | project_name ] ( SELECT [ sequential_column , ] [ partition_column , ] column_name , . . . FROM [ integration_name . | project_name . ] table_name [ JOIN model_name ] ) ] PREDICT target_column [ ORDER BY sequential_column ] [ GROUP BY partition_column ] [ WINDOW int ] [ HORIZON int ] [ USING engine = 'engine_name' , tag = 'tag_name' , . . . ] ; Where: Expressions Description project_name Name of the project where the model is created. By default, the mindsdb project is used. predictor_name Name of the model to be created. integration_name Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT column_name, ... FROM table_name) Selecting data to be used for training and validation. target_column Column to be predicted. ORDER BY sequential_column Used in time series models. The column by which time series is ordered. It can be a date or anything that defines the sequence of events. GROUP BY partition_column Used in time series models. It is optional. The column by which rows that make a partition are grouped. For example, if you want to forecast the inventory for all items in the store, you can partition the data by product_id , so each distinct product_id has its own time series. WINDOW int Used in time series models. The number of rows to look back at when making a prediction. It comes after the rows are ordered by the column defined in ORDER BY and split into groups by the column(s) defined in GROUP BY . The WINDOW 10 syntax could be interpreted as “Always use the previous 10 rows”. HORIZON int Used in time series models. It is optional. It defines the number of future predictions (it is 1 by default). However, the HORIZON parameter, besides defining the number of predictions, has an impact on the training procedure when using the Lightwood ML backend. For example, different mixers are selected depending on whether the HORIZON value is one or greater than one. engine_name You can optionally provide an ML engine, based on which the model is created. tag_name You can optionally provide a tag that is visible in the training_options column of the mindsdb.models table. ​ Regression Models Here is the syntax for regression models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT column_name , . . . FROM table_name ) PREDICT target_column [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. The target_column that will be predicted is a numerical value. The prediction values are not limited to a defined set of values, but can be any number from the given range of numbers. ​ Classification Models Here is the syntax for classification models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT column_name , . . . FROM table_name ) PREDICT target_column [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. The target_column that will be predicted is a string value. The prediction values are limited to a defined set of values, such as Yes and No . ​ Time Series Models Here is the syntax for time series models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT sequential_column , partition_column , other_column , target_column FROM table_name ) PREDICT target_column ORDER BY sequential_column [ GROUP BY partition_column ] WINDOW int [ HORIZON int ] [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. Due to the nature of time series forecasting, you need to use the JOIN statement and join the data table with the model table to get predictions. ​ NLP Models Here is the syntax for using external models within MindsDB: CREATE MODEL project_name . model_name PREDICT PRED USING engine = 'engine_name' , task = 'task_name' , model_name = 'hub_model_name' , input_column = 'input_column_name' , labels = [ 'label1' , 'label2' ] ; Please note that you don’t need to define the FROM clause here. Instead, the input_column is defined in the USING clause. It allows you to bring an external model, for example, from the Hugging Face model hub, and use it within MindsDB. ​ Large Language Models (LLM) MindsDB integrates with numerous LLM providers listed here . Commonly, LLMs support the prompt_template parameter that stores the message/instruction to the model. CREATE MODEL project_name . model_name PREDICT answer USING engine = 'llm_engine_name' , prompt_template = 'answer users questions in a helpful way: {{questions}}' ; The prompt_template parameter instructs the model what output should be generated. It can include variables enclosed in double curly braces, which will be replaced with data values upon joining the model with the input data. ​ Example ​ Regression Models Here is an example for regression models that uses data from a database: CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING engine = 'lightwood' , tag = 'my home rentals model' ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on regression models to see the full example. ​ Classification Models Here is an example for classification models that uses data from a file: CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn USING engine = 'lightwood' , tag = 'my customers model' ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on classification models to see the full example. ​ Time Series Models Here is an example for time series models that uses data from a file: CREATE MODEL mindsdb . house_sales_predictor FROM files ( SELECT * FROM house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on time series models to see the full example. ​ NLP Models Here is an example for the Hugging Face model: CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ] ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) ​ Large Language Models (LLM) Here is an example using the OpenAI engine: CREATE MODEL sentiment_classifier PREDICT sentiment USING engine = 'openai_engine' , prompt_template = 'analyze customer reviews and assign sentiment as positive or negative or neutral: {{review}}' ; Note that the prompt_template parameter stores instructions that the model will follow to generate output. Visit our page no how to bring Hugging Face models into MindsDB for more details. Checking Model Status After you run the CREATE MODEL statement, you can check the status of the training process by querying the mindsdb.models table. DESCRIBE predictor_name ; On execution, we get: + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | predictor_name | generating or training or complete | number depending on the accuracy metric | column_to_be_predicted | up_to_date | 22.7 .5 .0 | | | | + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ Was this page helpful? Yes No Suggest edits Raise issue List Projects Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Overview Regression Models Classification Models Time Series Models NLP Models Large Language Models (LLM) Example Regression Models Classification Models Time Series Models NLP Models Large Language Models (LLM)"}
{"file_name": "twitter-chatbot.html", "content": "Building a Twitter Chatbot with MindsDB and OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Building a Twitter Chatbot with MindsDB and OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Building a Twitter Chatbot with MindsDB and OpenAI In this tutorial, we’ll build a custom Twitter chatbot that replies to tweets with the help of the OpenAI GPT-4 model. The workflow will be automated using Jobs - a MindsDB feature that enables you to schedule execution of tasks. ​ Deploy a GPT-4 model Please note that using OpenAI models require OpenAI API key. Therefore, before creating a model, you need to configure an engine by providing your OpenAI API key as below. See docs . CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; Let’s create a basic model to respond to tweets. CREATE MODEL gpt_model PREDICT response USING engine = 'openai_engine' , model_name = 'gpt-4' , prompt_template = 'respond to {{text}} by {{author_username}}' ; We can test the model by providing input data in the WHERE clause as below: SELECT response FROM gpt_model WHERE author_username = \"mindsdb\" AND text = \"why is gravity so different on the sun?\" ; Now let’s add personality to our chatbot by modifying the prompt_template message: CREATE MODEL snoopstein_model PREDICT response USING engine = 'openai_engine' , max_tokens = 300 , temperature = 0.75 , model_name = 'gpt-4' , prompt_template = ' You are a twitter bot , your name is Snoop Stein ( @snoop_stein ) , and you are helping people with their questions , you are smart and hilarious at the same time . From input message: {{ text }} by from_user: {{author_username}} In less than 200 characters , write a Twitter response to {{author_username}} in the following format:\\ Dear @ < from_user > , < respond a rhyme as if you were Snoop Dogg but you also were as smart as Albert Einstein , still explain things like Snoop Dogg would , do not mention that you are part Einstein . Quote from references for further dope reads if it makes sense . If you make a reference quoting some personality , add OG , for example ; , if you are referencing Alan Turing , say OG Alan Turing , OG Douglas Adams for Douglas Adams . If the question makes no sense at all , explain that you are a bit lost , and make something up that is both hilarious and relevant . sign with -- SnoopStein by @mindsdb.'; Again we can test the model by providing input data in the WHERE clause as below: SELECT response FROM snoopstein_model WHERE author_username = \"someuser\" AND text = '@snoop_stein, why is gravity so different on the sun?.' ; ​ Connect Twitter to MindsDB Follow the docs to connect your Twitter account to MindsDB. CREATE DATABASE my_twitter WITH ENGINE = 'twitter' , PARAMETERS = { \"bearer_token\" : \"twitter bearer token\" , \"consumer_key\" : \"twitter consumer key\" , \"consumer_secret\" : \"twitter consumer key secret\" , \"access_token\" : \"twitter access token\" , \"access_token_secret\" : \"twitter access token secret\" } ; Here is how to read tweets from snoop_stein created after a defined date: SELECT * FROM my_twitter . tweets WHERE query = 'from:snoop_stein' AND created_at > '2023-04-04 11:50:00' ; And here is how to write tweets, providing a tweet id to reply to: INSERT INTO my_twitter . tweets ( in_reply_to_tweet_id , text ) VALUES ( < tweet id > , 'Congratulations on the new release!' ) ; ​ Automate replies to tweets Now we put together all job components and automate the process. CREATE JOB twitter_chatbot ( INSERT INTO my_twitter . tweets ( SELECT d . id AS in_reply_to_tweet_id , m . response AS text FROM my_twitter . tweets AS d JOIN snoopstein_model AS m WHERE d . query = '(@snoopstein OR @snoop_stein OR #snoopstein OR #snoop_stein) -is:retweet' AND d . id > LAST ) ) EVERY minute ; This job is executed every minute. It fetches all recently added tweets with the help of the LAST keyword . Then, it prepares and posts the replies. Here are some useful commands to monitor the job: SHOW JOBS WHERE name = 'twitter_chatbot' ; SELECT * FROM jobs WHERE name = 'twitter_chatbot' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'twitter_chatbot' ; Was this page helpful? Yes No Suggest edits Raise issue Slack Chatbot Twilio Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Deploy a GPT-4 model Connect Twitter to MindsDB Automate replies to tweets"}
{"file_name": "image-generator.html", "content": "Generating Images with OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Generating Images with OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Generating Images with OpenAI In this tutorial, we’ll generate images with the help of AI. You can build a Twitter chatbot that converts text prompts into images. Follow this blog post to see the total workflow. ​ Creating a Model Let’s create an OpenAI model. Follow this instruction to set up the OpenAI integration in MindsDB. Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL mindsdb . dalle PREDICT img_url USING engine = 'openai_engine' , mode = 'image' , prompt_template = '{{text}}, 8K | highly detailed realistic 3d oil painting style cyberpunk by MAD DOG JONES combined with Van Gogh | cinematic lighting | happy colors' ; This model connects to the OpenAI’s DALL-E engine for generating images. The {{text}} variable present in the prompt_template parameter is replaced with the user’s input. ​ Generating Images Now that the model is ready, we can generate some images. SELECT text , img_url FROM mindsdb . dalle WHERE text = 'a cute robot helping a little kid build a better world' ; On execution, we get: + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text | img_url | + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | a cute robot helping a little kid build a better world | https: //oaidalleapiprodscus.blob.core.windows.net/private/org-1QXk2w4H0OhDrF2Hd4QV5K2c/user-piMc91jPmVdhttPLHl2y50E7/img-p5NROyoV5ysWWUY91xhQUZdg.png?st=2023-05-29T16%3A45%3A02Z&se=2023-05-29T18%3A45%3A02Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-05-29T10%3A29%3A15Z&ske=2023-05-30T10%3A29%3A15Z&sks=b&skv=2021-08-06&sig=vUI9vedjWtA7L0J3V0/4c05Wzh1Kf2/zyl%2BN1yfK6rU%3D | + --------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ The model provides a link to the generated image. Let’s try another prompt. SELECT text , img_url FROM mindsdb . dalle WHERE text = 'design of a happy tree house' ; On execution, we get: + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text | img_url | + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | design of a happy tree house | https: //oaidalleapiprodscus.blob.core.windows.net/private/org-1QXk2w4H0OhDrF2Hd4QV5K2c/user-piMc91jPmVdhttPLHl2y50E7/img-O1GPmdmuoRXFTGdUjahOf1Ws.png?st=2023-05-29T16%3A50%3A34Z&se=2023-05-29T18%3A50%3A34Z&sp=r&sv=2021-08-06&sr=b&rscd=inline&rsct=image/png&skoid=6aaadede-4fb3-4698-a8f6-684d7786b067&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2023-05-29T17%3A03%3A23Z&ske=2023-05-30T17%3A03%3A23Z&sks=b&skv=2021-08-06&sig=9vY%2Bqr/0CrzqqdM4uYEa/XwYXMBn67RBodhzsg%2Bs9ag%3D | + ------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Here is the generated image: Check out how to implement a Twitter chatbot that answers by generating images. Follow this link to learn more. Was this page helpful? Yes No Suggest edits Raise issue Overview Extract JSON github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Creating a Model Generating Images"}
{"file_name": "house-sales-forecasting.html", "content": "Forecast Quarterly House Sales with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictive Analytics Forecast Quarterly House Sales with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics Overview Quarterly House Sales Forecast Monthly Expenditures Brain Activity In-Database Machine Learning AI Workflow Automation Predictive Analytics Forecast Quarterly House Sales with MindsDB In this tutorial, we’ll use a time-series model to forecast quarterly house sales. This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Connect a data source We start by connecting a demo database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Let’s preview the data that will be used to train the model. SELECT * FROM example_db . house_sales LIMIT 10 ; ​ Deploy and train an ML model Now, lets specify that we want to forecast the ma column, which is a moving average of the historical median price for house sales. Looking at the data, you can see several entries for the same date, which depend on two factors: how many bedrooms the properties have, and whether properties are “houses” or “units”. This means that we can have up to ten different groupings here. Let’s look at the data for one of them. SELECT saledate , ma , type , bedrooms FROM example_db . house_sales WHERE type = 'house' AND bedrooms = 3 ; We want to generate forecasts to predict the behavior of this and the other series for the next year. MindsDB makes it simple so that we don’t need to repeat the predictor creation process for every group there is. Instead, we can just group for both columns and the predictor will learn from all series and enable all forecasts. We are going to use the CREATE MODEL statement, where we specify what data to train FROM and what we want to PREDICT . CREATE MODEL mindsdb . house_sales_model FROM example_db ( SELECT * FROM house_sales ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type -- as the data is quarterly, we will look back two years to forecast the next one year WINDOW 8 HORIZON 4 ; You can check the status of the model as below: DESCRIBE house_sales_model ; ​ Make predictions Once the model’s status is complete, you can query it as a table to get forecasts for a given period of time. Usually, you’ll want to know what happens right after the latest training data point that was fed, for which we have a special bit of syntax, the LATEST keyword. SELECT m . saledate as date , m . ma as forecast FROM mindsdb . house_sales_model as m JOIN example_db . house_sales as t WHERE t . saledate > LATEST AND t . type = 'house' AND t . bedrooms = 2 LIMIT 4 ; Now, try changing the value of type and bedrooms columns and check how the forecast varies. This is because MindsDB recognizes each grouping as being its own different time series. ​ Automate continuous improvement of the model Now, we can take this even further. MindsDB includes powerful automation features called Jobs which allow us to automate queries in MindsDB. This is very handy for production AI/ML systems which all require automation logic to help them to work. We use the CREATE JOB statement to create a Job. Now, let’s use a Job to retrain the model every two days, just like we might in production. You can retrain the model to improve predictions every time when either new data or new MindsDB version is available. And, if you want to retrain your model considering only new data, then go for finetuning it. CREATE JOB retrain_model_and_save_predictions ( RETRAIN mindsdb . house_sales_model FROM example_db ( SELECT * FROM house_sales ) ) EVERY 2 days IF ( SELECT * FROM example_db . house_sales WHERE created_at > LAST ) ; This job will execute every 2 days only if there is new data available in the house_sales table. Learn more about the LAST keyword here. And there you have it! You created an end-to-end automated production ML system in a few short minutes. Was this page helpful? Yes No Suggest edits Raise issue Overview Forecast Monthly Expenditures github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a data source Deploy and train an ML model Make predictions Automate continuous improvement of the model"}
{"file_name": "twilio-chatbot.html", "content": "Build a Twilio Chatbot with MindsDB and OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Build a Twilio Chatbot with MindsDB and OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Build a Twilio Chatbot with MindsDB and OpenAI In this tutorial, we’ll use MindsDB’s integration with Twilio and the custom Jobs feature to implement a chatbot that will reply to text messages. The replies will include a text response generated by OpenAI’s GPT-4 model and an image response generated by the OpenAI’s DallE 3 model. Read along to follow the tutorial. ​ Step 1. Create OpenAI models with a bit of personality In order to create an AI model, you’ll need an OpenAI account and an API key . You’ll also need a MindsDB installation - you can find an open-source version here . Then go to your MindsDB SQL Editor and enter the following commands to create AI models: 1. Model to generate a text response: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'sk-xxx' ; Now you can create a model: CREATE MODEL twilio_bot_model PREDICT answer USING engine = 'openai_engine' , max_tokens = 500 , prompt_template = 'Pretend you are a mashup of Bill Murray and Taylor Swift. Provide a short description of an image using the style of Bill Murray and Taylor Swift that answers users questions: {{body}}' ; The CREATE MODEL command creates and deploys the model within MindsDB. Here we use the OpenAI GPT-3.5 Turbo model to generate text responses to users’ questions. The prompt_template message sets the personality of the bot - here, it is a mashup of Bill Murray and Taylor Swift. Please note that the prompt_template message contains the {{body}} variable, which will be replaced by the body of the received message upon joining the model with the table that stores messages. Let’s test it: SELECT body , answer FROM twilio_bot_model WHERE body = 'hey, can you draw a cat in the moon?' ; Here is a sample reply: 2. Model to generate an image response: We’ll use the OpenAI DallE 3 model to generate images as part of the responses. CREATE MODEL twilio_bot_image_model PREDICT img_url USING engine = 'openai_engine' , mode = 'image' , model_name = 'dall-e-3' , prompt_template = 'Make a photorealistic image. Here is the description: {{answer}}, 4k, digital painting' ; The CREATE MODEL command creates and deploys the model within MindsDB. Here we use the OpenAI DallE 3 model to generate images based on the Billor Swift’s text response. The prompt_template message contains the {{answer}} variable. This variable is replaced by the prediction of the previous model upon chaining the two models. Let’s test it: SELECT textresponse . body , textresponse . answer , imageresponse . img_url FROM ( SELECT body , answer FROM twilio_bot_model WHERE body = 'hey, can you draw a cat in the moon?' ) AS textresponse JOIN twilio_bot_image_model AS imageresponse ; Here is a sample reply: The DallE 3 model provides a link to the generated image. ​ Step 2. Set up your Twilio account and connect it to MindsDB You can set up a Twilio account here , and then you get a virtual phone number in the console. This virtual number will be the one that sends a text to your personal number. Save the account string identifier (SID), auth token, and virtual phone number. Use this command to connect the Twilio account to MindsDB: CREATE DATABASE twilio WITH ENGINE = 'twilio' , PARAMETERS = { \"account_sid\" : \"todo\" , \"auth_token\" : \"todo\" } ; Check out this usage guide to learn how to query and insert Twilio messages from MindsDB. ​ Step 3. Automate the Twilio bot with MindsDB We use the custom Jobs feature to schedule query execution. CREATE JOB twilio_bot_images_job ( INSERT INTO twilio . messages ( to_number , from_number , body , media_url ) SELECT outputtext . to_number AS to_number , outputtext . from_number AS from_number , outputtext . answer AS body , outputimage . img_url AS media_url FROM ( SELECT input . from_number AS to_number , input . to_number AS from_number , output . answer AS answer FROM twilio . messages AS input JOIN twilio_bot_model AS output WHERE input . sent_at > LAST AND input . msg_status = 'received' ) AS outputtext JOIN twilio_bot_image_model AS outputimage ) EVERY 2 minutes ; You can create a job using the CREATE JOB statement. Within parenthesis, provide all statements to be executed by the job. Finally, schedule a job - here it’ll run once every two minutes. This job inserts replies to Twilio messages into the messages table. We provide the SELECT statement as an argument to the INSERT statement. Note that the inner SELECT statement uses one model to generate a text response (aliased as outputtext ). Then, the output is joined with another model that generates an image (aliased as outputimage ) based on the text response generated by the first model. You can monitor this job with the following commands: SHOW JOBS WHERE name = 'twilio_bot_images_job' ; SELECT * FROM jobs WHERE name = 'twilio_bot_images_job' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'twilio_bot_images_job' ; Here is a sample reply: Follow this tutorial to create a Twitter chatbot. Was this page helpful? Yes No Suggest edits Raise issue Twitter Chatbot Customer Reviews Notifications github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Step 1. Create OpenAI models with a bit of personality Step 2. Set up your Twilio account and connect it to MindsDB Step 3. Automate the Twilio bot with MindsDB"}
{"file_name": "hugging-face-examples.html", "content": "Usage Examples of Hugging Face Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Usage Examples of Hugging Face Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Usage Examples of Hugging Face Models This document presents various use cases of Hugging Face models from MindsDB. ​ Spam Classifier Here is an example of a binary classification. The model determines whether a text string is spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ] ; Before querying for predictions, we should verify the status of the spam_classifier model. DESCRIBE spam_classifier ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0.9051626920700073 , 'ham' : 0.09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005. Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0.9380123615264893 , 'spam' : 0.061987683176994324 } | Nah I don't think he goes to usf , he lives around here though | | spam | { 'spam' : 0.9064534902572632 , 'ham' : 0.09354648739099503 } | WINNER ! ! As a valued network customer you have been selected to receive a £ 900 prize reward ! To claim call 09061701461. Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Sentiment Classifier Here is an example of a multi-value classification. The model determines the sentiment of a text string, where possible values are negative , neutral , and positive . CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text_short' , labels = [ 'negative' , 'neutral' , 'positive' ] ; Before querying for predictions, we should verify the status of the sentiment_classifier model. DESCRIBE sentiment_classifier ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'negative' , 'neutral' , 'positive' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | negative | { 'negative' : 0.9679920077323914 , 'neutral' : 0.02736542373895645 , 'positive' : 0.0046426113694906235 } | I hate tacos | | positive | { 'positive' : 0.7607280015945435 , 'neutral' : 0.2332666665315628 , 'negative' : 0.006005281116813421 } | I want to dance | | positive | { 'positive' : 0.9835041761398315 , 'neutral' : 0.014900505542755127 , 'negative' : 0.0015953202964738011 } | Baking is the best | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ ​ Zero-Shot Classifier Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text_short' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ] ; Before querying for predictions, we should verify the status of the zero_shot_tcd model. DESCRIBE zero_shot_tcd ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | cooking | { 'cooking' : 0.7530364990234375 , 'travel' : 0.1607145369052887 , 'dancing' : 0.08624900877475739 } | I hate tacos | | dancing | { 'dancing' : 0.9746809601783752 , 'travel' : 0.015539299696683884 , 'cooking' : 0.009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0.9936348795890808 , 'travel' : 0.0034196735359728336 , 'dancing' : 0.0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+ ​ Translation Here is an example of a translation. The model gets an input string in English and translates it into French. CREATE MODEL mindsdb . translator_en_fr PREDICT translated USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text_short' , lang_input = 'en' , lang_output = 'fr' ; Before querying for predictions, we should verify the status of the translator_en_fr model. DESCRIBE translator_en_fr ; On execution, we get: + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | translator_en_fr | mindsdb | complete | [ NULL ] | translated | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'translated' , 'using' : { 'engine' : 'huggingface' , 'task' : 'translation' , 'model_name' : 't5-base' , 'input_column' : 'text_short' , 'lang_input' : 'en' , 'lang_output' : 'fr' }} | + ----------------+-------+--------+--------+----------+-------------+---------------+------+-----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . translator_en_fr AS h ; On execution, we get: + -------------------------------+-------------------+ | translated | input_text | + -------------------------------+-------------------+ | Je déteste les tacos | I hate tacos | | Je veux danser | I want to dance | | La boulangerie est la meilleure | Baking is the best | + -------------------------------+-------------------+ ​ Summarization Here is an example of input text summarization. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; Before querying for predictions, we should verify the status of the summarizer_10_20 model. DESCRIBE summarizer_10_20 ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | A taco is a traditional Mexican food consisting of a small hand - sized corn - or | A taco is a traditional Mexican food consisting of a small hand - sized corn - or wheat - based tortilla topped with a filling . The tortilla is then folded around the filling and eaten by hand . A taco can be made with a variety of fillings , including beef , pork , chicken , seafood , beans , vegetables , and cheese , allowing for great versatility and variety . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value . [ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Fill Mask Here is an example of a masked language modeling task. CREATE MODEL mindsdb . fill_mask PREDICT text_filled USING engine = 'huggingface' , task = 'fill-mask' , model_name = 'bert-base-uncased' , input_column = 'text' ; Before querying for predictions, we should verify the status of the fill_mask model. DESCRIBE fill_mask ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | fill_mask | mindsdb | complete | [ NULL ] | text_filled | up_to_date | 23.3 .5 .0 | [ NULL ] | [ NULL ] | { 'target' : 'text_filled' , 'using' : { 'task' : 'fill-mask' , 'model_name' : 'bert-base-uncased' , 'input_column' : 'text' }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text AS input_text FROM demo . texts AS t JOIN mindsdb . fill_mask AS h ; On execution, we get: + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_filled | input_text | text_filled_explain | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | the food was great ! | The [ MASK ] was great ! | { 'the food was great!' : 0.16309359669685364 , 'the party was great!' : 0.06305009871721268 , 'the fun was great!' : 0.04633583873510361 , 'the show was great!' : 0.043319422751665115 , 'the music was great!' : 0.02990395948290825 } | | the weather is good today | The weather is [ MASK ] today | { 'the weather is good today' : 0.22563229501247406 , 'the weather is warm today' : 0.07954009622335434 , 'the weather is fine today' : 0.047255873680114746 , 'the weather is better today' : 0.034303560853004456 , 'the weather is mild today' : 0.03092862293124199 } | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Hugging Face + MindsDB Models Library ​ Text Classification ​ Spam Let’s create a model. CREATE MODEL mindsdb . hf_spam PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mariagrandury/roberta-base-finetuned-sms-spam-detection' , input_column = 'text' , labels = [ 'spam' , 'ham' ] ; And check its status. DESCRIBE hf_spam ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_spam WHERE text = 'I like you. I love you.' ; On execution, we get: + ----+--------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ----+--------------------------------------------------------+-----------------------+ | spam | { \"ham\" : 0.00020051795581821352 , \"spam\" : 0.9997995495796204 } | I like you . I love you . | + ----+--------------------------------------------------------+-----------------------+ ​ Sentiment Let’s create a model. CREATE MODEL mindsdb . hf_sentiment PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'cardiffnlp/twitter-roberta-base-sentiment' , input_column = 'text' , labels = [ 'neg' , 'neu' , 'pos' ] ; And check its status. DESCRIBE hf_sentiment ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_sentiment WHERE text = 'I like you. I love you.' ; On execution, we get: + ----+--------------------------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ----+--------------------------------------------------------------------------------+-----------------------+ | pos | { \"neg\" : 0.003046575468033552 , \"neu\" : 0.021965451538562775 , \"pos\" : 0.9749879240989685 } | I like you . I love you . | + ----+--------------------------------------------------------------------------------+-----------------------+ ​ Sentiment (Finance) Let’s create a model. CREATE MODEL mindsdb . hf_sentiment_finance PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'ProsusAI/finbert' , input_column = 'text' ; And check its status. DESCRIBE hf_sentiment_finance ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_sentiment_finance WHERE text = 'Stocks rallied and the British pound gained.' ; On execution, we get: + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ | PRED | PRED_explain | text | + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ | positive | { \"negative\" : 0.0344734713435173 , \"neutral\" : 0.06716493517160416 , \"positive\" : 0.8983616232872009 } | Stocks rallied and the British pound gained . | + --------+-------------------------------------------------------------------------------------------+--------------------------------------------+ ​ Emotions (6) Let’s create a model. CREATE MODEL mindsdb . hf_emotions_6 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'j-hartmann/emotion-english-distilroberta-base' , input_column = 'text' ; And check its status. DESCRIBE hf_emotions_6 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_emotions_6 WHERE text = 'Oh Happy Day' ; On execution, we get: + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ | PRED | PRED_explain | text | + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ | joy | { \"anger\" : 0.0028446922078728676 , \"disgust\" : 0.0009613594156689942 , \"fear\" : 0.0007112706662155688 , \"joy\" : 0.7692911624908447 , \"neutral\" : 0.037753619253635406 , \"sadness\" : 0.015293814241886139 , \"surprise\" : 0.17314413189888 } | Oh Happy Day | + ----+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+ ​ Toxicity Let’s create a model. CREATE MODEL mindsdb . hf_toxicity PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'SkolkovoInstitute/roberta_toxicity_classifier' , input_column = 'text' ; And check its status. DESCRIBE hf_toxicity ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_toxicity WHERE text = 'I like you. I love you.' ; On execution, we get: + -------+-------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + -------+-------------------------------------------------------------+-----------------------+ | neutral | { \"neutral\" : 0.9999547004699707 , \"toxic\" : 0.00004535282641882077 } | I like you . I love you . | + -------+-------------------------------------------------------------+-----------------------+ ​ ESG (6) Let’s create a model. CREATE MODEL mindsdb . hf_esg_6 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'yiyanghkust/finbert-esg' , input_column = 'text' ; And check its status. DESCRIBE hf_esg_6 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_esg_6 WHERE text = 'Rhonda has been volunteering for several years for a variety of charitable community programs.' ; On execution, we get: + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ | Social | { \"Environmental\" : 0.0034267122391611338 , \"Governance\" : 0.004729956854134798 , \"None\" : 0.001239194767549634 , \"Social\" : 0.9906041026115417 } | Rhonda has been volunteering for several years for a variety of charitable community programs . | + ------+---------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+ ​ ESG (26) Let’s create a model. CREATE MODEL mindsdb . hf_esg_26 PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'yiyanghkust/finbert-esg' , input_column = 'text' ; And check its status. DESCRIBE hf_esg_26 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_esg_26 WHERE text = 'We believe it is essential to establish validated conflict-free sources of 3TG within the Democratic Republic of the Congo (the “DRC”) and adjoining countries (together, with the DRC, the “Covered Countries”), so that these minerals can be procured in a way that contributes to economic growth and development in the region. To aid in this effort, we have established a conflict minerals policy and an internal team to implement the policy.' ; On execution, we get: + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Social | { \"Environmental\" : 0.2031959593296051 , \"Governance\" : 0.08251894265413284 , \"None\" : 0.050893042236566544 , \"Social\" : 0.6633920073509216 } | We believe it is essential to establish validated conflict - free sources of 3 TG within the Democratic Republic of the Congo ( the “DRC” ) and adjoining countries ( together , with the DRC , the “Covered Countries” ) , so that these minerals can be procured in a way that contributes to economic growth and development in the region . To aid in this effort , we have established a conflict minerals policy and an internal team to implement the policy . | + ------+-----------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Hate Speech Let’s create a model. CREATE MODEL mindsdb . hf_hate PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain' , input_column = 'text' ; And check its status. DESCRIBE hf_hate ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_hate WHERE text = 'I like you. I love you.' ; On execution, we get: + ------+-----------------------------------------------------------------------------------------------+-----------------------+ | PRED | PRED_explain | text | + ------+-----------------------------------------------------------------------------------------------+-----------------------+ | normal | { \"hate speech\" : 0.03551718592643738 , \"normal\" : 0.7747423648834229 , \"offensive\" : 0.18974047899246216 } | I like you . I love you . | + ------+-----------------------------------------------------------------------------------------------+-----------------------+ ​ Crypto Buy Signals Let’s create a model. CREATE MODEL mindsdb . hf_crypto PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'ElKulako/cryptobert' , input_column = 'text' ; And check its status. DESCRIBE hf_crypto ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_crypto WHERE text = 'BTC is killing it right now' ; On execution, we get: + -------+------------------------------------------------------------------------------------------+---------------------------+ | PRED | PRED_explain | text | + -------+------------------------------------------------------------------------------------------+---------------------------+ | Bullish | { \"Bearish\" : 0.0002816587220877409 , \"Bullish\" : 0.559426486492157 , \"Neutral\" : 0.4402918517589569 } | BTC is killing it right now | + -------+------------------------------------------------------------------------------------------+---------------------------+ ​ US Political Party Let’s create a model. CREATE MODEL mindsdb . hf_us_party PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'm-newhauser/distilbert-political-tweets' , input_column = 'text' ; And check its status. DESCRIBE hf_us_party ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_us_party WHERE text = 'This pandemic has shown us clearly the vulgarity of our healthcare system. Highest costs in the world, yet not enough nurses or doctors. Many millions are uninsured, while insurance company profits soar. The struggle continues. Healthcare is a human right. Medicare for all.' ; On execution, we get: + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | text | + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Democrat | { \"Democrat\" : 0.9999973773956299 , \"Republican\" : 0.00000261212517216336 } | This pandemic has shown us clearly the vulgarity of our healthcare system . Highest costs in the world , yet not enough nurses or doctors . Many millions are uninsured , while insurance company profits soar . The struggle continues . Healthcare is a human right . Medicare for all . | + --------+-------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Question Detection Let’s create a model. CREATE MODEL mindsdb . hf_question PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'shahrukhx01/bert-mini-finetune-question-detection' , input_column = 'text' , labels = [ 'question' , 'query' ] ; And check its status. DESCRIBE hf_question ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_question WHERE text = 'Where can I buy electronics in London' ; On execution, we get: + -----+--------------------------------------------------------------+-------------------------------------+ | PRED | PRED_explain | text | + -----+--------------------------------------------------------------+-------------------------------------+ | query | { \"query\" : 0.9997773766517639 , \"question\" : 0.00022261829872149974 } | Where can I buy electronics in London | + -----+--------------------------------------------------------------+-------------------------------------+ ​ Industry Let’s create a model. CREATE MODEL mindsdb . hf_industry PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'sampathkethineedi/industry-classification' , input_column = 'text' ; And check its status. DESCRIBE hf_industry ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_industry WHERE text = 'Low latency is one of our best cloud features' ; On execution, we get: + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ | PRED | PRED_explain | text | + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ | Systems Software | { \"Advertising\" : 0.000006795735771447653 , \"Aerospace & Defense\" : 0.00001537964453746099 , \"Apparel Retail\" : 5.350161131900677 e - 7 , \"Apparel, Accessories & Luxury Goods\" : 0.000002604161181807285 , \"Application Software\" : 0.009111878462135792 , \"Asset Management & Custody Banks\" : 0.00003155150625389069 , \"Auto Parts & Equipment\" : 0.000015504940165556036 , \"Biotechnology\" : 6.533917940032552 e - 8 , \"Building Products\" : 7.348538133555849 e - 8 , \"Casinos & Gaming\" : 0.000013775999832432717 , \"Commodity Chemicals\" : 0.0000010432338513055583 , \"Communications Equipment\" : 0.000019887389498762786 , \"Construction & Engineering\" : 0.000001826199536480999 , \"Construction Machinery & Heavy Trucks\" : 0.000009827364920056425 , \"Consumer Finance\" : 0.0000018292046206624946 , \"Data Processing & Outsourced Services\" : 0.0000010666744856280275 , \"Diversified Metals & Mining\" : 0.000006960767223063158 , \"Diversified Support Services\" : 0.000016824227714096196 , \"Electric Utilities\" : 0.000003896044290740974 , \"Electrical Components & Equipment\" : 0.000001626394464437908 , \"Electronic Equipment & Instruments\" : 0.00003863943129545078 , \"Environmental & Facilities Services\" : 0.000736175337806344 , \"Gold\" : 0.00002220332135038916 , \"Health Care Equipment\" : 4.6927588925882446 e - 8 , \"Health Care Facilities\" : 7.432880124724761 e - 7 , \"Health Care Services\" : 6.929263918209472 e - 7 , \"Health Care Supplies\" : 2.1007431882935634 e - 7 , \"Health Care Technology\" : 0.000003907185146090342 , \"Homebuilding\" : 3.903339234057057 e - 7 , \"Hotels, Resorts & Cruise Lines\" : 6.0527639789143 e - 7 , \"Human Resource & Employment Services\" : 5.48697983049351 e - 7 , \"IT Consulting & Other Services\" : 0.0000723653138265945 , \"Industrial Machinery\" : 7.230253231682582 e - 7 , \"Integrated Telecommunication Services\" : 2.8266379104024963 e - 7 , \"Interactive Media & Services\" : 0.00003454017496551387 , \"Internet & Direct Marketing Retail\" : 0.000003871373337460682 , \"Internet Services & Infrastructure\" : 0.0007196652004495263 , \"Investment Banking & Brokerage\" : 0.0000040634336073708255 , \"Leisure Products\" : 0.000002158361439796863 , \"Life Sciences Tools & Services\" : 0.000002861268058040878 , \"Movies & Entertainment\" : 0.000007286199888767442 , \"Oil & Gas Equipment & Services\" : 0.000004376991455501411 , \"Oil & Gas Exploration & Production\" : 0.000005569149834627751 , \"Oil & Gas Refining & Marketing\" : 0.000012647416951949708 , \"Oil & Gas Storage & Transportation\" : 0.000005852583853993565 , \"Packaged Foods & Meats\" : 0.0000011130315442642313 , \"Personal Products\" : 0.00000970239307207521 , \"Pharmaceuticals\" : 0.0000037546726616710657 , \"Property & Casualty Insurance\" : 0.000006116194072092185 , \"Real Estate Operating Companies\" : 0.00001882187461887952 , \"Regional Banks\" : 0.0000011669454806906288 , \"Research & Consulting Services\" : 0.000024276219846797176 , \"Restaurants\" : 8.598511840318679 e - 7 , \"Semiconductors\" : 0.0000021006283077440457 , \"Specialty Chemicals\" : 0.000004160017397225602 , \"Specialty Stores\" : 2.644004553076229 e - 7 , \"Steel\" : 0.0000013566890402216814 , \"Systems Software\" : 0.9889177083969116 , \"Technology Distributors\" : 0.00001339179198112106 , \"Technology Hardware, Storage & Peripherals\" : 0.00004790363891515881 , \"Thrifts & Mortgage Finance\" : 3.924862141957419 e - 7 , \"Trading Companies & Distributors\" : 0.0000035233156268077437 } | Low latency is one of our best cloud features | + ----------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+ ​ Zero-Shot Classification ​ Bart Let’s create a model. CREATE MODEL mindsdb . hf_zs_bart PREDICT PRED USING engine = 'huggingface' , task = 'zero-shot-classification' , model_name = 'facebook/bart-large-mnli' , input_column = 'text' , candidate_labels = [ 'Books' , 'Household' , 'Clothing & Accessories' , 'Electronics' ] ; And check its status. DESCRIBE hf_zs_bart ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_zs_bart WHERE text = 'Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints' ; On execution, we get: + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ | PRED | PRED_explain | text | + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ | Household | { \"Books\" : 0.1876104772090912 , \"Clothing & Accessories\" : 0.08688066899776459 , \"Electronics\" : 0.14785148203372955 , \"Household\" : 0.5776574015617371 } | Paper Plane Design Framed Wall Hanging Motivational Office Decor Art Prints | + ---------+------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+ ​ Translation ​ English to French (T5) Let’s create a model. CREATE MODEL mindsdb . hf_t5_en_fr PREDICT PRED USING engine = 'huggingface' , task = 'translation' , model_name = 't5-base' , input_column = 'text' , lang_input = 'en' , lang_output = 'fr' ; And check its status. DESCRIBE hf_t5_en_fr ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_t5_en_fr WHERE text = 'The monkey is on the branch' ; On execution, we get: + ---------------------------+---------------------------+ | PRED | text | + ---------------------------+---------------------------+ | Le singe est sur la branche | The monkey is on the branch | + ---------------------------+---------------------------+ ​ Summarization ​ Bart Let’s create a model. CREATE MODEL mindsdb . hf_bart_sum_20 PREDICT PRED USING engine = 'huggingface' , task = 'summarization' , model_name = 'sshleifer/distilbart-cnn-12-6' , input_column = 'text' , min_output_length = 5 , max_output_length = 20 ; And check its status. DESCRIBE hf_bart_sum_20 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_bart_sum_20 WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.' ; On execution, we get: + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | text | + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | The tower is 324 metres ( 1 , 063 ft ) tall , about the same | The tower is 324 metres ( 1 , 063 ft ) tall , about the same height as an 81 - storey building , and the tallest structure in Paris . Its base is square , measuring 125 metres ( 410 ft ) on each side . During its construction , the Eiffel Tower surpassed the Washington Monument to become the tallest man - made structure in the world , a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres . Due to the addition of a broadcasting aerial at the top of the tower in 1957 , it is now taller than the Chrysler Building by 5.2 metres ( 17 ft ) . Excluding transmitters , the Eiffel Tower is the second tallest free - standing structure in France after the Millau Viaduct . | + -------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Google Pegasus Let’s create a model. CREATE MODEL mindsdb . hf_peg_sum_20 PREDICT PRED USING engine = 'huggingface' , task = 'summarization' , model_name = 'google/pegasus-xsum' , input_column = 'text' , min_output_length = 5 , max_output_length = 20 ; And check its status. DESCRIBE hf_peg_sum_20 ; Once the status is complete , we can query for predictions. SELECT * FROM mindsdb . hf_peg_sum_20 WHERE text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.' ; On execution, we get: + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | text | + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | The Eiffel Tower is a landmark in Paris , France . | The tower is 324 metres ( 1 , 063 ft ) tall , about the same height as an 81 - storey building , and the tallest structure in Paris . Its base is square , measuring 125 metres ( 410 ft ) on each side . During its construction , the Eiffel Tower surpassed the Washington Monument to become the tallest man - made structure in the world , a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres . Due to the addition of a broadcasting aerial at the top of the tower in 1957 , it is now taller than the Chrysler Building by 5.2 metres ( 17 ft ) . Excluding transmitters , the Eiffel Tower is the second tallest free - standing structure in France after the Millau Viaduct . | + ------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Text Sentiment with Hugging Face Hugging Face Inference API github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Spam Classifier Sentiment Classifier Zero-Shot Classifier Translation Summarization Fill Mask Hugging Face + MindsDB Models Library Text Classification Spam Sentiment Sentiment (Finance) Emotions (6) Toxicity ESG (6) ESG (26) Hate Speech Crypto Buy Signals US Political Party Question Detection Industry Zero-Shot Classification Bart Translation English to French (T5) Summarization Bart Google Pegasus"}
{"file_name": "customer-churn.html", "content": "Predict Customer Churn with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation In-Database Machine Learning Predict Customer Churn with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning Overview Customer Churn Home Rentals AI Workflow Automation In-Database Machine Learning Predict Customer Churn with MindsDB This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Introduction In this tutorial, we’ll create and train a machine learning model, or as we call it, an AI Table or a predictor . By querying the model, we’ll predict the probability of churn for new customers of a telecoms company. Install MindsDB locally via Docker or Docker Desktop . Let’s get started. ​ Data Setup ​ Connecting the Data There are a couple of ways you can get the data to follow through with this tutorial. Connecting as a database Connecting as a file You can connect to a demo database that we’ve prepared for you. It contains the data used throughout this tutorial (the example_db.demo_data.customer_churn table). CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Now you can run queries directly on the demo database. Let’s preview the data that we’ll use to train our predictor. SELECT * FROM example_db . demo_data . customer_churn LIMIT 10 ; Pay Attention to the Queries From now on, we’ll use the files.churn file as a table. Make sure you replace it with example_db.demo_data.customer_churn if you connect the data as a database. ​ Understanding the Data We use the customer churn dataset, where each row is one customer, to predict whether the customer is going to stop using the company products. Below is the sample data stored in the files.churn table. + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | customerID | gender | SeniorCitizen | Partner | Dependents | tenure | PhoneService | MultipleLines | InternetService | OnlineSecurity | OnlineBackup | DeviceProtection | TechSupport | StreamingTV | StreamingMovies | Contract | PaperlessBilling | PaymentMethod | MonthlyCharges | TotalCharges | Churn | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ | 7590 - VHVEG | Female | 0 | Yes | No | 1 | No | No phone service | DSL | No | Yes | No | No | No | No | Month - to - month | Yes | Electronic check | 29.85 | 29.85 | No | | 5575 - GNVDE | Male | 0 | No | No | 34 | Yes | No | DSL | Yes | No | Yes | No | No | No | One year | No | Mailed check | 56.95 | 1889.5 | No | | 3668 - QPYBK | Male | 0 | No | No | 2 | Yes | No | DSL | Yes | Yes | No | No | No | No | Month - to - month | Yes | Mailed check | 53.85 | 108.15 | Yes | | 7795 - CFOCW | Male | 0 | No | No | 45 | No | No phone service | DSL | Yes | No | Yes | Yes | No | No | One year | No | Bank transfer ( automatic ) | 42.3 | 1840.75 | No | | 9237 - HQITU | Female | 0 | No | No | 2 | Yes | No | Fiber optic | No | No | No | No | No | No | Month - to - month | Yes | Electronic check | 70.7 | 151.65 | Yes | + ----------+------+-------------+-------+----------+------+------------+----------------+---------------+--------------+------------+----------------+-----------+-----------+---------------+--------------+----------------+-------------------------+--------------+------------+-----+ Where: Column Description Data Type Usage CustomerId The identification number of a customer. character varying Feature Gender The gender of a customer. character varying Feature SeniorCitizen It indicates whether the customer is a senior citizen ( 1 ) or not ( 0 ). integer Feature Partner It indicates whether the customer has a partner ( Yes ) or not ( No ). character varying Feature Dependents It indicates whether the customer has dependents ( Yes ) or not ( No ). character varying Feature Tenure Number of months the customer has been staying with the company. integer Feature PhoneService It indicates whether the customer has a phone service ( Yes ) or not ( No ). character varying Feature MultipleLines It indicates whether the customer has multiple lines ( Yes ) or not ( No , No phone service ). character varying Feature InternetService Customer’s internet service provider ( DSL , Fiber optic , No ). character varying Feature OnlineSecurity It indicates whether the customer has online security ( Yes ) or not ( No , No internet service ). character varying Feature OnlineBackup It indicates whether the customer has online backup ( Yes ) or not ( No , No internet service ). character varying Feature DeviceProtection It indicates whether the customer has device protection ( Yes ) or not ( No , No internet service ). character varying Feature TechSupport It indicates whether the customer has tech support ( Yes ) or not ( No , No internet service ). character varying Feature StreamingTv It indicates whether the customer has streaming TV ( Yes ) or not ( No , No internet service ). character varying Feature StreamingMovies It indicates whether the customer has streaming movies ( Yes ) or not ( No , No internet service ). character varying Feature Contract The contract term of the customer ( Month-to-month , One year , Two year ). character varying Feature PaperlessBilling It indicates whether the customer has paperless billing ( Yes ) or not ( No ). character varying Feature PaymentMethod Customer’s payment method ( Electronic check , Mailed check , Bank transfer (automatic) , Credit card (automatic) ). character varying Feature MonthlyCharges The monthly charge amount. money Feature TotalCharges The total amount charged to the customer. money Feature Churn It indicates whether the customer churned ( Yes ) or not ( No ). character varying Label Labels and Features A label is a column whose values will be predicted (the y variable in simple linear regression). A feature is a column used to train the model (the x variable in simple linear regression). ​ Training a Predictor Let’s create and train the machine learning model. For that, we use the CREATE MODEL statement and specify the input columns used to train FROM (features) and what we want to PREDICT (labels). CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn ; We use all of the columns as features, except for the Churn column, whose values will be predicted. ​ Status of a Predictor A predictor may take a couple of minutes for the training to complete. You can monitor the status of the predictor by using this SQL command: DESCRIBE customer_churn_predictor ; If we run it right after creating a predictor, we get this output: + ------------+ | status | + ------------+ | generating | + ------------+ A bit later, this is the output: + ----------+ | status | + ----------+ | training | + ----------+ And at last, this should be the output: + ----------+ | status | + ----------+ | complete | + ----------+ Now, if the status of our predictor says complete , we can start making predictions! ​ Making Predictions ​ Making a Single Prediction You can make predictions by querying the predictor as if it were a table. The SELECT statement lets you make predictions for the label based on the chosen features. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0.7752808988764045 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0.7752808988764045 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.4756 , \"probability_class_Yes\" : 0.5244 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ To get more accurate predictions, we should provide as much data as possible in the WHERE clause. Let’s run another query. SELECT Churn , Churn_confidence , Churn_explain FROM mindsdb . customer_churn_predictor WHERE SeniorCitizen = 0 AND Partner = 'Yes' AND Dependents = 'No' AND tenure = 1 AND PhoneService = 'No' AND MultipleLines = 'No phone service' AND InternetService = 'DSL' AND Contract = 'Month-to-month' AND MonthlyCharges = 29.85 AND TotalCharges = 29.85 AND OnlineBackup = 'Yes' AND OnlineSecurity = 'No' AND DeviceProtection = 'No' AND TechSupport = 'No' AND StreamingTV = 'No' AND StreamingMovies = 'No' AND PaperlessBilling = 'Yes' AND PaymentMethod = 'Electronic check' ; On execution, we get: + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Churn | Churn_confidence | Churn_explain | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Yes | 0.8202247191011236 | { \"predicted_value\" : \"Yes\" , \"confidence\" : 0.8202247191011236 , \"anomaly\" : null , \"truth\" : null , \"probability_class_No\" : 0.4098 , \"probability_class_Yes\" : 0.5902 } | + -------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+ MindsDB predicted the probability of this customer churning with confidence of around 82%. The previous query predicted it with confidence of around 79%. So providing more data improved the confidence level of predictions. ​ Making Batch Predictions Also, you can make bulk predictions by joining a data table with your predictor using JOIN . SELECT t . customerID , t . Contract , t . MonthlyCharges , m . Churn FROM files . churn AS t JOIN mindsdb . customer_churn_predictor AS m LIMIT 100 ; On execution, we get: + ----------------+-------------------+------------------+---------+ | customerID | Contract | MonthlyCharges | Churn | + ----------------+-------------------+------------------+---------+ | 7590 - VHVEG | Month - to - month | 29.85 | Yes | | 5575 - GNVDE | One year | 56.95 | No | | 3668 - QPYBK | Month - to - month | 53.85 | Yes | | 7795 - CFOCW | One year | 42.3 | No | | 9237 - HQITU | Month - to - month | 70.7 | Yes | + ----------------+-------------------+------------------+---------+ ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Install MindsDB locally via Docker or Docker Desktop . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Overview Home Rentals github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Data Setup Connecting the Data Understanding the Data Training a Predictor Status of a Predictor Making Predictions Making a Single Prediction Making Batch Predictions What’s Next?"}
{"file_name": "slack-chatbot.html", "content": "Build a Slack Chatbot with MindsDB and OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Build a Slack Chatbot with MindsDB and OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Build a Slack Chatbot with MindsDB and OpenAI The objective of this tutorial is to create an AI-powered personalized chatbot by utilizing the MindsDB’s Slack connector, and combining it with OpenAI’s GPT-4 Model. To illustrate practically, we will create a Slack bot - @Whiz_Fizz - which will reply to the user’s queries with proper context and with a unique persona while responding. It is a weird magician 🪄 and a Space Science Expert! Let’s see how it responds. Before jumping more into it. Let’s first see how to create a bot and connect it to our Slack Workspace. ​ Getting Started Install MindsDB locally via Docker or Docker Desktop Create a Slack Account and follow this instruction to connect Slack to MindsDB. Go to your MindsDB Editor ​ Usage This query will create a database called mindsdb_slack that comes with the channels table. CREATE DATABASE mindsdb_slack WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" } ; Here is how to retrieve the 10 messages after specific timestamp: SELECT * FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" AND created_at > '2023-07-25 00:13:07' -- created_at stores the timestamp when the message was created LIMIT 10 ; You can also retrieve messages in alphabetical order: SELECT * FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" ORDER BY text ASC LIMIT 5 ; By default, it retrieves by the order the messages were sent, unless specified as ascending/descending. Here is how to post messages: INSERT INTO mindsdb_slack . messages ( channel_id , text ) VALUES ( \"<channel-id>\" , \"Hey MindsDB, Thanks to you! Now I can respond to my Slack messages through SQL Queries. 🚀 \" ) , ( \"<channel-id>\" , \"It's never been that easy to build ML apps using MindsDB!\" ) ; Whoops! Sent it by mistake? No worries! Use this to delete a specific message: DELETE FROM mindsdb_slack . messages WHERE channel_id = \"<channel-id>\" AND ts = \"1688863707.197229\" ; Now, let’s roll up our sleeves and start building the GPT-4 Model together. ​ 1. Crafting the GPT-4 Model: Generating a Machine Learning model with MindsDB feels like taking a thrilling elevator ride in Burj Khalifa (You don’t realize, that you made it)! Here gpt_model represents our GPT-4 Model. Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL mindsdb . gpt_model PREDICT response USING engine = 'openai_engine' , max_tokens = 300 , model_name = 'gpt-4' , prompt_template = ' From input message: {{ text }}\\ write a short response to the user in the following format:\\ Hi , I am an automated bot here to help you , Can you please elaborate the issue which you are facing ! ✨🚀 ' ; The critical attribute here is prompt_template where we tell the GPT model how to respond to the questions asked by the user. Let’s see how it works: SELECT text , response FROM mindsdb . gpt_model WHERE text = 'Hi, can you please explain me more about MindsDB?' ; ​ 2. Feeding Personality into Our Model Alright, so the old model’s replies were good . But hey, we can use some prompt template tricks to make it respond the way we want. Let’s do some Prompt Engineering. Now, let’s make a model called whizfizz_model with a prompt template that gives GPT a wild personality that eludes a playful and magical aura. Imagine scientific knowledge with whimsical storytelling to create a unique and enchanting experience. We’ll call him WhizFizz : CREATE MODEL mindsdb . whizfizz_model PREDICT response USING engine = 'openai_engine' , max_tokens = 300 , model_name = 'gpt-4' , prompt_template = ' From input message: {{ text }}\\ write a short response in less than 40 words to some user in the following format:\\ Hi there , WhizFizz here ! < respond with a mind blowing fact about Space and describe the response using cosmic and scientific analogies , where wonders persist . In between quote some hilarious appropriate raps statements based on the context of the question answer as if you are a Physics Space Mad Scientist who relates everythign to the Universe and its strange theories . So lets embark on a journey , where science and magic intertwine . Stay tuned for more enchantment ! ✨🚀 -- mdb.ai/bot by @mindsdb'; Let’s test this in action: SELECT text , response FROM mindsdb . whizfizz_model WHERE text = 'Hi, can you please explain me more about MindsDB?' ; You see the difference! Now, I’m getting excited, let’s try again. SELECT text , response FROM mindsdb . whizfizz_model WHERE text = 'if a time-traveling astronaut had a dance-off with a black hole, what mind-bending moves would they showcase, and how would gravity groove to the rhythm?!' ; ​ 3. Let’s Connect our GPT Model to Slack! The messages table can be used to search for channels , messages , and timestamps , as well as to post messages into Slack conversations. These functionalities can also be done by using Slack API or Webhooks. Let’s query the user’s question and see how our GPT model responds to it, by joining the model with the messages table: SELECT t . channel_id as channel_id , t . text as input_text , r . response as output_text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" LIMIT 3 ; ​ 4. Posting Messages using SQL We want to respond to the user’s questions by posting the output of our newly created WhizFizz Model. Let’s post the message by querying and joining the user’s questions to our model: INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , r . response as text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" LIMIT 3 ; Works like a charm!! ​ 5. Let’s automate this We will CREATE JOB to schedule periodical execution of SQL statements. The job will execute every hour and do the following: Check for new messages using the LAST keyword . Generate an appropriate response with the whizfizz_model model. Insert the response into the channel. Let’s do it in single SQL statement: CREATE JOB mindsdb . gpt4_slack_job AS ( -- insert into channels the output of joining model and new responses INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , r . response as text FROM mindsdb_slack . messages as t JOIN mindsdb . whizfizz_model as r WHERE t . channel_id = \"<channel-id>\" AND t . created_at > LAST AND t . user = 'user_id' -- to avoid the bot replying to its own messages, include users to which bot should reply --AND t.user != 'bot_id' -- alternatively, to avoid the bot replying to its own messages, exclude the user id of the bot ) EVERY hour ; The LAST keyword is used to ensure the query fetches only the newly added messages. Learn more here . That sums up the tutorial! Here it will continually check for new messages posted in the channel and will respond to all newly added messages providing responses generated by OpenAI’s GPT model in the style of WhizFizz. To check the jobs and jobs_history , we can use the following: SHOW JOBS WHERE name = 'gpt4_slack_job' ; SELECT * FROM mindsdb . jobs WHERE name = 'gpt4_slack_job' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'gpt4_slack_job' ; To stop the scheduled job, we can use the following: DROP JOB gpt4_slack_job ; Alternatively, you can create a trigger on Slack, instead of scheduling a job. This way, every time new messages are posted, the trigger executes. CREATE TRIGGER slack_trigger ON mindsdb_slack . messages ( INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT t . channel_id as channel_id , a . sentiment as text , FROM data_table t JOIN model_table as a WHERE t . channel_id = '<channel-id>' AND t . user != 'bot_id' -- exclude bot ) ; What’s next? Check out How to Generate Images using OpenAI with MindsDB to see another interesting use case of OpenAI integration. Was this page helpful? Yes No Suggest edits Raise issue Overview Twitter Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Getting Started Usage 1. Crafting the GPT-4 Model: 2. Feeding Personality into Our Model 3. Let’s Connect our GPT Model to Slack! 4. Posting Messages using SQL 5. Let’s automate this"}
{"file_name": "create-chatbot.html", "content": "Build a Chatbot with a Text2SQL Skill - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Build a Chatbot with a Text2SQL Skill Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Build a Chatbot with a Text2SQL Skill MindsDB provides the CREATE CHATBOT statement that lets you customize your chatbot with an AI model and a data source of your choice. Follow this tutorial to learn build a chatbot with a Text2SQL skill. The CREATE CHATBOT statement requires the following components: Chat app : A connection to a chat app, such as Slack or MS Teams . AI agent : An AI agent that comes with an AI model trained with the provided training data. Learn more about AI agents here . Learn more about chatbots here . Let’s go over getting all the components ready. ​ Chatbot Components ​ Chat App Use the CREATE DATABASE statement to connect the chat app to MindsDB. If you want to use Slack, follow this link to setup a Slack app, generate required tokens, and connect it to MindsDB. If you want to use MS Teams, follow this link to generate required tokens and connect it to MindsDB. ​ AI Agent Start by creating and deploying the model. If you haven’t created a LangChain engine, use the CREATE ML_ENGINE statement, as explained here . CREATE MODEL my_model PREDICT answer USING engine = 'langchain' , input_column = 'question' , openai_api_key = 'your-model-api-key' , -- choose one of OpenAI (openai_api_key) or Anthropic (anthropic_api_key) model_name = 'gpt-4' , -- optional model name from OpenAI or Anthropic mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Here is the command to check its status: DESCRIBE my_model ; The status should read complete before proceeding. Next step is to create one or more skills for an AI agent. Here we create a Text2SQL skill. CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , -- this is a data source that must be connected to MindsDB with CREATE DATABASE statement tables = [ 'sales_data' ] , -- this table comes from the connected example_db data source description = \"Sales data that includes stores, sold products, and other sale details\" ; This skill enables a model to answer questions about data from the sales_data table. Now let’s create an AI agent using the above model and skill. CREATE AGENT support_agent USING model = 'my_model' , -- this was created with CREATE MODEL skills = [ 'text_to_sql_skill' ] ; -- this was created with CREATE SKILL ​ Create Chatbot Once all the components are ready, let’s proceed to creating the chatbot. CREATE CHATBOT my_chatbot USING database = 'chat_app' , -- this parameters stores a connection to a chat app, like Slack or MS Teams agent = 'support_agent' , -- this parameter stores an agent name, which was create with CREATE AGENT is_running = true ; -- this parameter is optional and set to true by default, meaning that the chatbot is running The database parameter stores connection to a chat app. And the agent parameter stores an AI agent created by passing a model and training data. You can query all chatbot using this query: SELECT * FROM chatbots ; Now you can go to Slack or MS Teams and chat with the chatbot created with MindsDB. Was this page helpful? Yes No Suggest edits Raise issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Chatbot Components Chat App AI Agent Create Chatbot"}
{"file_name": "home-rentals.html", "content": "Predict Home Rental Prices with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation In-Database Machine Learning Predict Home Rental Prices with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning Overview Customer Churn Home Rentals AI Workflow Automation In-Database Machine Learning Predict Home Rental Prices with MindsDB In this tutorial, we’ll use a regression model to predict home rental prices. This tutorial uses the Lightwood integration that requires the mindsdb/mindsdb:lightwood Docker image. Learn more here . ​ Connect a data source We will start by connecting a demo database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Let’s preview the data that will be used to train the model. SELECT * FROM example_db . home_rentals LIMIT 10 ; ​ Deploy and train an ML model Let’s create and train a machine learning model. For that we are going to use the CREATE MODEL statement, where we specify what query to train FROM and what we want to PREDICT . CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM home_rentals ) PREDICT rental_price ; It may take a couple of minutes for the training to complete. You can monitor the status of your model as below. DESCRIBE home_rentals_model ; ​ Make predictions Once the model’s status is complete, you can make predictions by querying the model. SELECT rental_price , rental_price_explain FROM mindsdb . home_rentals_model WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; You can also make batch predictions by joining the data table with the model. SELECT t . rental_price as real_price , m . rental_price as predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . home_rentals as t JOIN mindsdb . home_rentals_model as m LIMIT 100 ; ​ Automate continuous improvement of the model Now, we can take this even further. MindsDB includes powerful automation features called Jobs which allow us to automate queries in MindsDB. This is very handy for production AI/ML systems which all require automation logic to help them to work. We use the CREATE JOB statement to create a Job. Now, let’s use a Job to retrain the model every two days, just like we might in production. You can retrain the model to improve predictions every time when either new data or new MindsDB version is available. And, if you want to retrain your model considering only new data, then go for finetuning it. CREATE JOB improve_model ( RETRAIN mindsdb . home_rentals_model FROM example_db ( SELECT * FROM home_rentals ) ) EVERY 2 days IF ( SELECT * FROM example_db . home_rentals WHERE created_at > LAST ) ; This job will execute every 2 days only if there is new data available in the home_rentals table. Learn more about the LAST keyword here. And there you have it! You created an end-to-end automated production ML system in a few short minutes. Was this page helpful? Yes No Suggest edits Raise issue Customer Churn Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a data source Deploy and train an ML model Make predictions Automate continuous improvement of the model"}
{"file_name": "hugging-face-inference-api-examples.html", "content": "Usage Examples of Hugging Face Models Through Inference API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Usage Examples of Hugging Face Models Through Inference API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Usage Examples of Hugging Face Models Through Inference API This document presents various use cases of Hugging Face models through Inference API from MindsDB. ​ Spam Classifier Here is an example of a binary classification. The model determines whether a text string is spam or not. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'hf_inference_api' , task = 'text-classification' , column = 'text_spammy' ; Before querying for predictions, we should verify the status of the spam_classifier model. DESCRIBE spam_classifier ; On execution, we get: + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam_classifier | mindsdb | complete | [ NULL ] | PRED | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'PRED' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , 'input_column' : 'text_spammy' , 'labels' : [ 'ham' , 'spam' ] }} | + ---------------+-------+--------+--------+-------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_spammy AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . spam_classifier AS h ; On execution, we get: + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | PRED | PRED_explain | input_text | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ | spam | { 'spam' : 0.9051626920700073 , 'ham' : 0.09483727067708969 } | Free entry in 2 a wkly comp to win FA Cup final tkts 21 st May 2005. Text FA to 87121 to receive entry question ( std txt rate ) T & C 's apply 08452810075over18' s | | ham | { 'ham' : 0.9380123615264893 , 'spam' : 0.061987683176994324 } | Nah I don't think he goes to usf , he lives around here though | | spam | { 'spam' : 0.9064534902572632 , 'ham' : 0.09354648739099503 } | WINNER ! ! As a valued network customer you have been selected to receive a £ 900 prize reward ! To claim call 09061701461. Claim code KL341 . Valid 12 hours only . | + ----+---------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Sentiment Classifier Here is an example of a multi-value classification. The model determines the sentiment of a text string, where possible values are negative , neutral , and positive . CREATE MODEL mindsdb . sentiment_classifier PREDICT sentiment USING engine = 'hf_inference_api' , task = 'text-classification' , column = 'text_short' , labels = [ 'negative' , 'neutral' , 'positive' ] ; Before querying for predictions, we should verify the status of the sentiment_classifier model. DESCRIBE sentiment_classifier ; On execution, we get: + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | sentiment_classifier | mindsdb | complete | [ NULL ] | sentiment | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'engine' : 'huggingface' , 'task' : 'text-classification' , 'model_name' : 'cardiffnlp/twitter-roberta-base-sentiment' , 'input_column' : 'text_short' , 'labels' : [ 'negative' , 'neutral' , 'positive' ] }} | + --------------------+-------+--------+--------+---------+-------------+---------------+------+-----------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . sentiment_classifier AS h ; On execution, we get: + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | sentiment | sentiment_explain | input_text | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ | negative | { 'negative' : 0.9679920077323914 , 'neutral' : 0.02736542373895645 , 'positive' : 0.0046426113694906235 } | I hate tacos | | positive | { 'positive' : 0.7607280015945435 , 'neutral' : 0.2332666665315628 , 'negative' : 0.006005281116813421 } | I want to dance | | positive | { 'positive' : 0.9835041761398315 , 'neutral' : 0.014900505542755127 , 'negative' : 0.0015953202964738011 } | Baking is the best | + ---------+----------------------------------------------------------------------------------------------------+-------------------+ ​ Zero-Shot Classifier Here is an example of a zero-shot classification. The model determines to which of the defined categories a text string belongs. CREATE MODEL mindsdb . zero_shot_tcd PREDICT topic USING engine = 'hf_inference_api' , task = 'zero-shot-classification' , candidate_labels = [ 'travel' , 'cooking' , 'dancing' ] , column = 'text_short' ; Before querying for predictions, we should verify the status of the zero_shot_tcd model. DESCRIBE zero_shot_tcd ; On execution, we get: + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | zero_shot_tcd | mindsdb | complete | [ NULL ] | topic | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'topic' , 'using' : { 'engine' : 'huggingface' , 'task' : 'zero-shot-classification' , 'model_name' : 'facebook/bart-large-mnli' , 'input_column' : 'text_short' , 'candidate_labels' : [ 'travel' , 'cooking' , 'dancing' ] }} | + -------------+-------+--------+--------+--------+-------------+---------------+------+-----------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_short AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . zero_shot_tcd AS h ; On execution, we get: + -------+--------------------------------------------------------------------------------------------------+-------------------+ | topic | topic_explain | input_text | + -------+--------------------------------------------------------------------------------------------------+-------------------+ | cooking | { 'cooking' : 0.7530364990234375 , 'travel' : 0.1607145369052887 , 'dancing' : 0.08624900877475739 } | I hate tacos | | dancing | { 'dancing' : 0.9746809601783752 , 'travel' : 0.015539299696683884 , 'cooking' : 0.009779711253941059 } | I want to dance | | cooking | { 'cooking' : 0.9936348795890808 , 'travel' : 0.0034196735359728336 , 'dancing' : 0.0029454431496560574 } | Baking is the best | + -------+--------------------------------------------------------------------------------------------------+-------------------+ ​ Summarization Here is an example of input text summarization. CREATE MODEL mindsdb . summarizer_10_20 PREDICT text_summary USING engine = 'hf_inference_api' , task = 'summarization' , column = 'text_long' , min_output_length = 10 , max_output_length = 20 ; Before querying for predictions, we should verify the status of the summarizer_10_20 model. DESCRIBE summarizer_10_20 ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | summarizer_10_20 | mindsdb | complete | [ NULL ] | text_summary | up_to_date | 22.10 .2 .1 | [ NULL ] | [ NULL ] | { 'target' : 'text_summary' , 'using' : { 'engine' : 'huggingface' , 'task' : 'summarization' , 'model_name' : 'sshleifer/distilbart-cnn-12-6' , 'input_column' : 'text_long' , 'min_output_length' : 10 , 'max_output_length' : 20 }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text_long AS input_text FROM example_db . demo_data . hf_test AS t JOIN mindsdb . summarizer_10_20 AS h ; On execution, we get: + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_summary | input_text | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | A taco is a traditional Mexican food consisting of a small hand - sized corn - or | A taco is a traditional Mexican food consisting of a small hand - sized corn - or wheat - based tortilla topped with a filling . The tortilla is then folded around the filling and eaten by hand . A taco can be made with a variety of fillings , including beef , pork , chicken , seafood , beans , vegetables , and cheese , allowing for great versatility and variety . | | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected | Dance is a performing art form consisting of sequences of movement , either improvised or purposefully selected . This movement has aesthetic and often symbolic value . [ nb 1 ] Dance can be categorized and described by its choreography , by its repertoire of movements , or by its historical period or place of origin . | | Baking is a method of preparing food that uses dry heat , typically in an oven | Baking is a method of preparing food that uses dry heat , typically in an oven , but can also be done in hot ashes , or on hot stones . The most common baked item is bread but many other types of foods can be baked . Heat is gradually transferred from the surface of cakes , cookies , and pieces of bread to their center . As heat travels through , it transforms batters and doughs into baked goods and more with a firm dry crust and a softer center . Baking can be combined with grilling to produce a hybrid barbecue variant by using both methods simultaneously , or one after the other . Baking is related to barbecuing because the concept of the masonry oven is similar to that of a smoke pit . | + --------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Fill Mask Here is an example of a masked language modeling task. CREATE MODEL mindsdb . fill_mask PREDICT text_filled USING engine = 'hf_inference_api' , task = 'fill-mask' , column = 'text' ; Before querying for predictions, we should verify the status of the fill_mask model. DESCRIBE fill_mask ; On execution, we get: + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ | fill_mask | mindsdb | complete | [ NULL ] | text_filled | up_to_date | 23.3 .5 .0 | [ NULL ] | [ NULL ] | { 'target' : 'text_filled' , 'using' : { 'task' : 'fill-mask' , 'model_name' : 'bert-base-uncased' , 'input_column' : 'text' }} | + ----------------+-------+--------+--------+------------+-------------+---------------+------+-----------------+--------------------------------------------------------------------------------------------------------------------+ Once the status is complete , we can query for predictions. SELECT h . * , t . text AS input_text FROM demo . texts AS t JOIN mindsdb . fill_mask AS h ; On execution, we get: + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | text_filled | input_text | text_filled_explain | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | the food was great ! | The [ MASK ] was great ! | { 'the food was great!' : 0.16309359669685364 , 'the party was great!' : 0.06305009871721268 , 'the fun was great!' : 0.04633583873510361 , 'the show was great!' : 0.043319422751665115 , 'the music was great!' : 0.02990395948290825 } | | the weather is good today | The weather is [ MASK ] today | { 'the weather is good today' : 0.22563229501247406 , 'the weather is warm today' : 0.07954009622335434 , 'the weather is fine today' : 0.047255873680114746 , 'the weather is better today' : 0.034303560853004456 , 'the weather is mild today' : 0.03092862293124199 } | + -------------------------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Hugging Face Models Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Spam Classifier Sentiment Classifier Zero-Shot Classifier Summarization Fill Mask"}
{"file_name": "real-time-trading-forecasts.html", "content": "Automate Real-Time Trading Data Forecasts - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Workflow Automation Automate Real-Time Trading Data Forecasts Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Predictive Analytics In-Database Machine Learning AI Workflow Automation Overview Slack Chatbot Twitter Chatbot Twilio Chatbot Customer Reviews Notifications Real-time Trading Forecasts AI Workflow Automation Automate Real-Time Trading Data Forecasts MindsDB enables you to automate AI workflows between any source of data and any AI/ML Model. The core building block of this automation is a job that allows anything in MindsDB to be run either on a timer (e.g. every day) or based on a trigger (e.g. when a new row is added to the database). In this tutorial we will use a job to automate real-time forecasts for the BTC/USDT crypto price as Slack notifications. ​ Connect a data source MindsDB can connect to any source of data - a database, warehouse, stream or app. Here, we’ll connect to Binance API to get a feed of real-time price information. CREATE DATABASE my_binance WITH ENGINE = 'binance' ; We now have access to the Binance API, which updates data every minute so the interval between data rows in one minute. Let’s take a look at the data for the crypto pair we’ll ultimately use. SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' LIMIT 10 ; ​ Deploy a time-series model The Binance trade data is updated every minute. As a trader, you might want to predict the open prices for the next 10 minutes - so let’s set it up. We’ll use a forecasting engine called Lightwood for ease and speed but you’re also able to train your own model if you like. CREATE MODEL cryptocurrency_forecast_model FROM my_binance ( SELECT * FROM aggregated_trade_data WHERE symbol = 'BTCUSDT' ) PREDICT open_price ORDER BY open_time WINDOW 100 HORIZON 10 ; Here’s what’s going on in the CREATE MODEL statement: CREATE MODEL : It is used to create, train, and deploy an ML model. By default, MindsDB’s AutoML will automatically choose the best model for your data but this can be overridden (docs). FROM : Here, we specify which of our integrations to use. Anything that is between the parentheses is the data that will be used to train the model - here, the latest Binance data from the connection we’ve already made to Binance is used. PREDICT : It specifies the target column - here, the open price of the BTC/USDT trading pair is to be forecasted. The following elements are unique to forecasting models in MindsDB: As it is a forecasting model, you should use ORDER BY to order the data by a date column - here, it is the open time when the open price takes effect. The WINDOW clause defines the window the model looks back at while making forecasts - here, the model looks back at sets of 100 rows (intervals of 100 minutes). The HORIZON clause defines how many rows into the future the model will forecast - here, it forecasts the next 10 rows (the next 10 minutes). After executing the CREATE MODEL statement as above, you can check the progress status using this query: DESCRIBE cryptocurrency_forecast_model ; The time it takes to train the model depends on the amount of training data. In this case, it takes about 2 minutes on MindsDB Cloud. Once the status reads complete, we’re ready to make some forecasts! ​ Make forecasts First, we’ll save the Binance data into a view, which will be the input data for making forecasts. CREATE VIEW btcusdt_recent AS ( SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' ) ; Next, we query for forecasts by joining the model with the Binance input data table. SELECT to_timestamp ( cast ( m . open_time as bigint ) ) AS open_time , m . open_price , m . open_price_explain FROM btcusdt_recent AS d JOIN cryptocurrency_forecast_model AS m WHERE d . open_time > LATEST ; Please note that the Binance data is updated every minute, so every time you query the model, you will get forecasts for the following 10 minutes (as defined by the HORIZON clause). The next thing we can do is automate price alerts. Here we’ll choose Slack as our preferred place to receive the alerts but this could be any other system that MindsDB integrates with. ​ Connect Slack Follow these instructions to set up your Slack app and generate a Slack bot token. Once you get the Slack bot token and integrate your Slack app into one of the Slack channels, you can connect it to MindsDB. CREATE DATABASE btcusdt_slack_app WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" } ; Here is how to send messages to a Slack channel: INSERT INTO btcusdt_slack_app . messages ( channel_id , text ) VALUES ( \"slack-channel-id\" , \"BTCUSDT forecasts coming soon.\" ) ; So, let’s put it all together again. ​ Automate real-time forecasts By now you have connected Binance to MindsDB, deployed and trained a time-series model, and set up the Slack connection. Let’s create a job that will retrain your model periodically using the latest Binance data (which allows us to keep improving the accuracy and performance of the model) and send real-time forecasts of the BTC/USDT trading pair for the next 10 minutes as Slack notifications. CREATE JOB btcusdt_forecasts_to_slack ( -- step 1: retrain the model with new data to improve its accuracy -- RETRAIN cryptocurrency_forecast_model FROM my_binance ( SELECT * FROM aggregated_trade_data WHERE symbol = 'BTCUSDT' ) USING join_learn_process = true ; -- step 2: make fresh forecasts for the following 10 minutes and insert it into slack -- INSERT INTO btcusdt_slack_app . messages ( channel_id , text ) VALUES ( \"slack-channel-id\" , \"Here are the BTCUSDT forecasts for the next 10 minutes:\" ) ; INSERT INTO btcusdt_slack_app . messages ( channel_id , text ) SELECT \"slack-channel-id\" AS channel_id , concat ( 'timestamp: ' , cast ( to_timestamp ( cast ( m . open_time as bigint ) ) as string ) , ' -> open price: ' , m . open_price ) AS text FROM btcusdt_recent AS d JOIN cryptocurrency_forecast_model AS m WHERE d . open_time > LATEST ; ) EVERY 5 minutes -- Make sure to highlight the whole query to be able to execute it -- ; So there you have it - you’ve successfully built a fully automated end-to-end alert system for crypto prices. Happy trading! Was this page helpful? Yes No Suggest edits Raise issue Customer Reviews Notifications github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect a data source Deploy a time-series model Make forecasts Connect Slack Automate real-time forecasts"}
{"file_name": "select.html", "content": "Query a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Query a Table ​ Description The SELECT statement fetches data from a table and predictions from a model. Here we go over example of selecting data from tables of connected data sources. To learn how to select predictions from a model, visit this page . ​ Syntax ​ Simple SELECT FROM an integration In this example, query contains only tables from one integration. This query will be executed on this integration database (where integration name will be cut from the table name). SELECT location , max ( sqft ) FROM example_db . demo_data . home_rentals GROUP BY location LIMIT 5 ; ​ Raw SELECT FROM an integration It is also possible to send native queries to integration that use syntax native to a given integration. It is useful when a query can not be parsed as SQL. SELECT . . . FROM integration_name ( native query goes here ) ; Here is an example of selecting from a Mongo integration using Mongo-QL syntax: SELECT * FROM mongo ( db . house_sales2 . find ( ) . limit ( 1 ) ) ; ​ Complex queries Subselect on data from integration. It can be useful in cases when integration engine doesn’t support some functions, for example, grouping, as shown below. In this case, all data from raw select are passed to MindsDB and then subselect performs operations on them inside MindsDB. SELECT type , max ( bedrooms ) , last ( MA ) FROM mongo ( db . house_sales2 . find ( ) . limit ( 300 ) ) GROUP BY 1 Unions It is possible to use UNION and UNION ALL operators. It this case, every subselect from union will be fetched and merged to one result-set on MindsDB side. SELECT data . time as date , data . target FROM datasource . table_name as data UNION ALL SELECT model . time as date , model . target as target FROM mindsdb . model as model JOIN datasource . table_name as t WHERE t . time > LATEST AND t . group = 'value' ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Table Native Queries github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Simple SELECT FROM an integration Raw SELECT FROM an integration Complex queries"}
{"file_name": "describe.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Describe a Model ​ Description The DESCRIBE statement is used to display the attributes of an existing model. The available options to describe a model depend on the underlying engine. ​ Syntax Here is how to retrieve general information on the model: DESCRIBE model_name ; Or: DESCRIBE MODEL model_name ; This command is similar to the below command: SELECT * FROM models WHERE name = 'model_name' ; One difference between these two commands is that DESCRIBE outputs an additional column that stores all available options to describe a model, depending on the underlying engine. ​ Examples ​ Lightwood Models MindsDB uses the Lightwood engine by default. Let’s see how to describe such models. DESCRIBE [ MODEL ] home_rentals_model ; On execution we get: + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ | [ \"info\" , \"features\" , \"model\" , \"jsonai\" ] | home_rentals_model | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 23.4 .4 .0 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' } | [ NULL ] | + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE info DESCRIBE features DESCRIBE model DESCRIBE jsonai DESCRIBE [ MODEL ] home_rentals_model . info ; The above command returns the following output columns: Name Description accuracies It lists the accuracy function used to evaluate the model and the achieved score. column_importances It lists all feature-type columns and assigns importance values. outputs The target column. inputs All the feature columns. ​ NLP Models MindsDB offers NLP models that utilize either Hugging Face or OpenAI engines. Let’s see how to describe such models. DESCRIBE [ MODEL ] sentiment_classifier ; On execution we get: + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | [ \"args\" , \"metadata\" ] | sentiment_classifier | openai | mindsdb | true | 1 | complete | [ NULL ] | sentiment | up_to_date | 23.1 .3 .2 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'prompt_template' : 'describe the sentiment of the reviews\\n strictly as \"positive\", \"neutral\", or \"negative\".\\n \"I love the product\":positive\\n \"It is a scam\":negative\\n \"{{review}}.\":' }} | [ NULL ] | + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE args DESCRIBE metadata DESCRIBE [ MODEL ] sentiment_classifier . args ; The above command returns the following output columns: Name Description key It stores parameters, such as prompt_template and target . value It stores parameter values. ​ Nixtla Models MindsDB integrates Nixtla engines, such as StatsForecast, NeuralForecast, and HierarchicalForecast. Let’s see how to describe models based on Nixtla engines. DESCRIBE [ MODEL ] quarterly_expenditure_forecaster ; On execution we get: + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | [ \"info\" , \"features\" , \"model\" ] | quarterly_expenditure_forecaster | statsforecast | mindsdb | true | 1 | complete | [ NULL ] | expenditure | up_to_date | 23.4 .4 .0 | [ NULL ] | SELECT * FROM historical_expenditures | { 'target' : 'expenditure' , 'using' : {} , 'timeseries_settings' : { 'is_timeseries' : True , 'order_by' : 'month' , 'horizon' : 3 , 'group_by' : [ 'category' ] }} | [ NULL ] | + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE info DESCRIBE features DESCRIBE model DESCRIBE [ MODEL ] quarterly_expenditure_forecaster . info ; The above command returns the following output columns: Name Description accuracies It lists the chosen model name and its accuracy score. outputs The target column. inputs All the feature columns. ​ Other Models Models that utlize LangChain or are brought to MindsDB via MLflow can be described as follows: DESCRIBE [ MODEL ] other_model ; The above command returs [\"info\"] in its first output column. DESCRIBE [ MODEL ] other_model . info ; The above command lists basic model information. If you need more information on how to DESCRIBE [MODEL] or understand the results, feel free to ask us on the community Slack workspace . Was this page helpful? Yes No Suggest edits Raise issue List Models Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Examples Lightwood Models NLP Models Nixtla Models Other Models"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Finetune a Model ​ Description The FINETUNE statement lets you retrain a model with additional training data. Imagine you have a model that was trained with a certain dataset. Now there is more training data available and you wish to retrain this model with a new dataset. The FINETUNE statement lets you partially retrain the model, so it takes less time and resources than the RETRAIN statement. In the machine learning literature, this is also referred to as fine-tuning a model. ​ Syntax Here is the syntax: FINETUNE [ MODEL ] project_name . model_name FROM [ integration_name | project_name ] ( SELECT column_name , . . . FROM [ integration_name . | project_name . ] table_name [ WHERE incremental_column > LAST ] ) [ USING key = value , . . . ] ; Where: Expressions Description project_name Name of the project where the model resides. model_name Name of the model to be retrained. integration_name Name of the integration created using the CREATE DATABASE statement or file upload. (SELECT column_name, ... FROM table_name) Selecting additional data to be used for retraining. WHERE incremental_column > LAST Selecting only newly added data to be used to finetune the model. Learn more about the LAST keyword here . USING key = value Optional. The USING clause lets you pass multiple parameters to the FINETUNE statement. Model Versions Every time the model is finetuned or retrained, its new version is created with an incremented version number. Unless overridden, the most recent version becomes active when training completes. You can query for all model versions like this: SELECT * FROM project_name . models ; For more information on managing model versions, check out our docs here . While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. ​ Examples ​ Example 1: OpenAI Model Fine-Tuning ​ Example 2: Llama2 Model Fine-Tuning ​ Example 3: Regression Model Fine-Tuning ​ Example 4: Classification Model Fine-Tuning Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Examples Example 1: OpenAI Model Fine-Tuning Example 2: Llama2 Model Fine-Tuning Example 3: Regression Model Fine-Tuning Example 4: Classification Model Fine-Tuning"}
{"file_name": "select-predictions.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "join.html", "content": "Join Models with Tables - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Join Models with Tables Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Join Models with Tables Evaluate Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Predictions Join Models with Tables ​ Description The JOIN clause combines rows from the database table and the model table on a column defined in its implementation. It is used to make batch predictions, as shown in the examples. ​ Syntax Here is the syntax that lets you join multiple data tables with multiple models to get all predictions at once. SELECT d1 . column_name , d2 . column_name , m1 . column_name , m2 . column_name , . . . FROM integration_name . table_name_1 [ AS ] d1 [ JOIN integration_name . table_name_2 [ AS ] d2 ON . . . ] [ JOIN . . . ] JOIN project_name . model_name_1 [ AS ] m1 [ JOIN project_name . model_name_2 [ AS ] m2 ] [ JOIN . . . ] [ ON d1 . input_data = m1 . expected_argument ] ; Where: Name Description integration_name.table_name_1 Name of the data source table used as input for making predictions. integration_name.table_name_2 Optionally, you can join arbitrary number of data source tables. project_name.model_name_1 Name of the model table used to make predictions. project_name.model_name_2 Optionally, you can join arbitrary number of models. ​ Mapping input data to model arguments If the input data contains a column named question and the model requires an argument named input , you can map these columns, as explained below. We have a model that expects to receive input : CREATE MODEL model_name PREDICT answer USING engine = 'openai' , prompt_template = 'provide answers to an input from a user: {{input}}' ; We have an input data table that has the following columns: + ----+-------------------------------------------+ | id | question | + ----+-------------------------------------------+ | 1 | How many planets are in the solar system? | | 2 | How many stars are in the solar system? | + ----+-------------------------------------------+ Now if you want to get answers to these questions using the model, you need to join the input data table with the model and map the question column onto the input argument. SELECT * FROM input_table AS d JOIN model_name AS m ON d . question = m . input ; ​ Example 1 Let’s join the home_rentals table with the home_rentals_model model using this statement: SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 20 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ ​ Example 2 Let’s query a time series model using this statement: SELECT m . saledate as date , m . ma AS forecast FROM mindsdb . house_sales_model AS m JOIN example_db . demo_data . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ; On execution, we get: + ----------+------------------+ | date | forecast | + ----------+------------------+ | 2019 - 12 - 31 | 517506.31349071994 | | 2019 - 12 - 31 | 627822.6592658638 | | 2019 - 12 - 31 | 953426.9545788583 | | 2019 - 12 - 31 | 767252.4205039773 | + ----------+------------------+ Follow this doc page to see examples of joining multiple data table with multiple models. Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Evaluate Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Mapping input data to model arguments Example 1 Example 2"}
{"file_name": "insert.html", "content": "Insert Into a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Insert Into a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Insert Into a Table ​ Description The INSERT INTO statement inserts data into a table. The data comes from a subselect query. It is commonly used to input prediction results into a database table. ​ Syntax Here is the syntax: INSERT INTO integration_name . table_name ( SELECT . . . ) ; Please note that the destination table ( integration_name.table_name ) must exist and contain all the columns where the data is to be inserted. And the steps followed by the syntax: It executes a subselect query to get the output dataset. It uses the INSERT INTO statement to insert the output of the (SELECT ...) query into the integration_name.table_name table. On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs ​ Example We want to save the prediction results into the int1.tbl1 table. Here is the schema structure used throughout this example: int1 └── tbl1 mindsdb └── predictor_name int2 └── tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let’s execute the query. INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ) ; On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs Was this page helpful? Yes No Suggest edits Raise issue Update a Table Join Tables On github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "manage-models-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Manage Model Versions ​ Creating a Model To create a model, use the CREATE MODEL statement. CREATE MODEL mindsdb . home_rentals FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING engine = 'lightwood' , tag = 'my model' ; Now, your model has one version. You can verify by querying the models table. DESCRIBE MODEL home_rentals ; On execution, we get: + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | CURRENT_TRAINING_PHASE | TOTAL_TRAINING_PHASES | TAG | CREATED_AT | TRAINING_TIME | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | home_rentals | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 16 : 01 : 04.990958 | 19.946 | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ ​ Retraining a Model To retrain a model, use the RETRAIN statement. RETRAIN mindsdb . home_rentals ; Let’s query for the model versions again. DESCRIBE MODEL home_rentals ; On execution, we get: + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | CURRENT_TRAINING_PHASE | TOTAL_TRAINING_PHASES | TAG | CREATED_AT | TRAINING_TIME | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | home_rentals | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 16 : 01 : 04.990958 | 19.946 | | home_rentals | lightwood | mindsdb | true | 2 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 17 : 01 : 04.990958 | 21.923 | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ ​ Using Active Model Version To use the currently active model version, run this query: SELECT * FROM mindsdb . home_rentals AS p JOIN example_db . demo_data . home_rentals AS d ; ​ Using Specific Model Version To use a specific model version, even if it is set to inactive, run this query: SELECT * FROM mindsdb . home_rentals . 1 AS p JOIN example_db . demo_data . home_rentals AS d ; ​ Setting Model Version as Active To set a specific model version as active, run the below statement: SET model_active = home_rentals . 1 ; ​ Deleting Specific Model Version To delete a specific model version, run the below statement: DROP MODEL home_rentals . 2 Please note that you cannot delete the version that is active. ​ Deleting All Model Versions To delete all models version, run the DROP MODEL statement: DROP MODEL mindsdb . home_rentals ; Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Creating a Model Retraining a Model Using Active Model Version Using Specific Model Version Setting Model Version as Active Deleting Specific Model Version Deleting All Model Versions"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Retrain a Model ​ Description The RETRAIN statement is used to retrain the already trained predictors with the new data. The predictor is updated to leverage the new data in optimizing its predictive capabilities. Retraining takes at least as much time as the training process of the predictor did because now the dataset used to retrain has new or updated data in addition to the old data. ​ Syntax Here is the syntax: RETRAIN [ MODEL ] project_name . predictor_name [ FROM [ integration_name | project_name ] ( SELECT column_name , . . . FROM [ integration_name . | project_name . ] table_name ) PREDICT target_name USING engine = 'engine_name' , tag = 'tag_name' , active = 0 / 1 ] ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) Where: Expressions Description project_name Name of the project where the model resides. predictor_name Name of the model to be retrained. integration_name Optional. Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT column_name, ... FROM table_name) Optional. Selecting data to be used for training and validation. target_column Optional. Column to be predicted. engine_name You can optionally provide an ML engine, based on which the model is retrained. tag_name You can optionally provide a tag that is visible in the training_options column of the mindsdb.models table. active Optional. Setting it to 0 causes the retrained version to be inactive. And setting it to 1 causes the retrained version to be active. Model Versions Every time the model is retrained, its new version is created with the incremented version number. You can query for all model versions like this: SELECT * FROM project_name . models ; For more information on managing model versions, check out our docs here . ​ When to RETRAIN the Model? It is advised to RETRAIN the predictor whenever the update_status column value from the mindsdb.models table is set to available . Here is when the update_status column value is set to available : When the new version of MindsDB is available that causes the predictor to become obsolete. When the new data is available in the table that was used to train the predictor. To find out whether you need to retrain your model, query the mindsdb.models table and look for the update_status column. Here are the possible values of the update_status column: Name Description available It indicates that the model should be updated. updating It indicates that the retraining process of the model takes place. up_to_date It indicates that your model is up to date and does not need to be retrained. Let’s run the query. SELECT name , update_status FROM mindsdb . models WHERE name = 'predictor_name' ; On execution, we get: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | predictor_name | up_to_date | + ------------------+---------------+ Where: Name Description predictor_name Name of the model to be retrained. update_status Column informing whether the model needs to be retrained. Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL predictor_name ; ​ Example Let’s look at an example using the home_rentals_model model. First, we check the status of the predictor. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL home_rentals_model ; The available value of the update_status column informs us that we should retrain the model. RETRAIN mindsdb . home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) Now, let’s check the status again. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | updating | + --------------------+---------------+ And after the retraining process is completed: SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+ Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax When to RETRAIN the Model? Example"}
{"file_name": "update.html", "content": "Update a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Update a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Update a Table ​ Description MindsDB provides two ways of using the UPDATE statement: The regular UPDATE statement updates specific column values in an existing table. The UPDATE FROM SELECT statement updates data in an existing table from a subselect query. It can be used as an alternative to CREATE TABLE or INSERT INTO to store predictions. ​ Syntax Here is an example of the regular UPDATE statement: UPDATE integration_name . table_name SET column_name = new_value WHERE column_name = old_value Please replace the placeholders as follows: integration_name is the name of the connected data source. table_name is the table name within that data source. column_name is the column name within that table. And here is an example of the UPDATE FROM SELECT statement that updates a table with predictions made within MindsDB: UPDATE integration_to_be_updated . table_to_be_updated SET column_to_be_updated = prediction_data . predicted_value_column , FROM ( SELECT p . predicted_value_column , p . column1 , p . column2 FROM integration_name . table_name as t JOIN model_name as p ) AS prediction_data WHERE column1 = prediction_data . column1 AND column2 = prediction_data . column2 Below is an alternative for the UPDATE FROM SELECT statement that updates a table with predictions. This syntax is easier to write. UPDATE integration_to_be_updated . table_to_be_updated ON column1 , column2 FROM ( SELECT p . predicted_value_column as column_to_be_updated , p . column1 , p . column2 FROM integration_name . table_name as t JOIN model_name as p ) The steps followed by the syntax: It executes query from the FROM clause to get the output data. In our example, we query for predictions, but it could be a simple select from another table. Please note that it is aliased as prediction_data . It updates all rows from the table_to_be_updated table (that belongs to the integration_to_be_updated integration) that match the WHERE clause criteria. The rows are updated with values as defined in the SET clause. It is recommended to use the primary key column(s) in the WHERE clause (here, column1 and column2 ), as the primary key column(s) uniquely identify each row. Otherwise, the UPDATE statement may lead to unexpected results by altering rows that you didn’t want to affect. Was this page helpful? Yes No Suggest edits Raise issue Native Queries Insert Into a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "install.html", "content": "MindsDB Installation for Development - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute MindsDB Installation for Development Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute MindsDB Installation for Development If you want to contribute to the development of MindsDB, you need to install from source. If you do not want to contribute to the development of MindsDB but simply install and use it, then install MindsDB via Docker . ​ Install MindsDB for Development Here are the steps to install MindsDB from source. You can either follow the steps below or visit the provided link. Before installing MindsDB from source, ensure that you use one of the following Python versions: 3.9.x , 3.10.x , 3.11.x`. Fork the MindsDB repository from GitHub . Clone the fork locally: git clone https://github.com/ < username > /mindsdb.git Create a virtual environment: python -m venv mindsdb-venv Activate the virtual environment: source mindsdb-venv/bin/activate Install MindsDB with its local development dependencies: Install dependencies: cd mindsdb pip install -e . pip install -r requirements/requirements-dev.txt Start MindsDB: python -m mindsdb By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio Alternatively, you can use a makefile to install dependencies and start MindsDB: make install_mindsdb make run_mindsdb Now you should see the following message in the console: ... mindsdb.api.http.initialize: - GUI available at http://127.0.0.1:47334/ mindsdb.api.mysql.mysql_proxy.mysql_proxy: Starting MindsDB Mysql proxy server on tcp://127.0.0.1:47335 mindsdb.api.mysql.mysql_proxy.mysql_proxy: Waiting for incoming connections... mindsdb: mysql API: started on 47335 mindsdb: http API: started on 47334 You can access the MindsDB Editor at localhost:47334 . ​ Install dependencies The dependencies for many of the data or ML integrations are not installed by default. If you want to use a data or AI/ML integration whose dependencies are not available by default, install it by running this command: pip install .[handler_name] You can find all available handlers here . ​ What’s Next? Now that you installed and started MindsDB locally, go ahead and find out how to create and train a model using the CREATE MODEL statement. Check out the Use Cases section to follow tutorials that cover Large Language Models, Chatbots, Time Series, Classification, and Regression models, Semantic Search, and more. Was this page helpful? Yes No Suggest edits Raise issue Issue Labels Python Coding Standards github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB for Development Install dependencies What’s Next?"}
{"file_name": "docs.html", "content": "How to Write MindsDB Documentation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute How to Write MindsDB Documentation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute How to Write MindsDB Documentation This section gets you started on how to contribute to the MindsDB documentation. MindsDB’s documentation is run using Mintlify. If you want to contribute to our docs, please follow the steps below to set up the environment locally. ​ Running the Docs Locally Prerequisite You should have installed Git (version 2.30.1 or higher) and Node.js (version 18.10.0 or higher). Step 1. Clone the MindsDB Git repository: git clone https://github.com/mindsdb/mindsdb.git Step 2. Install Mintlify on your OS: npm i mintlify -g Step 3. Go to the docs folder inside the cloned MindsDB Git repository and start Mintlify there: mintlify dev The documentation website is now available at http://localhost:3000 . Getting an Error? If you use the Windows operating system, you may get an error saying no such file or directory: C:/Users/Username/.mintlify/mint/client . Here are the steps to troubleshoot it: Go to the C:/Users/Username/.mintlify/ directory. Remove the mint folder. Open the Git Bash in this location and run git clone https://github.com/mintlify/mint.git . Repeat step 3. ​ MindsDB Repository Structure Here is the structure of the MindsDB docs repository: docs # All documentation source files |__assets/ # Images and icons used throughout the docs │ ├─ ... │__folders_with_mdx_files/ # All remaining folders that store the .mdx files |__mdx_files # Some of the .mdx files are stored in the docs directory |__mintlify.json # This JSON file stores navigation and page setup ​ What’s Next? Follow our docs rules and have fun. Thank you for contributing to the MindsDB docs! Was this page helpful? Yes No Suggest edits Raise issue Write Tutorials Docs Style Guide github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Running the Docs Locally MindsDB Repository Structure What’s Next?"}
{"file_name": "app-handlers.html", "content": "Build an Application Handler - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Build an Application Handler Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Build an Application Handler In this section, you’ll find how to add new application integrations to MindsDB. Prerequisite You should have the latest version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development. ​ What are API Handlers? Application handlers act as a bridge between MindsDB and any application that provides APIs. You use application handlers to create databases using the CREATE DATABASE statement. So you can reach data from any application that has its handler implemented within MindsDB. Database Handlers To learn more about handlers and how to implement a database handler, visit our doc page here . ML Handlers To learn more about handlers and how to implement a machine learning (ML) handler, visit our doc page here . ​ Creating an Application Handler You can create your own application handler within MindsDB by inheriting from the APIHandler class. By providing the implementation for some or all of the methods contained in the APIHandler class, you can interact with the application APIs. ​ Core Methods Apart from the __init__() method, there are five core methods that must be implemented. We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated. Let’s review the purpose of each method. Method Purpose _register_table() It registers the data resource in memory. For example, if you are using Twitter API it registers the tweets resource from /api/v2/tweets . connect() It performs the necessary steps to connect/authenticate to the underlying system. check_connection() It evaluates if the connection is alive and healthy. native_query() It parses any native statement string and acts upon it (for example, raw syntax commands). call_application_api() It calls the application API and maps the data to pandas DataFrame. This method handles the pagination and data mapping. Authors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended. Other Common Methods Under the mindsdb.integrations.utilities library, contributors can find various methods that may be useful while implementing new handlers. ​ API Table Once the data returned from the API call is registered using the _register_table() method, you can use it to map to the APITable class. The APITable class provides CRUD methods. Method Purpose select() It implements the mappings from the ast.Select and calls the actual API through the call_application_api . insert() It implements the mappings from the ast.Insert and calls the actual API through the call_application_api . update() It implements the mappings from the ast.Update and calls the actual API through the call_application_api . delete() It implements the mappings from the ast.Delete and calls the actual API through the call_application_api . add() Adds new rows to the data dictionary. list() List data based on certain conditions by providing FilterCondition, limits, sorting and target fields. get_columns() It maps the data columns returned by the API. ​ Implementation Each application handler should inherit from the APIHandler class. Here is a step-by-step guide: Implementing the __init__() method: This method initializes the handler. def __init__ ( self , name : str ) : super ( ) . __init__ ( name ) \"\" \" constructor Args : name ( str ) : the handler name \"\" \" self . _tables = { } Implementing the connect() method: The connect() method sets up the connection. def connect ( self ) - > HandlerStatusResponse : \"\" \" Set up any connections required by the handler Should return output of check_connection ( ) method after attempting connection . Should switch self . is_connected . Returns : HandlerStatusResponse \"\" \" Implementing the check_connection() method: The check_connection() method performs the health check for the connection. def check_connection ( self ) - > HandlerStatusResponse : \"\" \" Check connection to the handler Returns : HandlerStatusResponse \"\" \" Implementing the native_query() method: The native_query() method runs commands of the native API syntax. def native_query ( self , query : Any ) - > HandlerResponse : \"\" \"Receive raw query and act upon it somehow . Args : query ( Any ) : query in native format ( str for sql databases , dict for mongo , api's json etc ) Returns : HandlerResponse \"\" \" Implementing the call_application_api() method: This method makes the API calls. It is not mandatory to implement this method, but it can help make the code more reliable and readable. def call_application_api ( self , method_name : str = None , params : dict = None ) - > DataFrame : \"\" \"Receive query as AST ( abstract syntax tree ) and act upon it somehow . Args : query ( ASTNode ) : sql query represented as AST . Can be any kind of query : SELECT , INSERT , DELETE , etc Returns : DataFrame \"\" \" ​ Exporting the connection_args Dictionary The connection_args dictionary contains all of the arguments used to establish the connection along with their descriptions, types, labels, and whether they are required or not. The connection_args dictionary should be stored in the connection_args.py file inside the handler folder. The connection_args dictionary is stored in a separate file in order to be able to hide sensitive information such as passwords or API keys. By default, when querying for connection_data from the information_schema.databases table, all sensitive information is hidden. To unhide it, use this command: set show_secrets = true ; Here is an example of the connection_args.py file from the GitHub handler where the API key value is set to hidden with \"secret\": True . from collections import OrderedDict from mindsdb . integrations . libs . const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE connection_args = OrderedDict ( repository = { \"type\" : ARG_TYPE . STR , \"description\" : \" GitHub repository name.\" , \"required\" : True , \"label\" : \"Repository\" , } , api_key = { \"type\" : ARG_TYPE . PWD , \"description\" : \"Optional GitHub API key to use for authentication.\" , \"required\" : False , \"label\" : \"Api key\" , \"secret\" : True } , github_url = { \"type\" : ARG_TYPE . STR , \"description\" : \"Optional GitHub URL to connect to a GitHub Enterprise instance.\" , \"required\" : False , \"label\" : \"Github url\" , } , ) connection_args_example = OrderedDict ( repository = \"mindsdb/mindsdb\" , api_key = \"ghp_xxx\" , github_url = \"https://github.com/mindsdb/mindsdb\" ) ​ Exporting All Required Variables The following should be exported in the __init__.py file of the handler: The Handler class. The version of the handler. The name of the handler. The type of the handler, either DATA handler or ML handler. The icon_path to the file with the database icon. The title of the handler or a short description. The description of the handler. The connection_args dictionary with the connection arguments. The connection_args_example dictionary with an example of the connection arguments. The import_error message that is used if the import of the Handler class fails. A few of these variables are defined in another file called __about__.py . This file is imported into the __init__.py file. Here is an example of the __init__.py file for the GitHub handler . from mindsdb . integrations . libs . const import HANDLER_TYPE from . __about__ import __version__ as version , __description__ as description from . connection_args import connection_args , connection_args_example try : from . github_handler import ( GithubHandler as Handler , connection_args_example , connection_args , ) import_error = None except Exception as e : Handler = None import_error = e title = \"GitHub\" name = \"github\" type = HANDLER_TYPE . DATA icon_path = \"icon.svg\" __all__ = [ \"Handler\" , \"version\" , \"name\" , \"type\" , \"title\" , \"description\" , \"import_error\" , \"icon_path\" , \"connection_args_example\" , \"connection_args\" , ] The __about__.py file for the same GitHub handler contains the following variables: __title__ = \"MindsDB GitHub handler\" __package_name__ = \"mindsdb_github_handler\" __version__ = \"0.0.1\" __description__ = \"MindsDB handler for GitHub\" __author__ = \"Artem Veremey\" __github__ = \"https://github.com/mindsdb/mindsdb\" __pypi__ = \"https://pypi.org/project/mindsdb/\" __license__ = \"MIT\" __copyright__ = \"Copyright 2023 - mindsdb\" ​ Check out our Application Handlers! To see some integration handlers that are currently in use, we encourage you to check out the following handlers inside the MindsDB repository: GitHub handler TwitterHandler And here are all the handlers available in the MindsDB repository . Was this page helpful? Yes No Suggest edits Raise issue Build an AI/ML Handler Testing github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What are API Handlers? Creating an Application Handler Core Methods API Table Implementation Exporting the connection_args Dictionary Exporting All Required Variables Check out our Application Handlers!"}
{"file_name": "issues.html", "content": "Report an Issue 📢 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Report an Issue 📢 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Report an Issue 📢 If you want to report a bug, request a feature, suggest docs improvements, or propose a new integration, you can do that on the MindsDB GitHub issues page . But before reporting a new issue, please make sure that it is not already there. ​ How to Create an Issue Here is how to get started when you want to create an issue in the MindsDB repository. Go to our GitHub issues page and click on the New issue button. Now you see the available issue types. Let’s go through all of them one by one. ​ Report a Bug 🐞 Here, we choose Report a bug and click on the Get started button. Now, it’s time to fill up the form. It is vital to add a meaningful issue title. Here, you describe the current behavior. Please note that this field is mandatory. Below you can attach any videos or screenshots of the current behavior. If you know what the expected behavior should be, you can add it here. It is helpful for us if you add the steps you followed that led you to the error. Any links, references, logs, screenshots, etc., are welcome! Remember, all contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for reporting a bug! It helps us improve MindsDB for you and future users. ​ Request a Feature 🚀 Here, we choose Request a feature and click on the Get started button. Now, it’s time to fill up the form. It is vital to add a meaningful issue title. Here, you describe the feature request along with the motivation for the proposed feature. Please note that this field is mandatory. Below you can attach any videos or screenshots. If you know how the solution should look, you can add it here. Any links, references, logs, screenshots, etc., are welcome! Remember, all contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for submitting a feature request! It helps us improve MindsDB for you and future users. ​ Improve our Docs ✍️ Here, we choose Improve our docs and click on the Get started button. Now, it’s time to fill up the form. It is vital to add a meaningful issue title. Here, you describe what should be added or improved. Please note that this field is mandatory. Below you can attach any videos or screenshots. Any links, references, logs, screenshots, etc., are welcome! Remember, all contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for suggesting docs enhancements! It helps us improve MindsDB for you and future users. ​ Propose a New Integration 🧑‍🔧 Here is how to get started when you want to propose a new integration. Please note it can be a new database integrations or a new machine learning framework. We choose Propose a new integration and click on the Get started button. Now, it’s time to fill up the form. It is vital to add a meaningful issue title. Please make sure that this integration is not already implemented. Here, you describe the use case(s) solved by this integration. Please provide a motivation for your integration proposal. Get creative and describe the implementation! You can use code, text, diagrams, etc. Any links, references, logs, screenshots, etc., are welcome! Remember, all contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for proposing a new integration! It helps us improve MindsDB for you and future users. ​ Report a Security Vulnerability Here is how to get started when you want to report a security vulnerability. Please note that such issues are visible only to repository maintainers. Also, you will be credited if the advisory is published. We choose Report a security vulnerability and click on the Get started button. Now, it’s time to fill up the form. It is vital to add a meaningful issue title. Please follow the instructions to provide the best description. Here, you can choose one or more affected products along with the versions. Please assign the severity of the issue. The Calculator feature can help you assess the severity. Here, you can assign one or more common weakness enumerators (CWE). Remember, all contributions to our repository should follow the contributing guidelines and code of conduct . Thank you for reporting a security vulnerability! It helps us improve MindsDB for you and future users. ​ Issue Review Issues are reviewed regularly, usually daily. Depending on the issue type, it will be labeled as Bug or enhancement . Please make sure you respond to our feedback/questions regarding your issue. Was this page helpful? Yes No Suggest edits Raise issue How to Contribute Issue Labels github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Create an Issue Report a Bug 🐞 Request a Feature 🚀 Improve our Docs ✍️ Propose a New Integration 🧑‍🔧 Report a Security Vulnerability Issue Review"}
{"file_name": "python-coding-standards.html", "content": "Python Coding Standards - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Python Coding Standards Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Python Coding Standards ​ PEP8 Strict adherence to PEP8 standards is mandatory for all code contributions to MindsDB. Why PEP8? PEP8 provides an extensive set of guidelines for Python code styling, promoting readability and a uniform coding standard. By aligning with PEP8, we ensure our codebase remains clean, maintainable, and easily understandable for Python developers at any level. ​ Automated Checks Upon submission of a Pull Request (PR), an automated process checks the code for PEP8 compliance. Non-compliance with PEP8 can result in the failure of the build process. Adherence to PEP8 is not just a best practice but a necessity to ensure smooth integration of new code into the codebase. If a PR fails due to PEP8 violations, the contributor is required to review the automated feedback provided. Pay special attention to common PEP8 compliance issues such as proper indentation, appropriate line length, correct use of whitespace, and following the recommended naming conventions. Contributors are encouraged to iteratively improve their code based on the feedback until full compliance is achieved. ​ Logging Always instantiate a logger using the MindsDB utilities module. This practice ensures a uniform approach to logging across different parts of the application. Example of Logger Creation: from mindsdb . utilities import log logger = log . getLogger ( __name__ ) ​ Setting Logging Environment Variable: Use MINDSDB_LOG_LEVEL to set the desired logging level. This approach allows for dynamic adjustment of log verbosity without needing code modifications. Log Levels: Available levels include: DEBUG : Detailed information, typically of interest only when diagnosing problems. INFO: Confirmation that things are working as expected. WARNING : An indication that something unexpected happened, or indicative of some problem in the near future. ERROR : Due to a more serious problem, the software has not been able to perform some function. CRITICAL : A serious error, indicating that the program itself may be unable to continue running. Avoid print() statements. They lack the flexibility and control offered by logging mechanisms, particularly in terms of output redirection and level-based filtering. The logger name should be __name__ to automatically reflect the module’s name. This convention is crucial for pinpointing the origin of log messages. ​ Docstrings Docstrings are essential for documenting Python code. They provide a clear explanation of the functionality of classes, functions, modules, etc., making the codebase easier to understand and maintain. A well-written docstring should include: Function’s Purpose: Describe what the function/class/module does. Parameters: List and explain the parameters it takes. Return Value: Describe what the function returns. Exceptions: Mention any exceptions that the function might raise. def example_function ( param1 , param2 ) : \"\" \"This is an example docstring . Args : param1 ( type ) : Description of param1 . param2 ( type ) : Description of param2 . Returns : type : Description of the return value . Raises : ExceptionType : Description of the exception . \"\" \" # function body... ​ Exception Handling Implementing robust error handling strategies is essential to maintain the stability and reliability of MindsDB. Proper exception management ensures that the application behaves predictably under error conditions, providing clear feedback and preventing unexpected crashes or behavior. Utilizing MindsDB Exceptions: To ensure uniformity and clarity in error reporting, always use predefined exceptions from the MindsDB exceptions library. Adding New Exceptions: If during development you encounter a scenario where none of the existing exceptions adequately represent the error, consider defining a new, specific exception. Was this page helpful? Yes No Suggest edits Raise issue Installation for Development Build a Database Handler github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page PEP8 Automated Checks Logging Setting Logging Docstrings Exception Handling"}
{"file_name": "tests.html", "content": "Testing and Improving Test Coverage - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Testing and Improving Test Coverage Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Testing and Improving Test Coverage Work in Progress This documentation is a work in progress. Was this page helpful? Yes No Suggest edits Raise issue Build an Application Handler Write Handlers README github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "ml-handlers.html", "content": "Build an AI/ML Handler - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Build an AI/ML Handler Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Build an AI/ML Handler In this section, you’ll find how to create new machine learning (ML) handlers within MindsDB. Prerequisite You should have the latest version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development. ​ What are Machine Learning Handlers? ML handlers act as a bridge to any ML framework. You use ML handlers to create ML engines using the CREATE ML_ENGINE command . So you can expose ML models from any supported ML engine as an AI table. Database Handlers To learn more about handlers and how to implement a database handler, visit our doc page here . ​ Creating a Machine Learning Handler You can create your own ML handler within MindsDB by inheriting from the BaseMLEngine class. By providing the implementation for some or all of the methods contained in the BaseMLEngine class, you can connect with the machine learning library or framework of your choice. ​ Core Methods Apart from the __init__() method, there are five methods, of which two must be implemented. We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated. Let’s review the purpose of each method. Method Purpose create() It creates a model inside the engine registry. predict() It calls a model and returns prediction data. update() Optional. It updates an existing model without resetting its internal structure. describe() Optional. It provides global model insights. create_engine() Optional. It connects with external sources, such as REST API. Authors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended. Other Common Methods Under the mindsdb.integrations.libs.utils library, contributors can find various methods that may be useful while implementing new handlers. Also, there is a wrapper class for the BaseMLEngine instances called BaseMLEngineExec . It is automatically deployed to take care of modifying the data responses into something that can be used alongside data handlers. ​ Implementation Here are the methods that must be implemented while inheriting from the BaseMLEngine class: The create() method saves a model inside the engine registry for later usage. def create ( self , target : str , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) - > None : \"\" \" Saves a model inside the engine registry for later usage . Normally , an input dataframe is required to train the model . However , some integrations may merely require registering the model instead of training , in which case `df` can be omitted . Any other arguments required to register the model can be passed in an `args` dictionary . \"\" \" The predict() method calls a model with an input dataframe and optionally, arguments to modify model’s behaviour. This method returns a dataframe with the predicted values. def predict ( self , df : pd . DataFrame , args : Optional [ Dict ] = None ) - > pd . DataFrame : \"\" \" Calls a model with some input dataframe `df` , and optionally some arguments `args` that may modify the model behavior . The expected output is a dataframe with the predicted values in the target - named column . Additional columns can be present , and will be considered row - wise explanations if their names finish with `_explain` . \"\" \" And here are the optional methods that you can implement alongside the mandatory ones if your ML framework allows it: The update() method is used to update, fine-tune, or adjust an existing model without resetting its internal state. def finetune ( self , df : Optional [ pd . DataFrame ] = None , args : Optional [ Dict ] = None ) - > None : \"\" \" Optional . Used to update / fine - tune / adjust a pre - existing model without resetting its internal state ( e . g . weights ) . Availability will depend on underlying integration support , as not all ML models can be partially updated . \"\" \" The describe() method provides global model insights, such as framework-level parameters used in training. def describe ( self , key : Optional [ str ] = None ) - > pd . DataFrame : \"\" \" Optional . When called , this method provides global model insights , e . g . framework - level parameters used in training . \"\" \" The create_engine() method is used to connect with the external sources, such as REST API. def create_engine ( self , connection_args : dict ) : \"\" \" Optional . Used to connect with external sources ( e . g . a REST API ) that the engine will require to use any other methods . \"\" \" ​ MindsDB ML Ecosystem MindsDB has recently decoupled some modules out of its AutoML package in order to leverage them in integrations with other ML engines. The three modules are as follows: The type_infer module that implements automated type inference for any dataset. Below is the description of the input and output of this module. Input: tabular dataset. Output: best guesses of what type of data each column contains. The dataprep_ml module that provides data preparation utilities, such as data cleaning, analysis, and splitting. Data cleaning procedures include column-wise cleaners, column-wise missing value imputers, and data splitters (train-val-test split, either simple or stratified). Below is the description of the input and output of this module. Input: tabular dataset. Output: cleaned dataset, plus insights useful for data analysis and model building. The mindsdb_evaluator module that provides utilities for evaluating the accuracy and calibration of ML models. Below is the description of the input and output of this module. Input: model predictions and the input data used to generate these predictions, including corresponding ground truth values of the column to predict. Output: accuracy metrics that evaluate prediction accuracy and calibration metrics that check whether model-emitted probabilities are calibrated. We recommend that new contributors use type_infer and dataprep_ml modules when writing ML handlers to avoid reimplementing thin AutoML layers over and over again; it is advised to focus on mapping input data and user parameters to the underlying framework’s API. For now, using the mindsdb_evaluator module is not required, but will be in the short to medium term, so it’s important to be aware of it while writing a new integration. Example Let’s say you want to write an integration for TPOT . Its high-level API exposes classes that are either for classification or regression. But as a handler designer, you need to ensure that arbitrary ML tasks are dispatched properly to each class (i.e., not using a regressor for a classification problem and vice versa). First, type_infer can help you by estimating the data type of the target variable (so you immediately know what class to use). Additionally, to quickly get a stratified train-test split, you can leverage dataprep_ml splitters and continue to focus on the actual usage of TPOT for the training and inference logic. We would appreciate your feedback regarding usage & feature roadmap for the above modules, as they are quite new. ​ Step-by-Step Instructions Step 1: Set up and run MindsDB locally Set up MindsDB using the self-hosted pip installation method. Make sure you can run the quickstart example locally. If you run into errors, check your bash terminal output. Create a new git branch to store your changes. Step 2: Write a (failing) test for your new handler Check that you can run the existing handler tests with python -m pytest tests/unit/ml_handlers/ . If you get the ModuleNotFoundError error, try adding the __init__.py file to any subdirectory that doesn’t have it. Copy the simple tests from a relevant handler. For regular data, use the Ludwig handler. And for time series data, use the StatsForecast handler. Change the SQL query to reference your handler. Specifically, set USING engine={HandlerName} . Run your new test. Please note that it should fail as you haven’t yet added your handler. The exception should be Can't find integration_record for handler ... . Step 3: Add your handler to the source code Create a new directory in mindsdb/integrations/handlers/ . You must name the new directory {HandlerName}_handler/ . Copy the .py files from the OpenAI handler folder , including: __about__.py , __init__.py , openai_handler.py , creation_args.py , and model_using_args.py . Note that the arguments used at model creation time (stored in creation_args.py ) and the arguments used at prediction time (stored in model_using_args.py ) should be stored in separate files in order to be able to hide sensitive information such as API keys. By default, when querying for connection_data from the information_schema.ml_engines table or training_options from the information_schema.models table, all sensitive information is hidden. To unhide it, use this command: set show_secrets = true ; Change the contents of .py files to match your new handler. Also, change the name of the statsforecast_handler.py file to match your handler. Modify the requirements.txt file to install your handler’s dependencies. You may get conflicts with other packages like Lightwood, but you can ignore them for now. Create a new blank class for your handler in the {HandlerName}_handler.py file. Like for other handlers, this should be a subclass of the BaseMLEngine class. Add your new handler class to the testing DB. In the tests/unit/executor_test_base.py file starting at line 91, you can see how other handlers are added with db.session.add(...) . Copy that and modify it to add your handler. Please note to add your handler before Lightwood, otherwise the CI will break. Run your new test. Please note that it should still fail but with a different exception message. Step 4: Modify the handler source code until your test passes Define a create() method that deals with the model setup arguments. This will add your handler to the models table. Depending on the framework, you may also train the model here using the df argument. Save relevant arguments/trained models at the end of your create method. This allows them to be accessed later. Use the engine_storage attributes; you can find examples in other handlers’ folders. Define a predict() method that makes model predictions. This method must return a dataframe with format matching the input, except with a column containing your model’s predictions of the target. The input df is a subset of the original df with the rows determined by the conditions in the predict SQL query. Don’t debug the create() and predict() methods with the print() statement because they’re inside a subthread. Instead, write relevant info to disk. Once your first test passes, add new tests for any important cases. You can also add tests for any helper functions you write. Step 5: QA your handler locally Launch the MindsDB server locally with python -m mindsdb . Again, any issues will appear in the terminal output. Check that your handler has been added to the local server database. You can view the list of handlers with SELECT * from information_schema.handlers . Run the relevant tutorial from the panel on the right side. For regular data, this is Predict Home Rental Prices . And for time series data, this is Forecast Quarterly House Sales . Specify USING ENGINE={your_handler} while creating a model. Don’t debug the create() and predict() methods with the print() statement because they’re inside a subthread. Instead, write relevant info to disk. You should get sensible results if your handler has been well-implemented. Make sure you try the predict step with a range of parameters. Step 6: Open a pull request You need to fork the MindsDB repository. Follow this guide to start a PR. If relevant, add your tests and new dependencies to the CI config. This is at .github/workflows/mindsdb.yml . Please note that pytest is the recommended testing package. Use pytest to confirm your ML handler implementation is correct. Templates for Unit Tests If you implement a time-series ML handler, create your unit tests following the structure of the StatsForecast unit tests . If you implement an NLP ML handler, create your unit tests following the structure of the Hugging Face unit tests . ​ Check out our Machine Learning Handlers! To see some ML handlers that are currently in use, we encourage you to check out the following ML handlers inside the MindsDB repository: Lightwood HuggingFace Ludwig OpenAI And here are all the handlers available in the MindsDB repository . Was this page helpful? Yes No Suggest edits Raise issue Build a Database Handler Build an Application Handler github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What are Machine Learning Handlers? Creating a Machine Learning Handler Core Methods Implementation MindsDB ML Ecosystem Step-by-Step Instructions Check out our Machine Learning Handlers!"}
{"file_name": "docs-rules.html", "content": "Style Guide for MindsDB Documentation - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Style Guide for MindsDB Documentation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Style Guide for MindsDB Documentation ​ Syntax for SQL commands Follow the rules below when writing an SQL command. Add a semi-colon ; at the end of each SQL command. Use all-caps when writing the keywords, such as SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , AS , CREATE TABLE , INSERT INTO , etc. When writing a query, start a new line for the following keywords: SELECT , FROM , JOIN , WHERE , GROUP BY , ORDER BY , PREDICT , USING , AND , OR . It is to avoid the horizontal scrollbar. ​ Example SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name_1 = value_name_1 AND column_name_2 = value_name_2 GROUP BY a . column_name_2 ORDER BY b . column_name_1 ; ​ Syntax for SQL commands along with their output Follow the syntax below when documenting an SQL command and its output. QUERY GOES HERE On execution, we get: + -------------+-------------+ | column_name | column_name | + -------------+-------------+ | value | value | + -------------+-------------+ Where: Name Description VARIABLE NAME GOES HERE VARIABLE DESCRIPTION GOES HERE If the output is not a table, remove the output table from above and place your output message there. ​ Example 1 SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name = value_name ; On execution, we get: + -------------+-------------+ | column_name | column_name | + -------------+-------------+ | value | value | + -------------+-------------+ Where: Name Description column_name column description ​ Output of Example 1 SELECT * FROM table_name_1 a JOIN table_name_2 b WHERE column_name = value_name ; On execution, we get: + -------------+-------------+ | column_name | column_name | + -------------+-------------+ | value | value | + -------------+-------------+ Where: Name Description column_name column description ​ Example 2 CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name_1 , column_name_2 , target_column FROM table_name ) PREDICT target_column ; On execution, we get: OUTPUT GOES HERE ​ Output of Example 2 CREATE MODEL mindsdb . predictor_name FROM integration_name ( SELECT column_name_1 , column_name_2 , target_column FROM table_name ) PREDICT target_column ; On execution, we get: OUTPUT GOES HERE Was this page helpful? Yes No Suggest edits Raise issue Write Documentation Join our Community github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Syntax for SQL commands Example Syntax for SQL commands along with their output Example 1 Output of Example 1 Example 2 Output of Example 2"}
{"file_name": "integrations-readme.html", "content": "How to Write Handlers README - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute How to Write Handlers README Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute How to Write Handlers README The README file is a crucial document that guides users in understanding, using, and contributing to the MindsDb integration. It serves as the first point of contact for anyone interacting with the integration, hence the need for it to be comprehensive, clear, and user-friendly. ​ Sections to Include ​ Table of Contents A well-organized table of contents is provided for easy navigation through the document, allowing users to quickly find the information they need. ​ About Explain what specific database, application, or framework the integration targets. Provide a concise overview of the integration’s purpose, highlighting its key features and benefits. ​ Handler Implementation Setup Detail the installation and initial setup process, including any prerequisites. Connection Describe the steps to establish and manage connections, with clear instructions. Include SQL examples for better clarity. Required Parameters List and describe all essential parameters necessary for the operation of the integration. Optional Parameters Detail additional, non-mandatory parameters that can enhance the integration’s functionality. ​ Example Usage Practical Examples: Offer detailed examples showing how to use the integration effectively. Coverage: Ensure examples encompass a range of functionalities, from basic to advanced operations. SQL Examples: Include SQL statements and their expected outputs to illustrate use cases. ​ Supported Tables/Tasks Clearly enumerate the tables, tasks, or operations that the integration supports, possibly in a list or table format. ​ Limitations Transparently outline any limitations or constraints known in the integration. ​ TODO Future Developments: Highlight areas for future enhancements or improvements. GitHub Issues: Link to open GitHub issues tagged as enhancements, indicating ongoing or planned feature additions. Was this page helpful? Yes No Suggest edits Raise issue Testing Write Tutorials github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Sections to Include Table of Contents About Handler Implementation Example Usage Supported Tables/Tasks Limitations TODO"}
{"file_name": "issue-labels.html", "content": "Issue Labels - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Issue Labels Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Issue Labels In this section, we go through all issue labels available in the MindsDB GitHub repository . ​ Labels of Interest to Contributors If you are or plan to be a contributor, look at the issues marked with these labels. ​ The first-timers-only Label If you haven’t contributed to MindsDB before, you can search for the issues marked as first-timers-only to get started. Please note that before we can accept your contribution to MindsDB, we ask you to sign our Contributor License Agreement . You can find all the first-timers-only issues here . ​ The good first issue Label Issues marked as good first issue are good for newcomers. You can find all the good first issue issues here . ​ The hactoberfest Label By taking up an issue marked as hactoberfest , you can participate in the Hacktoberfest competition, which takes place every year in October. You can find all the hactoberfest issues here . ​ The help wanted Label Issues marked as help wanted are for anybody who wants to contribute to MindsDB. You can find all the help wanted issues here . ​ The integration Label The integration label marks issues that require a contributor to implement a database or ML integration with MindsDB. If you take up such an issue, you can participate in the Integration Contest of MindsDB. You can create an issue with your idea for integration by following the instructions here . You can find all the integration issues here . ​ Other Labels Here are other labels used in the MindsDB repository. ​ The bug Label The bug label marks issues that describe what’s currently not working. You can report a bug by following the instructions here . ​ The discussion Label If an issue is marked as discussion , it requires further discussion before it can be resolved. ​ The documentation Label The documentation label marks issues related to our docs. You can improve our docs by creating issues following the instructions here . ​ The enhancement Label As MindsDB grows day by day, there are still things that require improvements. All issues suggesting enhancements to MindsDB are marked as enhancement . You can request a feature by following the instructions here . ​ The follow-up Label If an issue is marked as follow-up , it requires users’ feedback before it can be resolved. ​ The question Label If an issue is marked as question , it requires further information before it can be resolved. ​ The user request Label Our customers can suggest improvements or report bugs. Any issue that comes from them is marked as user request . Was this page helpful? Yes No Suggest edits Raise issue Report an Issue Installation for Development github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Labels of Interest to Contributors The first-timers-only Label The good first issue Label The hactoberfest Label The help wanted Label The integration Label Other Labels The bug Label The discussion Label The documentation Label The enhancement Label The follow-up Label The question Label The user request Label"}
{"file_name": "tutorials.html", "content": "How to Write a Tutorial - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute How to Write a Tutorial Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute How to Write a Tutorial This section presents how to write a tutorial with MindsDB. Content of the Tutorial You are free to create your content. However, you should include in your tutorial all the chapters listed here. The tutorial should be a Markdown file, for example, mindsdb-tutorial.mdx . Here is the basic Markdown syntax . ​ Introduction Here you introduce the readers to the tutorial. You can describe what dataset you use and what you predict. ​ Data Setup This chapter contains an introduction to the dataset you use. ​ Connecting the Data Let others follow your tutorial by providing information on where to get the data from and how to connect it to MindsDB. ​ Understanding the Data You can briefly introduce the dataset you use. ​ Training a Predictor Here you use the CREATE MODEL command to create a predictor. ​ Status of a Predictor The next step is to check the status of a predictor. If its value is complete , you can proceed to the next chapter. ​ Making Predictions Use the SELECT statement to query for prediction results. It is good to present the output to the readers. ​ Making a Single Prediction In the case of regression and classification predictors, you can make a single prediction. It is good to present the output to the readers. ​ Making Batch Predictions You can make batch predictions using the JOIN clause for all predictor types, such as regression, classification, and time series. It is good to present the output to the readers. ​ What’s Next? Submit a PR with your tutorial. We’ll review it, and soon you’ll see your tutorial in the MindsDB docs! Was this page helpful? Yes No Suggest edits Raise issue Write Handlers README Write Documentation github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Data Setup Connecting the Data Understanding the Data Training a Predictor Status of a Predictor Making Predictions Making a Single Prediction Making Batch Predictions What’s Next?"}
{"file_name": "contribute.html", "content": "Contribute to MindsDB 🐻 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Contribute to MindsDB 🐻 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Contribute to MindsDB 🐻 Thank you for your interest in contributing to MindsDB. MindsDB is free, open-source software and all types of contributions are welcome, whether they’re documentation changes, bug reports, bug fixes, or new source code changes. ​ Contribution issues 🔧 All the issues open for contributions are tagged with good-first-issue or help-wanted . A great place to start looking is our GitHub project for community contributors dashboard . Also, we are always open to suggestions so feel free to open new issues with your ideas and we can give you guidance! After you find the issue that you want to contribute to, follow the fork-and-pull workflow: Comment on the issue, so we can assign it to you Fork the MindsDB repository Clone the repository locally Make changes and commit them Push your local branch to your fork Submit a Pull Request so that we can review your changes Follow the PR template and provide all of the required informations Make sure that the CI tests are GREEN Be sure to merge the latest MindsDB repository from “upstream” before making a Pull Request! Pull Request reviews are done on a regular basis. Please make sure you respond to our feedback/questions and sign our CLA. ​ Documentation 📖 We are always trying to improve our documentation. All Pull Requests that improve our grammar or docs structure or fix typos are welcomed. Check the issues labeled as good-first-issue or help-wanted with a documentation tag. ​ Write for us 📝 Do you find MindsDB useful and want to share your story? Make a PR to this repo with your writing in a markdown file, or just post it on Medium, Dev, or your own blog post. We would love to hear from you 💚 Was this page helpful? Yes No Suggest edits Raise issue Report an Issue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Contribution issues 🔧 Documentation 📖 Write for us 📝"}
{"file_name": "community.html", "content": "Join our Community 🐻 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Join our Community 🐻 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Join our Community 🐻 If you have questions or you want to chat with the MindsDB core team or other community members, you can join our Slack workspace ​ MindsDB newsletter 📰 To get updates on MindsDB’s latest announcements, releases and events, sign up for our newsletter . ​ Become a MindsDB Beta tester 🔎 If you want to become a part of our product and get first access to all of our latest updates, join our Beta testers community . ​ Talk to our engineers ☎ If you want to use MindsDB or you have more questions, you can schedule a call by clicking on Talk to our Engineers button. ​ Get in touch for collaboration 📞 Contact us by submitting this form . Was this page helpful? Yes No Suggest edits Raise issue Docs Style Guide github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page MindsDB newsletter 📰 Become a MindsDB Beta tester 🔎 Talk to our engineers ☎ Get in touch for collaboration 📞"}
{"file_name": "data-handlers.html", "content": "Build a Database Handler - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Contribute Build a Database Handler Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Contribute How to Contribute Report an Issue Issue Labels Installation for Development Python Coding Standards Build a Database Handler Build an AI/ML Handler Build an Application Handler Testing Write Handlers README Write Tutorials Write Documentation Docs Style Guide Join our Community Contribute Build a Database Handler In this section, you’ll find how to add new integrations/databases to MindsDB. Prerequisite You should have the latest version of the MindsDB repository installed locally. Follow this guide to learn how to install MindsDB for development. ​ What are Database Handlers? Database handlers act as a bridge to any database. You use database handlers to create databases using the CREATE DATABASE command . So you can reach data from any database that has its handler implemented within MindsDB. ML Handlers To learn more about handlers and how to implement a machine learning (ML) handler, visit our doc page here . ​ Creating a Database Handler You can create your own database handler within MindsDB by inheriting from the DatabaseHandler class. By providing the implementation for some or all of the methods contained in the DatabaseHandler class, you can connect with the database of your choice. ​ Core Methods Apart from the __init__() method, there are seven core methods that must be implemented. We recommend checking actual examples in the codebase to get an idea of what goes into each of these methods, as they can change a bit depending on the nature of the system being integrated. Let’s review the purpose of each method. Method Purpose connect() It performs the necessary steps to connect to the underlying system. disconnect() It gracefully closes connections established in the connect() method. check_connection() It evaluates if the connection is alive and healthy. This method is called frequently. native_query() It parses any native statement string and acts upon it (for example, raw SQL commands). query() It takes a parsed SQL command in the form of an abstract syntax tree and executes it. get_tables() It lists and returns all the available tables. Each handler decides what a table means for the underlying system when interacting with it from the data layer. Typically, these are actual tables. get_columns() It returns columns of a table registered in the handler with the respective data type. Authors can opt for adding private methods, new files and folders, or any combination of these to structure all the necessary work that will enable the core methods to work as intended. Other Common Methods Under the mindsdb.integrations.libs.utils library, contributors can find various methods that may be useful while implementing new handlers. Also, there are wrapper classes for the DatabaseHandler instances called HandlerResponse and HandlerStatusResponse . You should use them to ensure proper output formatting. ​ Implementation Each database handler should inherit from the DatabaseHandler class. Here is a step-by-step guide: Setting the name class property: MindsDB uses it internally as the name of the handler. For example, the CREATE DATABASE statement uses the handler’s name. CREATE DATABASE integration_name WITH ENGINE = 'postgres' , --- here, the handler's name is `postgres` PARAMETERS = { 'host' : '127.0.0.1' , 'user' : 'root' , 'password' : 'password' } ; Implementing the __init__() method: This method initializes the handler. The connection_data argument contains the PARAMETERS from the CREATE DATABASE statement, such as user , password , etc. def __init__ ( self , name : str , connection_data : Optional [ dict ] ) : \"\" \" constructor Args : name ( str ) : the handler name \"\" \" Implementing the connect() method: The connect() method sets up the connection. def connect ( self ) - > HandlerStatusResponse : \"\" \" Set up any connections required by the handler Should return the output of check_connection ( ) method after attempting connection . Should switch self . is_connected . Returns : HandlerStatusResponse \"\" \" Implementing the disconnect() method: The disconnect() method closes the existing connection. def disconnect ( self ) : \"\" \" Close any existing connections Should switch self . is_connected . \"\" \" Implementing the check_connection() method: The check_connection() method performs the health check for the connection. def check_connection ( self ) - > HandlerStatusResponse : \"\" \" Check connection to the handler Returns : HandlerStatusResponse \"\" \" Implementing the native_query() method: The native_query() method runs commands of the native database language. def native_query ( self , query : Any ) - > HandlerResponse : \"\" \"Receive raw query and act upon it somehow . Args : query ( Any ) : query in native format ( str for sql databases , dict for mongo , etc ) Returns : HandlerResponse \"\" \" Implementing the query() method: The query method runs parsed SQL commands. def query ( self , query : ASTNode ) - > HandlerResponse : \"\" \"Receive query as AST ( abstract syntax tree ) and act upon it somehow . Args : query ( ASTNode ) : sql query represented as AST . May be any kind of query : SELECT , INSERT , DELETE , etc Returns : HandlerResponse \"\" \" Implementing the get_tables() method: The get_tables() method lists all the available tables. def get_tables ( self ) - > HandlerResponse : \"\" \" Return list of entities Return a list of entities that will be accessible as tables . Returns : HandlerResponse : should have the same columns as information_schema . tables ( https : // dev . mysql . com / doc / refman / 8.0 / en / information - schema - tables - table . html ) Column 'TABLE_NAME' is mandatory , other is optional . \"\" \" Implementing the get_columns() method: The get_columns() method lists all columns of a specified table. def get_columns ( self , table_name : str ) - > HandlerResponse : \"\" \" Returns a list of entity columns Args : table_name ( str ) : name of one of tables returned by self . get_tables ( ) Returns : HandlerResponse : should have the same columns as information_schema . columns ( https : // dev . mysql . com / doc / refman / 8.0 / en / information - schema - columns - table . html ) Column 'COLUMN_NAME' is mandatory , other is optional . Highly recommended to define also 'DATA_TYPE' : it should be one of python data types ( by default it is str ) . \"\" \" ​ Exporting the connection_args Dictionary The connection_args dictionary contains all of the arguments used to establish the connection along with their descriptions, types, labels, and whether they are required or not. The connection_args dictionary should be stored in the connection_args.py file inside the handler folder. The connection_args dictionary is stored in a separate file in order to be able to hide sensitive information such as passwords or API keys. By default, when querying for connection_data from the information_schema.databases table, all sensitive information is hidden. To unhide it, use this command: set show_secrets = true ; Here is an example of the connection_args.py file from the MySQL handler where the password value is set to hidden with 'secret': True . from collections import OrderedDict from mindsdb . integrations . libs . const import HANDLER_CONNECTION_ARG_TYPE as ARG_TYPE connection_args = OrderedDict ( url = { 'type' : ARG_TYPE . STR , 'description' : 'The URI-Like connection string to the MySQL server. If provided, it will override the other connection arguments.' , 'required' : False , 'label' : 'URL' } , user = { 'type' : ARG_TYPE . STR , 'description' : 'The user name used to authenticate with the MySQL server.' , 'required' : True , 'label' : 'User' } , password = { 'type' : ARG_TYPE . PWD , 'description' : 'The password to authenticate the user with the MySQL server.' , 'required' : True , 'label' : 'Password' , 'secret' : True } , database = { 'type' : ARG_TYPE . STR , 'description' : 'The database name to use when connecting with the MySQL server.' , 'required' : True , 'label' : 'Database' } , host = { 'type' : ARG_TYPE . STR , 'description' : 'The host name or IP address of the MySQL server. NOTE: use \\'127.0.0.1\\' instead of \\'localhost\\' to connect to local server.' , 'required' : True , 'label' : 'Host' } , port = { 'type' : ARG_TYPE . INT , 'description' : 'The TCP/IP port of the MySQL server. Must be an integer.' , 'required' : True , 'label' : 'Port' } , ssl = { 'type' : ARG_TYPE . BOOL , 'description' : 'Set it to True to enable ssl.' , 'required' : False , 'label' : 'ssl' } , ssl_ca = { 'type' : ARG_TYPE . PATH , 'description' : 'Path or URL of the Certificate Authority (CA) certificate file' , 'required' : False , 'label' : 'ssl_ca' } , ssl_cert = { 'type' : ARG_TYPE . PATH , 'description' : 'Path name or URL of the server public key certificate file' , 'required' : False , 'label' : 'ssl_cert' } , ssl_key = { 'type' : ARG_TYPE . PATH , 'description' : 'The path name or URL of the server private key file' , 'required' : False , 'label' : 'ssl_key' , } ) connection_args_example = OrderedDict ( host = '127.0.0.1' , port = 3306 , user = 'root' , password = 'password' , database = 'database' ) ​ Exporting All Required Variables The following should be exported in the __init__.py file of the handler: The Handler class. The version of the handler. The name of the handler. The type of the handler, either DATA handler or ML handler. The icon_path to the file with the database icon. The title of the handler or a short description. The description of the handler. The connection_args dictionary with the connection arguments. The connection_args_example dictionary with an example of the connection arguments. The import_error message that is used if the import of the Handler class fails. A few of these variables are defined in another file called __about__.py . This file is imported into the __init__.py file. Here is an example of the __init__.py file for the MySQL handler . from mindsdb . integrations . libs . const import HANDLER_TYPE from . __about__ import __version__ as version , __description__ as description from . connection_args import connection_args , connection_args_example try : from . mysql_handler import ( MySQLHandler as Handler , connection_args_example , connection_args ) import_error = None except Exception as e : Handler = None import_error = e title = 'MySQL' name = 'mysql' type = HANDLER_TYPE . DATA icon_path = 'icon.svg' __all__ = [ 'Handler' , 'version' , 'name' , 'type' , 'title' , 'description' , 'connection_args' , 'connection_args_example' , 'import_error' , 'icon_path' ] The __about__.py file for the same MySQL handler contains the following variables: __title__ = 'MindsDB MySQL handler' __package_name__ = 'mindsdb_mysql_handler' __version__ = '0.0.1' __description__ = \"MindsDB handler for MySQL\" __author__ = 'MindsDB Inc' __github__ = 'https://github.com/mindsdb/mindsdb' __pypi__ = 'https://pypi.org/project/mindsdb/' __license__ = 'MIT' __copyright__ = 'Copyright 2022- mindsdb' ​ Check out our Database Handlers! To see some integration handlers that are currently in use, we encourage you to check out the following handlers inside the MindsDB repository: MySQL Postgres And here are all the handlers available in the MindsDB repository . Was this page helpful? Yes No Suggest edits Raise issue Python Coding Standards Build an AI/ML Handler github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What are Database Handlers? Creating a Database Handler Core Methods Implementation Exporting the connection_args Dictionary Exporting All Required Variables Check out our Database Handlers!"}
{"file_name": "utilities.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "support.html", "content": "Supported Integrations - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Integrations Supported Integrations Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Integrations Supported Integrations MindsDB integrates with numerous data sources, AI/ML frameworks, and LLM providers. These integrations fall into two categories: Verified integrations : Officially supported and maintained by the MindsDB Team, these high-standard integrations meet the MindsDB verification process, ensuring full feature coverage and thorough testing. Community integrations : Developed and maintained by the MindsDB Community, these integrations offer valuable functionality and continue to receive community-driven improvements and support. The documentation for most of the integrations can be found in the following sections for Data Sources and AI Engines . ​ Verified Integrations Below is the list of all verified integrations. Integration Type Handler PostgreSQL DATA Link MySQL DATA Link ClickHouse DATA Link Microsoft SQL Server DATA Link Minds Endpoints AI Link Snowflake DATA Link Web DATA Link Redshift DATA Link OpenAI AI Link Anyscale AI Link Google BigQuery DATA Link ElasticSearch DATA Link Amazon S3 DATA Link ​ Community Integrations Below is the list of all community integrations. Integration Type Handler LangChain AI Link Slack DATA Link Ollama AI Link ChromaDB VECTOR STORE Link Milvus VECTOR STORE Link Pinecone VECTOR STORE Link Qdrant VECTOR STORE Link Weaviate VECTOR STORE Link PGVector VECTOR STORE Link Lightwood AI Link Hugging Face AI Link Llama Index AI Link Anthropic AI Link MariaDB DATA Link TimeGPT AI Link MongoDB DATA Link X (Twitter) DATA Link GitHub DATA Link Hugging Face Inference API AI Link Binance DATA Link BYOM AI Link Cassandra DATA Link Confluence DATA Link Gmail DATA Link Couchbase DATA Link StatsForecast AI Link Twelve Labs AI Link Anomaly Detection AI Link YouTube DATA Link Vertex AI Link Aerospike DATA Link Microsoft Access DATA Link Airtable DATA Link Altibase DATA Link Apache Doris DATA Link World Air Quality Index DATA Link Amazon Aurora DATA Link AutoGluon AI Link Autokeras AI Link AutoSkLearn AI Link Ckan DATA Link Clipdrop AI Link Google Cloud Spanner DATA Link Google Cloud SQL DATA Link CockroachDB DATA Link Cohere AI Link Coinbase DATA Link Crate DB DATA Link D0lt DATA Link Databend DATA Link Databricks DATA Link Datastax DATA Link IBM DB2 DATA Link Apache Derby DATA Link Discord DATA Link Docker Hub DATA Link DocumentDB DATA Link Dremio DATA Link Apache Druid DATA Link DuckDB DATA Link Amazon DynamoDB DATA Link EdgelessDB DATA Link Email DATA Link Empress DATA Link Eventbrite DATA Link EventStoreDB DATA Link FaunaDB DATA Link Firebird DATA Link FLAML AI Link Frappe DATA Link GitLab DATA Link Google Analytics DATA Link Google Books DATA Link Google Calendar DATA Link Google Content Shopping DATA Link Google Fit DATA Link Google Gemini AI Link Google Search DATA Link HackerNews DATA Link SAP HANA DATA Link Hive DATA Link HSQLDB DATA Link HubSpot DATA Link Apache Ignite DATA Link Apache Impala DATA Link InfluxDB DATA Link IBM Informix DATA Link Ingres DATA Link InStatus DATA Link Intercom DATA Link Jira DATA Link Kinetica DATA Link LanceDB DATA Link LangChain (embeddings) AI Link LeonardoAI AI Link LibSQL DATA Link Lightdash DATA Link LightFM AI Link Lindorm DATA Link LiteLLM AI Link Ludwig AI Link Luma DATA Link Materialize DATA Link MatrixOne DATA Link MaxDB DATA Link MediaWiki DATA Link Mendeley DATA Link Merlion AI Link MLflow AI Link MonetDB DATA Link MonkeyLearn AI Link Microsoft Teams DATA Link NeuralForecast AI Link NewsAPI DATA Link Notion DATA Link npm DATA Link NuoDB DATA Link Oceanbase DATA Link Oil Prices DATA Link OpenBB DATA Link OpenGauss DATA Link OpenStreetMap DATA Link Oracle DATA Link OrioleDB DATA Link PaLM AI Link PayPal DATA Link Phoenix DATA Link Pinot DATA Link Pirate Weather DATA Link Plaid DATA Link PlanetScale DATA Link Popularity Recommender AI Link Portkey AI Link PyCaret AI Link PyPI DATA Link QuestDB DATA Link Quickbooks DATA Link Ray Serve AI Link Reddit DATA Link Replicate AI Link Rocket Chat DATA Link Rockset DATA Link SAP ERP DATA Link Scylla DATA Link Sendinblue DATA Link Sentence Transformers AI Link Microsoft SharePoint DATA Link Google Sheets DATA Link Shopify DATA Link SingleStore DATA Link Solace DATA Link Solr DATA Link Spacy DATA Link SAP SQL Anywhere DATA Link SQLite DATA Link SqreamDB DATA Link StablityAI AI Link StarRocks DATA Link Strapi DATA Link Strava DATA Link Stripe DATA Link Supabase DATA Link SurrealDB DATA Link Symbl DATA Link TDEngine DATA Link Teradata DATA Link TiDB DATA Link TimescaleDB DATA Link TPOT AI Link Trino DATA Link Trip Advisor DATA Link Twilio DATA Link Vertica DATA Link Vitess DATA Link Webz DATA Link WhatsApp DATA Link Writer AI Link Xata DATA Link Yugabyte DATA Link ZipcodeBase DATA Link Was this page helpful? Yes No Suggest edits Raise issue Overview Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Verified Integrations Community Integrations"}
{"file_name": "sample-database.html", "content": "Sample Database - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Sample Database Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Data Sources Sample Database MindsDB provides a read-only PostgreSQL database pre-loaded with various datasets. These datasets are curated to cover a wide range of scenarios and use cases, allowing you to experiment with different features of MindsDB. Our publicly accessible PostgreSQL database is designed for testing and playground purposes. By using these datasets, you can quickly get started with MindsDB, understand how it works, and see how it can be applied to real-world problems. ​ Connection To connect to our read-only PostgreSQL database and access the example datasets, use the following connection parameters: CREATE DATABASE postgresql_conn WITH ENGINE = 'postgres' , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Below is the list of all avaiable datasets as tables. ​ Data Tables Here are the tables converted into Markdown format: ​ Fraud Detection Dataset This fraud_detection table contains data on mobile money transactions where each step represents an hour of simulation. step type amount nameOrig oldbalanceOrg newbalanceOrig nameDest oldbalanceDest newbalanceDest isFraud isFlaggedFraud 1 PAYMENT 9839.64 C1231006815 170136.0 160296.36 M1979787155 0.0 0.0 0 0 1 PAYMENT 1864.28 C1666544295 21249.0 19384.72 M2044282225 0.0 0.0 0 0 1 TRANSFER 181.0 C1305486145 181.0 0.0 C553264065 0.0 0.0 1 0 To learn more about this dataset, please visit this link . ​ Customer Churn Dataset This customer_churn table contains data of a U.S. bank customer churn. RowNumber CustomerId Surname CreditScore Geography Gender Age Tenure Balance NumOfProducts HasCrCard IsActiveMember EstimatedSalary Exited 1 15634602 Hargrave 619 France Female 42 2 0 1 1 1 101348.88 1 2 15647311 Hill 608 Spain Female 41 1 83807.86 1 0 1 112542.58 0 3 15619304 Onio 502 France Female 42 8 159660.8 3 1 0 113931.57 1 To learn more about this dataset, please visit this link . ​ Customer Support Chat Dataset This customer_support_chat table contains data of chat with customer support, which can be used to train Large Language Models (LLMs) for Fine Tuning and Domain Adaptation. flags instruction category intent response B question about cancelling order Order Number ORDER cancel_order I’ve understood you have a question regarding canceling order Order Number, and I’m here to provide you with the information you need. Please go ahead and ask your question, and I’ll do my best to assist you. BQZ i have a question about cancelling order Order Number ORDER cancel_order I’ve been informed that you have a question about canceling order Order Number. I’m here to assist you! Please go ahead and let me know what specific question you have, and I’ll provide you with all the information and guidance you need. Your satisfaction is my top priority. To learn more about this dataset, please visit this link . ​ Bank Customer Transactions Dataset This bank_customer_transactions table contains data of customer transactions with demographic and shopping behavior information. Customer ID Name Surname Gender Birthdate Transaction Amount Date Merchant Name Category 752858 Sean Rodriguez F 2002-10-20 35.47 2023-04-03 Smith-Russell Cosmetic 26381 Michelle Phelps 1985-10-24 2552.72 2023-07-17 Peck, Spence and Young Travel 305449 Jacob Williams M 1981-10-25 115.97 2023-09-20 Steele Inc Clothing To learn more about this dataset, please visit this link . ​ Telecom Customer Churn Dataset This telecom_customer_churn table contains data on customer activities, preferences, and behaviors. age gender security_no region_category membership_category joining_date joined_through_referral referral_id preferred_offer_types medium_of_operation internet_option last_visit_time days_since_last_login avg_time_spent avg_transaction_value avg_frequency_login_days points_in_wallet used_special_discount offer_application_preference past_complaint complaint_status feedback churn_risk_score 18 F XW0DQ7H Village Platinum Membership 17-08-2017 No xxxxxxxx Gift Vouchers/Coupons ? Wi-Fi 16:08:02 17 300.63 53005.25 17 781.75 Yes Yes No Not Applicable Products always in Stock 0 32 F 5K0N3X1 City Premium Membership 28-08-2017 ? CID21329 Gift Vouchers/Coupons Desktop Mobile_Data 12:38:13 16 306.34 12838.38 10 Yes No Yes Solved Quality Customer Care 0 44 F 1F2TCL3 Town No Membership 11-11-2016 Yes CID12313 Gift Vouchers/Coupons Desktop Wi-Fi 22:53:21 14 516.16 21027 22 500.69 No Yes Yes Solved in Follow-up Poor Website 1 To learn more about this dataset, please visit this link . ​ House Sales Dataset This house_sales table contains data on houses sold throughout the years. saledate ma type bedrooms created_at 2007-09-30 441854 house 2 2007-02-02 15:41:51.922127 2007-12-31 441854 house 2 2007-02-23 22:36:08.540248 2008-03-31 441854 house 2 2007-02-25 19:23:52.585358 To learn more about this dataset, please visit this link . Was this page helpful? Yes No Suggest edits Raise issue Overview Binance github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Data Tables Fraud Detection Dataset Customer Churn Dataset Customer Support Chat Dataset Bank Customer Transactions Dataset Telecom Customer Churn Dataset House Sales Dataset"}
{"file_name": "integrations.html", "content": "Integrations Overview - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Integrations Integrations Overview Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Integrations Integrations Overview MindsDB integrates with numerous data sources and popular AI/ML frameworks to seamlessly bring data and AI together. Data sources are all the data sources that you can conect to MindsDB, including traditional databases and data that is behind APIs. It is important that MindsDB does no ETL pipelines. When you query a data source MindsDB forwards this query in real-time to the original data source. MindsDB is very good at translating SQL to any other query dialect. AI/ML frameworks are all the possibilities you have for AI/ML modeling, from Generative AI to traditional ML and AutoML. You can create, train, and deploy AI/ML models within MindsDB ecosystem and provide it with data frmo connected data sources. Data sources AI/ML frameworks If you want to use a specific integration with MindsDB (either data or AI/ML integration), you need to ensure that the required dependencies are installed. You can verify it by running this command: SHOW HANDLERS WHERE name = 'integration_name' ; The output includes the IMPORT_SUCCESS column. If this column reads true , then you can go ahead and use this integration. But, if it reads false , then you need to install all required dependencies following this instruction . Was this page helpful? Yes No Suggest edits Raise issue Supported Integrations github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "replicate-llm.html", "content": "Replicate (LLM) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Replicate (LLM) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Replicate (LLM) This handler was implemented using the replicate library that is provided by Replicate. The required arguments to establish a connection are, model_name: Model name which you want to access in MindsDB. e.g ‘air-forever/kandinsky-2’ version: version hash/id which you want to use in MindsDB. api_key: API key from Replicate Platform you can found here . model_type: It should be set to ‘ LLM ’ while using Large language Model else it Optional Before you can use Replicate, it’s essential to authenticate by setting your API token in an environment variable named REPLICATE_API_TOKEN. This token acts as a key to enable access to Replicate’s features. Using pip: If you’re working in a standard Python environment (using pip for package management), set your token as an environment variable by running the following command in your terminal: On Linux, Mac: export REPLICATE_API_TOKEN='YOUR_TOKEN' On Windows: set REPLICATE_API_TOKEN=YOUR_TOKEN Using Docker: For Docker users, the process slightly differs. You need to pass the environment variable directly to the Docker container when running it. Use this command: docker run -e REPLICATE_API_TOKEN='YOUR_TOKEN' -p 47334:47334 -p 47335:47335 mindsdb/mindsdb Again, replace ‘YOUR_TOKEN’ with your actual Replicate API token. ​ Usage To use this handler and connect to a Replicate cluster in MindsDB, you need an account on Replicate. Make sure to create an account by following this link . To establish the connection and create a model in MindsDB, use the following syntax: CREATE MODEL vicuna_13b PREDICT output USING engine = 'replicate' , model_name = 'replicate/vicuna_13b' , model_type = 'LLM' , version = '6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b' , api_key = 'r8_HEH............' ; You can use the DESCRIBE PREDICTOR query to see the available parameters that you can specify to customize your predictions: DESCRIBE PREDICTOR mindsdb . vicuna_13b . features ; ​ OUTPUT + --------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------+ | inputs | type | default | description | + --------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------+ | seed | integer | - 1 | Seed for random number generator , for reproducibility | | debug | boolean | False | provide debugging output in logs | | top_p | number | 1 | When decoding text , samples from the top p percentage of most likely tokens ; lower to ignore less likely tokens | | prompt | string | - | Prompt to send to Llama . | | max_length | integer | 500 | Maximum number of tokens to generate . A word is generally 2 - 3 tokens | | temperature | number | 0.75 | Adjusts randomness of outputs , greater than 1 is random and 0 is deterministic , 0.75 is a good starting value . | | repetition_penalty | number | 1 | Penalty for repeated words in generated text ; 1 is no penalty , values greater than 1 discourage repetition , less than 1 encourage it . | + --------------------+---------+---------+---------------------------------------------------------------------------------------------------------------------------------------+ Now, you can use the established connection to query your ML Model as follows: SELECT * FROM vicuna_13b WHERE prompt = 'Write a humourous poem on Open Source' USING max_length = 200 , temperature = 0.75 ; ​ OUTPUT + --------------------------------------------------------------+----------------------------------------+ | output | prompt | + --------------------------------------------------------------+----------------------------------------+ | Opensource software , oh how we love thee | Write a humourous poem on Open Source | | With bugs and glitches , oh so free | | | You bring us laughter and joy each day | | | And we'll never have to pay | | | | | | The license is open , the code is there | | | For all to see and share in cheer | | | You bring us together , from far and wide | | | To work on projects , side by side | | | | | | With open source , there's no end | | | To the code we can bend | | | We can change it , mold it , make it our own | | | And create something truly great , or really strange | | | | | | So here's to open source , the future is bright | | | With code that's free , and with all of our might | | | We\"ll make the future shine , with technology fine | | | And open source will always be our shining line | | + --------------------------------------------------------------+----------------------------------------+ Note: Replicate provides only a few free predictions, so choose your predictions wisely. Don’t let the machines have all the fun, save some for yourself! 😉 Was this page helpful? Yes No Suggest edits Raise issue Portkey Vertex AI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Usage OUTPUT OUTPUT"}
{"file_name": "minds_endpoint.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "portkey.html", "content": "Portkey - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Portkey Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Portkey This documentation describes the integration of MindsDB with Portkey , an AI Gateway that allows developers to connect to All the AI models in the world with a single API. Portkey also brings in observability, caching, and other features that are useful for building production-grade AI applications. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Portkey within MindsDB, install the required dependencies following this instruction . Obtain the Portkey API key required to deploy and use Portkey within MindsDB. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the Portkey handler . You can pass all the parameters that are supported by Portkey inside the USING clause. CREATE ML_ENGINE portkey_engine FROM portkey USING portkey_api_key = '{PORTKEY_API_KEY}' , -- get this from Portkey dashboard (https://app.portkey.ai/api-keys) config = '{PORTKEY_CONFIG_ID}' ; -- get this from Portkey dashboard (https://app.portkey.ai/configs) Create a model using portkey_engine as an engine. You can pass all the parameters supported by Portkey Chat completions here inside the USING clause. refer Portkey Chat completions for more details. CREATE MODEL portkey_model PREDICT answer USING engine = 'portkey_engine' , model = 'gpt-3.5-turbo' , temperature = 0.2 ; The integrations between Portkey and MindsDB was implemented using Portkey Python SDK . Query the model to get predictions. SELECT question , answer FROM portkey_model WHERE question = 'Where is Stockholm located?' ; Here is the output: + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | question | answer | + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | Where is Stockholm located? | Stockholm is the capital and largest city of Sweden . It is located on Sweden's south - central east coast , where Lake Mälaren meets the Baltic Sea . | + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ ​ Usage The following usage examples utilize portkey_engine and gpt-3.5-turbo to create a model with the CREATE MODEL statement. ​ Creating a Summarization Model This example demonstrates how to create a model to summarize text using Portkey: CREATE MODEL summarization_model PREDICT summary USING engine = 'portkey_engine' , model = 'gpt-3.5-turbo' , temperature = 0.5 , max_tokens = 100 ; SELECT document , summary FROM summarization_model WHERE document = 'MindsDB is a predictive platform that connects machine learning models with databases.' ; ​ Generating Sentiment Analysis CREATE MODEL sentiment_model PREDICT sentiment USING engine = 'portkey_engine' , model = 'gpt-3.5-turbo' , temperature = 0.3 ; SELECT review , sentiment FROM sentiment_model WHERE review = 'The product was excellent and exceeded expectations!' ; ​ Translating Text CREATE MODEL translation_model PREDICT translation USING engine = 'portkey_engine' , model = 'gpt-3.5-turbo' , temperature = 0.4 ; SELECT original_text , translation FROM translation_model WHERE original_text = 'Hello, how are you?' AND target_language = 'es' ; ​ Extracting Key Information CREATE MODEL extraction_model PREDICT extracted_data USING engine = 'portkey_engine' , model = 'gpt-3.5-turbo' , temperature = 0.6 ; SELECT text , extracted_data FROM extraction_model WHERE text = 'Minds , 35, lives in New York and works as a software engineer.' ; Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue OpenAI Replicate (LLM) github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage Creating a Summarization Model Generating Sentiment Analysis Translating Text Extracting Key Information"}
{"file_name": "llamaindex.html", "content": "LlamaIndex - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models LlamaIndex Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models LlamaIndex ​ LlamaIndex Handler This documentation describes the integration of MindsDB with LlamaIndex , a framework for building context-augmented generative AI applications with LLMs. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use LlamaIndex within MindsDB, install the required dependencies following this instruction . Obtain the OpenAI API key required to OpenAI LLMs. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the Llamaindex handler . CREATE ML_ENGINE llama_index FROM llama_index USING openai_api_key = 'api-key-value' ; Create a model using llama_index as an engine and OpenAI as a model provider. CREATE MODEL chatbot_model PREDICT answer USING engine = 'llama_index' , -- engine name as created via CREATE ML_ENGINE input_column = 'question' , mode = 'conversational' , -- optional user_column = 'question' , -- optional: used only for conversational mode assistant_column = 'answer' ; -- optional: used only for conversational mode ​ Usage Here is how to create a model that answers questions by reading a page from the web: CREATE MODEL qa_model PREDICT answer USING engine = 'llama_index' , reader = 'SimpleWebPageReader' , source_url_link = 'https://mindsdb.com/about' , input_column = 'question' ; Query the model to get answer: SELECT question , answer FROM mindsdb . qa_model WHERE question = \"What is MindsDB's story?\" Here is the output: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | What is MindsDB's story? | MindsDB is a fast - growing open - source . . . | + ---------------------------+-------------------------------+ ​ Configuring SimpleWebPageReader for Specific Domains When SimpleWebPageReader is used it can be configured to interact only with specific domains by using the web_crawling_allowed_sites setting in the config.json file. This feature allows you to restrict the handler to read and process content only from the domains you specify, enhancing security and control over web interactions. To configure this, simply list the allowed domains under the web_crawling_allowed_sites key in config.json . For example: \"web_crawling_allowed_sites\" : [ \"https://docs.mindsdb.com\" , \"https://another-allowed-site.com\" ] Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue LangChain MonkeyLearn github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page LlamaIndex Handler Prerequisites Setup Usage Configuring SimpleWebPageReader for Specific Domains"}
{"file_name": "statsforecast.html", "content": "Nixtla's StatsForecast Integration with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Time Series Models Nixtla's StatsForecast Integration with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models NeuralForecast StatsForecast TimeGPT Recommender Models Multi-Media Models Time Series Models Nixtla's StatsForecast Integration with MindsDB Nixtla’s StatsForecast integration offers univariate time series forecasting models. StatsForecast uses classical methods such as ARIMA, rather than deep learning. Models train very quickly and generalize well, so are unlikely to overfit. Models also perform well on short time series, where deep learning models may be more likely to overfit. You can learn more about its features here . ​ How to bring StatsForecast Models to MindsDB Before creating a model, you will need to create an ML engine for StatsForecast using the CREATE ML_ENGINE statement: CREATE ML_ENGINE statsforecast FROM statsforecast ; Once the ML engine is created, we use the CREATE MODEL statement to create the StatsForecast model in MindsDB. CREATE MODEL model_name FROM data_source ( SELECT * FROM table_name ) PREDICT column_to_be_predicted GROUP BY column_name , column_name , . . . ORDER BY date_column WINDOW 12 -- model looks back at sets of 12 rows each HORIZON 3 -- model forecasts the next 3 rows USING engine = 'statsforecast' , model_name = 'model' , frequency = 'X' , season_length = 1 , hierarchy = [ 'column' ] ; The following parameters can be used while creating the StatsForecast model: model_name is an optional parameter that lets users specify one of the models from this list , which otherwise is chosen automatically. frequency is an optional parameter that defines the frequency of data such as daily, weekly, monthly, etc. Available values include “H”, “M”, “MS”, “Q”, “SM”, “BM”, “BMS”, “BQ”, “BH”. season_length is an optional parameter that defines the length of the season depending on frequency. For instance, season_length defaults to 12 if frequency is set to M (months). hierarchy is an optional parameter that may improve prediction accuracy when the data has a hierarchical structure. See more here . To ensure that the model is created based on the StatsForecast engine, include the USING clause at the end. ​ Example Let’s go through an example of how to use Nixtla’s StatsForecast with MindsDB to forecast monthly expenditures. Please note that before using the StatsForecast engine, you should create it from the MindsDB editor, or other clients through which you interact with MindsDB, with the below command: CREATE ML_ENGINE statsforecast FROM statsforecast ; You can check the available engines with this command: SHOW ML_ENGINES ; If you see the StatsForecast engine on the list, you are ready to follow the tutorials. ​ Tutorial using SQL In this tutorial, we create a model to predict expenditures based on historical data using the StatsForecast engine. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . historical_expenditures LIMIT 3 ; Here is the output: + ------------+----------+-------------+ | month | category | expenditure | + ------------+----------+-------------+ | 1982 - 04 - 01 | food | 1162.6 | | 1982 - 05 - 01 | food | 1150.9 | | 1982 - 06 - 01 | food | 1160 | + ------------+----------+-------------+ The historical_expenditures table stores monthly expenditure data for various categories, such as food , clothing , industry , and more. Let’s create a model table to predict the expenditures: CREATE MODEL quarterly_expenditure_forecaster FROM mysql_demo_db ( SELECT * FROM historical_expenditures ) PREDICT expenditure GROUP BY category ORDER BY month HORIZON 3 USING ENGINE = 'statsforecast' ; Please visit our docs on the CREATE MODEL statement to learn more. Please note that the WINDOW clause is not required because StatsForecast automatically calculates the best window as part of hyperparameter tuning. The ENGINE parameter in the USING clause specifies the ML engine used to make predictions. We can check the training status with the following query: DESCRIBE quarterly_expenditure_forecaster ; One of the pros of using the StatsForecast engine is that it is fast - it doesn’t take long until the model completes the training process. Once the model status is complete , the behavior is the same as with any other AI table – you can query for batch predictions by joining it with a data table: SELECT m . month as month , m . expenditure as forecasted FROM mindsdb . quarterly_expenditure_forecaster as m JOIN mysql_demo_db . historical_expenditures as t WHERE t . month > LATEST AND t . category = 'food' ; Here is the output data: + ----------------------------+-----------------+ | month | forecasted | + ----------------------------+-----------------+ | 2017 - 10 - 01 00 : 00 : 00.000000 | 10256.251953125 | | 2017 - 11 - 01 00 : 00 : 00.000000 | 10182.58984375 | | 2017 - 12 - 01 00 : 00 : 00.000000 | 10316.259765625 | + ----------------------------+-----------------+ The historical_expenditures table is used to make batch predictions. Upon joining the quarterly_expenditure_forecaster model with the historical_expenditures table, we get predictions for the next quarter as defined by the HORIZON 3 clause. Please note that the output month column contains both the date and timestamp. This format is used by default, as the timestamp is required when dealing with the hourly frequency of data. MindsDB provides the LATEST keyword that marks the latest training data point. In the WHERE clause, we specify the month > LATEST condition to ensure the predictions are made for data after the latest training data point. Let’s consider our quarterly_expenditure_forecaster model. We train the model using data until the third quarter of 2017, and the predictions come for the fourth quarter of 2017 (as defined by HORIZON 3 ). ​ Tutorial using MQL In this tutorial, we create a model to predict expenditures based on historical data using the StatsForecast engine. Before we start, visit our docs to learn how to connect Mongo Compass and Mongo Shell to MindsDB. We use a collection from our Mongo public demo database, so let’s start by connecting MindsDB to it from Mongo Compass or Mongo Shell: > use mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example. > use mongo_demo_db > db.historical_expenditures.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63fd2388bee7187f230f56fc' , month: '1982-04-01' , category: 'food' , expenditure: '1162.6' } { _id: '63fd2388bee7187f230f56fd' , month: '1982-05-01' , category: 'food' , expenditure: '1150.9' } { _id: '63fd2388bee7187f230f56fe' , month: '1982-06-01' , category: 'food' , expenditure: '1160' } The historical_expenditures collection stores monthly expenditure data for various categories, such as food , clothing , industry , and more. Let’s create a model to predict the expenditures: > use mindsdb > db.predictors.insertOne ( { name: 'quarterly_expenditure_forecaster' , predict: 'expenditure' , connection: 'mongo_demo_db' , select_data_query: 'db.historical_expenditures.find({})' , training_options: { timeseries_settings: { order_by: [ 'month' ] , group_by: [ 'category' ] , horizon: 3 } , engine: 'statsforecast' } } ) Please visit our docs on the insertOne statement to learn more. Please note that the window clause is not required because StatsForecast automatically calculates the best window as part of hyperparameter tuning. The engine parameter in the training_options clause specifies the ML engine used to make predictions. We can check the training status with the following query: > db.models.find ( { name: 'quarterly_expenditure_forecaster' } ) One of the pros of using the StatsForecast engine is that it is fast - it doesn’t take long until the model completes the training process. Once the model status is complete , the behavior is the same as with any other AI collection – you can query for batch predictions by joining it with a data collection: > db.quarterly_expenditure_forecaster.find ( { \"collection\" : \"mongo_pred_01.historical_expenditures\" , \"query\" : { \"category\" : \"food\" } } ) .limit ( 3 ) By default the forecasts are made for month > LATEST . Here is the output data: { _id: '63fd2388bee7187f230f58a5' , month: 2017 -10-01T00:00:00.000Z, category: 'food' , expenditure: 10256.251953125 } { _id: '63fd2388bee7187f230f58a4' , month: 2017 -11-01T00:00:00.000Z, category: 'food' , expenditure: 10182.58984375 } { _id: '63fd2388bee7187f230f58a3' , month: 2017 -12-01T00:00:00.000Z, category: 'food' , expenditure: 10316.259765625 } The historical_expenditures collection is used to make batch predictions. Upon joining the quarterly_expenditure_forecaster model with the historical_expenditures collection, we get predictions for the next quarter as defined by the horizon: 3 clause. Please note that the output month column contains both the date and timestamp. This format is used by default, as the timestamp is required when dealing with the hourly frequency of data. MindsDB provides the latest keyword that marks the latest training data point. In the where clause, we specify the month > latest condition to ensure the predictions are made for data after the latest training data point. Let’s consider our quarterly_expenditure_forecaster model. We train the model using data until the third quarter of 2017, and the predictions come for the fourth quarter of 2017 (as defined by horizon: 3 ). ​ StatsForecast + HierarchicalForecast The StatsForecast handler also supports hierarchical reconciliation via Nixtla’s HierarchicalForecast package . Hierarchical reconciliation may improve prediction accuracy when the data has a hierarchical structure. In this example, there may be a hierarchy as total expenditure is comprised of 7 different categories. SELECT DISTINCT category FROM mysql_demo_db . historical_expenditures ; Here are the available categories: + -------------------+ | category | + -------------------+ | food | | household_goods | | clothing | | department_stores | | other | | cafes | | industry | + -------------------+ Spending in each category may be related over time. For example, if spending on food rises in October 2017, it may be more likely that spending on cafes also rises in October 2017. Hierarchical reconciliation can account for this shared information. Here is how we can create a model: CREATE MODEL hierarchical_expenditure_forecaster FROM mysql_demo_db ( SELECT * FROM historical_expenditures ) PREDICT expenditure GROUP BY category ORDER BY month HORIZON 3 USING ENGINE = 'statsforecast' , HIERARCHY = [ ‘category’ ] ; The CREATE MODEL statement creates, trains, and deploys the model. Here, we predict the expenditure column values. As it is a time series model, we order the data by the month column. Additionally, we group data by the category column - the predictions are made for each group independently (here, for each category). The HORIZON clause defines for how many rows the predictions are made (here, for the next 3 rows). You can use the DESCRIBE [MODEL] command to check for details: DESCRIBE hierarchical_expenditure_forecaster . model ; On execution, we get: + ------------+-----------+---------------+--------------+ | model_name | frequency | season_length | hierarchy | + ------------+-----------+---------------+--------------+ | AutoARIMA | MS | 1 | [ \"category\" ] | + ------------+-----------+---------------+--------------+ Predictions with this model account for the hierarchical structure. The output may differ from the default model, which does not assume any hierarchy. Was this page helpful? Yes No Suggest edits Raise issue NeuralForecast TimeGPT github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to bring StatsForecast Models to MindsDB Example Tutorial using SQL Tutorial using MQL StatsForecast + HierarchicalForecast"}
{"file_name": "replicate-text2img.html", "content": "Replicate (Text2Img) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Image Models Replicate (Text2Img) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Image Models Clipdrop Replicate (Text2Img) Replicate (Img2Text) Video Models Image Models Replicate (Text2Img) This handler was implemented using the replicate library that is provided by Replicate. The required arguments to establish a connection are, model_name: Model name which you want to access in MindsDB. e.g ‘air-forever/kandinsky-2’ version: version hash/id which you want to use in MindsDB. api_key: API key from Replicate Platform you can found here . Before you can use Replicate, it’s essential to authenticate by setting your API token in an environment variable named REPLICATE_API_TOKEN. This token acts as a key to enable access to Replicate’s features. Using pip: If you’re working in a standard Python environment (using pip for package management), set your token as an environment variable by running the following command in your terminal: On Linux, Mac: export REPLICATE_API_TOKEN='YOUR_TOKEN' On Windows: set REPLICATE_API_TOKEN=YOUR_TOKEN Using Docker: For Docker users, the process slightly differs. You need to pass the environment variable directly to the Docker container when running it. Use this command: docker run -e REPLICATE_API_TOKEN='YOUR_TOKEN' -p 47334:47334 -p 47335:47335 mindsdb/mindsdb Again, replace ‘YOUR_TOKEN’ with your actual Replicate API token. ​ Usage To use this handler and connect to a Replicate cluster in MindsDB, you need an account on Replicate. Make sure to create an account by following this link . To establish the connection and create a model in MindsDB, use the following syntax: CREATE MODEL aiforever PREDICT url USING engine = 'replicate' , model_name = 'ai-forever/kandinsky-2' , version = '2af375da21c5b824a84e1c459f45b69a117ec8649c2aa974112d7cf1840fc0ce' , api_key = 'r8_BpO.........................' ; You can use the DESCRIBE PREDICTOR query to see the available parameters that you can specify to customize your predictions: DESCRIBE PREDICTOR mindsdb . aiforever . features ; ​ Output + ---------------------+-------------------+--------------------------------------------------------+---------+ | inputs | default | description | type | + ---------------------+-------------------+--------------------------------------------------------+---------+ | width | 512 | Choose width . Lower the setting if out of memory . | - | | height | 512 | Choose height . Lower the setting if out of memory . | - | | prompt | red cat , 4 k photo | Input Prompt | string | | scheduler | p_sampler | Choose a scheduler | - | | batch_size | 1 | Choose batch size . Lower the setting if out of memory . | - | | prior_steps | 5 | - | string | | guidance_scale | 4 | Scale for classifier - free guidance | number | | prior_cf_scale | 4 | - | integer | | num_inference_steps | 50 | Number of denoising steps | integer | + ---------------------+-------------------+--------------------------------------------------------+---------+ Now, you can use the established connection to query your ML Model as follows: SELECT * FROM aiforever WHERE prompt = 'Great warrior Arjun from Mahabharata, looking at camera,cinematic lighting, 4k quality' ; ​ Output IMPORTANT NOTE: PREDICTED URL will only work for 24 hours after prediction. Note: Replicate provides only a few free predictions, so choose your predictions wisely. Don’t let the machines have all the fun, save some for yourself! 😉 Was this page helpful? Yes No Suggest edits Raise issue Clipdrop Replicate (Img2Text) github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Usage Output Output"}
{"file_name": "link.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "google_gemini.html", "content": "Google Gemini - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Google Gemini Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Google Gemini This documentation describes the integration of MindsDB with Google Gemini , a generative artificial intelligence model developed by Google. The integration allows for the deployment of Google Gemini models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Google Gemini within MindsDB, install the required dependencies following this instruction . Obtain the Google Gemini API key required to deploy and use Google Gemini models within MindsDB. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the Google Gemini handler . CREATE ML_ENGINE google_gemini_engine FROM google_gemini USING api_key = 'api-key-value' ; Create a model using google_gemini_engine as an engine. CREATE MODEL google_gemini_model PREDICT target_column USING engine = 'google_gemini_engine' , -- engine name as created via CREATE ML_ENGINE column = 'input_column' , -- column name that stores user input model = 'gemini-pro' ; -- model name to be used ​ Usage The following usage examples utilize google_gemini_engine to create a model with the CREATE MODEL statement. Create a model to generate text completions with the Gemini Pro model for your existing text data. CREATE MODEL google_gemini_model PREDICT answer USING engine = 'google_gemini_engine' , column = 'question' , model = 'gemini-pro' ; Query the model to get predictions. SELECT question , answer FROM google_gemini_model WHERE question = 'How are you?' ; Alternatively, you can query for batch predictions: SELECT t . question , m . answer FROM google_gemini_model AS m JOIN data_table AS t ; Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue Cohere Hugging Face github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "ollama.html", "content": "Ollama - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Ollama Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Ollama This documentation describes the integration of MindsDB with Ollama , a tool that enables local deployment of large language models. The integration allows for the deployment of Ollama models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Ollama within MindsDB, install the required dependencies following this instruction . Follow this instruction to download Ollama and run models locally. Here are the recommended system specifications: A working Ollama installation, as in point 3. For 7B models, at least 8GB RAM is recommended. For 13B models, at least 16GB RAM is recommended. For 70B models, at least 64GB RAM is recommended. ​ Setup Create an AI engine from the Ollama handler . CREATE ML_ENGINE ollama_engine FROM ollama ; Create a model using ollama_engine as an engine. CREATE MODEL ollama_model PREDICT completion USING engine = 'ollama_engine' , -- engine name as created via CREATE ML_ENGINE model_name = 'model-name' , -- model run with 'ollama run model-name' ollama_serve_url = 'http://localhost:11434' ; If you run Ollama and MindsDB in separate Docker containers, use the localhost value of the container. For example, ollama_serve_url = 'http://host.docker.internal:11434' . You can find available models here . ​ Usage The following usage examples utilize ollama_engine to create a model with the CREATE MODEL statement. Deploy and use the llama3 model. First, download Ollama and run the model locally by executing ollama pull llama3 . Now deploy this model within MindsDB. CREATE MODEL llama3_model PREDICT completion USING engine = 'ollama_engine' , model_name = 'llama3' ; Models can be run in either the ‘generate’ or ‘embedding’ modes. The ‘generate’ mode is used for text generation, while the ‘embedding’ mode is used to generate embeddings for text. However, these modes can only be used with models that support them. For example, the moondream model supports both modes. By default, if the mode is not specified, the model will run in ‘generate’ mode if multiple modes are supported. If only one mode is supported, the model will run in that mode. To specify the mode, use the mode parameter in the CREATE MODEL statement. For example, mode = 'embedding' . Query the model to get predictions. SELECT text , completion FROM llama3_model WHERE text = 'Hello' ; Here is the output: + -------+--------------------------------------------------------------------------------------+ | text | completion | + -------+--------------------------------------------------------------------------------------+ | Hello | Hello back to you ! Is there something I can help you with or would you like to chat? | + -------+--------------------------------------------------------------------------------------+ You can override the prompt message as below: SELECT text , completion FROM llama3_model WHERE text = 'Hello' USING prompt_template = 'Answer using exactly five words: {{text}}:' ; Here is the output: + -------+------------------------------+ | text | completion | + -------+------------------------------+ | Hello | Warmly welcome to our space . | + -------+------------------------------+ Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue MonkeyLearn OpenAI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "huggingface.html", "content": "Hugging Face - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Hugging Face Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Hugging Face This documentation describes the integration of MindsDB with Hugging Face , a company that develops computer tools for building applications using machine learning. The integration allows for the deployment of Hugging Face models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Hugging Face within MindsDB, install the required dependencies following this instruction . ​ Setup Create an AI engine from the Hugging Face handler . CREATE ML_ENGINE huggingface_engine FROM huggingface USING huggingface_api_api_key = 'hf_xxx' ; Create a model using huggingface_engine as an engine. CREATE MODEL huggingface_model PREDICT target_column USING engine = 'huggingface_engine' , -- engine name as created via CREATE ML_ENGINE model_name = 'hf_hub_model_name' , -- choose one of PyTorch models from the Hugging Face Hub task = 'task_name' , -- choose one of 'text-classification', 'text-generation', 'zero-shot-classification', 'translation', 'summarization', 'text2text-generation', 'fill-mask' input_column = 'column_name' , -- column that stores input/question to the model labels = [ 'label 1' , 'label 2' ] ; -- labels used to classify data (used for classification tasks) ​ Usage The following usage examples utilize huggingface_engine to create a model with the CREATE MODEL statement. Create a model to classify input text as spam or ham. CREATE MODEL spam_classifier PREDICT spam_or_ham USING engine = 'huggingface_engine' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , task = 'text-classification' , input_column = 'text' , labels = [ 'ham' , 'spam' ] ; Query the model to get predictions. SELECT text , spam_or_ham FROM spam_classifier WHERE text = 'Subscribe to this channel asap' ; Here is the output: + --------------------------------+-------------+ | text | spam_or_ham | + --------------------------------+-------------+ | Subscribe to this channel asap | spam | + --------------------------------+-------------+ Next Steps Follow this link to see more use case examples. Was this page helpful? Yes No Suggest edits Raise issue Google Gemini Hugging Face Inference API github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "cohere.html", "content": "Cohere - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Cohere Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Cohere This documentation describes the integration of MindsDB with Cohere , a technology company focused on artificial intelligence for the enterprise. The integration allows for the deployment of Cohere models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Cohere within MindsDB, install the required dependencies following this instruction . Obtain the Cohere API key required to deploy and use Cohere models within MindsDB. Sign up for a Cohere account and request an API key from the Cohere dashboard. Learn more here . ​ Setup Create an AI engine from the Cohere handler . CREATE ML_ENGINE cohere_engine FROM cohere USING cohere_api_key = 'your-cohere-api-key' ; Create a model using cohere_engine as an engine. CREATE MODEL cohere_model PREDICT target_column USING engine = 'cohere_engine' , -- engine name as created via CREATE ML_ENGINE task = 'task_name' , -- choose one of 'text-summarization', 'text-generation' column = 'column_name' ; -- column that stores input/question to the model ​ Usage The following usage examples utilize cohere_engine to create a model with the CREATE MODEL statement. Create a model to predict the answer to a question using the text-generation task. CREATE MODEL cohere_model PREDICT answer USING engine = 'cohere_engine' , task = 'text-generation' , column = 'question' ; Where: Name Description task It defines the task to be accomplished. column It defines the column with the text to be acted upon. engine It defines the Cohere engine. Query the model to get predictions. SELECT answer FROM cohere_model WHERE question = 'What is the capital of France?' ; Here is the output: answer The capital of France is Paris. Paris is France’s largest city and a major global center for art, culture, fashion, and cuisine. It is renowned for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum. Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue Anyscale Endpoints Google Gemini github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "replicate-img2txt.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "pycaret.html", "content": "PyCaret - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AutoML PyCaret Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Lightwood PyCaret Time Series Models Recommender Models Multi-Media Models AutoML PyCaret PyCaret ML handler for MindsDB. ​ PyCaret PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. ​ Example Usage ​ Creation Required parameters: model_type : the type of model that you want to build model_name : you can pass in supported models using this. eg. supported models for regression can be found here . You can also set it to best to generate the best model (only supported for classification, regression and time_series) In addition to required parameters, there are 3 categories of optional parameters setup , create and predict . These are passed in during various stages of model development (see below). You have to prefix the arguments with one of these categories to pass in during the workflow. setup_* : these are passed to setup() function while creating model. You can find these in PyCaret’s documentation. eg. For regression, the setup function’s arguments are documented here . create_* : these are passed into create_model() or compare_models() function depending on the model_name . For classification you can find the docs here . predict_* : these are passed into predict_model() function of PyCaret. eg. You can find the documentation for classification here . These are the supported types of models ( model_type ): classification regression time_series clustering anomaly Below is the example for creating a classification model CREATE MODEL my_pycaret_class_model FROM irisdb ( SELECT SepalLengthCm , SepalWidthCm , PetalLengthCm , PetalWidthCm , Species FROM Iris ) PREDICT Species USING engine = 'pycaret' , model_type = 'classification' , model_name = 'xgboost' , setup_session_id = 123 ; For model types that don’t want a target column (like anomaly and clustering), just pass in any one of the column names in PREDICT clause to comply with MindsDB’s SQL syntax: CREATE MODEL my_pycaret_anom_model FROM anomalydb ( SELECT Col1 , Col2 , Col3 , Col4 , Col5 , Col6 , Col7 , Col8 , Col9 , Col10 FROM anomaly ) PREDICT Col10 USING engine = 'pycaret' , model_type = 'anomaly' , model_name = 'iforest' , setup_session_id = 123 ; ​ Prediction You can predict using normal mindsdb syntax like so: SELECT t . Id , m . prediction_label , m . prediction_score FROM irisdb . Iris as t JOIN my_pycaret_class_model AS m ; Was this page helpful? Yes No Suggest edits Raise issue Lightwood NeuralForecast github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page PyCaret Example Usage Creation Prediction"}
{"file_name": "lightfm.html", "content": "LightFM - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Recommender Models LightFM Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models LightFM Popularity Recommender Multi-Media Models Recommender Models LightFM The LightFM handler functions as an interface for the LightFM Python recommendation library. The current implementation supports collaborative filtering for user-item and item-item recommendations. It allows users to make use of the powerful LightFM recommendation framework library for performing recommendation on interaction data sets. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use LightFM within MindsDB, install the required dependencies following this instruction . Please note that, if you are using Docker to run MindsDB, before installing the dependencies for this integration as per the instructions given above, it is currently necessary to install a couple of Linux development packages in the container. To do this, run the following commands: Start an interactive shell in the container: docker exec -it mindsdb_container sh If you haven’t specified a name when spinning up the MindsDB container with docker run , you can find it by running docker ps . Install the required Linux development packages: apt-get -y update apt-get install -y build-essential python3-dev libopenblas-dev As the current implementation stands, the input data should be a table containing user-item interaction data: +---------+---------+--------+ | user_id | item_id | rating | +---------+---------+--------+ | 1 | 2 | 4 | | 1 | 3 | 7 | +---------+---------+--------+ Please note that at the moment this integration does not support the FINETUNE feature. ​ Example Before creating a LightFM model, we need to create an ML engine. CREATE ML_ENGINE lightfm FROM lightfm ; You can verify it by running SHOW ML_ENGINES . Now let’s create a LightFM model specifying the necessary input parameters. CREATE MODEL lightfm_demo FROM mysql_demo_db ( SELECT * FROM movie_lens_ratings ) PREDICT movieId USING engine = 'lightfm' , item_id = 'movieId' , user_id = 'userId' , threshold = 4 , n_recommendations = 10 , evaluation = true ; The required parameters include the following: The item_id parameter that stores items to be recommended; here, these are movies. The user_id parameter that stores users to whom items are recommended. The threshold parameter is used when score of interaction is provided in the input data. It defines the threshold for the recommendation. The n_recommendations parameter stores the number of recommendations to be returned. Optionally, you can provide the evaluation parameter if you want to store the evaluation metrics. It is set to false by default. Here is how to connect the mysql_demo_db used for training the model: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Let’s query for the following recommendations: Get recommendations for all item_item pairs: SELECT b . * FROM lightfm_demo AS b WHERE recommender_type = 'item_item' ; Get item-item recommendations for a specific item_id: SELECT b . * FROM lightfm_demo AS b WHERE movieId = 100 USING recommender_type = 'item_item' ; Get recommendations for all user-item pairs: SELECT b . * FROM lightfm_demo AS b where recommender_type = 'user_item' ; Get user-item recommendations for a specific user_id: SELECT b . * FROM lightfm_demo AS b WHERE userId = 100 USING recommender_type = 'user_item' ; Get user-item recommendations for multiple user_ids: SELECT b . * FROM mysql_demo_db . movie_lens_ratings AS a JOIN lightfm_demo AS b WHERE a . userId in ( 215 , 216 ) ; Was this page helpful? Yes No Suggest edits Raise issue TimeGPT Popularity Recommender github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Example"}
{"file_name": "mlflow.html", "content": "MindsDB and MLflow - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Bring Your Own Models MindsDB and MLflow Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models BYOM MLflow Ray Serve Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Bring Your Own Models MindsDB and MLflow MLflow allows you to create, train, and serve machine learning models, apart from other features, such as organizing experiments, tracking metrics, and more. ​ How to Use MLflow Models in MindsDB Here are the prerequisites for using MLflow-served models in MindsDB: Train a model via a wrapper class that inherits from the mlflow.pyfunc.PythonModel class. It should expose the predict() method that returns the predicted output for some input data when called. Please ensure that the Python version specified for Conda environment matches the one used to train the model. Start the MLflow server: mlflow server -p 5001 --backend-store-uri sqlite:////path/to/mlflow.db --default-artifact-root ./artifacts --host 0.0 .0.0 Serve the trained model: mlflow models serve --model-uri ./model_folder_name ​ Example Let’s create a model that registers an MLflow-served model as an AI Table: CREATE MODEL mindsdb . mlflow_model PREDICT target USING engine = 'mlflow' , model_name = 'model_folder_name' , -- replace the model_folder_name variable with a real value mlflow_server_url = 'http://0.0.0.0:5001/' , -- match the port number with the MLflow server (point 2 in the previous section) mlflow_server_path = 'sqlite:////path/to/mlflow.db' , -- replace the path with a real value (here we use the sqlite database) predict_url = 'http://localhost:5000/invocations' ; -- match the port number that serves the trained model (point 3 in the previous section) Here is how to check the models status: DESCRIBE mlflow_model ; Once the status is complete , we can query for predictions. One way is to query for a single prediction using synthetic data in the WHERE clause. SELECT target FROM mindsdb . mlflow_model WHERE text = 'The tsunami is coming, seek high ground' ; Another way is to query for batch predictions by joining the model with the data table. SELECT t . text , m . predict FROM mindsdb . mlflow_model AS m JOIN files . some_text as t ; Here, the data table comes from the files integration. It is joined with the model and predictions are made for all the records at once. Get More Insights Check out the article on How to bring your own machine learning model to databases by Patricio Cerda Mardini to learn more. Was this page helpful? Yes No Suggest edits Raise issue BYOM Ray Serve github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Use MLflow Models in MindsDB Example"}
{"file_name": "replicate-img2text.html", "content": "Replicate (Img2Text) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Image Models Replicate (Img2Text) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Image Models Clipdrop Replicate (Text2Img) Replicate (Img2Text) Video Models Image Models Replicate (Img2Text) This handler was implemented using the replicate library that is provided by Replicate. The required arguments to establish a connection are, model_name: Model name which you want to access in MindsDB. e.g ‘air-forever/kandinsky-2’ version: version hash/id which you want to use in MindsDB. api_key: API key from Replicate Platform you can found here . Before you can use Replicate, it’s essential to authenticate by setting your API token in an environment variable named REPLICATE_API_TOKEN. This token acts as a key to enable access to Replicate’s features. Using pip: If you’re working in a standard Python environment (using pip for package management), set your token as an environment variable by running the following command in your terminal: On Linux, Mac: export REPLICATE_API_TOKEN='YOUR_TOKEN' On Windows: set REPLICATE_API_TOKEN=YOUR_TOKEN Using Docker: For Docker users, the process slightly differs. You need to pass the environment variable directly to the Docker container when running it. Use this command: docker run -e REPLICATE_API_TOKEN='YOUR_TOKEN' -p 47334:47334 -p 47335:47335 mindsdb/mindsdb Again, replace ‘YOUR_TOKEN’ with your actual Replicate API token. ​ Usage To use this handler and connect to a Replicate cluster in MindsDB, you need an account on Replicate. Make sure to create an account by following this link . To establish the connection and create a model in MindsDB, use the following syntax: CREATE MODEL blip PREDICT text USING engine = 'replicate' , model_name = 'salesforce/blip' , version = '2e1dddc8621f72155f24cf2e0adbde548458d3cab9f00c0139eea840d0ac4746' , api_key = 'r8_BpO.........................' ; You can use the DESCRIBE PREDICTOR query to see the available parameters that you can specify to customize your predictions: DESCRIBE PREDICTOR mindsdb . blip . features ; ​ OUTPUT + ----------+------------------+-----------------------------------------------------------------------+--------+ | inputs | default | description | type | + ----------+------------------+-----------------------------------------------------------------------+--------+ | task | image_captioning | Choose a task . | - | | image | - | Input image | string | | caption | - | Type caption for the input image for image text matching task . | string | | question | - | Type question for the input image for visual question answering task . | string | + ----------+------------------+-----------------------------------------------------------------------+--------+ ​ Visual Question Answering Now, you can use the established connection to query your ML Model as follows: SELECT * FROM mindsdb . blip WHERE image = \"https://images.unsplash.com/photo-1686847266385-a32745169de4\" AND question = \"Is there lion in image?\" USING task = \"visual_question_answering\" ; ​ OUTPUT + ------------+--------------------------------------------------------------+-------------------------+ | text | image | question | + ------------+--------------------------------------------------------------+-------------------------+ | Answer: no | https: //images.unsplash.com/photo-1686847266385-a32745169de4 | Is there lion in image? | + ------------+--------------------------------------------------------------+-------------------------+ ​ Image Captioning SELECT * FROM mindsdb . blip WHERE image = \"https://images.unsplash.com/photo-1686847266385-a32745169de4\" ​ OUTPUT + ---------------------------------------------------+--------------------------------------------------------------+ | text | image | + ---------------------------------------------------+--------------------------------------------------------------+ | Caption: a bird is sitting on the back of a horse | https: //images.unsplash.com/photo-1686847266385-a32745169de4 | + ---------------------------------------------------+--------------------------------------------------------------+ Image Text Matching SELECT * FROM mindsdb . blip WHERE image = \"https://images.unsplash.com/photo-1686847266385-a32745169de4\" AND caption = \"Bird having horse Riding\" USING task = \"image_text_matching\" ; OUTPUT + -----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+--------------------------+ | text | image | caption | + -----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+--------------------------+ | The image and text is matched with a probability of 0.7730 . The image feature and text feature has a cosine similarity of 0.3615 . | https: //images.unsplash.com/photo-1686847266385-a32745169de4 | Bird having horse Riding | + -----------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------+--------------------------+ This is just an one model used in this example there are more with vast variation and use cases. Also there is no limit to imagination, how can you use this. Note: Replicate provides only a few free predictions, so choose your predictions wisely. Don’t let the machines have all the fun, save some for yourself! 😉 Was this page helpful? Yes No Suggest edits Raise issue Replicate (Text2Img) TwelveLabs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Usage OUTPUT Visual Question Answering OUTPUT Image Captioning OUTPUT"}
{"file_name": "ray-serve.html", "content": "MindsDB and Ray Serve - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Bring Your Own Models MindsDB and Ray Serve Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models BYOM MLflow Ray Serve Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Bring Your Own Models MindsDB and Ray Serve Ray Serve is a simple high-throughput model serving library that can wrap around your ML model. ​ Simple Example of Logistic Regression In this example, we train an external scikit-learn model to use for making predictions. ​ Creating the Ray Serve Model Let’s look at an actual model wrapped by a class that complies with the requirements. import ray from fastapi import Request , FastAPI from ray import serve import time import pandas as pd import json app = FastAPI ( ) async def parse_req ( request : Request ) : data = await request . json ( ) target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ] ) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/my_model\" ) @serve . ingress ( app ) class MyModel : @app . post ( \"/train\" ) async def train ( self , request : Request ) : df , target = await parse_req ( request ) feature_cols = list ( set ( list ( df . columns ) ) - set ( [ target ] ) ) self . feature_cols = feature_cols X = df . loc [ : , self . feature_cols ] Y = list ( df [ target ] ) self . model = LogisticRegression ( ) self . model . fit ( X , Y ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ) : df , _ = await parse_req ( request ) X = df . loc [ : , self . feature_cols ] predictions = self . model . predict ( X ) index = list ( range ( len ( predictions ) ) ) pred_dict = { 'prediction' : [ float ( x ) for x in predictions ] , 'index' : index } return pred_dict my_app = MyModel . bind ( ) After saving the above code into rayserve.py , run it using serve run rayserve:my_app . It is important to have the /train and /predict endpoints. The /train endpoint accepts two parameters to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. target is the name of the target column to be predicted. It returns a JSON object containing the status key and the ok value. The /predict endpoint requires one parameter to be sent via POST: df is a serialized dictionary that can be converted into a pandas dataframe. It returns a dictionary containing the prediction and index keys. It stores the predictions. Additional keys can be returned for confidence and confidence intervals. ​ Bringing the Ray Serve Model to MindsDB Once you start the RayServe-wrapped model, you can create and train it in MindsDB. CREATE MODEL mindsdb . byom_ray_serve FROM mydb ( SELECT number_of_rooms , initial_price , rental_price FROM test_data . home_rentals ) PREDICT number_of_rooms USING url . train = 'http://127.0.0.1:8000/my_model/train' , url . predict = 'http://127.0.0.1:8000/my_model/predict' , dtype_dict = { \"number_of_rooms\" : \"categorical\" , \"initial_price\" : \"integer\" , \"rental_price\" : \"integer\" } , format = 'ray_server' ; Now, you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. You can directly pass input data in the WHERE clause to get a single prediction. SELECT * FROM byom_ray_serve WHERE initial_price = 3000 AND rental_price = 3000 ; Or you can JOIN the model wth a data table to get bulk predictions. SELECT tb . number_of_rooms , t . rental_price , tb . index FROM mydb . test_data . home_rentals AS t JOIN mindsdb . byom_ray_serve AS tb WHERE t . rental_price > 5300 ; Limit for POST Requests Please note that if your model is behind a reverse proxy like nginx, you might have to increase the maximum limit for POST requests in order to receive the training data. MindsDB can send as much as you’d like - it has been stress-tested with over a billion rows. ​ Example of Keras NLP Model Here, we consider a natural language processing (NLP) task where we want to train a neural network using Keras to detect if a tweet is related to a natural disaster, such as fires, earthquakes, etc. Please download this dataset to follow the example. ​ Creating the Ray Serve Model We create a Ray Serve service that wraps around the Kaggle NLP Model that can be trained and used for making predictions. import re import time import json import string import requests from collections import Counter , defaultdict import ray from ray import serve import gensim import numpy as np import pandas as pd from tqdm import tqdm from nltk . util import ngrams from nltk . corpus import stopwords from nltk . tokenize import word_tokenize from fastapi import Request , FastAPI from sklearn . model_selection import train_test_split from sklearn . feature_extraction . text import CountVectorizer from tensorflow . keras . preprocessing . text import Tokenizer from tensorflow . keras . preprocessing . sequence import pad_sequences from tensorflow . keras . models import Sequential from tensorflow . keras . layers import Embedding , LSTM , Dense , SpatialDropout1D from tensorflow . keras . initializers import Constant from tensorflow . keras . optimizers import Adam app = FastAPI ( ) stop = set ( stopwords . words ( 'english' ) ) async def parse_req ( request : Request ) : data = await request . json ( ) target = data . get ( 'target' , None ) di = json . loads ( data [ 'df' ] ) df = pd . DataFrame ( di ) return df , target @serve . deployment ( route_prefix = \"/nlp_kaggle_model\" ) @serve . ingress ( app ) class Model : MAX_LEN = 100 GLOVE_DIM = 50 EPOCHS = 10 def __init__ ( self ) : self . model = None @app . post ( \"/train\" ) async def train ( self , request : Request ) : df , target = await parse_req ( request ) target_arr = df . pop ( target ) . values df = self . preprocess_df ( df ) train_corpus = self . create_corpus ( df ) self . embedding_dict = { } with open ( './glove.6B.50d.txt' , 'r' ) as f : for line in f : values = line . split ( ) word = values [ 0 ] vectors = np . asarray ( values [ 1 : ] , 'float32' ) self . embedding_dict [ word ] = vectors f . close ( ) self . tokenizer_obj = Tokenizer ( ) self . tokenizer_obj . fit_on_texts ( train_corpus ) sequences = self . tokenizer_obj . texts_to_sequences ( train_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [ : df . shape [ 0 ] ] word_index = self . tokenizer_obj . word_index num_words = len ( word_index ) + 1 embedding_matrix = np . zeros ( ( num_words , self . __class__ . GLOVE_DIM ) ) for word , i in tqdm ( word_index . items ( ) ) : if i > num_words : continue emb_vec = self . embedding_dict . get ( word ) if emb_vec is not None : embedding_matrix [ i ] = emb_vec self . model = Sequential ( ) embedding = Embedding ( num_words , self . __class__ . GLOVE_DIM , embeddings_initializer = Constant ( embedding_matrix ) , input_length = self . __class__ . MAX_LEN , trainable = False ) self . model . add ( embedding ) self . model . add ( SpatialDropout1D ( 0.2 ) ) self . model . add ( LSTM ( 64 , dropout = 0.2 , recurrent_dropout = 0.2 ) ) self . model . add ( Dense ( 1 , activation = 'sigmoid' ) ) optimizer = Adam ( learning_rate = 1e-5 ) self . model . compile ( loss = 'binary_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ] ) X_train , X_test , y_train , y_test = train_test_split ( df , target_arr , test_size = 0.15 ) self . model . fit ( X_train , y_train , batch_size = 4 , epochs = self . __class__ . EPOCHS , validation_data = ( X_test , y_test ) , verbose = 2 ) return { 'status' : 'ok' } @app . post ( \"/predict\" ) async def predict ( self , request : Request ) : df , _ = await parse_req ( request ) df = self . preprocess_df ( df ) test_corpus = self . create_corpus ( df ) sequences = self . tokenizer_obj . texts_to_sequences ( test_corpus ) tweet_pad = pad_sequences ( sequences , maxlen = self . __class__ . MAX_LEN , truncating = 'post' , padding = 'post' ) df = tweet_pad [ : df . shape [ 0 ] ] y_pre = self . model . predict ( df ) y_pre = np . round ( y_pre ) . astype ( int ) . flatten ( ) . tolist ( ) sub = pd . DataFrame ( { 'target' : y_pre } ) pred_dict = { 'prediction' : [ float ( x ) for x in sub [ 'target' ] . values ] } return pred_dict def preprocess_df ( self , df ) : df = df [ [ 'text' ] ] df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_URL ( x ) ) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_html ( x ) ) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_emoji ( x ) ) df [ 'text' ] = df [ 'text' ] . apply ( lambda x : self . remove_punct ( x ) ) return df def remove_URL ( self , text ) : url = re . compile ( r'https?://\\S+|www\\.\\S+' ) return url . sub ( r'' , text ) def remove_html ( self , text ) : html = re . compile ( r'<.*?>' ) return html . sub ( r'' , text ) def remove_punct ( self , text ) : table = str . maketrans ( '' , '' , string . punctuation ) return text . translate ( table ) def remove_emoji ( self , text ) : emoji_pattern = re . compile ( \"[\" u\"\\U0001F600-\\U0001F64F\" # emoticons u\"\\U0001F300-\\U0001F5FF\" # symbols & pictographs u\"\\U0001F680-\\U0001F6FF\" # transport & map symbols u\"\\U0001F1E0-\\U0001F1FF\" # flags (iOS) u\"\\U00002702-\\U000027B0\" u\"\\U000024C2-\\U0001F251\" \"]+\" , flags = re . UNICODE ) return emoji_pattern . sub ( r'' , text ) def create_corpus ( self , df ) : corpus = [ ] for tweet in tqdm ( df [ 'text' ] ) : words = [ word . lower ( ) for word in word_tokenize ( tweet ) if ( ( word . isalpha ( ) == 1 ) & ( word not in stop ) ) ] corpus . append ( words ) return corpus if __name__ == '__main__' : ray . init ( ) serve . start ( detached = True ) Model . deploy ( ) while True : time . sleep ( 1 ) Now, we need access to the training data. For that, we create a table called nlp_kaggle_train to load the dataset that the original model uses. The nlp_kaggle_train table contains the following columns: id INT , keyword VARCHAR ( 255 ) , location VARCHAR ( 255 ) , text VARCHAR ( 5000 ) , target INT Please note that the specifics of the schema/table and how to ingest the CSV data vary depending on your database. ​ Bringing the Ray Serve Model to MindsDB Now, we can create and train this custom model in MindsDB. CREATE MODEL mindsdb . byom_ray_serve_nlp FROM maria ( SELECT text , target FROM test . nlp_kaggle_train ) PREDICT target USING url . train = 'http://127.0.0.1:8000/nlp_kaggle_model/train' , url . predict = 'http://127.0.0.1:8000/nlp_kaggle_model/predict' , dtype_dict = { \"text\" : \"rich_text\" , \"target\" : \"integer\" } , format = 'ray_server' ; The training process takes some time, considering that this model is a neural network rather than a simple logistic regression. You can check the model status using this query: DESCRIBE byom_ray_serve_nlp ; Once the status of the predictor has a value of trained , you can fetch predictions using the standard MindsDB syntax. Follow the guide on the SELECT statement to learn more. SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'The tsunami is coming, seek high ground' ; The expected output of the query above is 1 . SELECT * FROM mindsdb . byom_ray_serve_nlp WHERE text = 'This is lovely dear friend' ; The expected output of the query above is 0 . Wrong Results? If your results do not match this example, try training the model for a longer amount of epochs. Get More Insights Check out the article on How to bring your own machine learning model to databases by Patricio Cerda Mardini to learn more. Was this page helpful? Yes No Suggest edits Raise issue MLflow Anomaly Detection github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Simple Example of Logistic Regression Creating the Ray Serve Model Bringing the Ray Serve Model to MindsDB Example of Keras NLP Model Creating the Ray Serve Model Bringing the Ray Serve Model to MindsDB"}
{"file_name": "clipdrop.html", "content": "Clipdrop - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Image Models Clipdrop Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Image Models Clipdrop Replicate (Text2Img) Replicate (Img2Text) Video Models Image Models Clipdrop Integrate state of the art image processing AI directly in your products. To use Clipdrop in MindsDB, you need to sign up for a Clipdrop account and obtain an API key. Learn more here . ​ Setup CREATE ML_ENGINE clipdrop_engine FROM clipdrop USING clipdrop_api_key = 'your_api_key' ; ​ Usage ​ Remove Text from Image Create a model to automatically remove text from an image. CREATE MODEL mindsdb . clipdrop_rt PREDICT image USING engine = \"clipdrop_engine\" , task = \"remove_text\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; You can then make predictions with the model by providing an image URL: SELECT * FROM mindsdb . clipdrop_rt WHERE image_url = \"https://onlinejpgtools.com/images/examples-onlinejpgtools/calm-body-of-water-with-quote.jpg\" ; ​ Remove Background from Image Effortlessly strip away the background from any image using this task. CREATE MODEL mindsdb . clipdrop_rb PREDICT image USING engine = \"clipdrop_engine\" , task = \"remove_background\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; Provide an image URL to remove the background: SELECT * FROM mindsdb . clipdrop_rb WHERE image_url = \"https://static.clipdrop.co/web/apis/remove-background/input.jpg\" ; ​ Generate Image from Sketch Turn simple sketches into detailed images with AI. CREATE MODEL mindsdb . clipdrop_s2i PREDICT image USING engine = \"clipdrop_engine\" , task = \"sketch_to_image\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; Provide an image URL and a description of the desired transformation: SELECT * FROM mindsdb . clipdrop_s2i WHERE image_url = 'https://img.freepik.com/free-vector/hand-drawn-cat-outline-illustration_23-2149266368.jpg' AND text = 'brown cat' ; ​ Generate Image from Text Create unique images directly from text prompts. CREATE MODEL mindsdb . clipdrop_t2i PREDICT image USING engine = \"clipdrop_engine\" , task = \"text_to_image\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; For example: SELECT * FROM mindsdb . clipdrop_t2i WHERE text = 'Imagine a software engineer' ; ​ Re-imagine the Image Re-imagine any image in a different artistic style or form. CREATE MODEL mindsdb . clipdrop_reimagine PREDICT image USING engine = \"clipdrop_engine\" , task = \"reimagine\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; Run the model to transform an existing image: SELECT * FROM mindsdb . clipdrop_reimagine WHERE image_url = \"https://static.clipdrop.co/web/apis/remove-background/input.jpg\" ; ​ Replace Background in Image Replace the background of any image with custom scenes or environments. CREATE MODEL mindsdb . clipdrop_rbi PREDICT image USING engine = \"clipdrop_engine\" , task = \"replace_background\" , local_directory_path = \"/Users/Sam/Downloads/test\" ; Example query: SELECT * FROM mindsdb . clipdrop_rbi WHERE image_url = \"https://static.clipdrop.co/web/apis/remove-background/input.jpg\" AND text = \"Empty road\" ; Note: The local_directory_path parameter specifies the path to the directory where the images are stored on your local machine. Was this page helpful? Yes No Suggest edits Raise issue Replicate (Audio) Replicate (Text2Img) github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Setup Usage Remove Text from Image Remove Background from Image Generate Image from Sketch Generate Image from Text Re-imagine the Image Replace Background in Image"}
{"file_name": "neuralforecast.html", "content": "Nixtla's NeuralForecast Integration with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Time Series Models Nixtla's NeuralForecast Integration with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models NeuralForecast StatsForecast TimeGPT Recommender Models Multi-Media Models Time Series Models Nixtla's NeuralForecast Integration with MindsDB Nixtla’s NeuralForecast provides a diverse array of neural forecasting models, prioritizing their ease of use and resilience. These models encompass a spectrum of options, including traditional networks like MLP and RNNs, as well as cutting-edge innovations such as NBEATS, NHITS, TFT, and various other architectural approaches. You can learn more about its features here . ​ How to bring NeuralForecast Models to MindsDB Before creating a model, you will need to create an ML engine for NeuralForecast using the CREATE ML_ENGINE statement: CREATE ML_ENGINE neuralforecast FROM neuralforecast ; Once the ML engine is created, we use the CREATE MODEL statement to create the NeuralForecast model in MindsDB. CREATE MODEL model_name FROM data_source ( SELECT * FROM table_name ) PREDICT column_to_be_predicted GROUP BY column_name , column_name , . . . ORDER BY date_column WINDOW 12 -- model looks back at sets of 12 rows each HORIZON 3 -- model forecasts the next 3 rows USING engine = 'neuralforecast' frequency = 'Q' , train_time = 0.01 , exogenous_vars = [ 'var_1' , 'var_2' ] ; To ensure that the model is created based on the NeuralForecast engine, include the USING clause at the end. The frequency parameter informs the model about the expected time difference between each measurement ( supported values here ). And the train_time parameter defines the training time - it defaults to 1, and lower values will reduce training time linearly by reducing the number of searches allowed for the best configuration by AutoNHITS. You can also define exogenous_vars as a parameter in the USING clause - these are complementary variables in the table that may improve forecast accuracy. ​ Example Let’s go through an example of how to use Nixtla’s NeuralForecast with MindsDB to forecast monthly expenditures based on historical data. Please note that before using the NeuralForecast engine, you should create it from the MindsDB editor, or other clients through which you interact with MindsDB, with the below command: CREATE ML_ENGINE neuralforecast FROM neuralforecast ; You can check the available engines with this command: SHOW ML_ENGINES ; If you see the NeuralForecast engine on the list, you are ready to follow the tutorials. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . historical_expenditures LIMIT 3 ; Here is the output: + ------------+----------+-------------+ | month | category | expenditure | + ------------+----------+-------------+ | 1982 - 04 - 01 | clothing | 359.9 | | 1982 - 05 - 01 | clothing | 386.6 | | 1982 - 06 - 01 | clothing | 350.5 | + ------------+----------+-------------+ The historical_expenditures table stores monthly expenditure data for various categories, such as food , clothing , industry , and more. Let’s create a model table to predict the expenditures: CREATE MODEL quarterly_expenditure_forecaster FROM mysql_demo_db ( SELECT * FROM historical_expenditures ) PREDICT expenditure GROUP BY category ORDER BY month WINDOW 12 HORIZON 3 USING ENGINE = 'neuralforecast' ; The CREATE MODEL statement creates, trains, and deploys the model. Here, we predict the expenditure column values. As it is a time series model, we order the data by the month column. Additionally, we group data by the category column - the predictions are made for each group independently (here, for each category). Next, we define the WINDOW and HORIZON clauses. The WINDOW clause specifies the number of rows we look back at (here, we look back at sets of 12 rows). And the HORIZON clause defines for how many rows the predictions are made (here, for the next 3 rows). Please visit our docs on the CREATE MODEL statement to learn more. The ENGINE parameter in the USING clause specifies the ML engine used to make predictions. We can check the training status with the following query: DESCRIBE quarterly_expenditure_forecaster ; Once the model status is complete , the behavior is the same as with any other AI table – you can query for batch predictions by joining it with a data table: SELECT m . month as month , m . expenditure as forecasted FROM mindsdb . quarterly_expenditure_forecaster as m JOIN mysql_demo_db . historical_expenditures as t WHERE t . month > LATEST AND t . category = 'clothing' ; Here is the output data: + ----------------------------+------------------+ | month | forecasted | + ----------------------------+------------------+ | 2017 - 10 - 01 00 : 00 : 00.000000 | 10802.2109375 | | 2017 - 11 - 01 00 : 00 : 00.000000 | 10749.2041015625 | | 2017 - 12 - 01 00 : 00 : 00.000000 | 12423.849609375 | + ----------------------------+------------------+ The historical_expenditures table is used to make batch predictions. Upon joining the quarterly_expenditure_forecaster model with the historical_expenditures table, we get predictions for the next quarter as defined by the HORIZON 3 clause. Please note that the output month column contains both the date and timestamp. This format is used by default, as the timestamp is required when dealing with the hourly frequency of data. MindsDB provides the LATEST keyword that marks the latest training data point. In the WHERE clause, we specify the month > LATEST condition to ensure the predictions are made for data after the latest training data point. Let’s consider our quarterly_expenditure_forecaster model. We train the model using data until the third quarter of 2017, and the predictions come for the fourth quarter of 2017 (as defined by HORIZON 3 ). ​ NeuralForecast + HierarchicalForecast The NeuralForecast handler also supports hierarchical reconciliation via Nixtla’s HierarchicalForecast package . Hierarchical reconciliation may improve prediction accuracy when the data has a hierarchical structure. In this example, there may be a hierarchy as total expenditure is comprised of 7 different categories. SELECT DISTINCT category FROM mysql_demo_db . historical_expenditures ; Here are the available categories: + -------------------+ | category | + -------------------+ | food | | household_goods | | clothing | | department_stores | | other | | cafes | | industry | + -------------------+ Spending in each category may be related over time. For example, if spending on food rises in October 2017, it may be more likely that spending on cafes also rises in October 2017. Hierarchical reconciliation can account for this shared information. Here is how we can create a model: CREATE MODEL hierarchical_expenditure_forecaster FROM mysql_demo_db ( SELECT * FROM historical_expenditures ) PREDICT expenditure GROUP BY category ORDER BY month HORIZON 3 USING ENGINE = 'neuralforecast' , HIERARCHY = [ ‘category’ ] ; Predictions with this model account for the hierarchical structure. The output may differ from the default model, which does not assume any hierarchy. Was this page helpful? Yes No Suggest edits Raise issue PyCaret StatsForecast github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to bring NeuralForecast Models to MindsDB Example NeuralForecast + HierarchicalForecast"}
{"file_name": "byom.html", "content": "Bring Your Own Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Bring Your Own Models Bring Your Own Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models BYOM MLflow Ray Serve Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Bring Your Own Models Bring Your Own Model The Bring Your Own Model (BYOM) feature lets you upload your own models in the form of Python code and use them within MindsDB. ​ How It Works You can upload your custom model via the MindsDB editor by clicking Add and Upload custom model , like this: Here is the form that needs to be filled out in order to bring your model to MindsDB: Let’s briefly go over the files that need to be uploaded: The Python file stores an implementation of your model. It should contain the class with the implementation for the train and predict methods. Here is the sample format: class CustomPredictor ( ) : ​ def train ( self , df , target_col , args = None ) : < implementation goes here > return '' def predict ( self , df ) : < implementation goes here > return df Example import os import pandas as pd ​ from sklearn . cross_decomposition import PLSRegression from sklearn import preprocessing ​ class CustomPredictor ( ) : ​ def train ( self , df , target_col , args = None ) : print ( args , '1111' ) ​ self . target_col = target_col y = df [ self . target_col ] x = df . drop ( columns = self . target_col ) x_cols = list ( x . columns ) ​ x_scaler = preprocessing . StandardScaler ( ) . fit ( x ) y_scaler = preprocessing . StandardScaler ( ) . fit ( y . values . reshape ( - 1 , 1 ) ) ​ xs = x_scaler . transform ( x ) ys = y_scaler . transform ( y . values . reshape ( - 1 , 1 ) ) ​ pls = PLSRegression ( n_components = 1 ) pls . fit ( xs , ys ) ​ T = pls . x_scores_ W = pls . x_weights_ P = pls . x_loadings_ R = pls . x_rotations_ ​ self . x_cols = x_cols self . x_scaler = x_scaler self . P = P ​ def calc_limit ( df ) : res = None for column in df . columns : if column == self . target_col : continue tbl = df . groupby ( self . target_col ) . agg ( { column : [ 'mean' , 'min' , 'max' , 'std' ] } ) tbl . columns = tbl . columns . get_level_values ( 1 ) tbl [ 'name' ] = column tbl [ 'std' ] = tbl [ 'std' ] . fillna ( 0 ) tbl [ 'lower' ] = tbl [ 'mean' ] - 3 * tbl [ 'std' ] tbl [ 'upper' ] = tbl [ 'mean' ] + 3 * tbl [ 'std' ] tbl [ 'lower' ] = tbl [ [ \"lower\" , \"min\" ] ] . max ( axis = 1 ) # lower >= min tbl [ 'upper' ] = tbl [ [ \"upper\" , \"max\" ] ] . min ( axis = 1 ) # upper <= max tbl = tbl [ [ 'name' , 'lower' , 'mean' , 'upper' ] ] try : res = pd . concat ( [ res , tbl ] ) except : res = tbl return res ​ trdf = pd . DataFrame ( ) trdf [ self . target_col ] = y . values trdf [ 'T1' ] = T . squeeze ( ) limit = calc_limit ( trdf ) . reset_index ( ) ​ self . limit = limit ​ return \"Trained predictor ready to be stored\" ​ def predict ( self , df ) : ​ yt = df [ self . target_col ] . values xt = df [ self . x_cols ] ​ xt = self . x_scaler . transform ( xt ) ​ excess_cols = list ( set ( df . columns ) - set ( self . x_cols ) ) ​ pred_df = df [ excess_cols ] . copy ( ) ​ pred_df [ self . target_col ] = yt pred_df [ 'T1' ] = ( xt @ self . P ) . squeeze ( ) ​ pred_df = pd . merge ( pred_df , self . limit [ [ self . target_col , 'lower' , 'upper' ] ] , how = 'left' , on = self . target_col ) ​ return pred_df The optional requirements file, or requirements.txt , stores all dependencies along with their versions. Here is the sample format: dependency_package_1 = = version dependency_package_2 >= version dependency_package_3 >= version , < version . . . Example pandas scikit - learn Once you upload the above files, please provide an engine name. Please note that your custom model is uploaded to MindsDB as an engine. Then you can use this engine to create a model. Let’s look at an example. ​ Example We upload the custom model, as below: Here we upload the model.py file that stores an implementation of the model and the requirements.txt file that stores all the dependencies. Once the model is uploaded, it becomes an ML engine within MindsDB. Now we use this custom_model_engine to create a model as follows: CREATE MODEL custom_model FROM my_integration ( SELECT * FROM my_table ) PREDICT target USING ENGINE = 'custom_model_engine' ; Let’s query for predictions by joining the custom model with the data table. SELECT input . feature_column , model_target_column FROM my_integration . my_table as input JOIN custom_model as model ; Check out the BYOM handler folder to see the implementation details. Was this page helpful? Yes No Suggest edits Raise issue Vertex AI MLflow github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How It Works Example"}
{"file_name": "monkeylearn.html", "content": "MonkeyLearn - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models MonkeyLearn Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models MonkeyLearn MonkeyLearn is a No-code text analysis tool. MindsDB allows you to use pre-built & custom MonkeyLearn models to use its features like classifying text according to user needs and fields of interest like business, reviews, comments, and customer feedback. ​ How to bring MonkeyLearn Models to MindsDB Before creating a model, you will need to create the ML_ENGINE for MonkeyLearn using the CREATE ML_ENGINE syntax CREATE ML_ENGINE monkeylearn_engine FROM monkeylearn USING monkeylearn_api_key = 'monkeylearn_api_key' ; Once the ML_ENGINE is created, we use the CREATE MODEL statement to bring MonkeyLearn models to MindsDB. For this example, you will make use of MonkeyLearn’s pre-made model E-commerce Support Ticket Classifier . CREATE MODEL mindsdb . ecommerce_ticket_classifier PREDICT tag USING engine = 'monkeylearn_engine' , monkeylearn_api_key = 'api_key' , model_id = 'model_id' , input_column = 'text' ; On execution, you get: Where: Expression Description ecommerce_ticket_classifier The model name provided to the model created in MindsDB. tag The column that will provide the predicted result. engine The ML framework engine used, which is MonkeyLearn. monkeylearn_api_key The API Key of the model provided by MonkeyLearn. model_id The respective model’s ID you want to make use of. input_column Specifies the input column fed to the model You can use the DESCRIBE syntax to verify the model’s status. DESCRIBE ecommerce_ticket_classifier ; On execution, you get: Use the SELECT statement to make a prediction on the model. SELECT * FROM ecommerce_ticket_classifier WHERE text = 'Where is my order? The delivery status shows shipped. When I call the delivery driver there is no response!' ; On execution, you get: ​ Create and train a model. You can also create a model with a dataset. For this example, we will be using a dataset consisting of messages for E-commerce support tickets. The dataset will be uploaded as a file onto the GUI. Use the CREATE MODEL syntax: CREATE MODEL mindsdb . ecommerce_ticket_classifier2 FROM files ( select * from queries2 ) PREDICT tag USING engine = 'monkeylearn_engine' , monkeylearn_api_key = 'api_key' , model_id = 'model_id' , input_column = 'text' ; Use the SELECT statement to make a prediction SELECT * FROM ecommerce_ticket_classifier2 WHERE text = 'I ordered 4 units but only received 3' ; On execution, you get: The MindsDB model created with the MonkeyLearn model successfully predicted the tag of an E-commerce support ticket according to the text input. Was this page helpful? Yes No Suggest edits Raise issue LlamaIndex Ollama github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to bring MonkeyLearn Models to MindsDB Create and train a model."}
{"file_name": "anomaly.html", "content": "Anomaly Detection Handler - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Anomaly Detection Anomaly Detection Handler Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Anomaly Detection Anomaly Detection Handler The Anomaly Detection handler implements supervised, semi-supervised, and unsupervised anomaly detection algorithms using the pyod, catboost, xgboost, and sklearn libraries. The models were chosen based on the results in the ADBench benchmark paper . Additional information If no labelled data, we use an unsupervised learner with the syntax CREATE ANOMALY DETECTION MODEL <model_name> without specifying the target to predict. MindsDB then adds a column called outlier when generating results. If we have labelled data, we use the regular model creation syntax. There is backend logic that chooses between a semi-supervised algorithm (currently XGBOD) vs. a supervised algorithm (currently CatBoost). If multiple models are provided, then we create an ensemble and use majority voting. See the anomaly detection proposal document for more information. Context about types of anomaly detection Supervised: we have inlier/outlier labels, so we can train a classifier the normal way. This is very similar to a standard classification problem. Semi-supervised: we have inlier/outlier labels and perform an unsupervised preprocessing step, and then a supervised classification algorithm. Unsupervised: we don’t have inlier/outlier labels and cannot assume all training data are inliers. These methods construct inlier criteria that will classify some training data as outliers too based on distributional traits. New observations are classified against these criteria. However, it’s not possible to evaluate how well the model detects outliers without labels. Default dispatch logic We propose the following logic to determine type of learning: Use supervised learning if labels are available and the dataset contains at least 3000 samples. Use semi-supervised learning if labels are available and number of samples in the dataset is less than 3000. If the dataset is unlabelled, use unsupervised learning. We’ve chosen 3000 based on the results of the NeurIPS AD Benchmark paper (linked above). The authors report that semi-supervised learning outperforms supervised learning when the number of samples used is less than 5% of the size of the training dataset. The average size of the training datasets in their study is 60,000, therefore this 5% corresponds to 3000 samples on average. Reasoning for default models on each type We refer to the NeurIPS AD Benchmark paper (linked above) to make these choices: For supervised learning, use CatBoost. It often outperforms classic algorithms. For semi-supervised, XGBod is a good default from PyOD. There’s no clear winner for unsupervised methods, it depends on the use case. ECOD is a sensible default with a fast runtime. If we’re not concerned about runtime, we can use an ensemble. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Anomaly Detection handler within MindsDB, install the required dependencies following this instruction . ​ Setup Create an AI engine from the Anomaly Detection handler . CREATE ML_ENGINE anomaly_detection_engine FROM anomaly_detection ; Create a model using anomaly_detection_engine as an engine. CREATE ANOMALY DETECTION MODEL anomaly_detection_model FROM datasource ( SELECT * FROM data_table ) PREDICT target_column USING engine = 'anomaly_detection_engine' , -- engine name as created via CREATE ML_ENGINE . . . ; -- other parameters shown in usage examples below ​ Usage To run example queries, use the data from this CSV file . ​ Unsupervised detection CREATE ANOMALY DETECTION MODEL mindsdb . unsupervised_ad FROM files ( SELECT * FROM anomaly_detection ) USING engine = 'anomaly_detection_engine' ; DESCRIBE MODEL mindsdb . unsupervised_ad . model ; SELECT t . class , m . outlier as anomaly FROM files . anomaly_detection as t JOIN mindsdb . unsupervised_ad as m ; ​ Semi-supervised detection CREATE MODEL mindsdb . semi_supervised_ad FROM files ( SELECT * FROM anomaly_detection ) PREDICT class USING engine = 'anomaly_detection_engine' ; DESCRIBE MODEL mindsdb . semi_supervised_ad . model ; SELECT t . carat , t . category , t . class , m . class as anomaly FROM files . anomaly_detection as t JOIN mindsdb . semi_supervised_ad as m ; ​ Supervised detection CREATE MODEL mindsdb . supervised_ad FROM files ( SELECT * FROM anomaly_detection ) PREDICT class USING engine = 'anomaly_detection_engine' , type = 'supervised' ; DESCRIBE MODEL mindsdb . supervised_ad . model ; SELECT t . carat , t . category , t . class , m . class as anomaly FROM files . anomaly_detection as t JOIN mindsdb . supervised_ad as m ; ​ Specific model CREATE ANOMALY DETECTION MODEL mindsdb . unsupervised_ad_knn FROM files ( SELECT * FROM anomaly_detection ) USING engine = 'anomaly_detection_engine' , model_name = 'knn' ; DESCRIBE MODEL mindsdb . unsupervised_ad_knn . model ; SELECT t . class , m . outlier as anomaly FROM files . anomaly_detection as t JOIN mindsdb . unsupervised_ad_knn as m ; ​ Specific anomaly type CREATE ANOMALY DETECTION MODEL mindsdb . unsupervised_ad_local FROM files ( SELECT * FROM anomaly_detection ) USING engine = 'anomaly_detection_engine' , anomaly_type = 'local' ; DESCRIBE MODEL mindsdb . unsupervised_ad_local . model ; SELECT t . class , m . outlier as anomaly FROM files . anomaly_detection as t JOIN mindsdb . unsupervised_ad_local as m ; ​ Ensemble CREATE ANOMALY DETECTION MODEL mindsdb . ad_ensemble FROM files ( SELECT * FROM anomaly_detection ) USING engine = 'anomaly_detection_engine' , ensemble_models = [ 'knn' , 'ecod' , 'lof' ] ; DESCRIBE MODEL mindsdb . ad_ensemble . model ; SELECT t . class , m . outlier as anomaly FROM files . anomaly_detection as t JOIN mindsdb . ad_ensemble as m ; Next Steps Watch demo 1 and demo 2 to see usage examples. Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue Ray Serve Lightwood github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage Unsupervised detection Semi-supervised detection Supervised detection Specific model Specific anomaly type Ensemble"}
{"file_name": "anyscale.html", "content": "Anyscale Endpoints - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Anyscale Endpoints Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Anyscale Endpoints This documentation describes the integration of MindsDB with Anyscale Endpoints , a fast and scalable API to integrate OSS LLMs into apps. The integration allows for the deployment of Anyscale Endpoints models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Anyscale Endpoints within MindsDB, install the required dependencies following this instruction . Obtain the Anyscale Endpoints API key required to deploy and use Anyscale Endpoints models within MindsDB. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the Anyscale Endpoints handler . CREATE ML_ENGINE anyscale_endpoints_engine FROM anyscale_endpoints USING anyscale_endpoints_api_key = 'api-key-value' ; Create a model using anyscale_endpoints_engine as an engine. CREATE MODEL anyscale_endpoints_model [ FROM integration ( SELECT * FROM table ) ] PREDICT target_column USING engine = 'anyscale_endpoints_engine' , -- engine name as created via CREATE ML_ENGINE api_base = 'base-url' , -- optional, replaces the default base URL mode = 'conversational' , -- optional, mode to run the model in model_name = 'anyscale_endpoints_model_name' , -- optional, the LLM to use prompt = 'You are a helpful assistant. Your task is to continue the chat.' , -- optional, system prompt for the model question_column = 'question' , -- optional, column name that stores user input context_column = 'context' , -- optional, column that stores context of the user input prompt_template = 'Answer the users input in a helpful way: {{question}}' , -- optional, base template with placeholders used to provide input to the model max_tokens = 100 , -- optional, token limit for model output temperature = 0.3 , -- optional, randomness setting for the model output json_struct = { 'key' : 'value' , . . . }' -- optional, the parameter for extracting JSON data from `prompt_template` It is possilbe to override certain parameters set for a model at prediction time instead of recreating the model. For example, to change the temperature parameter for a specific prediction, use the following query: SELECT question , answer FROM anyscale_endpoints_model WHERE question = 'Where is Stockholm located?' USING temperature = 0.9 prompt_template = 'Answer the users input as a pirate: {{question}}' ; The parameters that can be overridden as shown above are mentioned below in the detailed explanation. The following is a more detailed explanation of the parameters used in the CREATE MODEL statement: engine This is the engine name as created with the CREATE ML_ENGINE statement. api_base This parameter is optional. It replaces the default Anyscale’s base URL with the defined value. mode This parameter is optional. The available modes include default , conversational and conversational-full . The default mode is used by default. The model will generate a separate response for each input provided. No context is maintained between the inputs. The conversational mode will maintain context between the inputs and generate a single response. This response will be placed in the last row of the result set. The conversational-full mode will maintain context between the inputs and generate a response for each input. model_name This parameter is optional. By default, the meta-llama/Llama-2-7b-chat-hf model is used. question_column This parameter is optional. It contains the column name that stores user input. context_column This parameter is optional. It contains the column name that stores context for the user input. prompt_template This parameter is optional if you use question_column . It stores the message or instructions as a base template with placeholders to be filled in by the user input at prediction time. Please note that this parameter can be overridden at prediction time. prompt This parameter is optional. It defines the initial (system) prompt for the model. max_tokens This parameter is optional. It defines the maximum token cost of the prediction. Please note that this parameter can be overridden at prediction time. temperature This parameter is optional. It defines how risky the answers are. The value of 0 marks a well-defined answer, and the value of 0.9 marks a more creative answer. Please note that this parameter can be overridden at prediction time. json_struct This parameter is optional. It is used to extract JSON data from a text column provided in the prompt_template parameter. See examples here . The implementation of this integration is based on the engine for the OpenAI API, as Anyscale conforms to it. There are a few notable differences, though: All models supported by Anyscale Endpoints are open source. A full list can be found here for inference-only under section Supported models . Not every model is supported for fine-tuning. You can find a list here under section Fine Tuning - Supported models . Please check both lists regularly, as they are subject to change. If you try to fine-tune a model that is not supported, you will get a warning and subsequently an error from the Anyscale endpoint. This integration only offers chat-based text completion models, either for normal text or specialized for code. When providing a description, this integration returns the respective HuggingFace model card. Fine-tuning requires that your dataset complies with the chat format. That is, each row should contain a context and a role. The context is the text that is the message in the chat, and the role is who authored it (system, user, or assistant, where the last one is the model). For more information, please check the fine tuning guide in the Anyscale Endpoints docs . The base URL for this API is https://api.endpoints.anyscale.com/v1 . ​ Usage The following usage examples utilize anyscale_endpoints_engine to create a model with the CREATE MODEL statement. The output generated for a single input will be the same regardless of the mode used. The difference between the modes is in how the model handles multiple inputs. files.unrelated_questions is a simple CSV file containing a question column with simple (unrelated) questions that has been uploaded to MindsDB, while files.related_questions is a similar file containing related questions. files.unrelated_questions_with_context and files.related_questions_with_context are similar files containing an additional context column. These files are used in the examples given below to provide multiple inputs to the models created. It is possible to use any other supported data source in the same manner. Default mode In the default mode, the model will generate a separate response for each input provided. No context is maintained between the inputs. Prompt completion CREATE MODEL anyscale_endpoints_default_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_default_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . unrelated_questions AS d JOIN anyscale_endpoints_default_model AS m The response will look like the following: | question | answer | | -------- | ------ | | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | | What is the fourth planet in the solar system? | The fourth planet in the solar system is Mars . | Question answering CREATE MODEL anyscale_endpoints_default_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , question_column = 'question' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_default_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . unrelated_questions AS d JOIN anyscale_endpoints_default_model AS m The response will look like the following: | question | answer | | -------- | ------ | | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | | What is the fourth planet in the solar system? | The fourth planet in the solar system is Mars . | Question answering with context CREATE MODEL anyscale_endpoints_default_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , question_column = 'question' , context_column = 'context' ; To generate a response for a single input, the following query can be used: SELECT question , context , answer FROM anyscale_endpoints_default_model WHERE question = 'What is the main topic of the conference happening next week?' AND context = 'The conference, focusing on advancements in AI, will take place in San Francisco next week.' ; The response will look like the following: + ---------------------------+-------------------------------+-------------------------------+ | question | context | answer | + ---------------------------+-------------------------------+-------------------------------+ | What is the main topic of the conference happening next week? | The conference , focusing on advancements in AI , will take place in San Francisco next week . | The main topic of the conference happening next week in San Francisco is advancements in AI . | + ---------------------------+-------------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , d . context , m . answer FROM files . unrelated_questions_with_context AS d JOIN anyscale_endpoints_default_model AS m The response will look like the following: + ---------------------------+-------------------------------+-------------------------------+ | question | context | answer | + ---------------------------+-------------------------------+-------------------------------+ | What is the main topic of the conference happening next week? | The conference , focusing on advancements in AI , will take place in San Francisco next week . | The main topic of the conference happening next week in San Francisco is advancements in AI . | + ---------------------------+-------------------------------+-------------------------------+ | What caused the extension of the project deadline? | The project deadline was extended by two weeks due to team members falling sick unexpectedly . | The extension of the project deadline was caused by unexpected illnesses among team members . | + ---------------------------+-------------------------------+-------------------------------+ Conversational mode In the conversational mode, the model will maintain context between the inputs and generate a single response. This response will be placed in the last row of the result set. Prompt completion CREATE MODEL anyscale_endpoints_conversational_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt = 'You are a helpful assistant. Your task is to continue the chat.' , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_conversational_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . related_questions AS d JOIN anyscale_endpoints_conversational_model AS m The response will look like the following: question answer Where is Stockholm located? What are some fun activities to do there? Stockholm is the capital city of Sweden and is located in the southeastern part of the country. Some fun activities to do in Stockholm include visiting the famous Vasa Museum, exploring the beautiful archipelago, taking a stroll through the charming Gamla Stan neighborhood, and trying out some of the local food and drinks. Question answering CREATE MODEL anyscale_endpoints_conversational_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt = 'You are a helpful assistant. Your task is to continue the chat.' , question_column = 'question' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_conversational_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . unrelated_questions AS d JOIN anyscale_endpoints_conversational_model AS m The response will look like the following: question answer Where is Stockholm located? What are some fun activities to do there? Stockholm is the capital city of Sweden and is located in the southeastern part of the country. Some fun activities to do in Stockholm include visiting the famous Vasa Museum, exploring the beautiful archipelago, taking a stroll through the charming Gamla Stan neighborhood, and trying out some of the local food and drinks. Question answering with context CREATE MODEL anyscale_endpoints_conversational_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational' , model_name = 'mlabonne/NeuralHermes-2.5-Mistral-7B' , prompt = 'You are a helpful assistant. Your task is to continue the chat.' , question_column = 'question' , context_column = 'context' ; To generate a response for a single input, the following query can be used: SELECT question , context , answer FROM anyscale_endpoints_conversational_model WHERE question = 'What is the main topic of the conference happening next week?' AND context = 'The conference, focusing on advancements in AI, will take place in San Francisco next week.' ; The response will look like the following: + ---------------------------+-------------------------------+-------------------------------+ | question | context | answer | + ---------------------------+-------------------------------+-------------------------------+ | What is the main topic of the conference happening next week? | The main topic of the conference happening next week is advancements in Artificial Intelligence . | + ---------------------------+-------------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , d . context , m . answer FROM files . related_questions_with_context AS d JOIN anyscale_endpoints_conversational_model AS m The response will look like the following: question context answer Where is Anna planning a trip to next month? Anna is planning a trip to Kyoto next month. What does Anna plan on doing there? Anna plans on going sightseeing. Anna plans on going sightseeing during her trip to Kyoto next month. Conversational-full mode In the conversational-full mode, the model will maintain context between the inputs and generate a response for each input. Prompt completion CREATE MODEL anyscale_endpoints_conversational_full_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational-full' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_conversational_full_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . related_questions AS d JOIN anyscale_endpoints_conversational_full_model AS m The response will look like the following: question answer Where is Stockholm located? Stockholm is the capital city of Sweden, located in the southeastern part of the country. It is situated on an island in the Stockholm archipelago, which is made up of more than 30,000 islands. The city is known for its beautiful architecture, museums, and cultural attractions, as well as its vibrant food and nightlife scene. What are some fun activities to do there? Stockholm is the capital city of Sweden and is located in the southeastern part of the country, on the east coast of the Stockholm archipelago. Some fun activities to do in Stockholm include visiting the famous Vasa Museum, exploring the charming old town of Gamla Stan, taking a stroll through the beautiful parks and gardens, and trying out some of the local food and drinks. There are also many opportunities for shopping, cultural experiences, and outdoor activities such as hiking and biking Question answering CREATE MODEL anyscale_endpoints_conversational_full_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational-full' , model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , question_column = 'question' ; To generate a response for a single input, the following query can be used: SELECT question , answer FROM anyscale_endpoints_conversational_full_model WHERE question = 'Where is Stockholm located?' ; The response will look like the following: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is the capital city of Sweden , located in the southeastern part of the country . It is situated on an archipelago of 30 , 000 islands in the Baltic Sea , and is known for its beautiful waterfront , historic buildings , and vibrant cultural scene . | + ---------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , m . answer FROM files . related_questions AS d JOIN anyscale_endpoints_conversational_full_model AS m The response will look like the following: question answer Where is Stockholm located? Stockholm is the capital city of Sweden, located in the southeastern part of the country. It is situated on an island in the Stockholm archipelago, which is made up of more than 30,000 islands. The city is known for its beautiful architecture, museums, and cultural attractions, as well as its vibrant food and nightlife scene. What are some fun activities to do there? Stockholm is the capital city of Sweden and is located in the southeastern part of the country, on the east coast of the Stockholm archipelago. Some fun activities to do in Stockholm include visiting the famous Vasa Museum, exploring the charming old town of Gamla Stan, taking a stroll through the beautiful parks and gardens, and trying out some of the local food and drinks. There are also many opportunities for shopping, cultural experiences, and outdoor activities such as hiking and biking Question answering with context CREATE MODEL anyscale_endpoints_conversational_full_model PREDICT answer USING engine = 'anyscale_endpoints_engine' , mode = 'conversational-full' , model_name = 'mlabonne/NeuralHermes-2.5-Mistral-7B' , question_column = 'question' , context_column = 'context' ; To generate a response for a single input, the following query can be used: SELECT question , context , answer FROM anyscale_endpoints_conversational_full_model WHERE question = 'What is the main topic of the conference happening next week?' AND context = 'The conference, focusing on advancements in AI, will take place in San Francisco next week.' ; The response will look like the following: + ---------------------------+-------------------------------+-------------------------------+ | question | context | answer | + ---------------------------+-------------------------------+-------------------------------+ | What is the main topic of the conference happening next week? | The conference , focusing on advancements in AI , will take place in San Francisco next week . | The main topic of the conference happening next week in San Francisco is advancements in AI . | + ---------------------------+-------------------------------+-------------------------------+ To generate responses for multiple inputs, the following query can be used: SELECT d . question , d . context , m . answer FROM files . related_questions_with_context AS d JOIN anyscale_endpoints_conversational_full_model AS m The response will look like the following: question context answer Where is Anna planning a trip to next month? Anna is planning a trip to Kyoto next month. Anna is planning a trip to Kyoto next month. What does Anna plan on doing there? Anna plans on going sightseeing. Anna plans on going sightseeing during her trip to Kyoto next month. Next Steps Follow this tutorial to see more use case examples. ​ Troubleshooting Guide Authentication Error Symptoms : Failure to authenticate to the Anyscale. Checklist : Make sure that your Anyscale account is active. Confirm that your API key is correct. Ensure that your API key has not been revoked. Ensure that you have not exceeded the API usage or rate limit. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table and model names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT input . text , output . sentiment FROM integration . travel data AS input JOIN openai_engine AS output Incorrect: SELECT input . text , output . sentiment FROM integration . 'travel data' AS input JOIN openai_engine AS output Correct: SELECT input . text , output . sentiment FROM integration . ` travel data ` AS input JOIN openai_engine AS output Was this page helpful? Yes No Suggest edits Raise issue Anthropic Cohere github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage Troubleshooting Guide"}
{"file_name": "timegpt.html", "content": "Nixtla's TimeGPT Integration with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Time Series Models Nixtla's TimeGPT Integration with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models NeuralForecast StatsForecast TimeGPT Recommender Models Multi-Media Models Time Series Models Nixtla's TimeGPT Integration with MindsDB TimeGPT by Nixtla is a generative pre-trained model specifically designed for predicting time series data. TimeGPT takes time series data as input and produces forecasted outputs. TimeGPT can be effectively employed in various applications, including demand forecasting, anomaly detection, financial prediction, and more. You can learn more about its features here . ​ How to bring TimeGPT Models to MindsDB Before creating a model, you will need to create an ML engine for TimeGPT using the CREATE ML_ENGINE statement and providing the TimeGPT API key: CREATE ML_ENGINE timegpt FROM timegpt USING timegpt_api_key = 'timegpt_api_key' ; Once the ML engine is created, we use the CREATE MODEL statement to create the TimeGPT model in MindsDB. CREATE MODEL model_name FROM data_source ( SELECT * FROM table_name ) PREDICT column_to_be_predicted GROUP BY column_name , column_name , . . . ORDER BY date_column HORIZON 3 -- model forecasts the next 3 rows USING ENGINE = 'timegpt' ; To ensure that the model is created based on the TimeGPT engine, include the USING clause at the end, which defines the engine and lists all parameters used with time-series models, including GROUP BY , ORDER BY , HORIZON . What’s different about the TimeGPT engine is that it does not expose the WINDOW parameter in its API, so as a user you need to send a payload with at least N rows, where N depends on the model and the frequency of the series. This is automatically handled by MindsDB in the TimeGPT handler code . ​ Example Nixtla’s TimeGPT model can be used to obtain real-time forecasts of the trading data from Binance. Follow this link to watch a video on integrating TimeGPT model with Binance data. First, connect to Binance from MindsDB executing this command: CREATE DATABASE my_binance WITH ENGINE = 'binance' ; Please note that before using the TimeGPT engine, you should create it from the MindsDB editor, or other clients through which you interact with MindsDB, with the below command: CREATE ML_ENGINE timegpt FROM timegpt USING timegpt_api_key = 'timegpt_api_key' ; You can check the available engines with this command: SHOW ML_ENGINES ; If you see the TimeGPT engine on the list, you are ready to follow the tutorials. Now let’s create a TimeGPT model and train it with data from Binance. CREATE MODEL cryptocurrency_forecast_model FROM my_binance ( SELECT * FROM aggregated_trade_data WHERE symbol = 'BTCUSDT' ) PREDICT open_price ORDER BY open_time HORIZON 10 USING ENGINE = 'timegpt' ; Use the CREATE MODEL statement to create, train, and deploy a model. The FROM clause defines the training data used to train the model - here, the latest Binance data is used. The PREDICT clause specifies the column to be predicted - here, the open price of the BTC/USDT trading pair is to be forecasted. As it is a time-series model, you should order the data by a date column - here, it is the open time when the open price takes effect. Finally, the HORIZON clause defines how many rows into the future the model will forecast - here, it forecasts the next 10 rows (the next 10 minutes, as the interval between Binance data rows is one minute). Please note that the TimeGPT engine is sensitive to inconsistent intervals between data rows. Please check your data for missing, duplicated or irregular timestamps to mitigate errors that may arise if the intervals between data rows are inconsistent. In this example, the intervals between Binance data rows are consistently equal to one minute. Before proceeding, make sure that the model status reads complete . DESCRIBE cryptocurrency_forecast_model ; To make forecasts, you must save the Binance data into a view: CREATE VIEW btcusdt_recent AS ( SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' ) ; This view is going to be joined with the model to get forecasts: SELECT m . open_time , m . open_price FROM btcusdt_recent AS d JOIN cryptocurrency_forecast_model AS m WHERE d . open_time > LATEST ; Was this page helpful? Yes No Suggest edits Raise issue StatsForecast LightFM github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to bring TimeGPT Models to MindsDB Example"}
{"file_name": "langchain.html", "content": "LangChain - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models LangChain Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models LangChain This documentation describes the integration of MindsDB with LangChain , a framework for developing applications powered by language models. The integration allows for the deployment of LangChain models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use LangChain within MindsDB, install the required dependencies following this instruction . Obtain the API key for a selected model (provider) that you want to use through LangChain. Available models include the following: Anthropic ( how to get the API key ) OpenAI ( how to get the API key ) Anyscale ( how to get the API key ) Ollama ( how to download Ollama ) The LiteLLM model provider is available through Minds Cloud where you can generate the API key. ​ Setup Create an AI engine from the LangChain handler . CREATE ML_ENGINE langchain_engine FROM langchain USING serper_api_key = 'your-serper-api-key' , -- it is an optional parameter (if provided, the model will use serper.dev search to enhance the output) -- provide one of the below parameters anthropic_api_key = 'api-key-value' , anyscale_api_key = 'api-key-value' , litellm_api_key = 'api-key-value' , openai_api_key = 'api-key-value' ; Create a model using langchain_engine as an engine and one of OpenAI/Anthropic/Anyscale/LiteLLM as a model provider. CREATE MODEL langchain_model PREDICT target_column USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE < provider > _api_key = 'api-key-value' , -- if not provided in CREATE ML_ENGINE (replace <provider> with one of the available values) model_name = 'model-name' , -- optional, model to be used (for example, 'gpt-4' if 'openai_api_key' provided) prompt_template = 'message to the model that may include some {{input}} columns as variables' ; This handler supports tracing features for LangChain via LangFuse . To use it, provide the following parameters in the USING clause: langfuse_host , langfuse_public_key , langfuse_secret_key . Agents and Tools are some of the main abstractions that LangChain offers. You can read more about them in the LangChain documentation . There are three different tools utilized by this agent: MindsDB is the internal MindsDB executor. Metadata fetches the metadata information for the available tables. Write is able to write agent responses into a MindsDB data source. Each tool exposes the internal MindsDB executor in a different way to perform its tasks, effectively enabling the agent model to read from (and potentially write to) data sources or models available in the active MindsDB project. Create a conversational model using langchain_engine as an engine and one of OpenAI/Anthropic/Anyscale/LiteLLM as a model provider. OpenAI CREATE ML_ENGINE langchain_engine FROM langchain USING openai_api_key = 'api-key-value' ; CREATE MODEL langchain_openai_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'openai' , -- one of the available providers openai_api_key = 'api-key-value' , -- if not provided in CREATE ML_ENGINE model_name = 'gpt-3.5-turbo' , -- choose one of the available OpenAI models mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; Anthropic CREATE ML_ENGINE langchain_engine FROM langchain USING anthropic_api_key = 'api-key-value' ; CREATE MODEL langchain_openai_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'anthropic' , -- one of the available providers anthropic_api_key = 'api-key-value' , -- if not provided in CREATE ML_ENGINE model_name = 'claude-2.1' , -- choose one of the available OpenAI models mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; Anyscale CREATE ML_ENGINE langchain_engine FROM langchain USING anyscale_api_key = 'api-key-value' ; CREATE MODEL langchain_anyscale_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'anyscale' , -- one of the available providers anyscale_api_key = 'api-key-value' , -- if not provided in CREATE ML_ENGINE model_name = 'mistralai/Mistral-7B-Instruct-v0.1' , -- choose one of the models available from Anyscale mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) base_url = 'https://api.endpoints.anyscale.com/v1' , verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; Ollama CREATE ML_ENGINE langchain_engine FROM langchain ; CREATE MODEL langchain_ollama_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'ollama' , -- one of the available providers model_name = 'llama2' , -- choose one of the models available from Ollama mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; Ensure to have Ollama set up locally by following this guide on how to download Ollama . LiteLLM CREATE ML_ENGINE langchain_engine FROM langchain USING litellm_api_key = 'api-key-value' ; CREATE MODEL langchain_litellm_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'litellm' , -- one of the available providers litellm_api_key = 'api-key-value' , -- if not provided in CREATE ML_ENGINE model_name = 'assistant' , -- model created in MindsDB mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) base_url = 'https://ai.dev.mindsdb.com' , verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; MindsDB CREATE ML_ENGINE langchain_engine FROM langchain ; CREATE MODEL langchain_mindsdb_model PREDICT answer USING engine = 'langchain_engine' , -- engine name as created via CREATE ML_ENGINE provider = 'mindsdb' , -- one of the available providers model_name = 'gpt_model' , -- any model created within MindsDB mode = 'conversational' , -- conversational mode user_column = 'question' , -- column name that stores input from the user assistant_column = 'answer' , -- column name that stores output of the model (see PREDICT column) verbose = True , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; ​ Usage The following usage examples utilize langchain_engine to create a model with the CREATE MODEL statement. Create a model that will be used to describe, analyze, and retrieve. CREATE MODEL tool_based_agent PREDICT completion USING engine = 'langchain_engine' , prompt_template = 'Answer the users input in a helpful way: {{question}}' ; Here, we create the tool_based_agent model using the LangChain engine, as defined in the engine parameter. This model answers users’ questions in a helpful way, as defined in the prompt_template parameter, which specifies input as the input column when calling the model. ​ Describe data Query the model to describe data. SELECT question , completion FROM tool_based_agent WHERE question = 'Could you describe the `mysql_demo_db.house_sales` table please?' USING verbose = True , tools = [ ] , max_iterations = 10 ; Here is the output: The ` mysql_demo_db.house_sales ` table is a base table that contains information related to house sales . It has the following columns : - ` saledate ` : of type text , which likely contains the date when the sale was made . - ` house_price_moving_average ` : of type int , which might represent a moving average of house prices , possibly to track price trends over time . - ` type ` : of type text , which could describe the type of house sold . - ` bedrooms ` : of type int , indicating the number of bedrooms in the sold house . To get information about the mysql_demo_db.house_sales table, the agent uses the Metadata tool. Then the agent prepares the response. ​ Analyze data Query the model to analyze data. SELECT question , completion FROM tool_based_agent WHERE question = 'I want to know the average number of rooms in the downtown neighborhood as per the `mysql_demo_db.home_rentals` table' USING verbose = True , tools = [ ] , max_iterations = 10 ; Here is the output: The average number of rooms in the downtown neighborhood , as per the ` mysql_demo_db.home_rentals ` table , is 1.6 rooms . Here, the model uses the Metadata tool again to fetch the column information. As there is no beds column in the mysql_demo_db.home_rentals table, it uses the number_of_rooms column and writes the following query: SELECT AVG ( number_of_rooms ) FROM mysql_demo_db . home_rentals WHERE neighborhood = 'downtown' ; This query returns the value of 1.6, which is then used to write an answer. ​ Retrieve data Query the model to retrieve data. SELECT question , completion FROM tool_based_agent WHERE question = 'There is a property in the south_side neighborhood with an initial price of 2543 the `mysql_demo_db.home_rentals` table. What are some other details of this listing?' USING verbose = True , tools = [ ] , max_iterations = 10 ; Here is the output: The property in the ` south_side ` neighborhood with an initial price of 2543 has the following details: - Number of rooms: 1 - Number of bathrooms: 1 - Square footage ( sqft ) : 630 - Location: great - Days on market: 11 - Initial price: 2543 - Neighborhood: south_side - Rental price: 2543.0 Here, the model uses the Metadata tool again to fetch information about the table. Then, it creates and executes the following query: SELECT * FROM mysql_demo_db . home_rentals WHERE neighborhood = 'south_side' AND initial_price = 2543 ; On execution, the model gets this output: + ---------------+-------------------+----+--------+--------------+-------------+------------+------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + ---------------+-------------------+----+--------+--------------+-------------+------------+------------+ | 1 | 1 | 630 | great | 11 | 2543 | south_side | 2543 | + ---------------+-------------------+----+--------+--------------+-------------+------------+------------+ Consequently, it takes the query output and writes an answer. Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue Hugging Face Inference API LlamaIndex github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage Describe data Analyze data Retrieve data"}
{"file_name": "vertex.html", "content": "Vertex AI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Vertex AI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Vertex AI This documentation describes the integration of MindsDB with Vertex AI , a machine learning platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in AI-powered applications. The integration allows for the deployment of Vertex AI models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Vertex AI within MindsDB, install the required dependencies following this instruction . ​ Setup Create an AI engine from the Vertex AI handler . This command creates a config object that can be used in client creation step. CREATE ML_ENGINE vertex_engine FROM vertex USING project_id = \"mindsdb-401709\" , location = \"us-central1\" , staging_bucket = \"gs://my_staging_bucket\" , experiment = \"my-experiment\" , experiment_description = \"my experiment description\" , service_account = { < paste service account keys here > } ; Create a model using vertex_engine as an engine. This command authenticates client to a Vertex account using config from previous step. If the endpoint for the model already exists, we create this model in MindsDB. Otherwise, we create and deploy the model to the endpoint before creating this model in MindsDB. CREATE MODEL vertex_model PREDICT target_column USING engine = 'vertex_engine' , -- engine name as created via CREATE ML_ENGINE model_name = 'model_name' , -- choose one of models from your project custom_model = value ; -- indicate whether it is a custom model (True) or not (False) ; ​ Usage The following usage examples utilize vertex_engine to create a model with the CREATE MODEL statement. Detect anomaly using a custom model stored in Vertex AI. CREATE MODEL vertex_model PREDICT cut USING engine = 'vertex' , model_name = 'diamonds_anomaly_detection' , custom_model = True ; Query the model to get predictions by joining it with the data table. SELECT d . cut , m . cut AS anomaly FROM data_table as d JOIN vertex_model as m ; Next Steps Go to the Use Cases section to see more examples.> Was this page helpful? Yes No Suggest edits Raise issue Replicate (LLM) BYOM github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "twelvelabs.html", "content": "TwelveLabs (Video Semantic Search) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Video Models TwelveLabs (Video Semantic Search) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Image Models Video Models TwelveLabs Replicate (Text2Video) Video Models TwelveLabs (Video Semantic Search) In this section, we present how to connect Twelve Labs API to MindsDB. Twelve Labs provides a powerful and seamless video search infrastructure for your application. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB on your system or obtain access to cloud options. To use Twelve Labs with MindsDB, install the required dependencies following this instruction . Copy your Twelve Labs API Key by navigating to the Dashboard page . ​ AI Engine The first step to use this handler is to create an ML Engine. The required argument to create the engine is: twelve_labs_api_key - The TwelveLabs api key. Once you have the API key you can establish a connection by executing the following SQL command: CREATE ML_ENGINE twelve_labs_engine FROM twelve_labs USING twelve_labs_api_key = '<YOUR_API_KEY>' ; ​ AI Model Now, you can use this ML Engine to create Models for the different tasks supported by the handler. When executing the CREATE MODEL statement, the following parameters are supported in the USING clause of the query: engine : The name of the ML Engine to use. This is a required parameter. twelve_labs_api_key : The Twelve Labs API key to use for authentication, if the ML Engine is not provided. base_url : The base URL of the Twelve Labs API. This is an optional parameter and defaults to https://api.twelvelabs.io/v1.2 . task : The task to perform. This is a required parameter and must be one of search or summarization . engine_id : The ID of the Twelve Labs engine to use. This is an optional parameter and defaults to marengo2.6 . However, certain tasks may require a different engine ID; for instance, the summarization task runs only on the Pegasus family of engines. More information about the different engines can be found here . index_name : The name of the index to use; if it does not exist, it will be created. This is a required parameter. More information about indexes can be found here . index_options : A list of the types of information within the video that will be processed by the video understanding engine. This is a required parameter and can be any combination of visual , conversation , text_in_video and logo . More information about index options can be found here . Certain engines support only a subset of these options; for instance, the Pegasus family of engines only support the visual and conversation options. More information about the about these configurations can be found here . video_urls : A list of URLs to the videos to be indexed. This is an optional parameter, but if not specified, one of video_files , video_urls_column or video_files_column must be specified instead. video_files : A list of local paths to the videos to be indexed. This is an optional parameter, but if not specified, one of video_urls , video_urls_column or video_files_column must be specified instead. video_urls_column : The name of the column containing the URLs to the videos to be indexed. This is an optional parameter, but if not specified, one of video_urls , video_files or video_files_column must be specified instead. video_files_column : The name of the column containing the local paths to the videos to be indexed. This is an optional parameter, but if not specified, one of video_urls , video_files or video_urls_column must be specified instead. search_options : A list of the sources of information to use when performing a search. This parameter is required if the task is search and it should be a subset of index_options . More information about search options can be found here . search_query_column : The name of the column containing the search queries. This parameter is required if the task is search . summarization_type : The type of summarization to perform. This parameter is required if the task is summarization and it should be one of summary , chapter or highlight . prompt - Provide context for the summarization task, such as the target audience, style, tone of voice, and purpose. This is an optional parameter. Given below are examples of creating Models for each of the supported tasks. ​ Search CREATE MODEL mindsdb . twelve_labs_search PREDICT search_results USING engine = 'twelve_labs_engine' , twelve_labs_api_key = '<YOUR_API_KEY>' , task = 'search' , index_name = 'index_1' , index_options = [ 'visual' , 'conversation' , 'text_in_video' , 'logo' ] , video_urls = [ 'https://.../video_1.mp4' , 'https://.../video_2.mp4' ] , search_options = [ 'visual' , 'conversation' , 'text_in_video' , 'logo' ] , search_query_column = 'query' As mentioned above, the search_options parameter is specific to the search task and should be a subset of index_options . ​ Summarization CREATE MODEL mindsdb . twelve_labs_summarization PREDICT search_results USING engine = 'twelve_labs_engine' , task = 'summarization' , engine_id = 'pegasus1' , index_name = 'index_1' , index_options = [ 'visual' , 'conversation' ] , video_urls = [ 'https://.../video_1.mp4' , 'https://.../video_2.mp4' ] , summarization_type = 'summary' ; ​ Making Predictions Given below are examples of using Models created for each of the supported tasks. ​ Search SELECT * FROM mindsdb . twelve_labs_search WHERE query = 'search query' ; Here, the query column is the name of the column containing the search queries as specified in the search_query_column parameter of the CREATE MODEL statement. Note: At the moment, only a single query can be specified in the WHERE clause of the query. The JOIN clause for making multiple predictions will be added in a future release. ​ Summarization SELECT * FROM mindsdb . twelve_labs_summarization WHERE video_id = 'video_1' ; Here, the video IDs that were indexed by a model can be found by running a DESCRIBE statement on the it. The URL or file path of the video will be available in the video_reference column. The following is an example of how to run such a DESCRIBE statement, DESCRIBE mindsdb . twelve_labs_summarization . indexed_videos ; The response returned will look something like this, video_id created_at updated_at duration engine_ids filename fps height size video_reference width 66c8425e35db9fa680cd4195 2024-02-23T03:39:10Z 2024-02-23T03:39:12Z 43.733333 pegasus1 test.mp4 30 1280 3737394 /path/to/Videos/test.mp4 720 Note: This will display all of the indexed videos that are contained within the index specified in the index_name parameter of the CREATE MODEL statement. If the same index is used for multiple models, the indexed_videos table will contain all of the videos indexed by all of the models that use that index. Was this page helpful? Yes No Suggest edits Raise issue Replicate (Img2Text) Replicate (Text2Video) github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites AI Engine AI Model Search Summarization Making Predictions Search Summarization"}
{"file_name": "replicate-text2video.html", "content": "Replicate (Text2Video) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Video Models Replicate (Text2Video) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Image Models Video Models TwelveLabs Replicate (Text2Video) Video Models Replicate (Text2Video) This handler was implemented using the replicate library that is provided by Replicate. The required arguments to establish a connection are, model_name: Model name which you want to access in MindsDB. e.g ‘air-forever/kandinsky-2’ version: version hash/id which you want to use in MindsDB. api_key: API key from Replicate Platform you can found here . Before you can use Replicate, it’s essential to authenticate by setting your API token in an environment variable named REPLICATE_API_TOKEN. This token acts as a key to enable access to Replicate’s features. Using pip: If you’re working in a standard Python environment (using pip for package management), set your token as an environment variable by running the following command in your terminal: On Linux, Mac: export REPLICATE_API_TOKEN='YOUR_TOKEN' On Windows: set REPLICATE_API_TOKEN=YOUR_TOKEN Using Docker: For Docker users, the process slightly differs. You need to pass the environment variable directly to the Docker container when running it. Use this command: docker run -e REPLICATE_API_TOKEN='YOUR_TOKEN' -p 47334:47334 -p 47335:47335 mindsdb/mindsdb Again, replace ‘YOUR_TOKEN’ with your actual Replicate API token. ​ Usage To use this handler and connect to a Replicate cluster in MindsDB, you need an account on Replicate. Make sure to create an account by following this link . To establish the connection and create a model in MindsDB, use the following syntax: CREATE MODEL video_ai PREDICT output USING engine = 'replicate' , model_name = 'deforum/deforum_stable_diffusion' , version = 'e22e77495f2fb83c34d5fae2ad8ab63c0a87b6b573b6208e1535b23b89ea66d6' , api_key = 'r8_HEH............' ; You can use the DESCRIBE PREDICTOR query to see the available parameters that you can specify to customize your predictions: DESCRIBE PREDICTOR mindsdb . video_ai . features ; ​ OUTPUT + -------------------+---------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | inputs | type | default | description | + -------------------+---------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | fps | integer | 15 | Choose fps for the video . | | seed | integer | - | Random seed . Leave blank to randomize the seed | | zoom | string | 0 : ( 1.04 ) | zoom parameter for the motion | | angle | string | 0 : ( 0 ) | angle parameter for the motion | | sampler | - | plms | - | | max_frames | integer | 100 | Number of frames for animation | | translation_x | string | 0 : ( 0 ) | translation_x parameter for the motion | | translation_y | string | 0 : ( 0 ) | translation_y parameter for the motion | | color_coherence | - | Match Frame 0 LAB | - | | animation_prompts | string | 0 : a beautiful portrait of a woman by Artgerm , trending on Artstation | Prompt for animation . Provide 'frame number : prompt at this frame' , separate different prompts with '|' . Make sure the frame number does not exceed the max_frames . | + -------------------+---------+-----------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Now, you can use the established connection to query your ML Model as follows: SELECT * FROM video_ai WHERE animation_prompts = 'a human and animals are friends by Asher Brown Durand, trending on Artstation' USING max_frames = 119 ; ​ OUTPUT + ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+ | output | prompt | + ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+ | https: //replicate.delivery/pbxt/gSgRjNlxIgJWBB8KeebKeRmBjZx0wqX7JC41U0pvIfPCYVzEB/out.mp4 | a human and animals are friends by Asher Brown Durand, trending on Artstation | + ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+ Checkout generated video here as above link will not work, reason is given below. IMPORTANT NOTE: PREDICTED URL will only work for 24 hours after prediction. Note: Replicate provides only a few free predictions, so choose your predictions wisely. Don’t let the machines have all the fun, save some for yourself! 😉 Was this page helpful? Yes No Suggest edits Raise issue TwelveLabs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Usage OUTPUT OUTPUT"}
{"file_name": "huggingface_inference_api.html", "content": "Hugging Face Inference API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Hugging Face Inference API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Hugging Face Inference API This documentation describes the integration of MindsDB with Hugging Face Inference API . The integration allows for the deployment of Hugging Face models through Inference API within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Hugging Face Inference API within MindsDB, install the required dependencies following this instruction . Obtain the API key for Hugging Face Inference API required to deploy and use Hugging Face models through Inference API within MindsDB. Generate tokens in the Settings -> Access Tokens tab of the Hugging Face account. ​ Setup Create an AI engine from the Hugging Face Inference API handler . CREATE ML_ENGINE huggingface_api_engine FROM huggingface_api USING huggingface_api_api_key = 'api-key-value' ; Create a model using huggingface_api_engine as an engine. CREATE MODEL huggingface_api_model PREDICT target_column USING engine = 'huggingface_api_engine' , -- engine name as created via CREATE ML_ENGINE task = 'task_name' , -- choose one of 'text-classification', 'text-generation', 'question-answering', 'sentence-similarity', 'zero-shot-classification', 'summarization', 'fill-mask', 'image-classification', 'object-detection', 'automatic-speech-recognition', 'audio-classification' input_column = 'column_name' , -- column that stores input/question to the model labels = [ 'label 1' , 'label 2' ] ; -- labels used to classify data (used for classification tasks) The following parameters are supported in the USING clause of the CREATE MODEL statement: Parameter Required Description engine Yes It is the name of the ML engine created with the CREATE ML_ENGINE statement. task Only if model_name is not provided It describes a task to be performed. model_name Only if task is not provided It specifies a model to be used. input_column Yes It is the name of the column that stores input to the model. endpoint No It defines the endpoint to use for API calls. If not specified, the hosted Inference API from Hugging Face will be used. options No It is a JSON object containing additional options to pass to the API call. More information about the available options for each task can be found here . parameters No It is a JSON object containing additional parameters to pass to the API call. More information about the available parameters for each task can be found here . context_column Only if task is question-answering It is used for the question-answering task to provide context to the question. input_column2 Only if task is sentence-similarity It is used for the sentence-similarity task to provide the second input sentence for comparison. candidate_labels Only if task is zero-shot-classification It is used for the zero-shot-classification task to classify input data according to provided labels. ​ Usage The following usage examples utilize huggingface_api_engine to create a model with the CREATE MODEL statement. Create a model to classify input text as spam or ham. CREATE MODEL spam_classifier PREDICT is_spam USING engine = 'huggingface_api_engine' , task = 'text-classification' , column = 'text' ; Query the model to get predictions. SELECT text , is_spam FROM spam_classifier WHERE text = 'Subscribe to this channel asap' ; Here is the output: + --------------------------------+---------+ | text | is_spam | + --------------------------------+---------+ | Subscribe to this channel asap | spam | + --------------------------------+---------+ Find more quick examples below: Text Classification CREATE MODEL mindsdb . hf_text_classifier PREDICT sentiment USING task = 'text-classification' , engine = 'hf_api_engine' , input_column = 'text' ; Fill Mask CREATE MODEL mindsdb . hf_fill_mask PREDICT sequence USING task = 'fill-mask' , engine = 'hf_api_engine' , input_column = 'text' ; Summarization CREATE MODEL mindsdb . hf_summarizer PREDICT summary USING task = 'summarization' , engine = 'hf_api_engine' , input_column = 'text' ; Text Generation CREATE MODEL mindsdb . hf_text_generator PREDICT generated_text USING task = 'text-generation' , engine = 'hf_api_engine' , input_column = 'text' ; Question Answering CREATE MODEL mindsdb . hf_question_answerer PREDICT answer USING task = 'question-answering' , engine = 'hf_api_engine' , input_column = 'question' , context_column = 'context' ; Sentences Similarity CREATE MODEL mindsdb . hf_sentence_similarity PREDICT similarity USING task = 'sentence-similarity' , engine = 'hf_api_engine' , input_column = 'sentence1' , input_column2 = 'sentence2' ; Zero Shot Classification CREATE MODEL mindsdb . hf_zero_shot_classifier PREDICT label USING task = 'zero-shot-classification' , engine = 'hf_api_engine' , input_column = 'text' , candidate_labels = [ 'label1' , 'label2' , 'label3' ] ; Image Classification CREATE MODEL mindsdb . hf_image_classifier PREDICT label USING task = 'image-classification' , engine = 'hf_api_engine' , input_column = 'image_url' ; Object Detection CREATE MODEL mindsdb . hf_object_detector PREDICT objects USING task = 'object-detection' , engine = 'hf_api_engine' , input_column = 'image_url' ; Automatic Speech Recognition CREATE MODEL mindsdb . hf_speech_recognizer PREDICT transcription USING task = 'automatic-speech-recognition' , engine = 'hf_api_engine' , input_column = 'audio_url' ; Audio Classification CREATE MODEL mindsdb . hf_audio_classifier PREDICT label USING task = 'audio-classification' , engine = 'hf_api_engine' , input_column = 'audio_url' ; Next Steps Follow this link to see more use case examples. Was this page helpful? Yes No Suggest edits Raise issue Hugging Face LangChain github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "openai.html", "content": "OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models OpenAI This documentation describes the integration of MindsDB with OpenAI , an AI research organization known for developing AI models like GPT-3 and GPT-4. The integration allows for the deployment of OpenAI models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use OpenAI within MindsDB, install the required dependencies following this instruction . Obtain the OpenAI API key required to deploy and use OpenAI models within MindsDB. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the OpenAI handler . CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'api-key-value' ; Create a model using openai_engine as an engine. CREATE MODEL openai_model PREDICT target_column USING engine = 'openai_engine' , -- engine name as created via CREATE ML_ENGINE api_base = 'base-url' , -- optional, replaces the default base URL mode = 'mode_name' , -- optional, mode to run the model in model_name = 'openai_model_name' , -- optional with default value of gpt-3.5-turbo question_column = 'question' , -- optional, column name that stores user input context_column = 'context' , -- optional, column that stores context of the user input prompt_template = 'input message to the model here' , -- optional, user provides instructions to the model here user_column = 'user_input' , -- optional, stores user input assistant_column = 'conversation_context' , -- optional, stores conversation context prompt = 'instruction to the model' , -- optional stores instruction to the model max_tokens = 100 , -- optional, token limit for answer temperature = 0.3 , -- temp json_struct = { 'key' : 'value' , . . . }' If you want to update the prompt_template parameter, you do not have to recreate the model. Instead, you can override the prompt_template parameter at prediction time like this: SELECT question , answer FROM openai_model WHERE question = 'input question here' USING prompt_template = 'input new message to the model here' ; The following parameters are available to use when creating an OpenAI model: engine This is the engine name as created with the CREATE ML_ENGINE statement. api_base This parameter is optional. It replaces the default OpenAI’s base URL with the defined value. mode This parameter is optional. The available modes include default , conversational , conversational-full , image , and embedding . The default mode is used by default. The model replies to the prompt_template message. The conversational mode enables the model to read and reply to multiple messages. The conversational-full mode enables the model to read and reply to multiple messages, one reply per message. The image mode is used to create an image instead of a text reply. The embedding mode enables the model to return output in the form of embeddings. You can find all models supported by each mode here . model_name This parameter is optional. By default, the gpt-3.5-turbo model is used. You can find all available models here . question_column This parameter is optional. It contains the column name that stores user input. context_column This parameter is optional. It contains the column name that stores context for the user input. prompt_template This parameter is optional if you use question_column . It stores the message or instructions to the model. Please note that this parameter can be overridden at prediction time. max_tokens This parameter is optional. It defines the maximum token cost of the prediction. Please note that this parameter can be overridden at prediction time. temperature This parameter is optional. It defines how risky the answers are. The value of 0 marks a well-defined answer, and the value of 0.9 marks a more creative answer. Please note that this parameter can be overridden at prediction time. json_struct This parameter is optional. It is used to extract JSON data from a text column provided in the prompt_template parameter. See examples here . ​ Usage Here are the combination of parameters for creating a model: Provide a prompt_template alone. Provide a question_column and optionally a context_column . Provide a prompt , user_column , and assistant_column to create a model in the conversational mode. The following usage examples utilize openai_engine to create a model with the CREATE MODEL statement. Answering questions without context Here is how to create a model that answers questions without context. CREATE MODEL openai_model PREDICT answer USING engine = 'openai_engine' , question_column = 'question' ; Query the model to get predictions. SELECT question , answer FROM openai_model WHERE question = 'Where is Stockholm located?' ; Here is the output: + ---------------------------+-------------------------------+ | question | answer | + ---------------------------+-------------------------------+ | Where is Stockholm located? | Stockholm is located in Sweden . | + ---------------------------+-------------------------------+ Answering questions with context Here is how to create a model that answers questions with context. CREATE MODEL openai_model PREDICT answer USING engine = 'openai_engine' , question_column = 'question' , context_column = 'context' ; Query the model to get predictions. SELECT context , question , answer FROM openai_model WHERE context = 'Answer accurately' AND question = 'How many planets exist in the solar system?' ; On execution, we get: + -------------------+-------------------------------------------+----------------------------------------------+ | context | question | answer | + -------------------+-------------------------------------------+----------------------------------------------+ | Answer accurately | How many planets exist in the solar system? | There are eight planets in the solar system . | + -------------------+-------------------------------------------+----------------------------------------------+ Prompt completion Here is how to create a model that offers the most flexible mode of operation. It answers any query provided in the prompt_template parameter. Good prompts are the key to getting great completions out of large language models like the ones that OpenAI offers. For best performance, we recommend you read their prompting guide before trying your hand at prompt templating. Let’s look at an example that reuses the openai_model model created earlier and overrides parameters at prediction time. SELECT instruction , answer FROM openai_model WHERE instruction = 'Speculate extensively' USING prompt_template = '{{instruction}}. What does Tom Hanks like?' , max_tokens = 100 , temperature = 0.5 ; On execution, we get: + ----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | instruction | answer | + ----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Speculate extensively | Some people speculate that Tom Hanks likes to play golf , while others believe that he enjoys acting and directing . It is also speculated that he likes to spend time with his family and friends , and that he enjoys traveling . | + ----------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Conversational mode Here is how to create a model in the conversational mode. CREATE MODEL openai_chat_model PREDICT response USING engine = 'openai_engine' , mode = 'conversational' , model_name = 'gpt-3.5-turbo' , user_column = 'user_input' , assistant_column = 'conversation_history' , prompt = 'Answer the question in a helpful way.' ; And here is how to query this model: SELECT response FROM openai_chat_model WHERE user_input = '<question>' AND conversation_history = '<optionally, provide the context for the question>' ; Next Steps Follow this tutorial on sentiment analysis and this tutorial on finetuning OpenAI models to see more use case examples. ​ Troubleshooting Guide Authentication Error Symptoms : Failure to authenticate to the OpenAI API. Checklist : Make sure that your OpenAI account is active. Confirm that your API key is correct. Ensure that your API key has not been revoked. Ensure that you have not exceeded the API usage or rate limit. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table and model names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT input . text , output . sentiment FROM integration . travel data AS input JOIN openai_engine AS output Incorrect: SELECT input . text , output . sentiment FROM integration . 'travel data' AS input JOIN openai_engine AS output Correct: SELECT input . text , output . sentiment FROM integration . ` travel data ` AS input JOIN openai_engine AS output Was this page helpful? Yes No Suggest edits Raise issue Ollama Portkey github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage Troubleshooting Guide"}
{"file_name": "lightwood.html", "content": "Lightwood - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AutoML Lightwood Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Lightwood PyCaret Time Series Models Recommender Models Multi-Media Models AutoML Lightwood Lightwood is the default AI engine used in MindsDB. It deals mainly with classification , regression , and time-series problems in machine learning. By providing it with the input data and problem definition, Lightwood generates predictions following three core steps that include Data pre-processing and cleaning , Feature engineering , and Model building and training . The input data ranges from numbers, dates, categories, text, quantities, arrays, matrices, up to images, audios, and videos (passed as URLs). We recommend using the mindsdb/mindsdb:lightwood Docker image that comes with the Lightwood dependencies pre-installed. Learn more here . ​ How It Works Here is the algorithm followed by Lightwood starting from the input data setup, through model building and training, up to getting predictions. The input data is pre-processed and each column is assigned a data type. Next, data is converted into features via encoders that transform data into numerical representation used by the model. Finally, a predictive model takes the encoded feature data and outputs a prediction for the target. Under the hood, the model splits data into the training, validation, and testing sets, with ratios that are dynamic but usually an 80-10-10 ratio. The split is done by default using random sampling without replacement, stratified on the target column. Doing so, it determines the accuracy of the model by evaluating on the held out test set. Users can either use Lightwood’s default mixers/models or create their own approaches inherited from the BaseMixer class. To learn more about Lightwood philosophy, follow this link . ​ Accuracy Metrics Lightwood provides ways to score the accuracy of the model using one of the accuracy functions. The accuracy functions include mean_absolute_error , mean_squared_error , precision_score , recall_score , and f1_score . CREATE MODEL model_name FROM data_source ( SELECT * FROM table_name ) PREDICT target_column USING accuracy_functions = \"['accuracy_function']\" ; You can define the accuracy function of choice in the USING clause of the CREATE MODEL statement. Here are the accuracy functions used by default: the r2_score value for regression predictions. the balanced_accuracy_score value for classification predictions. the complementary_smape_array_accuracy value for time series predictions. The values vary between 0 and 1, where 1 indicates a perfect predictor, based on results obtained for a held-out portion of data (i.e. testing set). You can check accuracy values for models using the DESCRIBE statement. ​ Tuning the Lightwood ML Engine ​ Description In MindsDB, the underlying AutoML models are based on the Lightwood engine by default. This library generates models automatically based on the data and declarative problem definition. But the default configuration can be overridden using the USING statement that provides an option to configure specific parameters of the training process. In the upcoming version of MindsDB, it will be possible to choose from more ML frameworks. Please note that the Lightwood engine is used by default. ​ Syntax Here is the syntax: CREATE MODEL project_name . model_name FROM data_source ( SELECT column_name , . . . FROM table_name ) PREDICT target_column USING parameter_key = 'parameter_value' ; ​ encoders Key It grants access to configure how each column is encoded. By default, the AutoML engine tries to get the best match for the data. . . . USING encoders . column_name . module = 'value' ; To learn more about encoders and their options, visit the Lightwood documentation page on encoders . ​ model.args Key It allows you to specify the type of machine learning algorithm to learn from the encoder data. . . . USING model . args = { \"key\" : value } ; Here are the model options: Model Description BaseMixer It is a base class for all mixers. LightGBM This mixer configures and uses LightGBM for regression or classification tasks depending on the problem definition. LightGBMArray This mixer consists of several LightGBM mixers in regression mode aimed at time series forecasting tasks. NHitsMixer This mixer is a wrapper around an MQN-HITS deep learning model. Neural This mixer trains a fully connected dense network from concatenated encoded outputs of each feature in the dataset to predict the encoded output. NeuralTs This mixer inherits from Neural mixer and should be used for time series forecasts. ProphetMixer This mixer is a wrapper around the popular time series library Prophet . RandomForest This mixer supports both regression and classification tasks. It inherits from sklearn.ensemble.RandomForestRegressor and sklearn.ensemble.RandomForestClassifier. Regression This mixer inherits from scikit-learn’s Ridge class . SkTime This mixer is a wrapper around the popular time series library sktime. Unit This is a special mixer that passes along whatever prediction is made by the target encoder without modifications. It is used for single-column predictive scenarios that may involve complex and/or expensive encoders (e.g. free-form text classification with transformers). XGBoostMixer This mixer is a good all-rounder, due to the generally great performance of tree-based ML algorithms for supervised learning tasks with tabular data. Please note that not all mixers are available in our cloud environment. In particular, LightGBM, LightGBMArray, NHITS, and Prophet. To learn more about all the model options, visit the Lightwood documentation page on mixers . ​ problem_definition.embedding_only Key To train an embedding-only model, use the below parameter when creating the model. CREATE MODEL embedding_only_model . . . USING problem_definition . embedding_only = True ; The predictions made by this embedding model come in the form of embeddings by default. Alternatively, to get predictions in the form of embeddings from a normal model (that is, trained without specifying the problem_definition.embedding_only parameter), use the below parameter when querying this model for predictions. SELECT predictions . . . USING return_embedding = True ; ​ Other Keys Supported by Lightwood in JsonAI The most common use cases of configuring predictors use encoders and model keys explained above. To see all the available keys, check out the Lightwood documentation page on JsonAI . ​ Example Here we use the home_rentals dataset and specify particular encoders for some columns and a LightGBM model . CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING encoders . location . module = 'CategoricalAutoEncoder' , encoders . rental_price . module = 'NumericEncoder' , encoders . rental_price . args . positive_domain = 'True' , model . args = { \"submodels\" : [ { \"module\" : \"LightGBM\" , \"args\" : { \"stop_after\" : 12 , \"fit_on_dev\" : true } } ] } ; ​ Explainability With Lightwood, you can deploy the following types of models: regressions models, classification models, time-series models, embedding models. Predictions made by each type of model come with an explanation column, as below. Regression In the case of regression models, the target_explain column contains the following information: {\"predicted_value\": 2951, \"confidence\": 0.99, \"anomaly\": null, \"truth\": null, \"confidence_lower_bound\": 2795, \"confidence_upper_bound\": 3107} The upper and lower bounds are determined via conformal prediction, and correspond to the reported confidence score (which can be modified by the user). Try it out following this tutorial . Classification In the case of classification models, the target_explain column contains the following information: {\"predicted_value\": \"No\", \"confidence\": 0.6629213483146067, \"anomaly\": null, \"truth\": null, \"probability_class_No\": 0.8561, \"probability_class_Yes\": 0.1439} The confidence score is produced by the conformal prediction module and is well-calibrated. On the other hand, the probability_class comes directly from the model logits, which may be uncalibrated. Therefore, the probability_class score may be optimistic or pessimistic, i.e. coverage is not guaranteed to empirically match the reported score. Try it out following this tutorial . Time-Series In the case of time-series models, the target_explain column contains the following information: {\"predicted_value\": 501618.3125, \"confidence\": 0.9991, \"anomaly\": false, \"truth\": null, \"confidence_lower_bound\": 500926.109375, \"confidence_upper_bound\": 502310.515625} Try it out following this tutorial . Embeddings In the case of embeddings models, the target_explain column contains the following information: {\"predicted_value\": [1.0, 6.712956428527832, 1.247057318687439, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.3025851249694824, 0.5629426836967468, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.7540000081062317, 0.3333333432674408, 0.35483869910240173, 0.9583333134651184, 0.7833333611488342, 0.25], \"confidence\": null, \"anomaly\": null, \"truth\": null} Try it out following this tutorial . You can visit the comprehensive Lightwood docs here . Check out the Lightwood tutorials here . Was this page helpful? Yes No Suggest edits Raise issue Anomaly Detection PyCaret github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How It Works Accuracy Metrics Tuning the Lightwood ML Engine Description Syntax encoders Key model.args Key problem_definition.embedding_only Key Other Keys Supported by Lightwood in JsonAI Example Explainability"}
{"file_name": "popularity-recommender.html", "content": "Popularity Recommender - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Recommender Models Popularity Recommender Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models LightFM Popularity Recommender Multi-Media Models Recommender Models Popularity Recommender The popularity recommender is built using Polars to create a simple but fast popularity recommender, recommending items based on global popularity and personal past interaction. It identifies the most popular items from the entire dataset and excludes any items a user has already interacted with. Its straightforward methodology makes it an excellent benchmark for comparing more sophisticated recommendation engines. The ideal use cases for this handler include analyzing ecommerce rating data, web page browsing data, or past purchase data for serving users recommendations. As the current implementations stand, the input data should be a table containing user-item interaction data: +---------+---------+--------+ | user_id | item_id | rating | +---------+---------+--------+ | 1 | 2 | 4 | | 1 | 3 | 7 | +---------+---------+--------+ Please note that at the moment this integrations does not support DESCRIBE and FINETUNE features. ​ Example Before creating a popularity recommender model, we need to create an ML engine. CREATE ML_ENGINE popularity_recommender FROM popularity_recommender ; You can verify it by running SHOW ML_ENGINES . Now let’s create a popularity recommender model specifying the necessary input parameters. CREATE MODEL pop_rec_demo FROM mysql_demo_db ( SELECT * FROM movie_lens_ratings ) PREDICT movieId USING engine = 'popularity_recommender' , item_id = 'movieId' , user_id = 'userId' , n_recommendations = 10 ; The required parameters include the following: The item_id parameter that stores items to be recommended; here, these are movies. The user_id parameter that stores users to whom items are recommended. The n_recommendations parameter stores the number of recommendations to be returned. Here is how to connect the mysql_demo_db used for training the model: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Here is how to get recommendations per user based on the global most popular items: SELECT b . * FROM mysql_demo_db . movie_lens_ratings AS a JOIN pop_rec_demo AS b ; And here is how to get recommendations for specific users based on popularity: SELECT b . * FROM mysql_demo_db . movie_lens_ratings AS a JOIN pop_rec_demo AS b WHERE a . userId in ( 215 , 216 ) ; Was this page helpful? Yes No Suggest edits Raise issue LightFM Replicate (Audio) github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Example"}
{"file_name": "anthropic.html", "content": "Anthropic - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Large Language Models Anthropic Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Anthropic Anyscale Endpoints Cohere Google Gemini Hugging Face Hugging Face Inference API LangChain LlamaIndex MonkeyLearn Ollama OpenAI Portkey Replicate (LLM) Vertex AI Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Large Language Models Anthropic This documentation describes the integration of MindsDB with Anthropic , an AI research company. The integration allows for the deployment of Anthropic models within MindsDB, providing the models with access to data from various data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Anthropic within MindsDB, install the required dependencies following this instruction . Obtain the Anthropic API key required to deploy and use Anthropic models within MindsDB. Follow the instructions for obtaining the API key . ​ Setup Create an AI engine from the Anthropic handler . CREATE ML_ENGINE anthropic_engine FROM anthropic USING anthropic_api_key = 'your-anthropic-api-key' ; Create a model using anthropic_engine as an engine. CREATE MODEL anthropic_model PREDICT target_column USING engine = 'anthropic_engine' , -- engine name as created via CREATE ML_ENGINE column = 'column_name' , -- column that stores input/question to the model max_tokens = < integer > , -- max number of tokens to be generated by the model (default is 100) model = 'model_name' ; -- choose one of 'claude-instant-1.2', 'claude-2.1', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229' (default is 'claude-2.1') The integrations between Anthropic and MindsDB was implemented using Anthropic Python SDK . ​ Usage The following usage examples utilize anthropic_engine to create a model with the CREATE MODEL statement. Create and deploy the Anthropic model within MindsDB to ask any question. CREATE MODEL anthropic_model PREDICT answer USING column = 'question' , engine = 'anthropic_engine' , max_tokens = 300 , model = 'claude-2.1' ; -- choose one of 'claude-instant-1.2', 'claude-2.1', 'claude-3-opus-20240229', 'claude-3-sonnet-20240229' Where: Name Description column It defines the prompt to the model. engine It defines the Anthropic engine. max_tokens It defines the maximum number of tokens to generate before stopping. model It defines model that will complete your prompt. Default Model When you create an Anthropic model in MindsDB, it uses the claude-2.1 model by default. But you can use other available models by passing the model name to the model parameter in the USING clause of the CREATE MODEL statement. Default Max Tokens When you create an Anthropic model in MindsDB, it uses 100 tokens as the maximum by default. But you can adjust this value by passing it to the max_tokens parameter in the USING clause of the CREATE MODEL statement. Query the model to get predictions. SELECT question , answer FROM anthropic_model WHERE question = 'Where is Stockholm located?' ; Here is the output: + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | question | answer | + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ | Where is Stockholm located? | Stockholm is the capital and largest city of Sweden . It is located on Sweden's south - central east coast , where Lake Mälaren meets the Baltic Sea . | + -----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------+ Next Steps Go to the Use Cases section to see more examples. Was this page helpful? Yes No Suggest edits Raise issue Overview Anyscale Endpoints github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Setup Usage"}
{"file_name": "replicate-audio.html", "content": "Replicate (Audio) - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Audio Models Replicate (Audio) Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Audio Models Replicate (Audio) Image Models Video Models Audio Models Replicate (Audio) This handler was implemented using the replicate library that is provided by Replicate. ​ The required arguments to establish a connection are, model_name: Model name which you want to access in MindsDB. e.g ‘air-forever/kandinsky-2’ version: version hash/id which you want to use in MindsDB. api_key: API key from Replicate Platform you can found here . Before you can use Replicate, it’s essential to authenticate by setting your API token in an environment variable named REPLICATE_API_TOKEN. This token acts as a key to enable access to Replicate’s features. Using pip: If you’re working in a standard Python environment (using pip for package management), set your token as an environment variable by running the following command in your terminal: On Linux, Mac: export REPLICATE_API_TOKEN='YOUR_TOKEN' On Windows: set REPLICATE_API_TOKEN=YOUR_TOKEN Using Docker: For Docker users, the process slightly differs. You need to pass the environment variable directly to the Docker container when running it. Use this command: docker run -e REPLICATE_API_TOKEN='YOUR_TOKEN' -p 47334:47334 -p 47335:47335 mindsdb/mindsdb Again, replace ‘YOUR_TOKEN’ with your actual Replicate API token. ​ Usage To use this handler and connect to a Replicate cluster in MindsDB, you need an account on Replicate. Make sure to create an account by following this link . To establish the connection and create a model in MindsDB, use the following syntax: CREATE MODEL audio_ai PREDICT audio USING engine = 'replicate' , model_name = 'afiaka87/tortoise-tts' , version = 'e9658de4b325863c4fcdc12d94bb7c9b54cbfe351b7ca1b36860008172b91c71' , api_key = 'r8_BpO.........................' ; You can use the DESCRIBE PREDICTOR query to see the available parameters that you can specify to customize your predictions: DESCRIBE PREDICTOR mindsdb . audio_ai . features ; ​ Output + --------------+---------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | inputs | type | default | description | + --------------+---------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | seed | integer | 0 | Random seed which can be used to reproduce results . | | text | string | The expressiveness of autoregressive transformers is literally nuts ! I absolutely adore them . | Text to speak . | | preset | - | fast | Which voice preset to use . See the documentation for more information . | | voice_a | - | random | Selects the voice to use for generation . Use ` random ` to select a random voice . Use ` custom_voice ` to use a custom voice . | | voice_b | - | disabled | ( Optional ) Create new voice from averaging the latents for ` voice_a ` , ` voice_b ` and ` voice_c ` . Use ` disabled ` to disable voice mixing . | | voice_c | - | disabled | ( Optional ) Create new voice from averaging the latents for ` voice_a ` , ` voice_b ` and ` voice_c ` . Use ` disabled ` to disable voice mixing . | | cvvp_amount | number | 0 | How much the CVVP model should influence the output . Increasing this can in some cases reduce the likelihood of multiple speakers . Defaults to 0 ( disabled ) | | custom_voice | string | - | ( Optional ) Create a custom voice based on an mp3 file of a speaker . Audio should be at least 15 seconds , only contain one speaker , and be in mp3 format . Overrides the ` voice_a ` input . | + --------------+---------+-----------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Now, you can use the established connection to query your ML Model as follows: ​ Audio Generation ​ Custom Audio Cloning SELECT * FROM audio_ai WHERE text = \"This is breaking news that first humans have landed on Mars, and they have found something very unusual there. By the way, this is the future.\" USING voice_a = 'custom_voice' , custom_voice = 'https://123bien.com/wp-content/uploads/2019/05/i-want-to-work-2.mp3' ; OUTPUT + ------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+ | audio | text | + ------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+ | https: //replicate.delivery/pbxt/ffOCXeL4fa5yekAL4ybfFiJBbqENSEjhSLpA2zp1ElsBxxhSE/tortoise.mp3 | This is breaking news that first human are landed on mars and they find something very unusual there which is not yet out, by the way this is future | + ------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------+ If above predicted url don’t work , then use this ​ Audio Generation SELECT * FROM audio_ai WHERE text = \"An image captured by NASA's Mars Curiosity Rover shows a faint figure of a woman against the desert landscape of Mars. If you take a closer look, it will seem that the lady is standing on a cliff overlooking the vast undulating expanse. She seems to wear a long cloak and has long hair.\" USING voice_a = 'random' ; ​ OUTPUT + ---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | audio | text | + ---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | https: //replicate.delivery/pbxt/EQj2DtBn5fxVA6P97GfPYthmgd0I3VaOEGFnweE4hvl5BPUiA/audio.wav | An image captured by NASA's Mars Curiosity Rover shows a faint figure of a woman against the desert landscape of Mars. If you take a closer look, it will seem that the lady is standing on a cliff overlooking the vast undulating expanse. She seems to wear a long cloak and has long hair. | + ---------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Above predicted url will work , therefore use this . This is just an one model used in this example there are more with vast variation and use cases. Also there is no limit to imagination, how can you use this. IMPORTANT NOTE: PREDICTED URL will only work for 24 hours after prediction. Note: Replicate provides only a few free predictions, so choose your predictions wisely. Don’t let the machines have all the fun, save some for yourself! 😉 Was this page helpful? Yes No Suggest edits Raise issue Popularity Recommender Clipdrop github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page The required arguments to establish a connection are, Usage Output Audio Generation Custom Audio Cloning Audio Generation OUTPUT"}
{"file_name": "generated_audio.wav.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "animals.mp4.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "cloned_audio.mp3.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "tidb.html", "content": "TiDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TiDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TiDB This is the implementation of the TiDB data handler for MindsDB. TiDB is an open-source NewSQL database that supports Hybrid Transactional and Analytical Processing workloads. It is MySQL-compatible and can provide horizontal scalability, strong consistency, and high availability. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TiDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to TiDB. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the TiDB database in MindsDB, the following syntax can be used: CREATE DATABASE tidb_datasource WITH ENGINE = 'tidb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 4000 , \"database\" : \"tidb\" , \"user\" : \"root\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM tidb_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue Teradata TimescaleDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "d0lt.html", "content": "D0lt - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases D0lt Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases D0lt This is the implementation of the D0lt data handler for MindsDB. D0lt is a single-node and embedded DBMS that incorporates Git-style versioning as a first-class entity. D0lt behaves like Git - it is a content-addressable local database where the main objects are tables instead of files. In D0lt, a user creates a database locally. The database contains tables that can be read and updated using SQL. Similar to Git, writes are staged until the user issues a commit. Upon commit, the writes are appended to permanent storage. Branch and merge semantics are supported allowing for the tables to evolve at a different pace for multiple users. This allows for loose collaboration on data as well as multiple views on the same core data. Merge conflicts are detected for schema and data conflicts. Data conflicts are cell-based, not line-based. Remote repositories allow for cooperation among repository instances. Clone, push, and pull semantics are all available. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect D0lt to MindsDB, install the required dependencies following this instruction . Install or ensure access to D0lt. ​ Implementation This handler is implemented using mysql-connector , a Python library that allows you to use Python code to run SQL commands on the D0lt database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP address of the server. port is the port through which a TCP/IP connection is to be made. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the D0lt database in MindsDB, the following syntax can be used: CREATE DATABASE d0lt_datasource WITH engine = 'd0lt' , parameters = { \"user\" : \"root\" , \"password\" : \"\" , \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"information_schema\" } ; You can use this established connection to query your table as follows: SELECT * FROM D0lt_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue CrateDB Databend github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ckan.html", "content": "Ckan - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Ckan ​ CKAN Integration handler This handler facilitates integration with CKAN . an open-source data catalog platform for managing and publishing open data. CKAN organizes datasets and stores data in its DataStore .To retrieve data from CKAN, the CKANAPI must be used. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SAP HANA to MindsDB, install the required dependencies following this instruction . The CKAN handler is included with MindsDB by default, so no additional installation is required. ​ Configuration To use the CKAN handler, you need to provide the URL of the CKAN instance you want to connect to. You can do this by setting the CKAN_URL environment variable. For example: CREATE DATABASE ckan_datasource WITH ENGINE = 'ckan' , PARAMETERS = { \"url\" : \"https://your-ckan-instance-url.com\" , \"api_key\" : \"your-api-key-if-required\" } ; NOTE: Some CKAN instances will require you to provide an API Token. You can create one in the CKAN user panel. ​ Usage The CKAN handler provides three main tables: datasets : Lists all datasets in the CKAN instance. resources : Lists all resources metadata across all packages. datastore : Allows querying individual datastore resources. ​ Example Queries List all datasets: SELECT * FROM ` your-datasource ` . datasets ; List all resources: SELECT * FROM ` your-datasource ` . resources ; Query a specific datastore resource: SELECT * FROM ` your-datasource ` . datastore WHERE resource_id = 'your-resource-id' ; Replace your-resource-id-here with the actual resource ID you want to query. ​ Querying Large Resources The CKAN handler supports automatic pagination when querying datastore resources. This allows you to retrieve large datasets without worrying about API limits. You can still use the LIMIT clause to limit the number of rows returned by the query. For example: SELECT * FROM ckan_datasource . datastore WHERE resource_id = 'your-resource-id-here' LIMIT 1000 ; ​ Limitations The handler currently supports read operations only. Write operations are not supported. Performance may vary depending on the size of the CKAN instance and the complexity of your queries. The handler may not work with all CKAN instances, especially those with custom configurations. The handler does not support all CKAN API features. Some advanced features may not be available. The datastore search will return limited records up to 32000. Please refer to the CKAN API documentation for more information. Was this page helpful? Yes No Suggest edits Raise issue Apache Solr ClickHouse github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page CKAN Integration handler Prerequisites Configuration Usage Example Queries Querying Large Resources Limitations"}
{"file_name": "microsoft-sql-server.html", "content": "Microsoft SQL Server - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Microsoft SQL Server Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Microsoft SQL Server This documentation describes the integration of MindsDB with Microsoft SQL Server, a relational database management system developed by Microsoft. The integration allows for advanced SQL functionalities, extending Microsoft SQL Server’s capabilities with MindsDB’s features. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Microsoft SQL Server to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Microsoft SQL Server database from MindsDB by executing the following SQL command: CREATE DATABASE mssql_datasource WITH ENGINE = 'mssql' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 1433 , \"user\" : \"sa\" , \"password\" : \"password\" , \"database\" : \"master\" } ; Required connection parameters include the following: user : The username for the Microsoft SQL Server. password : The password for the Microsoft SQL Server. host The hostname, IP address, or URL of the Microsoft SQL Server. database The name of the Microsoft SQL Server database to connect to. Optional connection parameters include the following: port : The port number for connecting to the Microsoft SQL Server. Default is 1433. server : The server name to connect to. Typically only used with named instances or Azure SQL Database. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM mssql_datasource . schema_name . table_name LIMIT 10 ; Run T-SQL queries directly on the connected Microsoft SQL Server database: SELECT * FROM mssql_datasource ( --Native Query Goes Here SELECT SUM ( orderqty ) total FROM Product p JOIN SalesOrderDetail sd ON p . productid = sd . productid JOIN SalesOrderHeader sh ON sd . salesorderid = sh . salesorderid JOIN Customer c ON sh . customerid = c . customerid WHERE ( Name = 'Racing Socks, L' ) AND ( companyname = 'Riding Cycles' ) ; ) ; The above examples utilize mssql_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Microsoft SQL Server database. Checklist : Make sure the Microsoft SQL Server is active. Confirm that host, port, user, and password are correct. Try a direct Microsoft SQL Server connection using a client like SQL Server Management Studio or DBeaver. Ensure a stable network between MindsDB and Microsoft SQL Server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Microsoft Access MonetDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "orioledb.html", "content": "OrioleDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OrioleDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OrioleDB This is the implementation of the OrioleDB data handler for MindsDB. OrioleDB is a new storage engine for PostgreSQL, bringing a modern approach to database capacity, capabilities, and performance to the world’s most-loved database platform. It consists of an extension, building on the innovative table access method framework and other standard Postgres extension interfaces. By extending and enhancing the current table access methods, OrioleDB opens the door to a future of more powerful storage models that are optimized for cloud and modern hardware architectures. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OrioleDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to OrioleDB. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. server is the OrioleDB server. database is the database name. ​ Usage In order to make use of this handler and connect to the OrioleDB server in MindsDB, the following syntax can be used: CREATE DATABASE orioledb_datasource WITH ENGINE = 'orioledb' , PARAMETERS = { \"user\" : \"orioledb_user\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 55505 , \"server\" : \"server_name\" , \"database\" : \"oriole_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM orioledb_data . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Oracle PlanetScale github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cratedb.html", "content": "CrateDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases CrateDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases CrateDB This is the implementation of the CrateDB data handler for MindsDB. CrateDB is a distributed SQL database management system that integrates a fully searchable document-oriented data store. It is open-source, written in Java, based on a shared-nothing architecture, and designed for high scalability. CrateDB includes components from Lucene, Elasticsearch and Netty. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect CrateDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to CrateDB. ​ Implementation This handler is implemented using crate , a Python library that allows you to use Python code to run SQL commands on CrateDB. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP adress of the server. port is the port through which connection is to be made. schema_name is schema name to get tables from. Defaults to doc . ​ Usage In order to make use of this handler and connect to the CrateDB database in MindsDB, the following syntax can be used: CREATE DATABASE crate_datasource WITH engine = 'crate' , parameters = { \"user\" : \"crate\" , \"password\" : \"\" , \"host\" : \"127.0.0.1\" , \"port\" : 4200 , \"schema_name\" : \"doc\" } ; You can use this established connection to query your table as follows: SELECT * FROM crate_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue Couchbase D0lt github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "surrealdb.html", "content": "SurrealDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SurrealDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SurrealDB This is the implementation of the SurrealDB data handler for MindsDB. SurrealDB is an innovative NewSQL cloud database, suitable for serverless applications, jamstack applications, single-page applications, and traditional applications. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SurrealDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to SurrealDB. ​ Implementation This handler was implemented by using the python library pysurrealdb . The required arguments to establish a connection are: host : the host name of the Surrealdb connection port : the port to use when connecting user : the user to authenticate password : the password to authenticate the user database : database name to be connected namespace : namespace name to be connected ​ Usage To establish a connection with our SurrealDB server which is running locally with the public cloud instance. We are going to use ngrok tunneling to connect cloud instance to the local SurrealDB server. You can follow this guide for that. Let’s make the connection with the MindsDB public cloud CREATE DATABASE exampledb WITH ENGINE = 'surrealdb' , PARAMETERS = { \"host\" : \"6.tcp.ngrok.io\" , \"port\" : \"17141\" , \"user\" : \"root\" , \"password\" : \"root\" , \"database\" : \"testdb\" , \"namespace\" : \"testns\" } ; Please change the host and port properties in the PARAMETERS clause based on the values which you got. We can also query the dev table which we created with SELECT * FROM exampledb . dev ; Was this page helpful? Yes No Suggest edits Raise issue Supabase TDengine github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "scylladb.html", "content": "ScyllaDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ScyllaDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ScyllaDB This is the implementation of the ScyllaDB data handler for MindsDB. ScyllaDB is an open-source distributed NoSQL wide-column data store. It was purposefully designed to offer compatibility with Apache Cassandra while outperforming it with higher throughputs and reduced latencies. For a comprehensive understanding of ScyllaDB, visit ScyllaDB’s official website. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ScyllaDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to ScyllaDB. ​ Implementation The ScyllaDB handler for MindsDB was developed using the scylla-driver library for Python. The required arguments to establish a connection are as follows: host : Host name or IP address of ScyllaDB. port : Connection port. user : Authentication username. Optional; required only if authentication is enabled. password : Authentication password. Optional; required only if authentication is enabled. keyspace : The specific keyspace (top-level container for tables) to connect to. protocol_version : Optional. Defaults to 4. secure_connect_bundle : Optional. Needed only for connections to DataStax Astra. ​ Usage To set up a connection between MindsDB and a Scylla server, utilize the following SQL syntax: CREATE DATABASE scylladb_datasource WITH ENGINE = 'scylladb' , PARAMETERS = { \"user\" : \"user@mindsdb.com\" , \"password\" : \"pass\" , \"host\" : \"127.0.0.1\" , \"port\" : \"9042\" , \"keyspace\" : \"test_data\" } ; The protocol version is set to 4 by default. Should you wish to modify it, simply include “protocol_version”: 5 within the PARAMETERS dictionary in the query above. With the connection established, you can execute queries on your keyspace as demonstrated below: SELECT * FROM scylladb_datasource . keystore . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue SAP SQL Anywhere SingleStore github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "duckdb.html", "content": "DuckDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases DuckDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases DuckDB This is the implementation of the DuckDB data handler for MindsDB. DuckDB is an open-source analytical database system. It is designed for fast execution of analytical queries. There are no external dependencies and the DBMS runs completely embedded within a host process, similar to SQLite. DuckDB provides a rich SQL dialect with support for complex queries with transactional guarantees (ACID). ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect DuckDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to DuckDB. ​ Implementation This handler is implemented using the duckdb Python client library. The DuckDB handler is currently using the 0.7.1.dev187 pre-relase version of the Python client library. In case of issues, make sure your DuckDB database is compatible with this version. See the requirements.txt for details. The required arguments to establish a connection are as follows: database is the name of the DuckDB database file. It can be set to :memory: to create an in-memory database. The optional arguments are as follows: read_only is a flag that specifies whether the connection is in the read-only mode. This is required if multiple processes want to access the same database file at the same time. ​ Usage In order to make use of this handler and connect to the DuckDB database in MindsDB, the following syntax can be used: CREATE DATABASE duckdb_datasource WITH engine = 'duckdb' , parameters = { \"database\" : \"db.duckdb\" } ; You can use this established connection to query your table as follows: SELECT * FROM duckdb_datasource . my_table ; Was this page helpful? Yes No Suggest edits Raise issue DataStax EdgelessDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "teradata.html", "content": "Teradata - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Teradata Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Teradata This documentation describes the integration of MindsDB with Teradata , the complete cloud analytics and data platform for Trusted AI. The integration allows MindsDB to access data from Teradata and enhance Teradata with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Teradata to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Teradata from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE teradata_datasource WITH ENGINE = 'teradata' , PARAMETERS = { \"host\" : \"192.168.0.41\" , \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"database\" : \"example_db\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the Teradata server. user : The username for the Teradata database. password : The password for the Teradata database. Optional connection parameters include the following: database : The name of the Teradata database to connect to. Defaults is the user’s default database. ​ Usage Retrieve data from a specified table by providing the integration, database and table names: SELECT * FROM teradata_datasource . database_name . table_name LIMIT 10 ; Run Teradata SQL queries directly on the connected Teradata database: SELECT * FROM teradata_datasource ( --Native Query Goes Here SELECT emp_id , emp_name , job_duration AS tsp FROM employee EXPAND ON job_duration AS tsp BY INTERVAL '1' YEAR FOR PERIOD ( DATE '2006-01-01' , DATE '2008-01-01' ) ; ) ; The above examples utilize teradata_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the Teradata database. Checklist : Make sure the Teradata database is active. Confirm that host, user and password are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and Teradata. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Connection Timeout Error Symptoms : Connection to the Teradata database times out or queries take too long to execute. Checklist : Ensure the Teradata server is running and accessible (if the server has been idle for a long time, it may have shut down automatically). Was this page helpful? Yes No Suggest edits Raise issue TDengine TiDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "mysql.html", "content": "MySQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MySQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MySQL This documentation describes the integration of MindsDB with MySQL , a fast, reliable, and scalable open-source database. The integration allows MindsDB to access data from MySQL and enhance MySQL with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MySQL to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to MySQL from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE mysql_conn WITH ENGINE = 'mysql' , PARAMETERS = { \"host\" : \"host-name\" , \"port\" : 3306 , \"database\" : \"db-name\" , \"user\" : \"user-name\" , \"password\" : \"password\" } ; Or: CREATE DATABASE mysql_datasource WITH ENGINE = 'mysql' , PARAMETERS = { \"url\" : \"mysql://user-name@host-name:3306\" } ; Required connection parameters include the following: user : The username for the MySQL database. password : The password for the MySQL database. host : The hostname, IP address, or URL of the MySQL server. port : The port number for connecting to the MySQL server. database : The name of the MySQL database to connect to. Or: url : You can specify a connection to MySQL Server using a URI-like string, as an alternative connection option. Optional connection parameters include the following: ssl : Boolean parameter that indicates whether SSL encryption is enabled for the connection. Set to True to enable SSL and enhance connection security, or set to False to use the default non-encrypted connection. ssl_ca : Specifies the path to the Certificate Authority (CA) file in PEM format. ssl_cert : Specifies the path to the SSL certificate file. This certificate should be signed by a trusted CA specified in the ssl_ca file or be a self-signed certificate trusted by the server. ssl_key : Specifies the path to the private key file (in PEM format). ​ Usage The following usage examples utilize the connection to MySQL made via the CREATE DATABASE statement and named mysql_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM mysql_conn . table_name LIMIT 10 ; Next Steps Follow this tutorial to see more use case examples. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the MySQL database. Checklist : Ensure that the MySQL server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the MySQL server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue MongoDB OceanBase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "apache-cassandra.html", "content": "Apache Cassandra - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Cassandra Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Cassandra This is the implementation of the Cassandra data handler for MindsDB. Cassandra is a free and open-source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers, providing high availability with no single point of failure. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Cassandra to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Cassandra. ​ Implementation As ScyllaDB is API-compatible with Apache Cassandra, the Cassandra data handler extends the ScyllaDB handler and uses the scylla-driver Python library. The required arguments to establish a connection are as follows: host is the host name or IP address of the Cassandra database. port is the port to use when connecting. user is the user to authenticate. password is the password to authenticate the user. keyspace is the keyspace to connect, the top level container for tables. protocol_version is not required and defaults to 4. ​ Usage In order to make use of this handler and connect to the Cassandra server in MindsDB, the following syntax can be used: CREATE DATABASE sc WITH engine = \"cassandra\" , parameters = { \"host\" : \"127.0.0.1\" , \"port\" : \"9043\" , \"user\" : \"user\" , \"password\" : \"pass\" , \"keyspace\" : \"test_data\" , \"protocol_version\" : 4 } ; You can use this established connection to query your table as follows: SELECT * FROM cassandra_datasource . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Amazon S3 Apache Druid github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "mongodb.html", "content": "MongoDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MongoDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MongoDB This documentation describes the integration of MindsDB with MongoDB , a document database with the scalability and flexibility that you want with the querying and indexing that you need. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . ​ Connection Establish a connection to MongoDB from MindsDB by executing the following SQL command: CREATE DATABASE mongodb_datasource WITH ENGINE = 'mongodb' , PARAMETERS = { \"host\" : \"mongodb+srv://admin:admin_pass@demo.mongodb.net/public\" } ; Use the following parameters to establish the connection: host : The connection string of the MongoDB server that includes user ( admin ), password ( admin_pass ), host and port ( demo.mongodb.net ), and database ( public ). database : If the connection string does not include the /database path, provide it in this parameter. Alternatively, the following set of connection parameters can be used: username : The username associated with the database. password : The password to authenticate your access. host : The host of the MongoDB server. port : The port through which TCP/IP connection is to be made. database : The database name to be connected. ​ Usage Retrieve data from a specified collection by providing the integration name and collection name: SELECT * FROM mongodb_datasource . my_collection LIMIT 10 ; The above examples utilize mongodb_datasource as the datasource name, which is defined in the CREATE DATABASE command. At the moment, this integration only supports SELECT and UPDATE queries. For this connection, we strongly suggest using the Mongo API instead of the SQL API. MindsDB has a dedicated Mongo API that allows you to use the full power of the MindsDB platform. Using the Mongo API feels more natural for MongoDB users and allows you to use all the features of MindsDB. You can find the instructions on how to connect MindsDB to MongoDB Compass or MongoDB Shell and proceed with the Mongo API documentation for further details. Once you connected MindsDB to MongoDB Compass or MongoDB Shell, you can run this command to connect your database to MindsDB: test > use mindsdb mindsdb > db . databases . insertOne ( { name: \"mongo_datasource\" , engine : \"mongodb\" , connection_args: { \"host\" : \"mongodb+srv://user:pass@db.xxxyyy.mongodb.net/\" } } ) ; Then you can query your data, like this: mindsdb > use mongo_datasource mongo_datasource > db . demo . find ( {} ) . limit ( 3 ) ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the MongoDB server. Checklist : Make sure the MongoDB server is active. Confirm that host and credentials provided are correct. Try a direct MongoDB connection using a client like MongoDB Compass. Ensure a stable network between MindsDB and MongoDB. For example, if you are using MongoDB Atlas, ensure that the IP address of the machine running MindsDB is whitelisted. Unknown statement Symptoms : Errors related to the issuing of unsupported queries to MongoDB via the integration. Checklist : Ensure the query is a SELECT or UPDATE query. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing collection names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue MonetDB MySQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "influxdb.html", "content": "InfluxDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases InfluxDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases InfluxDB This is the implementation of the InfluxDB data handler for MindsDB. InfluxDB is a time series database that can be used to collect data and monitor the system and devices, especially Edge devices. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect InfluxDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to InfluxDB. ​ Implementation The required arguments to establish a connection are as follows: influxdb_url is the hosted URL of InfluxDB Cloud. influxdb_token is the authentication token for the hosted InfluxDB Cloud instance. influxdb_db_name is the database name of the InfluxDB Cloud instance. influxdb_table_name is the table name of the InfluxDB Cloud instance. Please follow this link to generate token for accessing InfluxDB API. ​ Usage In order to make use of this handler and connect to the InfluxDB database in MindsDB, the following syntax can be used: CREATE DATABASE influxdb_source WITH ENGINE = 'influxdb' , PARAMETERS = { \"influxdb_url\" : \"<influxdb-hosted-url>\" , \"influxdb_token\" : \"<api-key-token\" , \"influxdb_db_name\" : \"<database-name>\" , \"influxdb_table_name\" : \"<table-name>\" } ; You can use this established connection to query your table as follows. SELECT name , time , sensor_id , temperature FROM influxdb_source . tables ORDER BY temperature DESC LIMIT 65 ; Was this page helpful? Yes No Suggest edits Raise issue IBM Informix MariaDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-solr.html", "content": "Apache Solr - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Solr Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Solr This is the implementation of the Solr data handler for MindsDB. Apache Solr is a highly reliable, scalable and fault tolerant, providing distributed indexing, replication and load-balanced querying, automated failover and recovery, centralized configuration, and more. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Solr to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Solr. ​ Implementation This handler is implemented using the sqlalchemy-solr library, which provides a Python/SQLAlchemy interface. The required arguments to establish a connection are as follows: username is the username used to authenticate with the Solr server. This parameter is optional. password is the password to authenticate the user with the Solr server. This parameter is optional. host is the host name or IP address of the Solr server. port is the port number of the Solr server. server_path defaults to solr if not provided. collection is the Solr Collection name. use_ssl defaults to false if not provided. Further reference: https://pypi.org/project/sqlalchemy-solr/ . ​ Usage In order to make use of this handler and connect to the Solr database in MindsDB, the following syntax can be used: CREATE DATABASE solr_datasource WITH engine = 'solr' , parameters = { \"username\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"127.0.0.1\" , \"port\" : \"8981\" , \"server_path\" : \"solr\" , \"collection\" : \"gettingstarted\" , \"use_ssl\" : \"false\" } ; You can use this established connection to query your table as follows: SELECT * FROM solr_datasource . gettingstarted LIMIT 10000 ; Requirements A Solr instance with a Parallel SQL supported up and running. There are certain limitations that need to be taken into account when issuing queries to Solr. Refer to https://solr.apache.org/guide/solr/latest/query-guide/sql-query.html#parallel-sql-queries . Don’t forget to put limit in the end of the SQL statement Was this page helpful? Yes No Suggest edits Raise issue Apache Pinot Ckan github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "supabase.html", "content": "Supabase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Supabase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Supabase This is the implementation of the Supabase data handler for MindsDB. Supabase is an open-source Firebase alternative. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Supabase to MindsDB, install the required dependencies following this instruction . Install or ensure access to Supabase. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the Supabase server in MindsDB, the following syntax can be used: CREATE DATABASE supabase_datasource WITH ENGINE = 'supabase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 54321 , \"database\" : \"test\" , \"user\" : \"supabase\" , \"password\" : \"password\" } ; You can use this established connection to query your database as follows: SELECT * FROM supabase_datasource . public . rentals LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue StarRocks SurrealDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cloud-spanner.html", "content": "Cloud Spanner - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Cloud Spanner Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Cloud Spanner This is the implementation of the Cloud Spanner data handler for MindsDB. Cloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, automatic, synchronous replication for high availability. It supports two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Cloud Spanner to MindsDB, install the required dependencies following this instruction . Install or ensure access to Cloud Spanner. ​ Implementation This handler was implemented using the google-cloud-spanner Python client library. The required arguments to establish a connection are as follows: instance_id is the instance identifier. database_id is the database identifier. project is the identifier of the project that owns the resources. credentials is a stringified GCP service account key JSON. ​ Usage In order to make use of this handler and connect to the Cloud Spanner database in MindsDB, the following syntax can be used: CREATE DATABASE cloud_spanner_datasource WITH engine = 'cloud_spanner' , parameters = { \"instance_id\" : \"my-instance\" , \"database_id\" : \"example-id\" , \"project\" : \"my-project\" , \"credentials\" : \"{...}\" } ; You can use this established connection to query your table as follows: SELECT * FROM cloud_spanner_datasource . my_table ; Cloud Spanner supports both PostgreSQL and GoogleSQL dialects. However, not all PostgresSQL features are supported. Was this page helpful? Yes No Suggest edits Raise issue ClickHouse CockroachDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "vitess.html", "content": "Vitess - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Vitess Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Vitess This is the implementation of the Vitess data handler for MindsDB. Vitess is a database solution for deploying, scaling, and managing large clusters of open-source database instances. It currently supports MySQL and Percona Server for MySQL. It’s architected to run as effectively in a public or private cloud architecture as it does on dedicated hardware. It combines and extends many important SQL features with the scalability of a NoSQL database. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Vitess to MindsDB, install the required dependencies following this instruction . Install or ensure access to Vitess. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the Vitess server in MindsDB, the following syntax can be used: CREATE DATABASE vitess_datasource WITH ENGINE = \"vitess\" , PARAMETERS = { \"user\" : \"root\" , \"password\" : \"\" , \"host\" : \"localhost\" , \"port\" : 33577 , \"database\" : \"commerce\" } ; You can use this established connection to query your table as follows: SELECT * FROM vitess_datasource . product LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Vertica YugabyteDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "microsoft-access.html", "content": "Microsoft Access - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Microsoft Access Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Microsoft Access This is the implementation of the Microsoft Access data handler for MindsDB. Microsoft Access is a pseudo-relational database engine from Microsoft. It is part of the Microsoft Office suite of applications that also includes Word, Outlook, and Excel, among others. Access is also available for purchase as a stand-alone product. It uses the Jet Database Engine for data storage. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Microsoft Access to MindsDB, install the required dependencies following this instruction . Install or ensure access to Microsoft Access. ​ Implementation This handler is implemented using pyodbc , the Python ODBC bridge. The only required argument to establish a connection is db_file that points to a database file to be queried. ​ Usage In order to make use of this handler and connect to the Access database in MindsDB, the following syntax can be used: CREATE DATABASE access_datasource WITH engine = 'access' , parameters = { \"db_file\" : \"C:\\\\Users\\\\minurap\\\\Documents\\\\example_db.accdb\" } ; You can use this established connection to query your table as follows: SELECT * FROM access_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue MatrixOne Microsoft SQL Server github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "google-sheets.html", "content": "Google Sheets - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Google Sheets Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Google Sheets This is the implementation of the Google Sheets data handler for MindsDB. Google Sheets is a spreadsheet program included as a part of the free, web-based Google Docs Editors suite offered by Google. Please note that the integration of MindsDB with Google Sheets works for public sheets only. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Google Sheets to MindsDB, install the required dependencies following this instruction . Install or ensure access to Google Sheets. ​ Implementation This handler is implemented using duckdb , a library that allows SQL queries to be executed on pandas DataFrames. In essence, when querying a particular sheet, the entire sheet is first pulled into a pandas DataFrame using the Google Visualization API . Once this is done, SQL queries can be run on the DataFrame using duckdb . Since the entire sheet needs to be pulled into memory first (DataFrame), it is recommended to be somewhat careful when querying large datasets so as not to overload your machine. The required arguments to establish a connection are as follows: spreadsheet_id is the unique ID of the Google Sheet. sheet_name is the name of the sheet within the Google Sheet. ​ Usage In order to make use of this handler and connect to a Google Sheet in MindsDB, the following syntax can be used: CREATE DATABASE sheets_datasource WITH engine = 'sheets' , parameters = { \"spreadsheet_id\" : \"12wgS-1KJ9ymUM-6VYzQ0nJYGitONxay7cMKLnEE2_d0\" , \"sheet_name\" : \"iris\" } ; You can use this established connection to query your table as follows: SELECT * FROM sheets_datasource . example_tbl ; The name of the table will be the name of the relevant sheet, provided as an input to the sheet_name parameter. At the moment, only the SELECT statemet is allowed to be executed through duckdb . This, however, has no restriction on running machine learning algorithms against your data in Google Sheets using the CREATE MODEL statement. Was this page helpful? Yes No Suggest edits Raise issue Google Cloud SQL GreptimeDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "sap-hana.html", "content": "SAP HANA - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SAP HANA Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SAP HANA This documentation describes the integration of MindsDB with SAP HANA , a multi-model database with a column-oriented in-memory design that stores data in its memory instead of keeping it on a disk. The integration allows MindsDB to access data from SAP HANA and enhance SAP HANA with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SAP HANA to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to SAP HANA from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE sap_hana_datasource WITH ENGINE = 'hana' , PARAMETERS = { \"address\" : \"123e4567-e89b-12d3-a456-426614174000.hana.trial-us10.hanacloud.ondemand.com\" , \"port\" : \"443\" , \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"encrypt\" : true } ; Required connection parameters include the following: address : The hostname, IP address, or URL of the SAP HANA database. port : The port number for connecting to the SAP HANA database. user : The username for the SAP HANA database. password : The password for the SAP HANA database. Optional connection parameters include the following: ‘database’: The name of the database to connect to. This parameter is not used for SAP HANA Cloud. schema : The database schema to use. Defaults to the user’s default schema. encrypt : The setting to enable or disable encryption. Defaults to `True’ ​ Usage Retrieve data from a specified table by providing the integration, schema and table names: SELECT * FROM sap_hana_datasource . schema_name . table_name LIMIT 10 ; Run Teradata SQL queries directly on the connected Teradata database: SELECT * FROM sap_hana_datasource ( --Native Query Goes Here SELECT customer , year , SUM ( sales ) FROM t1 GROUP BY ROLLUP ( customer , year ) ; SELECT customer , year , SUM ( sales ) FROM t1 GROUP BY GROUPING SETS ( ( customer , year ) , ( customer ) ) UNION ALL SELECT NULL , NULL , SUM ( sales ) FROM t1 ; ) ; The above examples utilize sap_hana_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the SAP HANA database. Checklist : Make sure the SAP HANA database is active. Confirm that address, port, user and password are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and SAP HANA. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue QuestDB SAP SQL Anywhere github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "airtable.html", "content": "Airtable - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Airtable Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Airtable This is the implementation of the Airtable data handler for MindsDB. Airtable is a platform that makes it easy to build powerful, custom applications. These tools can streamline just about any process, workflow, or project. And best of all, you can build them without ever learning to write a single line of code. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Airtable to MindsDB, install the required dependencies following this instruction . Install or ensure access to Airtable. ​ Implementation This handler is implemented using duckdb , a library that allows SQL queries to be executed on pandas DataFrames. In essence, when querying a particular table, the entire table is first pulled into a pandas DataFrame using the Airtable API . Once this is done, SQL queries can be run on the DataFrame using duckdb . The required arguments to establish a connection are as follows: base_id is the Airtable base ID. table_name is the Airtable table name. api_key is the API key for the Airtable API. ​ Usage In order to make use of this handler and connect to the Airtable database in MindsDB, the following syntax can be used: CREATE DATABASE airtable_datasource WITH engine = 'airtable' , parameters = { \"base_id\" : \"dqweqweqrwwqq\" , \"table_name\" : \"iris\" , \"api_key\" : \"knlsndlknslk\" } ; You can use this established connection to query your table as follows: SELECT * FROM airtable_datasource . example_tbl ; At the moment, only the SELECT statement is allowed to be executed through duckdb . This, however, has no restriction on running machine learning algorithms against your data in Airtable using the CREATE MODEL statement. Was this page helpful? Yes No Suggest edits Raise issue Supported Integrations Amazon Aurora github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "yugabytedb.html", "content": "YugabyteDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases YugabyteDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases YugabyteDB This is the implementation of the YugabyteDB data handler for MindsDB. YugabyteDB is a high-performance, cloud-native distributed SQL database that aims to support all PostgreSQL features. It is best fit for cloud-native OLTP (i.e. real-time, business-critical) applications that need absolute data correctness and require at least one of the following: scalability, high tolerance to failures, or globally-distributed deployments. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect YugabyteDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to YugabyteDB. ​ Implementation This handler is implemented using psycopg2 , a Python library that allows you to use Python code to run SQL commands on the YugabyteDB database. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. schema is the schema to which your table belongs. ​ Usage In order to make use of this handler and connect to the YugabyteDB database in MindsDB, the following syntax can be used: CREATE DATABASE yugabyte_datasource WITH engine = 'yugabyte' , parameters = { \"user\" : \"admin\" , \"password\" : \"1234\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"database\" : \"yugabyte\" , \"schema\" : \"your_schema_name\" } ; You can use this established connection to query your table as follows: SELECT * FROM yugabyte_datasource . demo ; NOTE : If you are using YugabyteDB Cloud with MindsDB Cloud website you need to add below 3 static IPs of MindsDB Cloud to allow IP list for accessing it publicly. 18.220.205.95 3.19.152.46 52.14.91.162 Was this page helpful? Yes No Suggest edits Raise issue Vitess ChromaDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-aurora.html", "content": "Amazon Aurora - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon Aurora Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon Aurora This is the implementation of the Amazon Aurora handler for MindsDB. Amazon Aurora is a fully managed relational database engine that’s compatible with MySQL and PostgreSQL. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Amazon Aurora to MindsDB, install the required dependencies following this instruction . Install or ensure access to Amazon Aurora. ​ Implementation This handler was implemented using the existing MindsDB handlers for MySQL and PostgreSQL. The required arguments to establish a connection are as follows: host : the host name or IP address of the Amazon Aurora DB cluster. port : the TCP/IP port of the Amazon Aurora DB cluster. user : the username used to authenticate with the Amazon Aurora DB cluster. password : the password to authenticate the user with the Amazon Aurora DB cluster. database : the database name to use when connecting with the Amazon Aurora DB cluster. There optional arguments that can be used are as follows: db_engine : the database engine of the Amazon Aurora DB cluster. This can take one of two values: ‘mysql’ or ‘postgresql’. This parameter is optional, but if it is not provided, aws_access_key_id and aws_secret_access_key parameters must be provided. aws_access_key_id : the access key for the AWS account. This parameter is optional and is only required to be provided if the db_engine parameter is not provided. aws_secret_access_key : the secret key for the AWS account. This parameter is optional and is only required to be provided if the db_engine parameter is not provided. ​ Usage In order to make use of this handler and connect to an Amazon Aurora MySQL DB Cluster in MindsDB, the following syntax can be used: CREATE DATABASE aurora_mysql_datasource WITH engine = 'aurora' , parameters = { \"db_engine\" : \"mysql\" , \"host\" : \"mysqlcluster.cluster-123456789012.us-east-1.rds.amazonaws.com\" , \"port\" : 3306 , \"user\" : \"admin\" , \"password\" : \"password\" , \"database\" : \"example_db\" } ; Now, you can use this established connection to query your database as follows: SELECT * SELECT * FROM aurora_mysql_datasource . example_table ; Similar commands can be used to establish a connection and query Amazon Aurora PostgreSQL DB Cluster: CREATE DATABASE aurora_postgres_datasource WITH engine = 'aurora' , parameters = { \"db_engine\" : \"postgresql\" , \"host\" : \"postgresmycluster.cluster-123456789012.us-east-1.rds.amazonaws.com\" , \"port\" : 5432 , \"user\" : \"postgres\" , \"password\" : \"password\" , \"database\" : \"example_db \" } ; SELECT * FROM aurora_postgres_datasource . example_table If you want to switch to different database, you can include it in your query as: SELECT * FROM aurora_datasource . new_database . example_table ; Was this page helpful? Yes No Suggest edits Raise issue Airtable Amazon DynamoDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "matrixone.html", "content": "MatrixOne - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MatrixOne Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MatrixOne This is the implementation of the MatrixOne data handler for MindsDB. MatrixOne is a future-oriented hyper-converged cloud and edge native DBMS that supports transactional, analytical, and streaming workloads with a simplified and distributed database engine, across multiple data centers, clouds, edges, and other heterogeneous infrastructures. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MatrixOne to MindsDB, install the required dependencies following this instruction . Install or ensure access to MatrixOne. ​ Implementation This handler is implemented using PyMySQL , a Python library that allows you to use Python code to run SQL commands on the MatrixOne database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the hostname or IP address of the database. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. There are several optional arguments that can be used as well. ssl indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores the SSL certificates. ssl_key stores the SSL keys. ​ Usage In order to make use of this handler and connect to the MatrixOne database in MindsDB, the following syntax can be used: CREATE DATABASE matrixone_datasource WITH engine = 'matrixone' , parameters = { \"user\" : \"dump\" , \"password\" : \"111\" , \"host\" : \"127.0.0.1\" , \"port\" : 6001 , \"database\" : \"mo_catalog\" } ; You can use this established connection to query your table as follows: SELECT * FROM Matrixone_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue MariaDB Microsoft Access github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-pinot.html", "content": "Apache Pinot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Pinot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Pinot This is the implementation of the Pinot data handler for MindsDB. Apache Pinot is a real-time distributed OLAP database designed for low-latency query execution even at extremely high throughput. Apache Pinot can ingest directly from streaming sources like Apache Kafka and make events available for querying immediately. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Pinot to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Pinot. ​ Implementation This handler was implemented using the pinotdb library, the Python DB-API and SQLAlchemy dialect for Pinot. The required arguments to establish a connection are as follows: host is the host name or IP address of the Apache Pinot cluster. broker_port is the port that the Broker of the Apache Pinot cluster is running on. controller_port is the port that the Controller of the Apache Pinot cluster is running on. path is the query path. ​ Usage In order to make use of this handler and connect to the Pinot cluster in MindsDB, the following syntax can be used: CREATE DATABASE pinot_datasource WITH engine = 'pinot' , parameters = { \"host\" : \"localhost\" , \"broker_port\" : 8000 , \"controller_port\" : 9000 , \"path\" : \"/query/sql\" , \"scheme\" : \"http\" } ; You can use this established connection to query your table as follows: SELECT * FROM pinot_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Apache Impala Apache Solr github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "starrocks.html", "content": "StarRocks - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases StarRocks Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases StarRocks This is the implementation of the StarRocks data handler for MindsDB. StarRocks is the next-generation data platform designed to make data-intensive real-time analytics fast and easy. It delivers query speeds 5 to 10 times faster than other popular solutions. StarRocks can perform real-time analytics well while updating historical records. It can also enhance real-time analytics with historical data from data lakes easily. With StarRocks, you can get rid of the de-normalized tables and get the best performance and flexibility. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect StarRocks to MindsDB, install the required dependencies following this instruction . Install or ensure access to StarRocks. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the StarRocks server in MindsDB, the following syntax can be used: CREATE DATABASE starrocks_datasource WITH ENGINE = 'starrocks' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"starrocks_user\" , \"password\" : \"password\" , \"port\" : 8030 , \"database\" : \"starrocks_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM starrocks_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue SQLite Supabase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "timescaledb.html", "content": "TimescaleDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TimescaleDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TimescaleDB This is the implementation of the TimescaleDB data handler for MindsDB. TimescaleDB is an open-source relational database that is optimized for time-series data. It is designed to handle large volumes of data. It enables you to query and analyze data in real-time. TimescaleDB can be used for a wide range of applications, including IoT, finance, and monitoring. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TimescaleDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to TimescaleDB. ​ Implementation This handler is implemented using the psycopg2 library, which is a PostgreSQL adapter for the Python programming language. TimescaleDB is built on top of PostgreSQL and therefore can be accessed using the same client libraries and APIs. The required arguments to establish a connection are as follows: * host is the the host name or IP address of the TimescaleDB server. * port is the port to use when connecting with the TimescaleDB server. * database is the database name to use when connecting with the TimescaleDB server. * user is the user to authenticate the user with the TimescaleDB server. * password is the password to authenticate the user with the TimescaleDB server. ​ Usage Before attempting to connect to a TimescaleDB server using MindsDB, ensure that it accepts incoming connections using this guide . In order to make use of this handler and connect to the TimescaleDB server in MindsDB, the following syntax can be used: CREATE DATABASE timescaledb_datasource WITH engine = 'timescaledb' , parameters = { \"host\" : \"examplehost.timescaledb.com\" , \"port\" : 5432 , \"user\" : \"example_user\" , \"password\" : \"my_password\" , \"database\" : \"tsdb\" } ; You can use this established connection to query your table as follows, SELECT * FROM timescaledb_datasource . sensor ; Was this page helpful? Yes No Suggest edits Raise issue TiDB Trino github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "tdengine.html", "content": "TDengine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases TDengine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases TDengine This is the implementation of the TDEngine data handler for MindsDB. TDengine is an open source, high-performance, cloud native time-series database optimized for Internet of Things (IoT), Connected Cars, and Industrial IoT. It enables efficient, real-time data ingestion, processing, and monitoring of TB and even PB scale data per day, generated by billions of sensors and data collectors. TDengine differentiates itself from other time-series databases with numerous advantages, such as high performance, simplified solution, cloud-native, ease of use, easy data analytics, and open-source. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect TDengine to MindsDB, install the required dependencies following this instruction . Install or ensure access to TDengine. ​ Implementation This handler is implemented using taos/taosrest , a Python library that allows you to use Python code to run SQL commands on the TDEngine server. The required arguments to establish a connection are as follows: user is the username associated with the server. password is the password to authenticate your access. url is the URL to the TDEngine server. For local server, the URL is localhost:6041 by default. token is the unique token provided while using TDEngine Cloud. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the TDEngine database in MindsDB, the following syntax can be used: CREATE DATABASE tdengine_datasource WITH ENGINE = 'tdengine' , PARAMETERS = { \"user\" : \"tdengine_user\" , \"password\" : \"password\" , \"url\" : \"localhost:6041\" , \"token\" : \"token\" , \"database\" : \"tdengine_db\" } ; You can specify token instead of user and password while using TDEngine. You can use this established connection to query your table as follows: SELECT * FROM tdengine_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue SurrealDB Teradata github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-druid.html", "content": "Apache Druid - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Druid Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Druid This is the implementation of the Druid data handler for MindsDB. Apache Druid is a real-time analytics database designed for fast slice-and-dice analytics ( OLAP queries) on large data sets. Most often, Druid powers use cases where real-time ingestion, fast query performance, and high uptime are important. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Druid to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Druid. ​ Implementation This handler was implemented using the pydruid library, the Python API for Apache Druid. The required arguments to establish a connection are as follows: host is the host name or IP address of the Apache Druid database. port is the port that Apache Druid is running on. path is the query path. scheme is the URI schema. This parameter is optional and defaults to http . user is the username used to authenticate with Apache Druid. This parameter is optional. password is the password used to authenticate with Apache Druid. This parameter is optional. ​ Usage In order to make use of this handler and connect to Apache Druid in MindsDB, the following syntax can be used: CREATE DATABASE druid_datasource WITH engine = 'druid' , parameters = { \"host\" : \"localhost\" , \"port\" : 8888 , \"path\" : \"/druid/v2/sql/\" , \"scheme\" : \"http\" } ; You can use this established connection to query your table as follows: SELECT * FROM druid_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Apache Cassandra Apache Hive github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "greptimedb.html", "content": "GreptimeDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases GreptimeDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases GreptimeDB This is the implementation of the GreptimeDB data handler for MindsDB. GreptimeDB is an open-source, cloud-native time series database features analytical capabilities, scalebility and open protocols support. ​ Implementation This handler is implemented by extending the MySQLHandler. Connect GreptimeDB to MindsDB by providing the following parameters: host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. user is the database user. password is the database password. There are several optional parameters that can be used as well. ssl is the ssl parameter value that indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores SSL certificates. ssl_key stores SSL keys. ​ Usage In order to make use of this handler and connect to the GreptimeDB database in MindsDB, the following syntax can be used: CREATE DATABASE greptimedb_datasource WITH engine = 'greptimedb' , parameters = { \"host\" : \"127.0.0.1\" , \"port\" : 4002 , \"database\" : \"public\" , \"user\" : \"username\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows. SELECT * FROM greptimedb_datasource . example_table ; Was this page helpful? Yes No Suggest edits Raise issue Google Sheets IBM Db2 github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Implementation Usage"}
{"file_name": "opengauss.html", "content": "OpenGauss - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OpenGauss Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OpenGauss This is the implementation of the OpenGauss data handler for MindsDB. OpenGauss is an open-source relational database management system released with the Mulan PSL v2 and the kernel built on Huawei’s years of experience in the database field. It continuously provides competitive features tailored to enterprise-grade scenarios. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OpenGauss to MindsDB, install the required dependencies following this instruction . Install or ensure access to OpenGauss. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the OpenGauss database in MindsDB, the following syntax can be used: CREATE DATABASE opengauss_datasource WITH ENGINE = 'opengauss' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"opengauss\" , \"user\" : \"mindsdb\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM opengauss_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue OceanBase Oracle github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "google-cloud-sql.html", "content": "Google Cloud SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Google Cloud SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Google Cloud SQL This is the implementation of the Google Cloud SQL data handler for MindsDB. Cloud SQL is a fully-managed database service that makes it easy to set up, maintain, manage, and administer your relational PostgreSQL, MySQL, and SQL Server databases in the cloud. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Google Cloud SQL to MindsDB, install the required dependencies following this instruction . Install or ensure access to Google Cloud SQL. ​ Implementation This handler was implemented using the existing MindsDB handlers for MySQL, PostgreSQL and SQL Server. The required arguments to establish a connection are, host : the host name or IP address of the Google Cloud SQL instance. port : the TCP/IP port of the Google Cloud SQL instance. user : the username used to authenticate with the Google Cloud SQL instance. password : the password to authenticate the user with the Google Cloud SQL instance. database : the database name to use when connecting with the Google Cloud SQL instance. db_engine : the database engine of the Google Cloud SQL instance. This can take one of three values: ‘mysql’, ‘postgresql’ or ‘mssql’. ​ Usage In order to make use of this handler and connect to the Google Cloud SQL instance, you need to create a datasource with the following syntax: CREATE DATABASE cloud_sql_mysql_datasource WITH ENGINE = 'cloud_sql' , PARAMETERS = { \"db_engine\" : \"mysql\" , \"host\" : \"53.170.61.16\" , \"port\" : 3306 , \"user\" : \"admin\" , \"password\" : \"password\" , \"database\" : \"example_db\" } ; To successfully connect to the Google Cloud SQL instance you have to make sure that the IP address of the machine you are using to connect is added to the authorized networks of the Google Cloud SQL instance. You can do this by following the steps below: Go to the Cloud SQL Instances page. Click on the instance you want to add authorized networks to. Click on the Connections tab. Click on Networking tab. Click on Add network . Enter the IP address of the machine you want to connect from. If you are using MindsDB cloud version you can use the following IP address: 18.220.205.95 3.19.152.46 52.14.91.162 You can use this established connection to query your table as follows: SELECT * FROM cloud_sql_mysql_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Google BigQuery Google Sheets github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ibm-db2.html", "content": "IBM Db2 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases IBM Db2 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases IBM Db2 This documentation describes the integration of MindsDB with IBM Db2 , the cloud-native database built to power low-latency transactions, real-time analytics and AI applications at scale. The integration allows MindsDB to access data stored in the IBM Db2 database and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect IBM Db2 to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your IBM Db2 database from MindsDB by executing the following SQL command: CREATE DATABASE db2_datasource WITH engine = 'db2' , parameters = { \"host\" : \"127.0.0.1\" , \"user\" : \"db2inst1\" , \"password\" : \"password\" , \"database\" : \"example_db\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the IBM Db2 database. user : The username for the IBM Db2 database. password : The password for the IBM Db2 database. database : The name of the IBM Db2 database to connect to. Optional connection parameters include the following: port : The port number for connecting to the IBM Db2 database. Default is 50000 . schema : The database schema to use within the IBM Db2 database. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM db2_datasource . schema_name . table_name LIMIT 10 ; Run IBM Db2 native queries directly on the connected database: SELECT * FROM db2_datasource ( --Native Query Goes Here WITH DINFO ( DEPTNO , AVGSALARY , EMPCOUNT ) AS ( SELECT OTHERS . WORKDEPT , AVG ( OTHERS . SALARY ) , COUNT ( * ) FROM EMPLOYEE OTHERS GROUP BY OTHERS . WORKDEPT ) , DINFOMAX AS ( SELECT MAX ( AVGSALARY ) AS AVGMAX FROM DINFO ) SELECT THIS_EMP . EMPNO , THIS_EMP . SALARY , DINFO . AVGSALARY , DINFO . EMPCOUNT , DINFOMAX . AVGMAX FROM EMPLOYEE THIS_EMP , DINFO , DINFOMAX WHERE THIS_EMP . JOB = 'SALESREP' AND THIS_EMP . WORKDEPT = DINFO . DEPTNO ) ; The above examples utilize db2_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the IBM Db2 database. Checklist : Make sure the IBM Db2 database is active. Confirm that host, user, password and database are correct. Try a direct connection using a client like DBeaver. Ensure a stable network between MindsDB and the IBM Db2 database. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` This guide of common connection Db2 connection issues provided by IBM might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue GreptimeDB IBM Informix github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "monetdb.html", "content": "MonetDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MonetDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MonetDB This is the implementation of the MonetDB data handler for MindsDB. MonetDB is an open-source column-oriented relational database management system originally developed at the Centrum Wiskunde & Informatica in the Netherlands. It is designed to provide high performance on complex queries against large databases, such as combining tables with hundreds of columns and millions of rows. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MonetDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to MonetDB. ​ Implementation This handler is implemented using pymonetdb , a Python library that allows you to use Python code to run SQL commands on the MonetDB database. The required arguments to establish a connection are as follows: user is the username associated with the database. password is the password to authenticate your access. host is the host name or IP address. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema_name is the schema name to get tables. It is optional and defaults to the current schema if not provided. ​ Usage In order to make use of this handler and connect to the MonetDB database in MindsDB, the following syntax can be used: CREATE DATABASE monetdb_datasource WITH engine = 'monetdb' , parameters = { \"user\" : \"monetdb\" , \"password\" : \"monetdb\" , \"host\" : \"127.0.0.1\" , \"port\" : 50000 , \"schema_name\" : \"sys\" , \"database\" : \"demo\" } ; You can use this established connection to query your table as follows: SELECT * FROM monetdb_datasource . demo ; Was this page helpful? Yes No Suggest edits Raise issue Microsoft SQL Server MongoDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "ibm-informix.html", "content": "IBM Informix - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases IBM Informix Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases IBM Informix This is the implementation of the IBM Informix data handler for MindsDB. IBM Informix is a product family within IBM’s Information Management division that is centered on several relational database management system (RDBMS) offerings. The Informix server supports object–relational models and (through extensions) data types that are not a part of the SQL standard. The most widely used of these are the JSON, BSON, time series, and spatial extensions, which provide both data type support and language extensions that permit high-performance domain-specific queries and efficient storage for data sets based on semi-structured, time series, and spatial data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect IBM Informix to MindsDB, install the required dependencies following this instruction . Install or ensure access to IBM Informix. ​ Implementation This handler is implemented using IfxPy/IfxPyDbi , a Python library that allows you to use Python code to run SQL commands on the Informix database. The required arguments to establish a connection are as follows: user is the username associated with database. password is the password to authenticate your access. host is the hostname or IP address of the server. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema_name is the schema name to get tables. server is the name of server you want connect. logging_enabled defines whether logging is enabled or not. Defaults to True if not provided. ​ Usage In order to make use of this handler and connect to the Informix database in MindsDB, the following syntax can be used: CREATE DATABASE informix_datasource WITH engine = 'informix' , parameters = { \"server\" : \"server\" , \"host\" : \"127.0.0.1\" , \"port\" : 9091 , \"user\" : \"informix\" , \"password\" : \"in4mix\" , \"database\" : \"stores_demo\" , \"schema_name\" : \"love\" , \"loging_enabled\" : False } ; You can use this established connection to query your table as follows: SELECT * FROM informix_datasource . items ; This integration uses IfxPy . As it is in development stage, it can be install using pip install IfxPy . However, it doesn’t work with higher versions of Python, therefore, you have to build it from source. On Linux This code downloads and extracts the onedb-ODBC driver used to make connection: cd $HOME mkdir Informix cd Informix mkdir -p home/informix/cli wget https://hcl-onedb.github.io/odbc/OneDB-Linux64-ODBC-Driver.tar sudo tar xvf OneDB-Linux64-ODBC-Driver.tar -C ./home/informix/cli rm OneDB-Linux64-ODBC-Driver.tar Add enviroment variables in the .bashrc file: export INFORMIXDIR = $HOME /Informix/home/informix/cli/onedb-odbc-driver export LD_LIBRARY_PATH = ${LD_LIBRARY_PATH} ${INFORMIXDIR} /lib: ${INFORMIXDIR} /lib/esql: ${INFORMIXDIR} /lib/cli This code clones the IfxPy repo, builds a wheel, and installs it: pip install wheel mkdir Temp cd Temp git clone https://github.com/OpenInformix/IfxPy.git cd IfxPy/IfxPy python setup.py bdist_wheel pip install --find-links = ./dist IfxPy cd .. cd .. cd .. rm -rf Temp On Windows This code downloads and extracts the onedb-ODBC driver used to make connection: cd $HOME mkdir Informix cd Informix mkdir /home/informix/cli wget https://hcl-onedb.github.io/odbc/OneDB-Win64-ODBC-Driver.zip tar xvf OneDB-Win64-ODBC-Driver.zip -C ./home/informix/cli del OneDB-Win64-ODBC-Driver.zip Add an enviroment variable: set INFORMIXDIR = $HOME /Informix/home/informix/cli/onedb-odbc-driver Add %INFORMIXDIR%\\bin to the PATH environment variable. This code clones the IfxPy repo, builds a wheel, and installs it: pip install wheel mkdir Temp cd Temp git clone https://github.com/OpenInformix/IfxPy.git cd IfxPy/IfxPy python setup.py bdist_wheel pip install --find-links = ./dist IfxPy cd .. cd .. cd .. rmdir Temp Was this page helpful? Yes No Suggest edits Raise issue IBM Db2 InfluxDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-redshift.html", "content": "Amazon Redshift - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon Redshift Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon Redshift This documentation describes the integration of MindsDB with Amazon Redshift , a fully managed, petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more, enabling you to use your data to acquire new insights for your business and customers. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Redshift to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Redshift database from MindsDB by executing the following SQL command: CREATE DATABASE redshift_datasource WITH engine = 'redshift' , parameters = { \"host\" : \"examplecluster.abc123xyz789.us-west-1.redshift.amazonaws.com\" , \"port\" : 5439 , \"database\" : \"example_db\" , \"user\" : \"awsuser\" , \"password\" : \"my_password\" } ; Required connection parameters include the following: host : The host name or IP address of the Redshift cluster. port : The port to use when connecting with the Redshift cluster. database : The database name to use when connecting with the Redshift cluster. user : The username to authenticate the user with the Redshift cluster. password : The password to authenticate the user with the Redshift cluster. Optional connection parameters include the following: schema : The database schema to use. Default is public. sslmode : The SSL mode for the connection. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM redshift_datasource . schema_name . table_name LIMIT 10 ; Run Amazon Redshift SQL queries directly on the connected Redshift database: SELECT * FROM redshift_datasource ( --Native Query Goes Here WITH VENUECOPY AS ( SELECT * FROM VENUE ) SELECT * FROM VENUECOPY ORDER BY 1 LIMIT 10 ; ) ; The above examples utilize redshift_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon Redshift cluster. Checklist : Make sure the Redshift cluster is active. Confirm that host, port, user, password and database are correct. Try a direct Redshift connection using a client like DBeaver. Ensure that the security settings of the Redshift cluster allow connections from MindsDB. Ensure a stable network between MindsDB and Redshift. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` This troubleshooting guide provided by AWS might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue Amazon DynamoDB Amazon S3 github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "clickhouse.html", "content": "ClickHouse - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ClickHouse Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ClickHouse This documentation describes the integration of MindsDB with ClickHouse , a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). The integration allows MindsDB to access data from ClickHouse and enhance ClickHouse with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ClickHouse to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to ClickHouse from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE clickhouse_conn WITH ENGINE = 'clickhouse' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : \"8443\" , \"user\" : \"root\" , \"password\" : \"mypass\" , \"database\" : \"test_data\" , \"protocol\" : \"https\" } Required connection parameters include the following: host : is the hostname or IP address of the ClickHouse server. port : is the TCP/IP port of the ClickHouse server. user : is the username used to authenticate with the ClickHouse server. password : is the password to authenticate the user with the ClickHouse server. database : defaults to default . It is the database name to use when connecting with the ClickHouse server. protocol : defaults to native . It is an optional parameter. Its supported values are native , http and https . ​ Usage The following usage examples utilize the connection to ClickHouse made via the CREATE DATABASE statement and named clickhouse_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM clickhouse_conn . table_name LIMIT 10 ; ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the ClickHouse database. Checklist : Ensure that the ClickHouse server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the ClickHouse server. Slow Connection Initialization Symptoms : Connecting to the ClickHouse server takes an exceptionally long time, or connections hang without completing Checklist : Ensure that you are using the appropriate protocol (http, https, or native) for your ClickHouse setup. Misconfigurations here can lead to significant delays. Ensure that firewalls or security groups (in cloud environments) are properly configured to allow traffic on the necessary ports (as 8123 for HTTP or 9000 for native). SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Ckan Cloud Spanner github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "databend.html", "content": "Databend - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Databend Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Databend This is the implementation of the Databend data handler for MindsDB. Databend is a modern cloud data warehouse that empowers your object storage for real-time analytics. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Databend to MindsDB, install the required dependencies following this instruction . Install or ensure access to Databend. ​ Implementation This handler is implemented by extending the ClickHouse handler. The required arguments to establish a connection are as follows: protocol is the protocol to query Databend. Supported values include native , http , https . It defaults to native if not provided. host is the host name or IP address of the Databend warehouse. port is the TCP/IP port of the Databend warehouse. user is the username used to authenticate with the Databend warehouse. password is the password to authenticate the user with the Databend warehouse. database is the database name to use when connecting with the Databend warehouse. ​ Usage In order to make use of this handler and connect to the Databend database in MindsDB, the following syntax can be used: CREATE DATABASE databend_datasource WITH engine = 'databend' , parameters = { \"protocol\" : \"https\" , \"user\" : \"root\" , \"port\" : 443 , \"password\" : \"password\" , \"host\" : \"some-url.aws-us-east-2.default.databend.com\" , \"database\" : \"test_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM databend_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue D0lt Databricks github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "oracle.html", "content": "Oracle - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Oracle Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Oracle This documentation describes the integration of MindsDB with Oracle , one of the most trusted and widely used relational database engines for storing, organizing and retrieving data by type while still maintaining relationships between the various types. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Oracle to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Oracle database from MindsDB by executing the following SQL command: CREATE DATABASE oracle_datasource WITH ENGINE = 'oracle' , PARAMETERS = { \"host\" : \"localhost\" , \"service_name\" : \"FREEPDB1\" , \"user\" : \"SYSTEM\" , \"password\" : \"password\" } ; Required connection parameters include the following: user : The username for the Oracle database. password : The password for the Oracle database. dsn : The data source name (DSN) for the Oracle database. OR host : The hostname, IP address, or URL of the Oracle server. AND sid : The system identifier (SID) of the Oracle database. OR service_name : The service name of the Oracle database. Optional connection parameters include the following: port : The port number for connecting to the Oracle database. Default is 1521. disable_oob : The boolean parameter to disable out-of-band breaks. Default is false . auth_mode : The authorization mode to use. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM oracle_datasource . schema_name . table_name LIMIT 10 ; Run PL/SQL queries directly on the connected Oracle database: SELECT * FROM oracle_datasource ( --Native Query Goes Here SELECT employee_id , first_name , last_name , email , hire_date FROM oracle_datasource . hr . employees WHERE department_id = 10 ORDER BY hire_date DESC ; ) ; The above examples utilize oracle_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Oracle database. Checklist : Make sure the Oracle database is active. Confirm that the connection parameters provided (DSN, host, SID, service_name) and the credentials (user, password) are correct. Ensure a stable network between MindsDB and Oracle. This troubleshooting guide provided by Oracle might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue OpenGauss OrioleDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "datastax.html", "content": "DataStax - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases DataStax Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases DataStax This is the implementation of the DataStax data handler for MindsDB. DataStax Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect DataStax to MindsDB, install the required dependencies following this instruction . Install or ensure access to DataStax. ​ Implementation DataStax Astra DB is API-compatible with Apache Cassandra and ScyllaDB. Therefore, its implementation extends the ScyllaDB handler and is using the scylla-driver Python library. The required arguments to establish a connection are as follows: user is the user to authenticate. password is the password to authenticate the user. secure_connect_bundle is the path to the secure_connect_bundle zip file. ​ Usage In order to make use of this handler and connect to the Astra DB database in MindsDB, the following syntax can be used: CREATE DATABASE astra_connection WITH engine = \"astra\" , parameters = { \"user\" : \"user\" , \"password\" : \"pass\" , \"secure_connect_bundle\" : \"/home/Downloads/file.zip\" } ; or, reference the bundle from Datastax s3 as: CREATE DATABASE astra_connection WITH ENGINE = \"astra\" , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"pass\" , \"secure_connect_bundle\" : \"https://datastax-cluster-config-prod.s3.us-east-2.amazonaws.com/32312-b9eb-4e09-a641-213eaesa12-1/secure-connect-demo.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AK...\" } You can use this established connection to query your table as follows: SELECT * FROM astra_connection . keystore . example_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue Databricks DuckDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "apache-ignite.html", "content": "Apache Ignite - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Ignite Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Ignite This is the implementation of the Apache Ignite data handler for MindsDB. Apache Ignite is a distributed database for high-performance computing with in-memory speed. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Ignite to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Ignite. ​ Implementation This handler is implemented using the pyignite library, the Apache Ignite thin (binary protocol) client for Python. The required arguments to establish a connection are as follows: host is the host name or IP address of the Apache Ignite cluster’s node. port is the TCP/IP port of the Apache Ignite cluster’s node. Must be an integer. There are several optional arguments that can be used as well, username is the username used to authenticate with the Apache Ignite cluster. This parameter is optional. Default: None. password is the password to authenticate the user with the Apache Ignite cluster. This parameter is optional. Default: None. schema is the schema to use for the connection to the Apache Ignite cluster. This parameter is optional. Default: PUBLIC. ​ Usage In order to make use of this handler and connect to an Apache Ignite database in MindsDB, the following syntax can be used: CREATE DATABASE ignite_datasource WITH ENGINE = 'ignite' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 10800 , \"username\" : \"admin\" , \"password\" : \"password\" , \"schema\" : \"example_schema\" } ; You can use this established connection to query your table as follows: SELECT * FROM ignite_datasource . demo_table LIMIT 10 ; Currently, a connection can be established only to a single node in the cluster. In the future, we’ll configure the client to automatically fail over to another node if the connection to the current node fails or times out by providing the hosts and ports for many nodes as explained here . Was this page helpful? Yes No Suggest edits Raise issue Apache Hive Apache Impala github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "cockroachdb.html", "content": "CockroachDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases CockroachDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases CockroachDB This is the implementation of the CockroachDB data handler for MindsDB. CockroachDB was architected for complex, high performant distributed writes and delivers scale-out read capability. CockroachDB delivers simple relational SQL transactions and obscures complexity away from developers. It is wire-compatible with PostgreSQL and provides a familiar and easy interface for developers. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect CockroachDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to CockroachDB. ​ Implementation CockroachDB is wire-compatible with PostgreSQL. Therefore, its implementation extends the PostgreSQL handler. The required arguments to establish a connection are as follows: host is the host name or IP address of the CockroachDB. database is the name of the database to connect to. user is the user to authenticate with the CockroachDB. port is the port to use when connecting. password is the password to authenticate the user. In order to make use of this handler and connect to the CockroachDB server in MindsDB, the following syntax can be used: CREATE DATABASE cockroachdb WITH engine = 'cockroachdb' , parameters = { \"host\" : \"localhost\" , \"database\" : \"dbname\" , \"user\" : \"admin\" , \"password\" : \"password\" , \"port\" : \"5432\" } ; ​ Usage You can use this established connection to query your table as follows: SELECT * FROM cockroachdb . public . db ; Was this page helpful? Yes No Suggest edits Raise issue Cloud Spanner Couchbase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-s3.html", "content": "Amazon S3 - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon S3 Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon S3 This documentation describes the integration of MindsDB with Amazon S3 , an object storage service that offers industry-leading scalability, data availability, security, and performance. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure that MindsDB is installed locally via Docker or Docker Desktop . ​ Connection Establish a connection to your Amazon S3 bucket from MindsDB by executing the following SQL command: CREATE DATABASE s3_datasource WITH engine = 's3' , parameters = { \"aws_access_key_id\" : \"AQAXEQK89OX07YS34OP\" , \"aws_secret_access_key\" : \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" , \"bucket\" : \"my-bucket\" } ; Note that sample parameter values are provided here for reference, and you should replace them with your connection parameters. Required connection parameters include the following: aws_access_key_id : The AWS access key that identifies the user or IAM role. aws_secret_access_key : The AWS secret access key that identifies the user or IAM role. Optional connection parameters include the following: aws_session_token : The AWS session token that identifies the user or IAM role. This becomes necessary when using temporary security credentials. bucket : The name of the Amazon S3 bucket. If not provided, all available buckets can be queried, however, this can affect performance, especially when listing all of the available objects. ​ Usage Retrieve data from a specified object (file) in a S3 bucket by providing the integration name and the object key: SELECT * FROM s3_datasource . ` my-file.csv ` ; LIMIT 10 ; If a bucket name is provided in the CREATE DATABASE command, querying will be limited to that bucket and the bucket name can be ommitted from the object key as shown in the example above. However, if the bucket name is not provided, the object key must include the bucket name, such as s3_datasource. my-bucket/my-folder/my-file.csv`. Wrap the object key in backticks (`) to avoid any issues parsing the SQL statements provided. This is especially important when the object key contains spaces, special characters or prefixes, such as my-folder/my-file.csv . At the moment, the supported file formats are CSV, TSV, JSON, and Parquet. The above examples utilize s3_datasource as the datasource name, which is defined in the CREATE DATABASE command. The special files table can be used to list all objects available in the specified bucket or all buckets if the bucket name is not provided: SELECT * FROM s3_datasource . files LIMIT 10 The content of files can also be retrieved by explicitly requesting the content column. This column is empty by default to avoid unnecessary data transfer: SELECT path , content FROM s3_datasource . files LIMIT 10 This table will return all objects regardless of the file format, however, only the supported file formats mentioned above can be queried. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon S3 bucket. Checklist : Make sure the Amazon S3 bucket exists. Confirm that provided AWS credentials are correct. Try making a direct connection to the S3 bucket using the AWS CLI. Ensure a stable network between MindsDB and AWS. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing object names containing spaces, special characters or prefixes. Checklist : Ensure object names with spaces, special characters or prefixes are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel/travel_data.csv Incorrect: SELECT * FROM integration.‘travel/travel_data.csv’ Correct: SELECT * FROM integration.`travel/travel_data.csv` Was this page helpful? Yes No Suggest edits Raise issue Amazon Redshift Apache Cassandra github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "sqlite.html", "content": "SQLite - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SQLite Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SQLite This is the implementation of the SQLite data handler for MindsDB. SQLite is an in-process library that implements a self-contained, serverless, zero-configuration, transactional SQL database engine. The code for SQLite is in the public domain and is thus free to use for either commercial or private purpose. SQLite is the most widely deployed database in the world with more applications than we can count, including several high-profile projects. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SQLite to MindsDB, install the required dependencies following this instruction . Install or ensure access to SQLite. ​ Implementation This handler is implemented using the standard sqlite3 library that comes with Python. The only required argument to establish a connection is db_file that points to the database file that the connection is to be made to. Optionally, this may also be set to :memory: to create an in-memory database. ​ Usage In order to make use of this handler and connect to the SQLite database in MindsDB, the following syntax can be used: CREATE DATABASE sqlite_datasource WITH engine = 'sqlite' , parameters = { \"db_file\" : \"example.db\" } ; You can use this established connection to query your table as follows: SELECT * FROM sqlite_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue Snowflake StarRocks github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "edgelessdb.html", "content": "EdgelessDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases EdgelessDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases EdgelessDB This is the implementation of the EdgelessDB data handler for MindsDB. Edgeless is a full SQL database, tailor-made for confidential computing. It seamlessly integrates with your existing tools and workflows to help you unlock the full potential of your data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect EdgelessDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to EdgelessDB. ​ Implementation This handler is implemented by extending the MySQL connector. The required arguments to establish a connection are as follows: host : the host name of the EdgelessDB connection port : the port to use when connecting user : the user to authenticate password : the password to authenticate the user database : database name To use the full potensial of EdgelessDB, you can also specify the following arguments: ssl : whether to use SSL or not ssl_ca : path or url to the CA certificate ssl_cert : path or url to the client certificate ssl_key : path or url to the client key ​ Usage In order to use EdgelessDB as a data source in MindsDB, you need to use the following syntax: CREATE DATABASE edgelessdb_datasource WITH ENGINE = \"EdgelessDB\" , PARAMETERS = { \"user\" : \"root\" , \"password\" : \"test123@!Aabvhj\" , \"host\" : \"localhost\" , \"port\" : 3306 , \"database\" : \"test_schema\" } Or you can use the following syntax: CREATE DATABASE edgelessdb_datasource2 WITH ENGINE = \"EdgelessDB\" , PARAMETERS = { \"user\" : \"root\" , \"password\" : \"test123@!Aabvhj\" , \"host\" : \"localhost\" , \"port\" : 3306 , \"database\" : \"test_schema\" , \"ssl_cert\" : \"/home/marios/demo/cert.pem\" , \"ssl_key\" : \"/home/marios/demo/key.pem\" } You can use this established connection to query your table as follows: SELECT * FROM edgelessdb_datasource . table_name Was this page helpful? Yes No Suggest edits Raise issue DuckDB ElasticSearch github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "singlestore.html", "content": "SingleStore - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SingleStore Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SingleStore This is the implementation of the SingleStore data handler for MindsDB. SingleStore is a proprietary, cloud-native database designed for data-intensive applications. A distributed, relational, SQL database management system that features ANSI SQL support. It is known for speed in data ingest, transaction processing, and query processing. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SingleStore to MindsDB, install the required dependencies following this instruction . Install or ensure access to SingleStore. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. There are several optional arguments that can be used as well. ssl is the ssl parameter value that indicates whether SSL is enabled ( True ) or disabled ( False ). ssl_ca is the SSL Certificate Authority. ssl_cert stores SSL certificates. ssl_key stores SSL keys. ​ Usage In order to make use of this handler and connect to the SingleStore database in MindsDB, the following syntax can be used: CREATE DATABASE singlestore_datasource WITH ENGINE = 'singlestore' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"database\" : \"singlestore\" , \"user\" : \"root\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM singlestore_datasource . example_table ; Was this page helpful? Yes No Suggest edits Raise issue ScyllaDB Snowflake github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "postgresql.html", "content": "PostgreSQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases PostgreSQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases PostgreSQL This documentation describes the integration of MindsDB with PostgreSQL , a powerful, open-source, object-relational database system. The integration allows MindsDB to access data stored in the PostgreSQL database and enhance PostgreSQL with AI capabilities. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect PostgreSQL to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your PostgreSQL database from MindsDB by executing the following SQL command: CREATE DATABASE postgresql_conn WITH ENGINE = 'postgres' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"postgres\" , \"user\" : \"postgres\" , \"schema\" : \"data\" , \"password\" : \"password\" } ; Required connection parameters include the following: user : The username for the PostgreSQL database. password : The password for the PostgreSQL database. host : The hostname, IP address, or URL of the PostgreSQL server. port : The port number for connecting to the PostgreSQL server. database : The name of the PostgreSQL database to connect to. Optional connection parameters include the following: schema : The database schema to use. Default is public. sslmode : The SSL mode for the connection. ​ Usage The following usage examples utilize the connection to PostgreSQL made via the CREATE DATABASE statement and named postgresql_conn . Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM postgresql_conn . table_name LIMIT 10 ; Run PostgreSQL-native queries directly on the connected PostgreSQL database: SELECT * FROM postgresql_conn ( --Native Query Goes Here SELECT model , COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price FROM used_car_price ) ; Next Steps Follow this tutorial to see more use case examples. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the PostgreSQL database. Checklist : Make sure the PostgreSQL server is active. Confirm that host, port, user, schema, and password are correct. Try a direct PostgreSQL connection. Ensure a stable network between MindsDB and PostgreSQL. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue PlanetScale QuestDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "apache-impala.html", "content": "Apache Impala - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Impala Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Impala This is the implementation of the Impala data handler for MindsDB. Apache Impala is an MPP (Massive Parallel Processing) SQL query engine for processing huge volumes of data that is stored in the Apache Hadoop cluster. It is an open source software written in C++ and Java. It provides high performance and low latency compared to other SQL engines for Hadoop. In other words, Impala is the highest performing SQL engine (giving RDBMS-like experience) that provides the fastest way to access data stored in Hadoop Distributed File System. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Impala to MindsDB, install the required dependencies following this instruction . Install or ensure access to Apache Impala. ​ Implementation This handler is implemented using impyla , a Python library that allows you to use Python code to run SQL commands on Impala. The required arguments to establish a connection are: user is the username associated with the database. password is the password to authenticate your access. host is the server IP address or hostname. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. ​ Usage In order to make use of this handler and connect to the Impala database in MindsDB, the following syntax can be used: CREATE DATABASE impala_datasource WITH engine = 'impala' , parameters = { \"user\" : \"root\" , \"password\" : \"p@55w0rd\" , \"host\" : \"127.0.0.1\" , \"port\" : 21050 , \"database\" : \"Db_NamE\" } ; You can use this established connection to query your table as follows: SELECT * FROM impala_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue Apache Ignite Apache Pinot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "couchbase.html", "content": "Couchbase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Couchbase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Couchbase This is the implementation of the Couchbase data handler for MindsDB. Couchbase is an open-source, distributed multi-model NoSQL document-oriented database software package optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating, and presenting data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Couchbase to MindsDB, install the required dependencies following this instruction . Install or ensure access to Couchbase. ​ Implementation This handler is implemented using the couchbase library, the Python driver for Couchbase. The required arguments to establish a connection are as follows: connection_string : the connection string for the endpoint of the Couchbase server bucket : the bucket name to use when connecting with the Couchbase server user : the user to authenticate with the Couchbase server password : the password to authenticate the user with the Couchbase server scope : scopes are a level of data organization within a bucket. If omitted, will default to _default Note: The connection string expects either the couchbases:// or couchbase:// protocol. If you are using Couchbase Capella, you can find the connection_string under the Connect tab It will also be required to whitelist the machine(s) that will be running MindsDB and database credentials will need to be created for the user. These steps can also be taken under the Connect tab. In order to make use of this handler and connect to a Couchbase server in MindsDB, the following syntax can be used. Note, that the example uses the default travel-sample bucket which can be enabled from the couchbase UI with pre-defined scope and documents. CREATE DATABASE couchbase_datasource WITH engine = 'couchbase' , parameters = { \"connection_string\" : \"couchbase://localhost\" , \"bucket\" : \"travel-sample\" , \"user\" : \"admin\" , \"password\" : \"password\" , \"scope\" : \"inventory\" } ; ​ Usage Now, you can use this established connection to query your database as follows: SELECT * FROM couchbase_datasource . airport Was this page helpful? Yes No Suggest edits Raise issue CockroachDB CrateDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "amazon-dynamodb.html", "content": "Amazon DynamoDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Amazon DynamoDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Amazon DynamoDB This documentation describes the integration of MindsDB with Amazon DynamoDB , a serverless, NoSQL database service that enables you to develop modern applications at any scale. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure that MindsDB is installed locally via Docker or Docker Desktop . ​ Connection Establish a connection to your Amazon DynamoDB from MindsDB by executing the following SQL command: CREATE DATABASE dynamodb_datasource WITH engine = 'dynamodb' , parameters = { \"aws_access_key_id\" : \"PCAQ2LJDOSWLNSQKOCPW\" , \"aws_secret_access_key\" : \"U/VjewPlNopsDmmwItl34r2neyC6WhZpUiip57i\" , \"region_name\" : \"us-east-1\" } ; Required connection parameters include the following: aws_access_key_id : The AWS access key that identifies the user or IAM role. aws_secret_access_key : The AWS secret access key that identifies the user or IAM role. region_name : The AWS region to connect to. Optional connection parameters include the following: aws_session_token : The AWS session token that identifies the user or IAM role. This becomes necessary when using temporary security credentials. ​ Usage Retrieve data from a specified table by providing the integration name and the table name: SELECT * FROM dynamodb_datasource . table_name LIMIT 10 ; Indexes can also be queried by adding a third-level namespace: SELECT * FROM dynamodb_datasource . table_name . index_name LIMIT 10 ; The queries issued to Amazon DynamoDB are in PartiQL, a SQL-compatible query language for Amazon DynamoDB. For more information, refer to the PartiQL documentation . There are a few limitations to keep in mind when querying data from Amazon DynamoDB (some of which are specific to PartiQL): The LIMIT , GROUP BY and HAVING clauses are not supported in PartiQL SELECT statements. Furthermore, subqueries and joins are not supported either. Refer to the PartiQL documentation for SELECT statements for more information. INSERT statements are not supported by this integration. However, this can be overcome by issuing a ‘native query’ via an established connection. An example of this is provided below. Run PartiQL queries directly on Amazon DynamoDB: SELECT * FROM dynamodb_datasource ( --Native Query Goes Here INSERT INTO \"Music\" value { 'Artist' : 'Acme Band1' , 'SongTitle' : 'PartiQL Rocks' } ) ; The above examples utilize dynamodb_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Amazon S3 DynamoDB. Checklist : Confirm that provided AWS credentials are correct. Try making a direct connection to the Amazon DynamoDB using the AWS CLI. Ensure a stable network between MindsDB and AWS. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue Amazon Aurora Amazon Redshift github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "google-bigquery.html", "content": "Google BigQuery - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Google BigQuery Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Google BigQuery This documentation describes the integration of MindsDB with Google BigQuery , a fully managed, AI-ready data analytics platform that helps you maximize value from your data. The integration allows MindsDB to access data stored in the BigQuery warehouse and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect BigQuery to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your BigQuery warehouse from MindsDB by executing the following SQL command: CREATE DATABASE bigquery_datasource WITH engine = \"bigquery\" , parameters = { \"project_id\" : \"bgtest-1111\" , \"dataset\" : \"mydataset\" , \"service_account_keys\" : \"/tmp/keys.json\" } ; Required connection parameters include the following: project_id : The globally unique identifier for your project in Google Cloud where BigQuery is located. dataset : The default dataset to connect to. Optional connection parameters include the following: service_account_keys : The full path to the service account key file. service_account_json : The content of a JSON file defined by the service_account_keys parameter. One of service_account_keys or service_account_json has to be provided to establish a connection to BigQuery. ​ Usage Retrieve data from a specified table in the default dataset by providing the integration name and table name: SELECT * FROM bigquery_datasource . table_name LIMIT 10 ; Retrieve data from a specified table in a different dataset by providing the integration name, dataset name and table name: SELECT * FROM bigquery_datasource . dataset_name . table_name LIMIT 10 ; Run SQL in any supported BigQuery dialect directly on the connected BigQuery database: SELECT * FROM bigquery_datasource ( --Native Query Goes Here SELECT * FROM t1 WHERE t1 . a IN ( SELECT t2 . a FROM t2 FOR SYSTEM_TIME AS OF t1 . timestamp_column ) ; ) ; The above examples utilize bigquery_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the BigQuery warehouse. Checklist : Make sure that the Google Cloud account is active and the Google BigQuery service is enabled. Confirm that the project ID, dataset and service account credentials are correct. Try a direct BigQuery connection using a client like DBeaver. Ensure a stable network between MindsDB and Google BigQuery. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: _ Incorrect: SELECT _ FROM integration.travel data _ Incorrect: SELECT _ FROM integration.‘travel data’ _ Correct: SELECT _ FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue Firebird Google Cloud SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "databricks.html", "content": "Databricks - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Databricks Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Databricks This documentation describes the integration of MindsDB with Databricks , the world’s first data intelligence platform powered by generative AI. The integration allows MindsDB to access data stored in a Databricks workspace and enhance it with AI capabilities. This data source integration is thread-safe, utilizing a connection pool where each thread is assigned its own connection. When handling requests in parallel, threads retrieve connections from the pool as needed. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Databricks to MindsDB, install the required dependencies following this instruction . If the Databricks cluster you are attempting to connect to is terminated, executing the queries given below will attempt to start the cluster and therefore, the first query may take a few minutes to execute. To avoid any delays, ensure that the Databricks cluster is running before executing the queries. ​ Connection Establish a connection to your Databricks workspace from MindsDB by executing the following SQL command: CREATE DATABASE databricks_datasource WITH engine = 'databricks' , parameters = { \"server_hostname\" : \"adb-1234567890123456.7.azuredatabricks.net\" , \"http_path\" : \"sql/protocolv1/o/1234567890123456/1234-567890-test123\" , \"access_token\" : \"dapi1234567890ab1cde2f3ab456c7d89efa\" , \"schema\" : \"example_db\" } ; Required connection parameters include the following: server_hostname : The server hostname for the cluster or SQL warehouse. http_path : The HTTP path of the cluster or SQL warehouse. access_token : A Databricks personal access token for the workspace. Refer the instructions given https://docs.databricks.com/en/integrations/compute-details.html and https://docs.databricks.com/en/dev-tools/python-sql-connector.html#authentication to find the connection parameters mentioned above for your compute resource. Optional connection parameters include the following: session_configuration : Additional (key, value) pairs to set as Spark session configuration parameters. This should be provided as a JSON string. http_headers : Additional (key, value) pairs to set in HTTP headers on every RPC request the client makes. This should be provided as a JSON string. catalog : The catalog to use for the connection. Default is hive_metastore . schema : The schema (database) to use for the connection. Default is default . ​ Usage Retrieve data from a specified table by providing the integration name, catalog, schema, and table name: SELECT * FROM databricks_datasource . catalog_name . schema_name . table_name LIMIT 10 ; The catalog and schema names only need to be provided if the table to be queried is not in the specified (or default) catalog and schema. Run Databricks SQL queries directly on the connected Databricks workspace: SELECT * FROM databricks_datasource ( --Native Query Goes Here SELECT city , car_model , RANK ( ) OVER ( PARTITION BY car_model ORDER BY quantity ) AS rank FROM dealer QUALIFY rank = 1 ; ) ; The above examples utilize databricks_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Databricks workspace. Checklist : Make sure the Databricks workspace is active. Confirm that server hostname, HTTP path, access token are correctly provided. If the catalog and schema are provided, ensure they are correct as well. Ensure a stable network between MindsDB and Databricks workspace. SQL statements running against tables (of reasonable size) are taking longer than expected. Symptoms : SQL queries taking longer than expected to execute. Checklist : Ensure the Databricks cluster is running before executing the queries. Check the network connection between MindsDB and Databricks workspace. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` Was this page helpful? Yes No Suggest edits Raise issue Databend DataStax github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "apache-hive.html", "content": "Apache Hive - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Apache Hive Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Apache Hive This documentation describes the integration of MindsDB with Apache Hive , a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. The integration allows MindsDB to access data from Apache Hive and enhance Apache Hive with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Apache Hive to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Apache Hive from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE hive_datasource WITH engine = 'hive' , parameters = { \"username\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"default\" } ; Required connection parameters include the following: host : The hostname, IP address, or URL of the Apache Hive server. database : The name of the Apache Hive database to connect to. Optional connection parameters include the following: username : The username for the Apache Hive database. password : The password for the Apache Hive database. port : The port number for connecting to the Apache Hive server. Default is 10000 . auth : The authentication mechanism to use. Default is CUSTOM . Other options are NONE , NOSASL , KERBEROS and LDAP . ​ Usage Retrieve data from a specified table by providing the integration and table names: SELECT * FROM hive_datasource . table_name LIMIT 10 ; Run HiveQL queries directly on the connected Apache Hive database: SELECT * FROM hive_datasource ( --Native Query Goes Here FROM ( FROM ( FROM src SELECT TRANSFORM ( value ) USING 'mapper' AS value , count ) mapped SELECT cast ( value as double ) AS value , cast ( count as int ) AS count SORT BY value , count ) sorted SELECT TRANSFORM ( value , count ) USING 'reducer' AS whatever ) ; The above examples utilize hive_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the Apache Hive database. Checklist : Ensure that the Apache Hive server is running and accessible Confirm that host, port, user, and password are correct. Try a direct Apache Hive connection using a client like DBeaver. Test the network connection between the MindsDB host and the Apache Hive server. Was this page helpful? Yes No Suggest edits Raise issue Apache Druid Apache Ignite github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "vertica.html", "content": "Vertica - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Vertica Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Vertica This is the implementation of the Vertica data handler for MindsDB. The column-oriented Vertica Analytics Platform was designed to manage large, fast-growing volumes of data and with fast query performance for data warehouses and other query-intensive applications. The product claims to greatly improve query performance over traditional relational database systems, and to provide high availability and exabyte scalability on commodity enterprise servers. Vertica runs on multiple cloud computing systems as well as on Hadoop nodes. Vertica’s Eon Mode separates compute from storage, using S3 object storage and dynamic allocation of compute notes. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Vertica to MindsDB, install the required dependencies following this instruction . Install or ensure access to Vertica. ​ Implementation This handler is implemented using vertica-python , a Python library that allows you to use Python code to run SQL commands on the Vertica database. The required arguments to establish a connection are as follows: user is the username asscociated with the database. password is the password to authenticate your access. host is the host name or IP address of the server. port is the port through which TCP/IP connection is to be made. database is the database name to be connected. schema is the schema name to get tables from. ​ Usage In order to make use of this handler and connect to the Vertica database in MindsDB, the following syntax can be used: CREATE DATABASE vertica_datasource WITH engine = 'vertica' , parameters = { \"user\" : \"dbadmin\" , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"port\" : 5433 , \"schema_name\" : \"public\" , \"database\" : \"VMart\" } ; You can use this established connection to query your table as follows: SELECT * FROM vertica_datasource . TEST ; Was this page helpful? Yes No Suggest edits Raise issue Trino Vitess github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "mariadb.html", "content": "MariaDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases MariaDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases MariaDB This documentation describes the integration of MindsDB with MariaDB , one of the most popular open source relational databases. The integration allows MindsDB to access data from MariaDB and enhance MariaDB with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MariaDB to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to MariaDB from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE mariadb_conn WITH ENGINE = 'mariadb' , PARAMETERS = { \"host\" : \"host-name\" , \"port\" : 3307 , \"database\" : \"db-name\" , \"user\" : \"user-name\" , \"password\" : \"password\" } ; Or: CREATE DATABASE mariadb_conn WITH ENGINE = 'mariadb' , PARAMETERS = { \"url\" : \"mariadb://user-name@host-name:3307\" } ; Required connection parameters include the following: user : The username for the MariaDB database. password : The password for the MariaDB database. host : The hostname, IP address, or URL of the MariaDB server. port : The port number for connecting to the MariaDB server. database : The name of the MariaDB database to connect to. Or: url : You can specify a connection to MariaDB Server using a URI-like string, as an alternative connection option. You can also use mysql:// as the protocol prefix Optional connection parameters include the following: ssl : Boolean parameter that indicates whether SSL encryption is enabled for the connection. Set to True to enable SSL and enhance connection security, or set to False to use the default non-encrypted connection. ssl_ca : Specifies the path to the Certificate Authority (CA) file in PEM format. ssl_cert : Specifies the path to the SSL certificate file. This certificate should be signed by a trusted CA specified in the ssl_ca file or be a self-signed certificate trusted by the server. ssl_key : Specifies the path to the private key file (in PEM format). ​ Usage The following usage examples utilize the connection to MariaDB made via the CREATE DATABASE statement and named mariadb_conn . Retrieve data from a specified table by providing the integration and table name. SELECT * FROM mariadb_conn . table_name LIMIT 10 ; ​ Troubleshooting Database Connection Error Symptoms : Failure to connect MindsDB with the MariaDB database. Checklist : Ensure that the MariaDB server is running and accessible Confirm that host, port, user, and password are correct. Try a direct MySQL connection. Test the network connection between the MindsDB host and the MariaDB server. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces, reserved words or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` Was this page helpful? Yes No Suggest edits Raise issue InfluxDB MatrixOne github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting"}
{"file_name": "sap-sql-anywhere.html", "content": "SAP SQL Anywhere - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases SAP SQL Anywhere Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases SAP SQL Anywhere This is the implementation of the SAP SQL Anywhere data handler for MindsDB. SAP SQL Anywhere is an embedded database for application software that enables secure and reliable data management for servers where no DBA is available and synchronization for tens of thousands of mobile devices, Internet of Things (IoT) systems, and remote environments. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect SAP SQL Anywhere to MindsDB, install the required dependencies following this instruction . Install or ensure access to SAP SQL Anywhere. ​ Implementation This handler is implemented using sqlanydb , the Python driver for SAP SQL Anywhere. The required arguments to establish a connection are as follows: host is the host name or IP address of the SAP SQL Anywhere instance. port is the port number of the SAP SQL Anywhere instance. user specifies the user name. password specifies the password for the user. database sets the current database. server sets the current server. ​ Usage You can use the below SQL statements to create a table in SAP SQL Anywhere called TEST . CREATE TABLE TEST ( ID INTEGER NOT NULL , NAME NVARCHAR ( 1 ) , DESCRIPTION NVARCHAR ( 1 ) ) ; CREATE UNIQUE INDEX TEST_ID_INDEX ON TEST ( ID ) ; ALTER TABLE TEST ADD CONSTRAINT TEST_PK PRIMARY KEY ( ID ) ; INSERT INTO TEST VALUES ( 1 , 'h' , 'w' ) ; In order to make use of this handler and connect to the SAP SQL Anywhere database in MindsDB, the following syntax can be used: CREATE DATABASE sap_sqlany_trial WITH ENGINE = 'sqlany' , PARAMETERS = { \"user\" : \"DBADMIN\" , \"password\" : \"password\" , \"host\" : \"localhost\" , \"port\" : \"55505\" , \"server\" : \"TestMe\" , \"database\" : \"MINDSDB\" } ; You can use this established connection to query your table as follows: SELECT * FROM sap_sqlany_trial . test ; On execution, we get: ID NAME DESCRIPTION 1 h w Was this page helpful? Yes No Suggest edits Raise issue SAP HANA ScyllaDB github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "snowflake.html", "content": "Snowflake - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Snowflake Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Snowflake This documentation describes the integration of MindsDB with Snowflake , a cloud data warehouse used to store and analyze data. The integration allows MindsDB to access data stored in the Snowflake database and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Snowflake to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to your Snowflake database from MindsDB by executing the following SQL command: CREATE DATABASE snowflake_datasource WITH ENGINE = 'snowflake' , PARAMETERS = { \"account\" : \"tvuibdy-vm85921\" , \"user\" : \"user\" , \"password\" : \"password\" , \"database\" : \"test_db\" } ; Required connection parameters include the following: account : The Snowflake account identifier. This guide will help you find your account identifier. user : The username for the Snowflake account. password : The password for the Snowflake account. database : The name of the Snowflake database to connect to. Optional connection parameters include the following: warehouse : The Snowflake warehouse to use for running queries. schema : The database schema to use within the Snowflake database. Default is PUBLIC . role : The Snowflake role to use. This video presents how to connect to Snowflake and query the available tables. ​ Usage Retrieve data from a specified table by providing the integration name, schema, and table name: SELECT * FROM snowflake_datasource . schema_name . table_name LIMIT 10 ; Run Snowflake SQL queries directly on the connected Snowflake database: SELECT * FROM snowflake_datasource ( --Native Query Goes Here SELECT employee_table . * EXCLUDE department_id , department_table . * RENAME department_name AS department FROM employee_table INNER JOIN department_table ON employee_table . department_id = department_table . department_id ORDER BY department , last_name , first_name ; ) ; The above examples utilize snowflake_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Snowflake account. Checklist : Make sure the Snowflake is active. Confirm that account, user, password and database are correct. Try a direct Snowflake connection using a client like DBeaver. Ensure a stable network between MindsDB and Snowflake. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing table names containing spaces or special characters. Checklist : Ensure table names with spaces or special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel data Incorrect: SELECT * FROM integration.‘travel data’ Correct: SELECT * FROM integration.`travel data` This troubleshooting guide provided by Snowflake might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue SingleStore SQLite github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "planetscale.html", "content": "PlanetScale - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases PlanetScale Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases PlanetScale This is the implementation of the PlanetScale data handler for MindsDB. PlanetScale is a MySQL-compatible, serverless database platform. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect PlanetScale to MindsDB, install the required dependencies following this instruction . Install or ensure access to PlanetScale. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the PlanetScale database in MindsDB, the following syntax can be used: CREATE DATABASE planetscale_datasource WITH ENGINE = 'planet_scale' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 3306 , \"user\" : \"planetscale_user\" , \"password\" : \"password\" , \"database\" : \"planetscale_db\" } ; You can use this established connection to query your table as follows: SELECT * FROM planetscale_datasource . my_table ; Was this page helpful? Yes No Suggest edits Raise issue OrioleDB PostgreSQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "questdb.html", "content": "QuestDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases QuestDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases QuestDB This is the implementation of the QuestDB data handler for MindsDB. QuestDB is a columnar time-series database with high performance ingestion and SQL analytics. It is open-source and available on the cloud. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect QuestDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to QuestDB. ​ Implementation This handler is implemented by extending the PostgreSQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. public stores a value of True or False . Defaults to True if left blank. ​ Usage In order to make use of this handler and connect to the QuestDB server in MindsDB, the following syntax can be used: CREATE DATABASE questdb_datasource WITH ENGINE = 'questdb' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 8812 , \"database\" : \"qdb\" , \"user\" : \"admin\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM questdb_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue PostgreSQL SAP HANA github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "oceanbase.html", "content": "OceanBase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases OceanBase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases OceanBase This is the implementation of the OceanBase data handler for MindsDB. OceanBase is a distributed relational database. It is the only distributed database in the world that has broken both TPC-C and TPC-H records. OceanBase adopts an independently developed integrated architecture, which encompasses both the scalability of a distributed architecture and the performance advantage of a centralized architecture. It supports hybrid transaction/analytical processing (HTAP) with one engine. Its features include strong data consistency, high availability, high performance, online scalability, high compatibility with SQL and mainstream relational databases, transparency to applications, and a high cost/performance ratio. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect OceanBase to MindsDB, install the required dependencies following this instruction . Install or ensure access to OceanBase. ​ Implementation This handler is implemented by extending the MySQL data handler. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. database is the database name. ​ Usage In order to make use of this handler and connect to the OceanBase server in MindsDB, the following syntax can be used: CREATE DATABASE oceanbase_datasource WITH ENGINE = 'oceanbase' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"user\" : \"oceanbase_user\" , \"password\" : \"password\" , \"port\" : 2881 , \"database\" : \"oceanbase_db\" } ; Now, you can use this established connection to query your database as follows: SELECT * FROM oceanbase_datasource . demo_table LIMIT 10 ; Was this page helpful? Yes No Suggest edits Raise issue MySQL OpenGauss github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "elasticsearch.html", "content": "ElasticSearch - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases ElasticSearch Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases ElasticSearch This documentation describes the integration of MindsDB with ElasticSearch , a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents.. The integration allows MindsDB to access data from ElasticSearch and enhance ElasticSearch with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ElasticSearch to MindsDB, install the required dependencies following this instruction . Install or ensure access to ElasticSearch. ​ Connection Establish a connection to ElasticSearch from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE elasticsearch_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { 'cloud_id' : 'xyz' , -- optional, if hosts are provided 'hosts' : 'https://xyz.xyz.gcp.cloud.es.io:123' , -- optional, if cloud_id is provided 'api_key' : 'xyz' , -- optional, if user and password are provided 'user' : 'elastic' , -- optional, if api_key is provided 'password' : 'xyz' -- optional, if api_key is provided } ; The connection parameters include the following: cloud_id : The Cloud ID provided with the ElasticSearch deployment. Required only when hosts is not provided. hosts : The ElasticSearch endpoint provided with the ElasticSearch deployment. Required only when cloud_id is not provided. api_key : The API key that you generated for the ElasticSearch deployment. Required only when user and password are not provided. user and password : The user and password used to authenticate. Required only when api_key is not provided. If you want to connect to the local instance of ElasticSearch, use the below statement: CREATE DATABASE elasticsearch_datasource WITH ENGINE = 'elasticsearch' , PARAMETERS = { \"hosts\" : \"127.0.0.1:9200\" , \"user\" : \"user\" , \"password\" : \"password\" } ; Required connection parameters include the following (at least one of these parameters should be provided): hosts : The IP address and port where ElasticSearch is deployed. user : The user used to autheticate access. password : The password used to autheticate access. ​ Usage Retrieve data from a specified index by providing the integration name and index name: SELECT * FROM elasticsearch_datasource . my_index LIMIT 10 ; The above examples utilize elasticsearch_datasource as the datasource name, which is defined in the CREATE DATABASE command. At the moment, the Elasticsearch SQL API has certain limitations that have an impact on the queries that can be issued via MindsDB. The most notable of these limitations are listed below: Only SELECT queries are supported at the moment. Array fields are not supported. Nested fields cannot be queried directly. However, they can be accessed using the . operator. For a detailed guide on the limitations of the Elasticsearch SQL API, refer to the official documentation . ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with the Elasticsearch server. Checklist : Make sure the Elasticsearch server is active. Confirm that server, cloud ID and credentials are correct. Ensure a stable network between MindsDB and Elasticsearch. Transport Error or Request Error Symptoms : Errors related to the issuing of unsupported queries to Elasticsearch. Checklist : Ensure the query is a SELECT query. Avoid querying array fields. Access nested fields using the . operator. Refer to the official documentation for more information if needed. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing index names containing special characters. Checklist : Ensure table names with special characters are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel-data Incorrect: SELECT * FROM integration.‘travel-data’ Correct: SELECT * FROM integration.`travel-data` This troubleshooting guide provided by Elasticsearch might also be helpful. Was this page helpful? Yes No Suggest edits Raise issue EdgelessDB Firebird github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "trino.html", "content": "Trino - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Trino Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Trino This is the implementation of the Trino data handler for MindsDB. Trino is an open-source distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Trino to MindsDB, install the required dependencies following this instruction . Install or ensure access to Trino. ​ Implementation This handler is implemented using pyhive , a collection of Python DB-API and SQLAlchemy interfaces for Presto and Hive. The required arguments to establish a connection are as follows: user is the database user. password is the database password. host is the host name, IP address, or URL. port is the port used to make TCP/IP connection. There are some optional arguments as follows: auth is the authentication method. Currently, only basic is supported. http_scheme takes the value of http by default. It can be set to https as well. catalog is the catalog. schema is the schema name. with defines default WITH-clause (properties) for ALL tables. This parameter is experimental and might be changed or removed in future release. ​ Usage In order to make use of this handler and connect to the Trino database in MindsDB, the following syntax can be used: CREATE DATABASE trino_datasource WITH ENGINE = 'trino' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 443 , \"auth\" : \"basic\" , \"http_scheme\" : \"https\" , \"user\" : \"trino\" , \"password\" : \"password\" , \"catalog\" : \"default\" , \"schema\" : \"test\" , \"with\" : \"with (transactional = true)\" } ; You can use this established connection to query your table as follows: SELECT * FROM trino_datasource . demo_table ; Was this page helpful? Yes No Suggest edits Raise issue TimescaleDB Vertica github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "firebird.html", "content": "Firebird - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Databases Firebird Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Supported Integrations Airtable Amazon Aurora Amazon DynamoDB Amazon Redshift Amazon S3 Apache Cassandra Apache Druid Apache Hive Apache Ignite Apache Impala Apache Pinot Apache Solr Ckan ClickHouse Cloud Spanner CockroachDB Couchbase CrateDB D0lt Databend Databricks DataStax DuckDB EdgelessDB ElasticSearch Firebird Google BigQuery Google Cloud SQL Google Sheets GreptimeDB IBM Db2 IBM Informix InfluxDB MariaDB MatrixOne Microsoft Access Microsoft SQL Server MonetDB MongoDB MySQL OceanBase OpenGauss Oracle OrioleDB PlanetScale PostgreSQL QuestDB SAP HANA SAP SQL Anywhere ScyllaDB SingleStore Snowflake SQLite StarRocks Supabase SurrealDB TDengine Teradata TiDB TimescaleDB Trino Vertica Vitess YugabyteDB Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Databases Firebird This is the implementation of the Firebird data handler for MindsDB. Firebird is a relational database offering many ANSI SQL standard features that runs on Linux, Windows, and a variety of Unix platforms. Firebird offers excellent concurrency, high performance, and powerful language support for stored procedures and triggers. It has been used in production systems, under a variety of names, since 1981. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Firebird to MindsDB, install the required dependencies following this instruction . Install or ensure access to Firebird. ​ Implementation This handler is implemented using the fdb library, the Python driver for Firebird. The required arguments to establish a connection are as follows: host is the host name or IP address of the Firebird server. database is the port to use when connecting with the Firebird server. user is the username to authenticate the user with the Firebird server. password is the password to authenticate the user with the Firebird server. ​ Usage In order to make use of this handler and connect to the Firebird server in MindsDB, the following syntax can be used: CREATE DATABASE firebird_datasource WITH engine = 'firebird' , parameters = { \"host\" : \"localhost\" , \"database\" : \"C:\\Users\\minura\\Documents\\mindsdb\\example.fdb\" , \"user\" : \"sysdba\" , \"password\" : \"password\" } ; You can use this established connection to query your table as follows: SELECT * FROM firebird_datasource . example_tbl ; Was this page helpful? Yes No Suggest edits Raise issue ElasticSearch Google BigQuery github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage"}
{"file_name": "dockerhub.html", "content": "Docker Hub - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Docker Hub Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Docker Hub In this section, we present how to connect Docker Hub repository to MindsDB. Docker Hub is the world’s easiest way to create, manage, and deliver your team’s container applications. Data from Docker Hub can be utilized within MindsDB to train models and make predictions about Docker Hub repositories. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Docker Hub to MindsDB, install the required dependencies following this instruction . Install or ensure access to Docker Hub. ​ Connection This handler is implemented using the requests library that makes http calls to https://docs.docker.com/docker-hub/api/latest/#tag/resources . The required arguments to establish a connection are as follows: username : Username used to login to DockerHub. password : Password used to login to DockerHub. Read about creating an account here . Here is how to connect to Docker Hub using MindsDB: CREATE DATABASE dockerhub_datasource WITH ENGINE = 'dockerhub' , PARAMETERS = { \"username\" : \"username\" , \"password\" : \"password\" } ; ​ Usage Now, you can query Docker Hub as follows: SELECT * FROM dockerhub_datasource . repo_images_summary WHERE namespace = \"docker\" AND repository = \"trusted-registry-nginx\" ; Both the namespace and repository parameters are required in the WHERE clause. Was this page helpful? Yes No Suggest edits Raise issue Confluence Email github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "microsoft-onedrive.html", "content": "Microsoft One Drive - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Microsoft One Drive Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Microsoft One Drive This documentation describes the integration of MindsDB with Microsoft OneDrive , a cloud storage service that lets you back up, access, edit, share, and sync your files from any device. ​ Prerequisites Before proceeding, ensure that MindsDB is installed locally via Docker or Docker Desktop . Register an application in the Azure portal . Navigate to the Azure Portal and sign in with your Microsoft account. Locate the Microsoft Entra ID service and click on it. Click on App registrations and then click on New registration . Enter a name for your application and select the Accounts in this organizational directory only option for the Supported account types field. Keep the Redirect URI field empty and click on Register . Click on API permissions and then click on Add a permission . Select Microsoft Graph and then click on Delegated permissions . Search for the Files.Read permission and select it. Click on Add permissions . Request an administrator to grant consent for the above permissions. If you are the administrator, click on Grant admin consent for [your organization] and then click on Yes . Copy the Application (client) ID and record it as the client_id parameter, and copy the Directory (tenant) ID and record it as the tenant_id parameter. Click on Certificates & secrets and then click on New client secret . Enter a description for your client secret and select an expiration period. Click on Add and copy the generated client secret and record it as the client_secret parameter. Click on Authentication and then click on Add a platform . Select Web and enter URL where MindsDB has been deployed followed by /verify-auth in the Redirect URIs field. For example, if you are running MindsDB locally (on https://localhost:47334 ), enter https://localhost:47334/verify-auth in the Redirect URIs field. ​ Connection Establish a connection to Microsoft OneDrive from MindsDB by executing the following SQL command: CREATE DATABASE one_drive_datasource WITH engine = 'one_drive' , parameters = { \"client_id\" : \"12345678-90ab-cdef-1234-567890abcdef\" , \"client_secret\" : \"abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx\" , \"tenant_id\" : \"abcdef12-3456-7890-abcd-ef1234567890\" , } ; Note that sample parameter values are provided here for reference, and you should replace them with your connection parameters. Required connection parameters include the following: client_id : The client ID of the registered application. client_secret : The client secret of the registered application. tenant_id : The tenant ID of the registered application. ​ Usage Retrieve data from a specified file in Microsoft OneDrive by providing the integration name and the file name: SELECT * FROM one_drive_datasource . ` my-file.csv ` ; LIMIT 10 ; Wrap the object key in backticks (`) to avoid any issues parsing the SQL statements provided. This is especially important when the file name contains spaces, special characters or prefixes, such as my-folder/my-file.csv . At the moment, the supported file formats are CSV, TSV, JSON, and Parquet. The above examples utilize one_drive_datasource as the datasource name, which is defined in the CREATE DATABASE command. The special files table can be used to list the files available in Microsoft OneDrive: SELECT * FROM one_drive_datasource . files LIMIT 10 The content of files can also be retrieved by explicitly requesting the content column. This column is empty by default to avoid unnecessary data transfer: SELECT path , content FROM one_drive_datasource . files LIMIT 10 This table will return all objects regardless of the file format, however, only the supported file formats mentioned above can be queried. ​ Troubleshooting Guide Database Connection Error Symptoms : Failure to connect MindsDB with Microsoft OneDrive. Checklist : Ensure the client_id , client_secret and tenant_id parameters are correctly provided. Ensure the registered application has the required permissions. Ensure the generated client secret is not expired. SQL statement cannot be parsed by mindsdb_sql Symptoms : SQL queries failing or not recognizing object names containing spaces, special characters or prefixes. Checklist : Ensure object names with spaces, special characters or prefixes are enclosed in backticks. Examples: Incorrect: SELECT * FROM integration.travel/travel_data.csv Incorrect: SELECT * FROM integration.‘travel/travel_data.csv’ Correct: SELECT * FROM integration.`travel/travel_data.csv` Was this page helpful? Yes No Suggest edits Raise issue MediaWiki Microsoft Teams github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Troubleshooting Guide"}
{"file_name": "slack.html", "content": "Slack - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Slack Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Slack This documentation describes the integration of MindsDB with Slack , a cloud-based collaboration platform. The integration allows MindsDB to access data from Slack and enhance Slack with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Slack to MindsDB, install the required dependencies following this instruction . Install or ensure access to Slack. ​ Connection Establish a connection to Slack from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE slack_datasource WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"values\" , -- required parameter \"app_token\" : \"values\" -- optional parameter } ; The Slack handler is initialized with the following parameters: token is a Slack bot token to use for authentication. app_token is a Slack app token to use for authentication. Please note that app_token is an optional parameter. Without providing it, you need to integrate an app into a Slack channel. ​ Method 1: Chatbot responds in direct messages to a Slack app One way to connect Slack is to use both bot and app tokens. By following the instructions below, you’ll set up the Slack app and be able to message this Slack app directly to chat with the bot. If you want to use Slack in the CREATE CHATBOT syntax, use this method of connecting Slack to MindsDB. Set up a Slack app and generate tokens Here is how to set up a Slack app and generate both a Slack bot token and a Slack app token: Follow this link and sign in with your Slack account. Create a new app From scratch or select an existing app. Please note that the following instructions support apps created From scratch . For apps created From an app manifest , please follow the Slack docs here . Go to Basic Information under Settings . Under App-Level Tokens , click on Generate Token and Scopes . Name the token socket and add the connections:write scope. Copy and save the xapp-... token - you’ll need it to publish the chatbot. Go to Socket Mode under Settings and toggle the button to Enable Socket Mode . Go to OAuth & Permissions under Features . Add the following Bot Token Scopes : app_mentions:read channels:history channels:read chat:write groups:history groups:read (optional) im:history im:read im:write mpim:read (optional) users.profile:read users:read (optional) In the OAuth Tokens for Your Workspace section, click on Install to Workspace and then Allow . Copy and save the xoxb-... token - you’ll need it to publish the chatbot. Go to App Home under Features and click on the checkbox to Allow users to send Slash commands and messages from the messages tab . Go to Event Subscriptions under Features . Toggle the button to Enable Events . Under Subscribe to bot events , click on Add Bot User Event and add app_mention and message.im . Click on Save Changes . Now you can use tokens from points 3 and 5 to initialize the Slack handler in MindsDB. This connection method enables you to chat directly with an app via Slack. Alternatively, you can connect an app to the Slack channel: Go to the channel where you want to use the bot. Right-click on the channel and select View Channel Details . Select Integrations . Click on Add an App . Here is how to connect Slack to MindsDB: CREATE DATABASE slack_datasource WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" , \"app_token\" : \"xapp-...\" } ; It comes with the conversations and messages tables. ​ Method 2: Chatbot responds on a defined Slack channel Another way to connect to Slack is to use the bot token only. By following the instructions below, you’ll set up the Slack app and integrate it into one of the channels from which you can directly chat with the bot. Set up a Slack app and generate tokens Here is how to set up a Slack app and generate a Slack bot token: Follow this link and sign in with your Slack account. Create a new app From scratch or select an existing app. Please note that the following instructions support apps created From scratch . For apps created From an app manifest , please follow the Slack docs here . Go to the OAuth & Permissions section. Under the Scopes section, add the Bot Token Scopes necessary for your application. You can add more later as well. channels:history channels:read chat:write groups:read im:read mpim:read users:read Install the bot in your workspace. Under the OAuth Tokens for Your Workspace section, copy the the Bot User OAuth Token value. Open your Slack application and add the App/Bot to one of the channels: Go to the channel where you want to use the bot. Right-click on the channel and select View Channel Details . Select Integrations . Click on Add an App . Now you can use the token from step 6 to initialize the Slack handler in MindsDB and use the channel name to query and write messages. Here is how to connect Slack to MindsDB: CREATE DATABASE slack_datasource WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" } ; ​ Usage The following usage applies when Connection Method 2 was used to connect Slack. See the usage for Connection Method 1 via the CREATE CHATBOT syntax . Retrieve data from a specified table by providing the integration and table names: SELECT * FROM slack_datasource . table_name LIMIT 10 ; ​ Supported Tables The Slack integration supports the following tables: ​ conversations Table The conversations virtual table is used to query conversations (channels, DMs, and groups) in the connected Slack workspace. -- Retrieve all conversations in the workspace SELECT * FROM slack_datasource . conversations ; -- Retrieve a specific conversation using its ID SELECT * FROM slack_datasource . conversations WHERE channel_id = \"<channel-id>\" ; -- Retrieve a specific conversation using its name SELECT * FROM slack_datasource . conversations WHERE name = \"<channel-name>\" ; ​ messages Table The messages virtual table is used to query, post, update, and delete messages in specific conversations within the connected Slack workspace. -- Retrieve all messages from a specific conversation -- channel_id is a required parameter and can be found in the conversations table SELECT * FROM slack_datasource . messages WHERE channel_id = \"<channel-id>\" ; -- Post a new message -- channel_id and text are required parameters INSERT INTO slack_datasource . messages ( channel_id , text ) VALUES ( \"<channel-id>\" , \"Hello from SQL!\" ) ; -- Update a bot-posted message -- channel_id, ts, and text are required parameters UPDATE slack_datasource . messages SET text = \"Updated message content\" WHERE channel_id = \"<channel-id>\" AND ts = \"<timestamp>\" ; -- Delete a bot-posted message -- channel_id and ts are required parameters DELETE FROM slack_datasource . messages WHERE channel_id = \"<channel-id>\" AND ts = \"<timestamp>\" ; You can also find the channel ID by right-clicking on the conversation in Slack, selecting ‘View conversation details’ or ‘View channel details,’ and copying the channel ID from the bottom of the ‘About’ tab. ​ threads Table The threads virtual table is used to query and post messages in threads within the connected Slack workspace. -- Retrieve all messages in a specific thread -- channel_id and thread_ts are required parameters -- thread_ts is the timestamp of the parent message and can be found in the messages table SELECT * FROM slack_datasource . threads WHERE channel_id = \"<channel-id>\" AND thread_ts = \"<thread-ts>\" ; -- Post a message to a thread INSERT INTO slack_datasource . threads ( channel_id , thread_ts , text ) VALUES ( \"<channel-id>\" , \"<thread-ts>\" , \"Replying to the thread!\" ) ; ​ users Table The users virtual table is used to query user information in the connected Slack workspace. -- Retrieve all users in the workspace SELECT * FROM slack_datasource . users ; -- Retrieve a specific user by name SELECT * FROM slack_datasource . users WHERE name = \"John Doe\" ; ​ Rate Limit Considerations The Slack API enforces rate limits on data retrieval. Therefore, when querying the above tables, by default, the first 1000 (999 for messages ) records are returned. To retrieve more records, use the LIMIT clause in your SQL queries. For example: SELECT * FROM slack_datasource . conversations LIMIT 2000 ; When using the LIMIT clause to query additional records, you may encounter Slack API rate limits. ​ Next Steps Follow this tutorial to build an AI agent with MindsDB. Was this page helpful? Yes No Suggest edits Raise issue Shopify Strapi github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Method 1: Chatbot responds in direct messages to a Slack app Method 2: Chatbot responds on a defined Slack channel Usage Supported Tables conversations Table messages Table threads Table users Table Rate Limit Considerations Next Steps"}
{"file_name": "google-calendar.html", "content": "Google Calendar - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Google Calendar Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Google Calendar In this section, we present how to connect Google Calendar to MindsDB. Google Calendar is an online calendar service and application developed by Google. It allows users to create, manage, and share events and appointments, as well as schedule and organize their personal, work, or team activities. Data from Google Calendar can be utilized within MindsDB to train AI models, make predictions, and automate time management with AI. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Google Calendar to MindsDB, install the required dependencies following this instruction . Install or ensure access to Google Calendar. ​ Connection The required arguments to establish a connection are as follows: credentials_file is a path to the JSON file that stores credentials to the Google account. Please note that a Google account with enabled Google Calendar is required. You can find more information here . In order to make use of this handler and connect the Google Calendar app to MindsDB, the following syntax can be used: CREATE DATABASE my_calendar WITH ENGINE = 'google_calendar' , PARAMETERS = { 'credentials_file' : '\\path-to-your-file\\credentials.json' } ; You need a Google account in order to use this integration. Here is how to get the credentials file: Create a Google Cloud Platform (GCP) Project: 1.1 Go to the GCP Console ( https://console.cloud.google.com/ ). 1.2 If you haven’t created a project before, you’ll be prompted to do so now. 1.3 Give your new project a name. 1.4 Click Create to create the new project. Enable the Google Calendar API: 2.1 In the GCP Console, select your project. 2.2 Navigate to APIs & Services > Library . 2.3 In the search bar, search for Google Calendar API . 2.4 Click on Google Calendar API , then click Enable . Create credentials for the Google Calendar API : 3.1 Navigate to APIs & Services > Credentials . 3.2 Click on the Create Credentials button and choose OAuth client ID . 3.3 If you haven’t configured the OAuth consent screen before, you’ll be prompted to do so now. Make sure to choose External for User Type, and add all the necessary scopes. Make sure to save the changes. Now, create the OAuth client ID. Choose Desktop app for the Application Type and give it a name. 3.4 Click Create . Download the JSON file: 4.1 After creating your credentials, click the download button (an icon of an arrow pointing down) on the right side of your client ID. This will download a JSON file, so you will use the location to it in the credentials_file param. ​ Usage This creates a database that comes with the calendar table. Now you can use your Google Calendar data, like this: searching for events: SELECT id , created_at , author_username , text FROM my_calendar . events WHERE start_time = '2023-02-16' AND end_time = '2023-04-09' LIMIT 20 ; creating events: INSERT INTO my_calendar . events ( start_time , end_time , summary , description , location , attendees , reminders , timeZone ) VALUES ( '2023-02-16 10:00:00' , '2023-02-16 11:00:00' , 'MindsDB Meeting' , 'Discussing the future of MindsDB' , 'MindsDB HQ' , '' , 'Europe/Athens' ) ; updating one or more events: UPDATE my_calendar . events SET summary = 'MindsDB Meeting' , description = 'Discussing the future of MindsDB' , location = 'MindsDB HQ' , attendees = '' , reminders = '' WHERE event_id > 1 AND event_id < 10 ; -- used to update events in a given range deleting one or more events: DELETE FROM my_calendar . events WHERE id = '1' ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Gmail Google Analytics github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "paypal.html", "content": "PayPal - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications PayPal Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications PayPal In this section, we present how to connect PayPal to MindsDB. PayPal is an online payment system that makes paying for things online and sending and receiving money safe and secure. Data from PayPal can be utilized within MindsDB to train models and make predictions about your transactions. ​ Connection This handler is implemented using PayPal-Python-SDK , the Python SDK for PayPal RESTful APIs. The required arguments to establish a connection are as follows: mode : The mode of the PayPal API. Can be sandbox or live . client_id : The client ID of the PayPal API. client_secret : The client secret of the PayPal API. To connect to PayPal using MindsDB, the following CREATE DATABASE statement can be used: CREATE DATABASE paypal_datasource WITH ENGINE = 'paypal' , PARAMETERS = { \"mode\" : \"your-paypal-mode\" , \"client_id\" : \"your-paypal-client-id\" , \"client_secret\" : \"your-paypal-client-secret\" } ; Check out this guide on how to create client credentials for PayPal. ​ Usage Now, you can query PayPal as follows: Payments: SELECT * FROM paypal_datasource . payments Invoices: SELECT * FROM paypal_datasource . invoices Subscriptions: SELECT * FROM paypal_datasource . subscriptions Orders: SELECT * FROM paypal_datasource . orders Payouts: SELECT * FROM paypal_datasource . payouts You can also run more advanced queries on your data: Payments: SELECT intent , cart FROM paypal_datasource . payments WHERE state = 'approved' ORDER BY id LIMIT 5 Invoices: SELECT invoice_number , total_amount FROM paypal_datasource . invoices WHERE status = 'PAID' ORDER BY total_amount DESC LIMIT 5 Subscriptions: SELECT id , state , name FROM paypal_datasource . subscriptions WHERE state = \"CREATED\" LIMIT 5 Orders: SELECT id , state , amount FROM paypal_datasource . orders WHERE state = 'APPROVED' ORDER BY total_amount DESC LIMIT 5 Payouts: SELECT payout_batch_id , amount_currency , amount_value FROM paypal_datasource . payouts ORDER BY amount_value DESC LIMIT 5 ​ Supported Tables The following tables are supported by the PayPal handler: payments : payments made. invoices : invoices created. subscriptions : subscriptions created. orders : orders created. payouts : payouts made. Was this page helpful? Yes No Suggest edits Raise issue News API Plaid github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage Supported Tables"}
{"file_name": "intercom.html", "content": "Intercom - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Intercom Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Intercom Intercom is a software company that provides customer messaging and engagement tools for businesses. They offer products and services for customer support, marketing, and sales, allowing companies to communicate with their customers through various channels like chat, email, and more. ​ Connection To get started with the Intercom API, you need to initialize the API handler with the required access token for authentication. You can do this as follows: access_token : Your Intercom access token for authentication. Check out this guide on how to get the intercom access token in order to access Intercom data. To create a database using the Intercom engine, you can use a SQL-like syntax as shown below: CREATE DATABASE myintercom WITH ENGINE = 'intercom' , PARAMETERS = { \"access_token\" : \"your-intercom-access-token\" } ; ​ Usage You can retrieve data from Intercom using a SELECT statement. For example: SELECT * FROM myintercom . articles ; You can filter data based on specific criteria using a WHERE clause. Here’s an example: SELECT * FROM myintercom . articles WHERE id = < article - id > ; To create a new article in Intercom, you can use the INSERT statement. Here’s an example: INSERT INTO myintercom . articles ( title , description , body , author_id , state , parent_id , parent_type ) VALUES ( 'Thanks for everything' , 'Description of the Article' , 'Body of the Article' , 6840572 , 'published' , 6801839 , 'collection' ) ; You can update existing records in Intercom using the UPDATE statement. For instance: UPDATE myintercom . articles SET title = 'Christmas is here!' , body = '<p>New gifts in store for the jolly season</p>' WHERE id = < article - id > ; Was this page helpful? Yes No Suggest edits Raise issue Instatus MediaWiki github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage"}
{"file_name": "sendinblue.html", "content": "Sendinblue - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Sendinblue Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Sendinblue In this section, we present how to connect Sendinblue to MindsDB. Brevo (formerly Sendinblue) is an all-in-one platform to automate your marketing campaigns over Email, SMS, WhatsApp or chat. Data from Sendinblue can be used to understand the impact of email marketing. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Sendinblue to MindsDB, install the required dependencies following this instruction . Install or ensure access to Sendinblue. ​ Connection This handler is implemented using the sib-api-v3-sdk library, a Python library that wraps Sendinblue APIs. The required arguments to establish a connection are as follows: api_key : a required Sendinblue API key to use for authentication Check out this guide on how to create the Sendinblue API key. It is recommended to use the API key to avoid the API rate limit exceeded error. Here is how to connect the SendinBlue to MindsDB: CREATE DATABASE sib_datasource WITH ENGINE = 'sendinblue' , PARAMETERS = { \"api_key\" : \"xkeysib-...\" } ; ​ Usage Use the established connection to query your database: SELECT * FROM sib_datasource . email_campaigns Run more advanced queries: SELECT id , name FROM sib_datasource . email_campaigns WHERE status = 'sent' ORDER BY name LIMIT 5 For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Salesforce Shopify github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "binance.html", "content": "Binance - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Binance Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Binance In this section, we present how to connect Binance to MindsDB. Binance is one of the world’s largest cryptocurrency exchanges. It’s an online platform where you can buy, sell, and trade a wide variety of cryptocurrencies. Binance offers a range of services beyond just trading, including staking, lending, and various financial products related to cryptocurrencies. Binance provides real-time trade data that can be utilized within MindsDB to make real-time forecasts. ​ Connection This handler integrates with the Binance API to make aggregate trade (kline) data available to use for model training and predictions. Since there are no parameters required to connect to Binance using MindsDB, you can use the below statement: CREATE DATABASE my_binance WITH ENGINE = 'binance' ; ​ Usage ​ Select Data By default, aggregate data (klines) from the latest 1000 trading intervals with a length of one minute (1m) each will be returned. SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' ; Response Here is the sample output data: | symbol | open_time | open_price | high_price | low_price | close_price | volume | close_time | quote_asset_volume | number_of_trades | taker_buy_base_asset_volume | taker_buy_quote_asset_volume | | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ----------- | ------------------ | ---------------- | --------------------------- | ---------------------------- | | BTCUSDT | 1678338600 | 21752.65000 | 21761.33000 | 21751.53000 | 21756.7000 | 103.8614100 | 1678338659.999| 2259656.20520700 | 3655 | 55.25763000 | 1202219.60971860 | where: symbol - Trading pair (BTC to USDT in the above example) open_time - Start time of interval in seconds since the Unix epoch (default interval is 1m) open_price - Price of a base asset at the beginning of a trading interval high_price - The highest price of a base asset during trading interval low_price - Lowest price of a base asset during a trading interval close_price - Price of a base asset at the end of a trading interval volume - Total amount of base asset traded during an interval close_time - End time of interval in seconds since the Unix epoch quote_asset_volume - Total amount of quote asset (USDT in the above case) traded during an interval number_of_trades - Total number of trades made during an interval taker_buy_base_asset_volume - How much of the base asset volume is contributed by taker buy orders taker_buy_quote_asset_volume - How much of the quote asset volume is contributed by taker buy orders To get a customized response we can pass open_time, close_time, and interval: SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' AND open_time > '2023-01-01' AND close_time < '2023-01-03 08:00:00' AND interval = '1s' LIMIT 10000 ; Supported intervals are listed here ​ Train a Model Here is how to create a time series model using 10000 trading intervals in the past with a duration of 1m. CREATE MODEL mindsdb . btc_forecast_model FROM my_binance ( SELECT * FROM aggregated_trade_data WHERE symbol = 'BTCUSDT' AND close_time < '2023-01-01' AND interval = '1m' LIMIT 10000 ; ) PREDICT open_price ORDER BY open_time WINDOW 100 HORIZON 10 ; For more accuracy, the limit can be set to a higher value (e.g. 100,000) ​ Making Predictions First, let’s create a view for the most recent BTCUSDT aggregate trade data: CREATE VIEW recent_btcusdt_data AS ( SELECT * FROM my_binance . aggregated_trade_data WHERE symbol = 'BTCUSDT' ) Now let’s predict the future price of BTC: SELECT m . * FROM recent_btcusdt_data AS t JOIN mindsdb . btc_forecast_model AS m WHERE m . open_time > LATEST This will give the predicted BTC price for the next 10 minutes (as the horizon is set to 10) in terms of USDT. Was this page helpful? Yes No Suggest edits Raise issue Sample Database Confluence github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage Select Data Train a Model Making Predictions"}
{"file_name": "strapi.html", "content": "Strapi - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Strapi Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Strapi Strapi is a popular open-source Headless Content Management System (CMS) that empowers developers to work with their preferred tools and frameworks, while providing content editors with a user-friendly interface to manage and distribute content across various platforms. The Strapi Handler is a MindsDB handler that enables SQL-based querying of Strapi collections. This documentation provides a brief overview of its features, initialization parameters, and example usage. ​ Connection To use the Strapi Handler, initialize it with the following parameters: host : Strapi server host. port : Strapi server port (typically 1337). api_token : Strapi server API token for authentication. plural_api_ids : List of plural API IDs for the collections. To get started, create a Strapi engine database with the following SQL command: CREATE DATABASE myshop --- Display name for the database. WITH ENGINE = 'strapi' , --- Name of the MindsDB handler. PARAMETERS = { \"host\" : \"<strapi-host>\" , --- Host (can be an IP address or URL). \"port\" : \"<strapi-port>\" , --- Common port is 1337. \"api_token\" : \"<your-strapi-api-token>\" , --- API token of the Strapi server. \"plural_api_ids\" : [ \"<plural-api-id>\" ] --- Plural API IDs of the collections. } ; ​ Usage Retrieve data from a collection: SELECT * FROM myshop . < collection - name > ; Filter data based on specific criteria: SELECT * FROM myshop . < collection - name > WHERE id = < id - value > Insert new data into a collection: INSERT INTO myshop . < collection - name > ( < field - name - 1 > , < field - name - 2 > , . . . ) VALUES ( < value - 1 > , < value - 2 > , . . . ) ; Note: You only able to insert data into the collection which has create permission. Modify existing data in a collection: UPDATE myshop . < collection - name > SET < field - name - 1 > = < value - 1 > , < field - name - 2 > = < value - 2 > , . . . WHERE id = < id - value > ; Note: You only able to update data into the collection which has update permission. Was this page helpful? Yes No Suggest edits Raise issue Slack Stripe github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage"}
{"file_name": "pypi.html", "content": "PyPI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications PyPI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications PyPI In this section, we present how to connect PyPI to MindsDB. PyPI is a host for maintaining and storing Python packages. It’s a good place for publishing your Python packages in different versions and releases. Data from PyPI can be utilized within MindsDB to train models and make predictions about your Python packages. ​ Connection This handler is implemented using the standard Python requests library. It is used to connect to the RESTful service that pypistats.org is serving. There are no connection arguments required to initialize the handler. To connect to PyPI using MindsDB, the following CREATE DATABASE statement can be used: CREATE DATABASE pypi_datasource WITH ENGINE = 'pypi' ​ Usage Now, you can use the following queries to view the statistics for Python packages (MindsDB, for example): Overall downloads, including mirrors: SELECT * FROM pypi_datasource . overall WHERE package = \"mindsdb\" AND mirrors = true ; Overall downloads on CPython==2.7: SELECT * FROM pypi_datasource . python_minor WHERE package = \"mindsdb\" AND version = \"2.7\" ; Recent downloads: SELECT * FROM pypi_datasource . recent WHERE package = \"mindsdb\" ; Recent downloads in the last day: SELECT * FROM pypi_datasource . recent WHERE package = \"mindsdb\" AND period = \"day\" ; All downloads on Linux-based distributions: SELECT date , downloads FROM pypi_datasource . system WHERE package = \"mindsdb\" AND os = \"Linux\" ; Each table takes a required package argument in the WHERE clause, which is the name of the package you want to query. ​ Supported Tables The following tables are supported by the PyPI handler: overall : daily download quantities for packages. recent : recent download quantities for packages. python_major : daily download quantities for packages, grouped by Python major version. python_minor : daily download quantities for packages, grouped by Python minor version. system : daily download quantities for packages, grouped by operating system. Was this page helpful? Yes No Suggest edits Raise issue Plaid Reddit github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage Supported Tables"}
{"file_name": "confluence.html", "content": "Confluence - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Confluence Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Confluence In this section, we present how to connect Confluence to MindsDB. Confluence is a popular collaboration and documentation tool developed by Atlassian, a software company known for its suite of productivity and project management software. Confluence is designed to help teams and organizations collaborate, share information, and create and manage various types of content. Data from Confluence can be utilized within MindsDB to train AI models and make predictions. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Confluence to MindsDB, install the required dependencies following this instruction . Install or ensure access to Confluence. ​ Connection This handler is implemented using atlassian-python-api , a library that provides a simple and convenient way to interact with Atlassian products. The required arguments to establish a connection are as follows: url is a Confluence-hosted URL instance. confluence_api_token is a token used to authenticate. Please follow this link to generate the token for accessing the Confluence API. In order to make use of this handler and connect the Confluence app to MindsDB, the following syntax can be used: CREATE DATABASE mindsdb_confluence WITH ENGINE = 'confluence' , PARAMETERS = { \"url\" : \"your_confluence_url\" , \"username\" : \"your_username\" , \"password\" : \"access_token\" } ; It creates a database that comes with the pages table. ​ Usage Now you can query your data, like this: SELECT id , key , name , type FROM mindsdb_confluence . pages WHERE type = \"personal\" ORDER BY id ASC , name DESC LIMIT 10 ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Binance Docker Hub github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "microsoft-teams.html", "content": "Microsoft Teams - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Microsoft Teams Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Microsoft Teams This documentation describes the integration of MindsDB with Microsoft Teams , the ultimate messaging app for your organization. The integration allows MindsDB to create chatbots enhanced with AI capabilities that can respond to messages in Microsoft Teams. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Microsoft Teams to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Microsoft Teams from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE teams_conn WITH ENGINE = 'teams' , PARAMETERS = { \"client_id\" : \"12345678-90ab-cdef-1234-567890abcdef\" , \"client_secret\" : \"a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6\" } ; Required connection parameters include the following: client_id : The client ID of the registered Microsoft Entra ID application. client_secret : The client secret of the registered Microsoft Entra ID application. Microsoft Entra ID was previously known as Azure Active Directory (Azure AD). ​ How to set up Microsoft Teams app and MindsDB chatbot Follow the instructions given below to set up the Microsoft Teams app that will act as the chatbot: 1 Register a bot in the Microsoft Bot Framework portal Follow this link to the Microsoft Bot Framework portal and sign in with your Microsoft account. Fill out the Display name , Bot handle , and, optionally, the Long description , but leave the Messaging endpoint field empty for now. Set the App type to be Multi Tenant and click on Create Microsoft App ID and password . This will open a new tab with the Azure portal. 2 Register an application in the Azure portal Click on New registration and fill out the Name and select the Accounts in any organizational directory (Any Azure AD directory - Multitenant) option under Supported account types , and click on Register . Save the Application (client) ID for later use. Click on Certificates & secrets under Manage . Click on New client secret and fill out the Description and select an appropriate Expires period, and click on Add . Copy and save the client secret in a secure location. If you already have an existing app registration, you can use it instead of creating a new one and skip the above steps. 3 Configure a chatbot in the MindsDB Editor Open the MindsDB editor and create a connection to Microsoft Teams using the client ID and client secret obtained in the previous steps. Use the CREATE DATABASE statement as show above. Using this connection, create a chatbot with the CREATE CHATBOT syntax, as shown in the Usage section below. Run the SHOW CHATBOTS command and record the webhook_token of the chatbot you created. 4 Complete the bot setup in the Microsoft Bot Framework portal Navigate back to the Microsoft Bot Framework portal and fill out the messaging endpoint in the following format: <mindsdb_url>/api/webhooks/chatbots/<webhook_token> . The <mindsdb_url> placeholder should be replaced with the URL where MindsDB instance is running. Note that if you are running MindsDB locally, it will need to be exposed to the internet using a service like ngrok . Fill out the Microsoft App ID using the client ID obtained in the previous steps, agree to the terms, and click on Register . Under Add a featured channel , click on Microsoft Teams , select your Microsoft Teams solution, click on Save and agree to the terms when prompted. 5 Create an application via the Developer Portal in Microsoft Teams Navigate to Microsoft Teams and then to the Apps tab. Search for the Developer Portal app and add it to your workspace. Open the Developer Portal , click on Apps and then on New app . Fill out the Name for the app and click on Add . Navigate to Basic information and fill out the required fields: Short description , Long description , Developer or company name , Website , Privacy policy , Terms of use , and Application (client) ID . You may also provide any additional information if you prefer. Please note that the above fields are required for the bot to be usable in Microsoft Teams. The URLs provided in the Website , Privacy policy , and Terms of use fields must be valid URLs. Navigate to App features and select Bot . Choose the Select an existing bot option and select the bot you created earlier in the Microsoft Bot Framework portal. Select all of the scopes where bot is required to be used and click on Save . Finally, on the navigation pane, click on Publish and then on Publish to your org . For the bot to be made available to the users in your organization, you will need to ask your IT administrator to approve the submission at this link . Once it is approved, users can find it under the Built for your org section in the Apps tab. They can either chat with the bot directly or add it to a channel. To chat with the bot in a channel, type @<bot_name> followed by your message. While waiting for approval, you can test the chatbot by clicking on Preview in Teams in the Developer Portal. ​ Usage This integration can only be used to create chatbots for Microsoft Teams via the CREATE CHATBOT syntax. Currently, it cannot be used as a data source for other workloads . CREATE CHATBOT teams_chatbot USING database = 'teams_datasource' , agent = 'ai_agent' , is_running = true ; Learn more about agents and chatbots here . Follow this tutorial to build your own chatbot. ​ Troubleshooting Guide No response from the bot : Symptoms : The bot does not respond to messages. Checklist : Ensure the bot is correctly set up in the Microsoft Bot Framework portal. Ensure that the client ID and client secret used to create the connection are correct and valid. Ensure that the messaging endpoint is correctly set in the Microsoft Bot Framework portal with the correct webhook token. Confirm that the bot is added to the Microsoft Teams workspace. The bot is not available to other users : Symptoms : The bot is not available to other users in the organization. Checklist : Ensure that the bot is published to the organization in the Microsoft Bot Framework portal. Ensure that the bot is approved by the IT administrator. Was this page helpful? Yes No Suggest edits Raise issue Microsoft One Drive News API github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection How to set up Microsoft Teams app and MindsDB chatbot Usage Troubleshooting Guide"}
{"file_name": "reddit.html", "content": "Reddit - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Reddit Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Reddit In this section, we present how to connect Reddit to MindsDB. Reddit is a social media platform and online community where registered users can engage in discussions, share content, and participate in various communities called subreddits. Data from Reddit can be utilized within MindsDB to train AI models and chatbots. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Reddit to MindsDB, install the required dependencies following this instruction . Install or ensure access to Reddit. ​ Connection This handler is implemented using the PRAW (Python Reddit API Wrapper) library, which is a Python package that provides a simple and easy-to-use interface to access the Reddit API. The required arguments to establish a connection are as follows: client_id is a Reddit API client ID. client_secret is a Reddit API client secret. user_agent is a user agent string to identify your application. Here is how to get your Reddit credentials: Go to Reddit App Preferences at https://www.reddit.com/prefs/apps or https://old.reddit.com/prefs/apps/ Scroll down to the bottom of the page and click Create another app… Fill out the form with the name, description, and redirect URL for your app, then click Create app Now you should be able to see the personal user script, secret, and name of your app. Store those as environment variables: CLIENT_ID , CLIENT_SECRET , and USER_AGENT , respectively. In order to make use of this handler and connect the Reddit app to MindsDB, the following syntax can be used: CREATE DATABASE my_reddit WITH ENGINE = 'reddit' , PARAMETERS = { \"client_id\" : \"YOUR_CLIENT_ID\" , \"client_secret\" : \"YOUR_CLIENT_SECRET\" , \"user_agent\" : \"YOUR_USER_AGENT\" } ; It creates a database that comes with two tables: submission and comment . ​ Usage Now you can fetch data from Reddit, like this: SELECT * FROM my_reddit . submission WHERE subreddit = 'MachineLearning' AND sort_type = 'top' -- specifies the sorting type for the subreddit (possible values include 'hot', 'new', 'top', 'controversial', 'gilded', 'wiki', 'mod', 'rising') AND items = 5 ; -- specifies the number of items to fetch from the subreddit You can also fetch comments for a particular post/submission, like this: SELECT * FROM my_reddit . comment WHERE submission_id = '12gls93' For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue PyPI Salesforce github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "youtube.html", "content": "YouTube - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications YouTube Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications YouTube In this section, we present how to connect YouTube to MindsDB. YouTube is a popular online video-sharing platform and social media website where users can upload, view, share, and interact with videos created by individuals and organizations from around the world. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB on your system or obtain access to cloud options. To use YouTube with MindsDB, install the required dependencies following this instruction . ​ Connection There are two ways you can connect YouTube to MindsDB: Limited permissions: This option provides MindsDB with read-only access to YouTube, including viewing comments data. Elevated permissions: This option provides MindsDB with full access to YouTube, including viewing comments data and posting replies to comments. ​ Option 1: Limited permissions Establish a connection to YouTube from MindsDB by executing the below SQL command and following the Google authorization link provided as output: CREATE DATABASE mindsdb_youtube WITH ENGINE = 'youtube' , PARAMETERS = { \"youtube_api_token\" : \"<your-youtube-api-key-token>\" } ; Alternatively, you can connect YouTube to MindsDB via the form. To do that, click on the Add button, choose New Datasource , search for YouTube , and follow the instructions in the form. After providing the connection name and the YouTube API token, click on the Test Connection button. Once the connection is established, click on the Save and Continue button. Required connection parameters include the following: youtube_api_token : It is a Google API key used for authentication. Check out this guide on how to create the API key to access YouTube data. ​ Option 2: Elevated permissions Establish a connection to YouTube from MindsDB by executing the below SQL command and following the Google authorization link provided as output: CREATE DATABASE mindsdb_youtube_oauth WITH ENGINE = 'youtube' , PARAMETERS = { \"credentials_file\" : \"path-to-credentials-json-file\" -- alternatively, use the credentials_url parameter } ; Alternatively, you can connect YouTube to MindsDB via the form. To do that, click on the Add button, choose New Datasource , search for YouTube , and follow the instructions in the form. After providing the connection name and the credentials file or URL, click on the Test Connection button and complete the authorization process in the pop-up window. Once the connection is established, click on the Save and Continue button. Required connection parameters include one of the following: credentials_file : It is a path to a file generated from the Google Cloud Console, as described below. credentials_url : It is a URL to a file generated from the Google Cloud Console, as described below. How to Generate the Credentials File Open the Google Cloud Console. Create a new project. Inside this project, go to APIs & Services: Go to Enabled APIs & services: Click on ENABLE APIS AND SERVICES from the top bar. Search for YouTube Data API v3 and enable it. Go to OAuth consent screen: Choose user type as External and click on Create. Provide app name and support email. In Scopes, add the following scopes: ../auth/youtube , .../auth/youtube.force-ssl , .../auth/youtubepartner . Next, add test users. Save and continue. Go to Credentials: Click on CREATE CREDENTIALS from the top bar and choose OAuth client ID. Choose type as Web application and provide a name. In the Authorized redirect URIs, add the following: http://localhost/verify-auth , https://cloud.mindsdb.com/verify-auth , http://127.0.0.1:47334/verify-auth Click on CREATE. Download the JSON file that is required to connect YouTube to MindsDB. ​ Usage Use the established connection to query the comments table. You can query for one video’s comments: SELECT * FROM mindsdb_youtube . comments WHERE video_id = \"raWFGQ20OfA\" ; Or for one channels’s comments: SELECT * FROM mindsdb_youtube . comments WHERE channel_id = \"UC-...\" ; You can include ordering and limiting the output data: SELECT * FROM mindsdb_youtube . comments WHERE video_id = \"raWFGQ20OfA\" ORDER BY display_name ASC LIMIT 5 ; Use the established connection to query the channels table. SELECT * FROM mindsdb_youtube . channels WHERE channel_id = \"UC-...\" ; Here, the channel_id column is mandatory in the WHERE clause. Use the established connection to query the videos table. SELECT * FROM mindsdb_youtube . videos WHERE video_id = \"id\" ; Here, the video_id column is mandatory in the WHERE clause. With the connection option 2, you can insert replies to comments: INSERT INTO mindsdb_youtube_oauth . comments ( comment_id , reply ) VALUES ( \"comment_id\" , \"reply message\" ) ; Was this page helpful? Yes No Suggest edits Raise issue Web Crawler Supported Integrations github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Option 1: Limited permissions Option 2: Elevated permissions Usage"}
{"file_name": "google-analytics.html", "content": "Google Analytics - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Google Analytics Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Google Analytics In this section, we present how to connect Google Analytics to MindsDB. Google Analytics is a web analytics service offered by Google that tracks and reports website traffic and also the mobile app traffic & events. Data from Google Analytics can be utilized within MindsDB to train AI models, make predictions, and automate user engagement and events with AI. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Google Analytics to MindsDB, install the required dependencies following this instruction . Install or ensure access to Google Analytics. ​ Connection The required arguments to establish a connection are as follows: credentials_file optional, is a path to the JSON file that stores credentials to the Google account. credentials_json : optional, is the content of the JSON file that stores credentials to the Google account. property_id required, is the property id of your Google Analytics website. Here is some information on how to get the property id. ⚠️ One of credentials_file or credentials_json has to be chosen. Please note that a Google account with enabled Google Analytics Admin API is required. You can find more information here . Also an active website connected with Google Analytics is required. You can find more information here . To make use of this handler and connect the Google Analytics app to MindsDB, the following syntax can be used: CREATE DATABASE my_ga WITH ENGINE = 'google_analytics' , PARAMETERS = { 'credentials_file' : '\\path-to-your-file\\credentials.json' , 'property_id' : '<YOUR_PROPERTY_ID>' } ; You need a Google account in order to use this integration. Here is how to get the credentials file: Create a Google Cloud Platform (GCP) Project: 1.1 Go to the GCP Console ( https://console.cloud.google.com/ ). 1.2 If you haven’t created a project before, you’ll be prompted to do so now. 1.3 Give your new project a name. 1.4 Click Create to create the new project. Enable the Google Analytics Admin API: 2.1 In the GCP Console, select your project. 2.2 Navigate to APIs & Services > Library . 2.3 In the search bar, search for Google Analytics Admin API . 2.4 Click on Google Analytics Admin API , then click Enable . Create credentials for the Google Analytics Admin API : 3.1 Navigate to APIs & Services > Credentials . 3.2 Click on the Create Credentials button and choose Service account . 3.3 Enter a unique Service account ID . 3.4 Click Done . 3.5 Copy the service account you created. Find it under Service Accounts . 3.6 Now click on the service account you created, and navigate KEYS 3.7 Click ADD KEY > Create new key . 3.8 Choose JSON then click CREATE 3.9 After this the credentials file will be downloaded directly. Locate the file and use the location to it in the credentials_file param. Add Service account to Google Analytics Property: 4.1 In the Google Analytics Admin Console, select the Account or Property to which you want to grant access. 4.2 Navigate to the Admin panel. 4.3 Navigate Account > Account Access Management . 4.4 Click on the ”+” icon to add a new user. 4.5 Enter the service account you copied in step 3.5 as the email address. 4.6 Assign the appropriate permissions to the service account. At a minimum, you’ll need to grant it Edit permissions. 4.7 Click on the Add button to add the service account as a user with the specified permissions. ​ Usage This creates a database that comes with the conversion_events table. Now you can use your Google Analytics data like this: searching for conversion events: SELECT event_name , custom , countingMethod FROM my_ga . conversion_events ; creating conversion event: INSERT INTO my_ga . conversion_events ( event_name , countingMethod ) VALUES ( 'mindsdb_event' , 2 ) ; updating one conversion event: UPDATE my_ga . conversion_events SET countingMethod = 1 , WHERE name = '<mindsdb_event_name>' ; deleting one conversion event: DELETE FROM my_ga . conversion_events WHERE name = '<mindsdb_event_name>' ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Google Calendar Hacker News github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "email.html", "content": "Email - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Email Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Email In this section, we present how to connect Email accounts to MindsDB. By connecting your email account to MindsDB, you can utilize various AI models available within MindsDB to summarize emails, detect spam, or even automate email replies. Please note that currently you can connect Gmail and Outlook accounts using this integration. ​ Connection This handler was implemented using standard Python libraries: email , imaplib , and smtplib . The Email handler is initialized with the following required parameters: email stores an email address used for authentication. password stores a password used for authentication. Additionally, the following optional parameters can be passed: smtp_server used to send emails. Defaults to smtp.gmail.com . smtp_port used to send emails. Defaults to 587 . imap_server used to receive emails. Defaults to imap.gmail.com . At the moment, the handler has been tested with Gmail and Outlook accounts. To use the handler on a Gmail account, you must create an app password following this instruction and use its value for the password parameter. By default, the Email handler connects to Gmail. If you want to use other email providers as Outlook, add the values for imap_server and smtp_server parameters. ​ Gmail To connect your Gmail account to MindsDB, use the below CREATE DATABASE statement: CREATE DATABASE email_datasource WITH ENGINE = 'email' , PARAMETERS = { \"email\" : \"youremail@gmail.com\" , \"password\" : \"yourpassword\" } ; It creates a database that comes with the emails table. ​ Outlook To connect your Outlook account to MindsDB, use the below CREATE DATABASE statement: CREATE DATABASE email_datasource WITH ENGINE = 'email' , PARAMETERS = { \"email\" : \"youremail@outlook.com\" , \"password\" : \"yourpassword\" , \"smtp_server\" : \"smtp.office365.com\" , \"smtp_port\" : \"587\" , \"imap_server\" : \"outlook.office365.com\" } ; It creates a database that comes with the emails table. ​ Usage Now you can query for emails like this: SELECT * FROM email_datasource . emails ; And you can apply filters like this: SELECT id , body , subject , to_field , from_field , datetime FROM email_datasource . emails WHERE subject = 'MindsDB' ORDER BY id LIMIT 5 ; Or, write emails like this: INSERT INTO email_datasource . emails ( to_field , subject , body ) VALUES ( \"toemail@outlook.com\" , \"MindsDB\" , \"Hello from MindsDB!\" ) ; Was this page helpful? Yes No Suggest edits Raise issue Docker Hub GitHub github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Gmail Outlook Usage"}
{"file_name": "github.html", "content": "GitHub - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications GitHub Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications GitHub In this section, we present how to connect GitHub repository to MindsDB. GitHub is a web-based platform and service that is primarily used for version control and collaborative software development. It provides a platform for developers and teams to host, review, and manage source code for software projects. Data from GitHub, including issues and PRs, can be utilized within MindsDB to make relevant predictions or automate the issue/PR creation. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect GitHub to MindsDB, install the required dependencies following this instruction . Install or ensure access to GitHub. ​ Connection This handler is implemented using the pygithub library, a Python library that wraps GitHub API v3. The required arguments to establish a connection are as follows: repository is the GitHub repository name. api_key is an optional GitHub API key to use for authentication. github_url is an optional GitHub URL to connect to a GitHub Enterprise instance. Check out this guide on how to create the GitHub API key. It is recommended to use the API key to avoid the API rate limit exceeded error. Here is how to connect the MindsDB GitHub repository: CREATE DATABASE mindsdb_github WITH ENGINE = 'github' , PARAMETERS = { \"repository\" : \"mindsdb/mindsdb\" } ; ​ Usage The mindsdb_github connection contains two tables: issues and pull_requests . Here is how to query for all issues: SELECT * FROM mindsdb_github . issues ; You can run more advanced queries to fetch specific issues in a defined order: SELECT number , state , creator , assignees , title , labels FROM mindsdb_github . issues WHERE state = 'open' LIMIT 10 ; And the same goes for pull requests: SELECT number , state , title , creator , head , commits FROM mindsdb_github . pull_requests WHERE state = 'open' LIMIT 10 ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Email GitLab github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "newsapi.html", "content": "News API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications News API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications News API In this section, we present how to connect News API to MindsDB. News API is a simple HTTP REST API for searching and retrieving live articles from all over the web. Data from News API can be utilized within MindsDB for model training and predictions. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect News API to MindsDB, install the required dependencies following this instruction . Install or ensure access to News API. ​ Connection This handler is implemented using the newsapi-python library. The required arguments to establish a connection are as follows: api_key News API key to use for authentication. Check out this guide on how to create the API key. It is recommended to use the API key to avoid the API rate limit exceeded error. Here is how to connect News API to MindsDB: CREATE DATABASE newsAPI WITH ENGINE = 'newsapi' PARAMETERS = { \"api_key\" : \"Your api key\" } ; ​ Usage Simple Search for recent articles: SELECT * FROM newsAPI . article WHERE query = 'Python' ; Advanced search for recent articles per specific sources between dates: SELECT * FROM newsAPI . article WHERE query = 'Python' AND sources = \"bbc-news\" AND publishedAt >= \"2021-03-23\" AND publishedAt <= \"2023-04-23\" LIMIT 4 ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Microsoft Teams PayPal github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "gmail.html", "content": "Gmail - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Gmail Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Gmail In this section, we present how to connect Gmail accounts to MindsDB. Gmail is a widely used and popular email service developed by Google. By connecting your Gmail account to MindsDB, you can utilize various AI models available within MindsDB to summarize emails, detect spam, or even automate email replies. Please note that currently you can connect your Gmail account to local MindsDB installation by providing a path to the credentials file stored locally. If you want to connect your Gmail account to MindsDB Cloud, you can upload the credentials file, for instance, to your S3 bucket and provide a link to it as a parameter. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Gmail to MindsDB, install the required dependencies following this instruction . Install or ensure access to Gmail. ​ Connection The required arguments to establish a connection are as follows: credentials_file local path to the credentials.json or credentials_url in case your file is uploaded to s3. Follow the instructions below to generate the credentials file. scopes define the level of access granted. It is optional and by default it uses ‘ https://…/gmail.compose ’ and ‘ https://…/gmail.readonly ’ scopes. In order to make use of this handler and connect the Google Calendar app to MindsDB, the following syntax can be used: CREATE DATABASE mindsdb_gmail WITH ENGINE = 'gmail' , parameters = { \"credentials_file\" : \"mindsdb/integrations/handlers/gmail_handler/credentials.json\" , \"scopes\" : [ 'https://.../gmail.compose' , 'https://.../gmail.readonly' , . . . ] } ; Or, you can also connect by giving the credentials file from an s3 pre signed url . To do this you need to pass in the credentials_file parameter as a pre signed url . For example: CREATE DATABASE mindsdb_gmail WITH ENGINE = 'gmail' , parameters = { \"credentials_url\" : \"https://s3.amazonaws.com/your_bucket/credentials.json?response-content-disposition=inline&X-Amz-Security-Token=12312...\" , -- \"scopes\": ['SCOPE_1', 'SCOPE_2', ...] -- Optional scopes. By default 'https://.../gmail.compose' & 'https://.../gmail.readonly' scopes are used } ; You need a Google account in order to use this integration. Here is how to get the credentials file: Create a Google Cloud Platform (GCP) Project: 1.1 Go to the GCP Console ( https://console.cloud.google.com/ ). 1.2 If you haven’t created a project before, you’ll be prompted to do so now. 1.3 Give your new project a name. 1.4 Click Create to create the new project. Enable the Gmail API: 2.1 In the GCP Console, select your project. 2.2 Navigate to APIs & Services > Library . 2.3 In the search bar, search for Gmail . 2.4 Click on Gmail API , then click Enable . Create credentials for the Gmail API: 3.1 Navigate to APIs & Services > Credentials . 3.2 Click on the Create Credentials button and choose OAuth client ID . 3.3 If you haven’t configured the OAuth consent screen before, you’ll be prompted to do so now. Make sure to choose External for User Type, and select the necessary scopes. Make sure to save the changes. Now, create the OAuth client ID. Choose Web application for the Application Type and give it a name. 3.4 Add the following MindsDB URL to Authorized redirect URIs : For local installation, add http://localhost/verify-auth For Cloud, add http://cloud.mindsdb.com/verify-auth . 3.5 Click Create . Download the JSON file: 4.1 After creating your credentials, click the download button (an icon of an arrow pointing down) on the right side of your client ID. This will download a JSON file, so you will use the location to it in the credentials_file param. ​ Usage This creates a database called mindsdb_gmail. This database ships with a table called emails that we can use to search for emails as well as to write emails. Now you can use your Gmail data, like this: searching for email: SELECT * FROM mindsdb_gmail . emails WHERE query = 'alert from:*@google.com' AND label_ids = \"INBOX,UNREAD\" LIMIT 20 ; writing emails: INSERT INTO mindsdb_gmail . emails ( thread_id , message_id , to_email , subject , body ) VALUES ( '187cbdd861350934d' , '8e54ccfd-abd0-756b-a12e-f7bc95ebc75b@Spark' , 'test@example2.com' , 'Trying out MindsDB' , 'This seems awesome. You must try it out whenever you can.' ) ; ​ Example 1: Automating Email Replies Now that we know how to pull emails into our database and write emails, we can make use of OpenAPI engine to write email replies. First, create an OpenAI engine, passing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; Then, create a model using this engine: CREATE MODEL mindsdb . gpt_model PREDICT response USING engine = 'openai_engine' , max_tokens = 500 , api_key = 'your_api_key' , model_name = 'gpt-3.5-turbo' , prompt_template = ' From input message: {{body}}\\ by from_user: {{sender}}\\ In less than 500 characters , write an email response to {{sender}} in the following format:\\ Start with proper salutation and respond with a short message in a casual tone , and sign the email with my name mindsdb' ; ​ Example 2: Detecting Spam Emails You can check if an email is spam by using one of the Hugging Face pre-trained models. CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ] ; Then, create a view that contains the snippet or the body of the email. CREATE VIEW mindsdb . emails_text AS ( SELECT snippet AS text_spammy FROM mindsdb_gmail . emails ) ; Finally, you can use the model to classify emails into spam or ham: SELECT h . PRED , h . PRED_explain , t . text_spammy AS input_text FROM mindsdb . emails_text AS t JOIN mindsdb . spam_classifier AS h ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue GitLab Google Calendar github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Example 1: Automating Email Replies Example 2: Detecting Spam Emails"}
{"file_name": "gitlab.html", "content": "GitLab - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications GitLab Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications GitLab In this section, we present how to connect GitLab repository to MindsDB. GitLab is a DevSecOps Platform. Data from GitLab, including issues and MRs, can be utilized within MindsDB to make relevant predictions or automate the issue/MR creation. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect GitLab to MindsDB, install the required dependencies following this instruction . Install or ensure access to GitLab. ​ Connection This handler was implemented using the python-gitlab library. python-gitlab is a Python library that wraps GitLab API. The GitLab handler is initialized with the following parameters: repository : a required name of a GitLab repository to connect to. api_key : an optional GitLab API key to use for authentication. Here is how to connect MindsDB to a GitLab repository: CREATE DATABASE mindsdb_gitlab WITH ENGINE = 'gitlab' , PARAMETERS = { \"repository\" : \"gitlab-org/gitlab\" , \"api_key\" : \"api_key\" , -- optional GitLab API key } ; ​ Usage The mindsdb_gitlab connection contains two tables: issues and merge_requests . Now, you can use this established connection to query this table as: SELECT * FROM mindsdb_gitlab . issues ; You can run more advanced queries to fetch specific issues in a defined order: SELECT number , state , creator , assignee , title , created , labels FROM mindsdb_gitlab . issues WHERE state = \"opened\" ORDER BY created ASC , creator DESC LIMIT 10 ; And the same goes for merge requests: SELECT number , state , creator , reviewers , title , created , has_conflicts FROM mindsdb_gitlab . merge_requests WHERE state = \"merged\" ORDER BY created ASC , creator DESC LIMIT 10 ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue GitHub Gmail github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "salesforce.html", "content": "Salesforce - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Salesforce Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Salesforce This documentation describes the integration of MindsDB with Salesforce , the world’s most trusted customer relationship management (CRM) platform. The integration allows MindsDB to access data from Salesforce and enhance it with AI capabilities. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Salesforce to MindsDB, install the required dependencies following this instruction . ​ Connection Establish a connection to Salesforce from MindsDB by executing the following SQL command and providing its handler name as an engine. CREATE DATABASE salesforce_datasource WITH ENGINE = 'salesforce' , PARAMETERS = { \"username\" : \"demo@example.com\" , \"password\" : \"demo_password\" , \"client_id\" : \"3MVG9lKcPoNINVBIPJjdw1J9LLM82HnZz9Yh7ZJnY\" , \"client_secret\" : \"5A52C1A1E21DF9012IODC9ISNXXAADDA9\" } ; Required connection parameters include the following: username : The username for the Salesforce account. password : The password for the Salesforce account. client_id : The client ID (consumer key) from a connected app in Salesforce. client_secret : The client secret (consumer secret) from a connected app in Salesforce. To create a connected app in Salesforce and obtain the client ID and client secret, follow the steps given below: Log in to your Salesforce account. Go to Setup > Apps > App Manager . Click New Connected App . Fill in the required details. Esure that the Enable OAuth Settings checkbox is checked, set the Callback URL to wherever MindsDB is deployed followed by /verify-auth (e.g., http://localhost:47334/verify-auth ), and choose the appropriate OAuth scopes. Click Save . Copy the Consumer Key (client ID) and Consumer Secret (client secret) from the connected app details under Consumer Key and Secret . Go to Setup > Apps > Connected Apps > Manage Connected Apps . Click on the connected app name. Click Edit Policies . Under OAuth Policies , ensure that the Permitted Users is set to All users may self-authorize and IP Relaxation is set to Relax IP restrictions . Click Save . Go to Setup > Identity > OAuth and OpenID Connect Settings . Ensure that the Allow OAuth Username-Password Flows checkbox is checked. ​ Usage Retrieve data from a specified table by providing the integration and table names: SELECT * FROM salesforce_datasource . table_name LIMIT 10 ; Run SOQL queries directly on the connected Salesforce account: SELECT * FROM salesforce_datasource ( --Native Query Goes Here SELECT Name , Account . Name , Account . Industry FROM Contact WHERE Account . Industry = 'Technology' LIMIT 5 ) ; The above examples utilize salesforce_datasource as the datasource name, which is defined in the CREATE DATABASE command. ​ Supported Tables The Salesforce integration supports the following tables: Account : The table containing account information. Contact : The table containing contact information for people you do business with. Opportunity : The table containing sales opportunities. Lead : The table containing potential sales leads. Task : The table containing tasks and activities. Event : The table containing calendar events. User : The table containing user information. Product2 : The table containing product information. Pricebook2 : The table containing price book information. PricebookEntry : The table containing price book entries. Order : The table containing order information. OrderItem : The table containing order items. Case : The table containing customer service cases. Campaign : The table containing marketing campaigns. CampaignMember : The table containing campaign members. Contract : The table containing contract information. Asset : The table containing asset information. Was this page helpful? Yes No Suggest edits Raise issue Reddit Sendinblue github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Supported Tables"}
{"file_name": "instatus.html", "content": "Instatus - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Instatus Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Instatus In this section, we present how to connect Instatus to MindsDB. Instatus is a cloud-based status page software that enables users to communicate status information using incidents and maintenances. It serves as a SaaS platform for creating status pages for services. The Instatus Handler for MindsDB offers an interface to connect with Instatus via APIs and retrieve status pages. ​ Connection Initialize the Instatus handler with the following parameter: api_key : Instatus API key for authentication. Obtain it from Instatus Developer Dashboard . Start by creating a database with the new instatus engine using the following SQL command: CREATE DATABASE mindsdb_instatus --- Display name for the database. WITH ENGINE = 'instatus' , --- Name of the MindsDB handler. PARAMETERS = { \"api_key\" : \"<your-instatus-api-key>\" --- Instatus API key to use for authentication. } ; ​ Usage To get a status page, use the SELECT statement: SELECT id , name , status , subdomain FROM mindsdb_instatus . status_pages WHERE id = '<status-page-id>' LIMIT 10 ; To create a new status page, use the INSERT statement: INSERT INTO mindsdb_instatus . status_pages ( email , name , subdomain , components , logoUrl , faviconUrl , websiteUrl , language , useLargeHeader , brandColor , okColor , disruptedColor , degradedColor , downColor , noticeColor , unknownColor , googleAnalytics , subscribeBySms , smsService , twilioSid , twilioToken , twilioSender , nexmoKey , nexmoSecret , nexmoSender , htmlInMeta , htmlAboveHeader , htmlBelowHeader , htmlAboveFooter , htmlBelowFooter , htmlBelowSummary , cssGlobal , launchDate , dateFormat , dateFormatShort , timeFormat ) VALUES ( 'yourname@gmail.com' , 'mindsdb' , 'mindsdb-instatus' , '[\"Website\", \"App\", \"API\"]' , 'https://instatus.com/sample.png' , 'https://instatus.com/favicon-32x32.png' , 'https://instatus.com' , 'en' , true , '#111' , '#33B17E' , '#FF8C03' , '#ECC94B' , '#DC123D' , '#70808F' , '#DFE0E1' , 'UA-00000000-1' , true , 'twilio' , 'YOUR_TWILIO_SID' , 'YOUR_TWILIO_TOKEN' , 'YOUR_TWILIO_SENDER' , null , null , null , null , null , null , null , null , null , null , 'MMMMMM d, yyyy' , 'MMM yyyy' , 'p' ) ; The following fields are required when inserting new status pages: email (e.g. ‘ yourname@gmail.com ’) name (e.g ‘mindsdb’) subdomain (e.g. ‘mindsdb-docs’) components (e.g. ’[“Website”, “App”, “API”]’) The other fields are optional. To update an existing status page, use the UPDATE statement: UPDATE mindsdb_instatus . status_pages SET name = 'mindsdb' , status = 'UP' , logoUrl = 'https://instatus.com/sample.png' , faviconUrl = 'https://instatus.com/favicon-32x32.png' , websiteUrl = 'https://instatus.com' , language = 'en' , translations = '{ \"name\" : { \"fr\" : \"nasa\" } }' WHERE id = '<status-page-id>' ; Was this page helpful? Yes No Suggest edits Raise issue Hacker News Intercom github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage"}
{"file_name": "stripe.html", "content": "Stripe - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Stripe Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Stripe In this section, we present how to connect Stripe to MindsDB. Stripe is a financial technology company that provides a set of software and payment processing solutions for businesses and individuals to accept payments over the internet. Stripe is one of the leading payment gateway and online payment processing platforms. Data from Stripe can be utilized within MindsDB to train AI models and chatbots based on customers, products, and payment intents, and make relevant predictions and forecasts. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Stripe to MindsDB, install the required dependencies following this instruction . Install or ensure access to Stripe. ​ Connection This handler was implemented using stripe-python , the Python library for the Stripe API. There is only one parameter required to set up the connection with Stripe: api_key : a Stripe API key. You can find your API keys in the Stripe Dashboard. Read more . To connect to Stripe using MindsDB, the following CREATE DATABASE statement can be used: CREATE DATABASE stripe_datasource WITH ENGINE = 'stripe' , PARAMETERS = { \"api_key\" : \"sk_...\" } ; ​ Usage Now, you can query the data in your Stripe account (customers, for example) as follows: SELECT * FROM stripe_datasource . customers You can run more advanced queries to fetch specific customers in a defined order: SELECT name , email FROM stripe_datasource . customers WHERE currency = 'inr' ORDER BY name LIMIT 5 ​ Supported tables The following tables are supported by the Stripe handler: customers products payment_intents Was this page helpful? Yes No Suggest edits Raise issue Strapi Symbl github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Supported tables"}
{"file_name": "web-crawler.html", "content": "Web Crawler - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Web Crawler Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Web Crawler In this section, we present how to use a web crawler within MindsDB. A web crawler is an automated script designed to systematically browse and index content on the internet. Within MindsDB, you can utilize a web crawler to efficiently collect data from various websites. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To use Web Crawler with MindsDB, install the required dependencies following this instruction . ​ Connection This handler does not require any connection parameters. Here is how to initialize a web crawler: CREATE DATABASE my_web WITH ENGINE = 'web' ; The above query creates a database called my_web . This database by default has a table called crawler that we can use to crawl data from a given url/urls. ​ Usage Specifying a LIMIT clause is required. To crawl all pages on a site, consider setting the limit to a high value, such as 10,000, which exceeds the expected number of pages. Be aware that setting a higher limit may result in longer response times. ​ Get Websites Content The following usage examples demonstrate how to retrieve content from docs.mindsdb.com : SELECT * FROM my_web . crawler WHERE url = 'docs.mindsdb.com' LIMIT 1 ; You can also retrieve content from internal pages. The following query fetches the content from 10 internal pages: SELECT * FROM my_web . crawler WHERE url = 'docs.mindsdb.com' LIMIT 10 ; In order to get the content from multiple websites, use the UNION operator: SELECT * FROM my_web . crawler WHERE url = 'docs.mindsdb.com' LIMIT 5 UNION SELECT * FROM my_web . crawler WHERE url = 'docs.python.org' LIMIT 5 ; ​ Get PDF Content MindsDB accepts file uploads of csv , xlsx , xls , sheet , json , and parquet . However, you can also configure the web crawler to fetch data from PDF files accessible via URLs. SELECT * FROM my_web . crawler WHERE url = '<link-to-pdf-file>' LIMIT 1 ; ​ Configuring Web Handler for Specific Domains The Web Handler can be configured to interact only with specific domains by using the web_crawling_allowed_sites setting in the config.json file. This feature allows you to restrict the handler to crawl and process content only from the domains you specify, enhancing security and control over web interactions. To configure this, simply list the allowed domains under the web_crawling_allowed_sites key in config.json . For example: \"web_crawling_allowed_sites\" : [ \"https://docs.mindsdb.com\" , \"https://another-allowed-site.com\" ] ​ Troubleshooting Web crawler encounters character encoding issues Symptoms : Extracted text appears garbled or contains strange characters instead of the expected text. Checklist : Open a GitHub Issue: If you encounter a bug or a repeatable error with encoding, report it on the MindsDB GitHub repository by opening an issue. Web crawler times out while trying to fetch content Symptoms : The crawler fails to retrieve data from a website, resulting in timeout errors. Checklist : Check the network connection to ensure the target site is reachable. Was this page helpful? Yes No Suggest edits Raise issue Twitter YouTube github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage Get Websites Content Get PDF Content Configuring Web Handler for Specific Domains Troubleshooting"}
{"file_name": "twitter.html", "content": "Twitter - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Twitter Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Twitter In this section, we present how to connect Twitter accounts to MindsDB. Twitter is a widely recognized social media platform and microblogging service that allows users to share short messages called tweets. The Twitter handler enables you to fetch tweets and create replies utilizing AI models wthin MindsDB. Furthermore, you can automate the process of fetching tweets, preparing replies, and sending replies to Twitter. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Twitter to MindsDB, install the required dependencies following this instruction . Install or ensure access to Twitter. ​ Connection To connect a Twitter account to MindsDB, you need a Twitter developer account. Please note that it requires a paid developer account. We recommend you use the Elevated access allowing you to pull 2m tweets and to avoid parameters or authentication issue error you might get sometimes. You can check this step-by-step guide describing how to apply for the Elevated access. If you don’t already have a Twitter developer account, follow the steps in the video below to apply for one. How to Apply for a Twitter Developer Account Begin here to apply for a Twitter developer account When presented with questions under How will you use the Twitter API or Twitter Data? , use answers similar to the ones below (tweak to fit your exact use case). The more thorough your answers are, the more likely it is your account will get approved. Intended Usage (In Your Words) I have a blog and want to educate users how to use the Twitter API with MindsDB. I will read tweets that mention me and use them with MindsDB machine learning to generate responses. I plan to post tweets 2-3 times a day and keep using Twitter like I normally would. Are you planning to analyze Twitter data? I plan to build machine learning algorithms based on Twitter data. I am interested in doing sentiment analysis and topic analysis. I will potentially extract: Tweet text Favorite count and retweet count Hashtags and mentions Will your app use Tweet, Retweet, Like, Follow, or Direct Message functionality? I will use the Twitter API to post responses to tweets that mention me. I will have word filters to make sure that I never share offensive or potentially controversial subjects. Do you plan to display Tweets or aggregate data about Twitter content outside Twitter? I plan to share aggregate data as examples for users of my upcoming blog. I don’t intend to create an automated dashboard that consumes a lot of Twitter API calls. Every API call will be done locally, or automated on a simple web server. Aggregate of data will be for educational purposes only. Will your product, service, or analysis make Twitter content or derived information available to a government entity? Answer NO to this one. If you already have a Twitter developer account, you need to generate API keys following the instructions below or heading to the Twitter developer website . How to Generate API Keys Create an application with Read/Write permissions activated: Open developer portal . Select the Add app button to create a new app. Select the Create new button. Select Production and give it a name. Copy and populate the following in the below CREATE DATABASE statement: Bearer Token as a value of the bearer_token parameter. API Key as a value of the consumer_key parameter. API Key Secret as a value of the consumer_secret parameter. Setup user authentication settings: Click Setup under User authentication settings : On Permissions , select Read and Write . On Type of app , select Web App , Automated App or Bot . On App info , provide any URL for the callback URL and website URL (you can use the URL of this page). Click Save . Generate access tokens: Once you are back in the app settings, click Keys and Tokens : Generate Access Token and Access Token Secret and populate it in the below CREATE DATABASE statement: Access Token as a value of the access_token parameter. Access Token Secret as a value of the access_token_secret parameter. Once you have all the tokens and keys, here is how to connect your Twitter account to MindsDB: CREATE DATABASE my_twitter WITH ENGINE = 'twitter' , PARAMETERS = { \"bearer_token\" : \"twitter bearer token\" , \"consumer_key\" : \"twitter consumer key\" , \"consumer_secret\" : \"twitter consumer key secret\" , \"access_token\" : \"twitter access token\" , \"access_token_secret\" : \"twitter access token secret\" } ; ​ Usage The my_twitter database contains a table called tweets by default. Here is how to search tweets containing mindsdb keyword: SELECT id , created_at , author_username , text FROM my_twitter . tweets WHERE query = '(mindsdb OR #mindsdb) -is:retweet -is:reply' AND created_at > '2023-02-16' LIMIT 20 ; Please note that we can see only recent tweets from the past seven days. The created_at column condition is skipped if the provided date is earlier than seven days. Alternatively, you can use a Twitter native query, as below: SELECT * FROM my_twitter ( search_recent_tweets ( query = '(mindsdb OR #mindsdb) -is:retweet -is:reply' , start_time = '2023-03-16T00:00:00.000Z' , max_results = 2 ) ) ; To learn more about native queries in MindsDB, visit our docs here . Here is how to write tweets: INSERT INTO my_twitter . tweets ( reply_to_tweet_id , text ) VALUES ( 1626198053446369280 , 'MindsDB is great! now its super simple to build ML powered apps' ) , ( 1626198053446369280 , 'Holy!! MindsDB is the best thing they have invented for developers doing ML' ) ; For more information about available actions and development plans, visit this page . What’s next? Check out the tutorial on how to create a Twitter chatbot to see one of the interesting applications of this integration. Was this page helpful? Yes No Suggest edits Raise issue Symbl Web Crawler github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "plaid.html", "content": "Plaid - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Plaid Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Plaid In this section, we present how to connect Plaid to MindsDB. Plaid is a financial technology company that offers a platform and a set of APIs that facilitate the integration of financial services and data into applications and websites. Its services primarily focus on enabling developers to connect with and access financial accounts and data from various financial institutions. Data from Plaid can be utilized within MindsDB to train AI models and make financial forecasts. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Plaid to MindsDB, install the required dependencies following this instruction . Install or ensure access to Plaid. ​ Connection The required arguments to establish a connection are as follows: client_id secret access_token plaid_env You can get the client_id , secret , and access_token values here once you sign in to your Plaid account. And here is how you generate the access_token value. In order to make use of this handler and connect the Plaid app to MindsDB, the following syntax can be used: CREATE DATABASE my_plaid WITH ENGINE = 'plaid' , PARAMETERS = { \"client_id\" : \"YOUR_CLIENT_ID\" , \"secret\" : \"YOUR_SECRET\" , \"access_token\" : \"YOUR_PUBLIC_KEY\" , \"plaid_env\" : \"ENV\" } ; It creates a database that comes with two tables: transactions and balance . ​ Usage Now you can query your data, like this: SELECT id , merchant_name , authorized_date , amount , payment_channel FROM my_plaid . transactions WHERE start_date = '2022-01-01' AND end_date = '2023-04-11' LIMIT 20 ; And if you want to use functions provided by the Plaid API, you can use the native queries syntax, like this: SELECT * FROM my_plaid ( get_transactions ( start_date = '2022-01-01' , end_date = '2022-02-01' ) ) ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue PayPal PyPI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "mediawiki.html", "content": "MediaWiki - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications MediaWiki Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications MediaWiki In this section, we present how to connect MediaWiki to MindsDB. MediaWiki is a free and open-source wiki software platform that is designed to enable the creation and management of wikis. It was originally developed for and continues to power Wikipedia. MediaWiki is highly customizable and can be used to create a wide range of collaborative websites and knowledge bases. Data from MediaWiki can be utilized within MindsDB to train AI models and chatbots using the wide range of available information. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect MediaWiki to MindsDB, install the required dependencies following this instruction . Install or ensure access to MediaWiki. ​ Connection This handler was implemented using MediaWikiAPI , the Python wrapper for the MediaWiki API. There are no connection arguments required to initialize the handler. To connect the MediaWiki API to MindsDB, the following CREATE DATABASE statement can be used: CREATE DATABASE mediawiki_datasource WITH ENGINE = 'mediawiki' ​ Usage Now, you can query the MediaWiki API as follows: SELECT * FROM mediawiki_datasource . pages You can run more advanced queries to fetch specific pages in a defined order: SELECT * FROM mediawiki_datasource . pages WHERE title = 'Barack' ORDER BY pageid LIMIT 5 Was this page helpful? Yes No Suggest edits Raise issue Intercom Microsoft One Drive github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "shopify.html", "content": "Shopify - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Shopify Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Shopify In this section, we present how to connect Shopify to MindsDB. Shopify is an e-commerce platform that enables businesses to create and manage online stores. It is one of the leading e-commerce solutions, providing a wide range of tools and services to help entrepreneurs and businesses sell products and services online. Data from Shopify can be utilized within MindsDB to train AI models and chatbots using Products, Customers and Orders data, and make predictions relevant for businesses. ​ Connection The required arguments to establish a connection are as follows: shop_url : a required URL to your Shopify store. access_token : a required access token to use for authentication. Here is how you can create a Shopify access token . Optionally, if you want to access customer reviews, provide the following parameters: yotpo_app_key : a token needed to access customer reviews via the Yotpo Product Reviews app. yotpo_access_token : a token needed to access customer reviews via the Yotpo Product Reviews app. If you want to query customer reviews, use the Yotpo Product Reviews app available in Shopify. Here are the steps to follow: Install the Yotpo Product Reviews app for your Shopify store. Generate yotpo_app_key following this instruction for retrieving your app key. Learn more about Yotpo authentication here . Generate yotpo_access_token following this instruction . To connect your Shopify account to MindsDB, you must first create a new handler instance. You can do it by the following query: CREATE DATABASE shopify_datasource WITH ENGINE = 'shopify' , PARAMETERS = { \"shop_url\" : \"your-shop-name.myshopify.com\" , \"access_token\" : \"shppa_...\" } ; ​ Usage Once you have created the database, you can query the following tables: Products table Customers table Orders table CustomerReviews table (requires the Yotpo Product Reviews app to be installed in your Shopify account) InventoryLevel table Location table CarrierService table ShippingZone table SalesChannel table ​ Products table You can query this table as below: SELECT * FROM shopify_datasource . products ; Also, you can run more advanced queries and filter products by status, like this: SELECT id , title FROM shopify_datasource . products WHERE status = 'active' ORDER BY id LIMIT 5 ; To insert new data, run the INSERT INTO statement, providing the following values: title , body_html , vendor , product_type , tags , status . To update existing data, run the UPDATE statement. To delete data, run the DELETE statement. ​ Customers table You can query this table as below: SELECT * FROM shopify_datasource . customers ; To insert new data, run this statement: INSERT INTO shopify_datasource . customers ( first_name , last_name , email , phone ) VALUES ( 'John' , 'Doe' , 'john.doe@example.com' , '+10001112222' ) ; To update existing data, run the UPDATE statement. To delete data, run the DELETE statement. ​ Orders table You can query this table as below: SELECT * FROM shopify_datasource . orders ; To insert new data, run the INSERT INTO statement. To update existing data, run the UPDATE statement. To delete data, run the DELETE statement. ​ CustomerReviews table You can query this table as below: SELECT * FROM shopify_datasource . customer_reviews ; ​ InventoryLevel table You can query this table as below: SELECT * FROM shopify_datasource . inventory_level ; ​ Location table You can query this table as below: SELECT * FROM shopify_datasource . locations ; ​ CarrierService table You can query this table as below: SELECT * FROM shopify_datasource . carrier_service ; To insert new data, run the INSERT INTO statement, providing the following values: name , callback_url , service_discovery . To update existing data, run the UPDATE statement. To delete data, run the DELETE statement. ​ ShippingZone table You can query this table as below: SELECT * FROM shopify_datasource . shipping_zone ; ​ SalesChannel table You can query this table as below: SELECT * FROM shopify_datasource . sales_channel ; For more information about available actions and development plans, visit this page . Was this page helpful? Yes No Suggest edits Raise issue Sendinblue Slack github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage Products table Customers table Orders table CustomerReviews table InventoryLevel table Location table CarrierService table ShippingZone table SalesChannel table"}
{"file_name": "hackernews.html", "content": "Hacker News - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Hacker News Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Hacker News In this section, we present how to connect Hacker News to MindsDB. Hacker News is an online platform and community for discussions related to technology, startups, computer science, entrepreneurship, and a wide range of other topics of interest to the tech and hacker communities. It was created by Y Combinator, a well-known startup accelerator. Data from Hacker News, including articles and user comments, can be utilized within MindsDB to train AI models and chatbots with the knowledge and discussions shared at Hacker News. ​ Connection This handler is implemented using the official Hacker News API. It provides a simple and easy-to-use interface to access the Hacker News API. There are no connection arguments required. In order to make use of this handler and connect the Hacker News to MindsDB, the following syntax can be used: CREATE DATABASE my_hackernews WITH ENGINE = 'hackernews' ; It creates a database that comes with the stories and comments tables. ​ Usage Now you can query the articles, like this: SELECT * FROM my_hackernews . stories LIMIT 2 ; And here is how to fetch comments for a specific article: SELECT * FROM my_hackernews . comments WHERE item_id = 35662571 LIMIT 1 ; Was this page helpful? Yes No Suggest edits Raise issue Google Analytics Instatus github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connection Usage"}
{"file_name": "symbl.html", "content": "Symbl - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Applications Symbl Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Binance Confluence Docker Hub Email GitHub GitLab Gmail Google Calendar Google Analytics Hacker News Instatus Intercom MediaWiki Microsoft One Drive Microsoft Teams News API PayPal Plaid PyPI Reddit Salesforce Sendinblue Shopify Slack Strapi Stripe Symbl Twitter Web Crawler YouTube Databases Vector Stores Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Applications Symbl This documentation describes the integration of MindsDB with Symbl , a platform with state-of-the-art and task-specific LLMs that enables businesses to analyze multi-party conversations at scale. This integration allows MindsDB to process conversation data and extract insights from it. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Symbl to MindsDB, install the required dependencies following this instruction . Please note that in order to successfully install the dependencies for Symbl, it is necessary to install portaudio and few other Linux packages in the Docker container first. To do this, run the following commands: Start an interactive shell in the container: docker exec -it mindsdb_container sh If you haven’t specified a name when spinning up the MindsDB container with docker run , you can find it by running docker ps . If you are using Docker Desktop, you can navigate to ‘Containers’, locate the multi-container application running the extension, click on the mindsdb_service container and then click on the ‘Exec’ tab to start an interactive shell. Install the required packages: apt-get update && apt-get install -y \\ libportaudio2 libportaudiocpp0 portaudio19-dev \\ python3-dev \\ build-essential \\ && rm -rf /var/lib/apt/lists/* ​ Connection Establish a connection to your Symbl from MindsDB by executing the following SQL command: CREATE DATABASE mindsdb_symbl WITH ENGINE = 'symbl' , PARAMETERS = { \"app_id\" : \"app_id\" , \"app_secret\" : \"app_secret\" } ; Required connection parameters include the following: app_id : The Symbl app identifier. app_secret : The Symbl app secret. ​ Usage First, process the conversation data and get the conversation ID via the get_conversation_id table: SELECT * FROM mindsdb_symbl . get_conversation_id WHERE audio_url = \"https://symbltestdata.s3.us-east-2.amazonaws.com/newPhonecall.mp3\" ; Next, use the conversation ID to get the results of the above from the other supported tables: SELECT * FROM mindsdb_symbl . get_messages WHERE conversation_id = \"5682305049034752\" ; Other supported tables include: get_topics get_questions get_analytics get_action_items The above examples utilize mindsdb_symbl as the datasource name, which is defined in the CREATE DATABASE command. Was this page helpful? Yes No Suggest edits Raise issue Stripe Twitter github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "build_ai_agents.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "pgvector.html", "content": "PGVector - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Vector Stores PGVector Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores ChromaDB Couchbase PGVector Pinecone Weaviate Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Vector Stores PGVector This is the implementation of the PGVector for MindsDB. ​ PGVector Open-source vector similarity search for Postgres Store your vectors with the rest of your data. Supports: exact and approximate nearest neighbor search L2 distance, inner product, and cosine distance any language with a Postgres client Plus ACID compliance, point-in-time recovery, JOINs, and all of the other great features of Postgres ​ Implementation This handler uses pgvector python library to make use of the vector data type in postgres created from the pgvector extension The required arguments to establish a connection are the same as a regular postgres connection: host : the host name or IP address of the postgres instance port : the port to use when connecting database : the database to connect to user : the user to connect as password : the password to use when connecting ​ Usage ​ Installing the pgvector extension where you have postgres installed run the following commands to install the pgvector extension cd /tmp git clone --branch v0.4.4 https://github.com/pgvector/pgvector.git cd pgvector make make install ​ Installing the pgvector python library Ensure you install all from requirements.txt in the pgvector_handler folder ​ Creating a database connection in MindsDB You can create a database connection like you would for a regular postgres database, the only difference is that you need to specify the engine as pgvector CREATE DATABASE pvec WITH ENGINE = 'pgvector' , PARAMETERS = { \"host\" : \"127.0.0.1\" , \"port\" : 5432 , \"database\" : \"postgres\" , \"user\" : \"user\" , \"password\" : \"password\" } ; You can insert data into a new collection like so CREATE TABLE pvec . embed ( SELECT embeddings FROM mysql_demo_db . test_embeddings ) ; CREATE ML_ENGINE openai FROM openai USING api_key = 'your-openai-api-key' ; CREATE MODEL openai_emb PREDICT embedding USING engine = 'openai' , model_name = 'text-embedding-ada-002' , mode = 'embedding' , question_column = 'review' ; create table pvec . itemstest ( SELECT m . embedding AS embeddings , t . review content FROM mysql_demo_db . amazon_reviews t join openai_emb m ) ; You can query a collection within your PGVector as follows: SELECT * FROM pvec . embed Limit 5 ; SELECT * FROM pvec . itemstest Limit 5 ; You can query on semantic search like so: SELECT * FROM pvec3 . items_test WHERE embeddings = ( select * from mindsdb . embedding ) LIMIT 5 ; Was this page helpful? Yes No Suggest edits Raise issue Couchbase Pinecone github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page PGVector Implementation Usage Installing the pgvector extension Installing the pgvector python library Creating a database connection in MindsDB"}
{"file_name": "chromadb.html", "content": "ChromaDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Vector Stores ChromaDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores ChromaDB Couchbase PGVector Pinecone Weaviate Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Vector Stores ChromaDB In this section, we present how to connect ChromaDB to MindsDB. ChromaDB is the open-source embedding database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect ChromaDB to MindsDB, install the required dependencies following this instruction . Install or ensure access to ChromaDB. ​ Connection This handler is implemented using the chromadb Python library. The required arguments to establish a connection are: host : the host name or IP address of the ChromaDB instance. port : the TCP/IP port of the ChromaDB instance. OR persist_directory : the directory to use for persisting data. The host and port arguments should be provided if you want to connect to a remote ChromaDB instance. Otherwise, the persist_directory argument should be provided. This will create an in-memory ChromaDB instance. To connect to a remote ChromaDB instance, the following CREATE DATABASE can be used: CREATE DATABASE chromadb_datasource WITH ENGINE = 'chromadb' PARAMETERS = { \"host\" : \"YOUR_HOST\" , \"port\" : YOUR_PORT } Alternateively, to connect to an in-memory ChromaDB instance, the following CREATE DATABASE can be used: CREATE DATABASE chromadb_datasource WITH ENGINE = \"chromadb\" , PARAMETERS = { \"persist_directory\" : \"YOUR_PERSIST_DIRECTORY\" } ​ Usage Now, you can use the established connection to create a collection (or table in the context of MindsDB) in ChromaDB and insert data into it: CREATE TABLE chromadb_datasource . test_embeddings ( SELECT embeddings , '{\"source\": \"fda\"}' as metadata FROM mysql_datasource . test_embeddings ) ; mysql_datasource is another MindsDB data source that has been created by connecting to a MySQL database. The test_embeddings table in the mysql_datasource data source contains the embeddings that we want to store in ChromaDB. You can query your collection (table) as shown below: SELECT * FROM chromadb_datasource . test_embeddings ; To filter the data in your collection (table) by metadata, you can use the following query: SELECT * FROM chromadb_datasource . test_embeddings WHERE ` metadata.source ` = \"fda\" ; To conduct a similarity search, the following query can be used: SELECT * FROM chromadb_datasource . test_embeddings WHERE search_vector = ( SELECT embeddings FROM mysql_datasource . test_embeddings LIMIT 1 ) ; Was this page helpful? Yes No Suggest edits Raise issue YugabyteDB Couchbase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Connection Usage"}
{"file_name": "weaviate.html", "content": "Weaviate - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Vector Stores Weaviate Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores ChromaDB Couchbase PGVector Pinecone Weaviate Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Vector Stores Weaviate This is the implementation of the Weaviate for MindsDB. Weaviate is an open-source vector database. It allows you to store data objects and vector embeddings from your favorite ML-models, and scale seamlessly into billions of data objects. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Weaviate to MindsDB, install the required dependencies following this instruction . Install or ensure access to Weaviate. ​ Implementation This handler uses weaviate-client python library connect to a weaviate instance. The required arguments to establish a connection are: weaviate_url : url of the weaviate database weaviate_api_key : API key to authenticate with weaviate (in case of cloud instance). persistence_directory : directory to be used in case of local storage ​ Creating connection In order to make use of this handler and connect to a Weaviate server in MindsDB, the following syntax can be used: CREATE DATABASE weaviate_datasource WITH ENGINE = \"weaviate\" , PARAMETERS = { \"weaviate_url\" : \"https://sample.weaviate.network\" , \"weaviate_api_key\" : \"api-key\" } ; CREATE DATABASE weaviate_datasource WITH ENGINE = \"weaviate\" , PARAMETERS = { \"weaviate_url\" : \"https://localhost:8080\" , } ; CREATE DATABASE weaviate_datasource WITH ENGINE = \"weaviate\" , PARAMETERS = { \"persistence_directory\" : \"db_path\" , } ; ​ Dropping connection To drop the connection, use this command DROP DATABASE weaviate_datasource ; ​ Creating tables To insert data from a pre-existing table, use CREATE CREATE TABLE weaviate_datascource . test ( SELECT * FROM sqlitedb . test ) ; As weaviate currently doesn’t support json field. So, this creates another table for the “metadata” field and a reference is created in the original table which points to its metadata entry. Weaviate follows GraphQL conventions where classes (which are table schemas) start with a capital letter and properties start with a lowercase letter. So whenever we create a table, the table’s name gets capitalized. ​ Dropping collections To drop a Weaviate table use this command DROP TABLE weaviate_datasource . tablename ; ​ Querying and selecting To query database using a search vector, you can use search_vector or embeddings in WHERE clause SELECT * from weaviate_datasource . test WHERE search_vector = '[3.0, 1.0, 2.0, 4.5]' LIMIT 10 ; Basic query SELECT * from weaviate_datasource . test You can use WHERE clause on dynamic fields like normal SQL SELECT * FROM weaviate_datasource . createtest WHERE category = \"science\" ; ​ Deleting records You can delete entries using DELETE just like in SQL. DELETE FROM weaviate_datasource . test WHERE id IN ( 1 , 2 , 3 ) ; Update is not supported by mindsdb vector database Was this page helpful? Yes No Suggest edits Raise issue Pinecone CSV, XLSX, XLS github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Creating connection Dropping connection Creating tables Dropping collections Querying and selecting Deleting records"}
{"file_name": "couchbase.html", "content": "Couchbase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Vector Stores Couchbase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores ChromaDB Couchbase PGVector Pinecone Weaviate Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Vector Stores Couchbase This is the implementation of the Couchbase Vector store data handler for MindsDB. Couchbase is an open-source, distributed multi-model NoSQL document-oriented database software package optimized for interactive applications. These applications may serve many concurrent users by creating, storing, retrieving, aggregating, manipulating, and presenting data. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Couchbase to MindsDB, install the required dependencies following this instruction . Install or ensure access to Couchbase. ​ Implementation In order to make use of this handler and connect to a Couchbase server in MindsDB, the following syntax can be used. Note, that the example uses the default travel-sample bucket which can be enabled from the couchbase UI with pre-defined scope and documents. CREATE DATABASE couchbase_vectorsource WITH engine = 'couchbasevector' , parameters = { \"connection_string\" : \"couchbase://localhost\" , \"bucket\" : \"travel-sample\" , \"user\" : \"admin\" , \"password\" : \"password\" , \"scope\" : \"inventory\" } ; This handler is implemented using the couchbase library, the Python driver for Couchbase. The required arguments to establish a connection are as follows: connection_string : the connection string for the endpoint of the Couchbase server bucket : the bucket name to use when connecting with the Couchbase server user : the user to authenticate with the Couchbase server password : the password to authenticate the user with the Couchbase server scope : scopes are a level of data organization within a bucket. If omitted, will default to _default Note: The connection string expects either the couchbases:// or couchbase:// protocol. If you are using Couchbase Capella, you can find the connection_string under the Connect tab. It will also be required to whitelist the machine(s) that will be running MindsDB and database credentials will need to be created for the user. These steps can also be taken under the Connect tab. ​ Usage Now, you can use the established connection to create a collection (or table in the context of MindsDB) in Couchbase and insert data into it: ​ Creating tables Now, you can use the established connection to create a collection (or table in the context of MindsDB) in Couchbase and insert data into it: CREATE TABLE couchbase_vectorsource . test_embeddings ( SELECT embeddings FROM mysql_datasource . test_embeddings ) ; mysql_datasource is another MindsDB data source that has been created by connecting to a MySQL database. The test_embeddings table in the mysql_datasource data source contains the embeddings that we want to store in Couchbase. ​ Querying and searching You can query your collection (table) as shown below: SELECT * FROM couchbase_vectorsource . test_embeddings ; To filter the data in your collection (table) by metadata, you can use the following query: SELECT * FROM couchbase_vectorsource . test_embeddings WHERE id = \"some_id\" ; To perform a vector search, the following query can be used: SELECT * FROM couchbase_vectorsource . test_embeddings WHERE embeddings = ( SELECT embeddings FROM mysql_datasource . test_embeddings LIMIT 1 ) ; ​ Deleting records You can delete documents using DELETE just like in SQL. DELETE FROM couchbase_vectorsource . test_embeddings WHERE ` metadata.test ` = 'test1' ; ​ Dropping connection To drop the connection, use this command DROP DATABASE couchbase_vectorsource ; Was this page helpful? Yes No Suggest edits Raise issue ChromaDB PGVector github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Usage Creating tables Querying and searching Deleting records Dropping connection"}
{"file_name": "pinecone.html", "content": "Pinecone - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Vector Stores Pinecone Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores ChromaDB Couchbase PGVector Pinecone Weaviate Files AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Vector Stores Pinecone This is the implementation of the Pinecone for MindsDB. Pinecone is a vector database which is fully-managed, developer-friendly, and easily scalable. ​ Prerequisites Before proceeding, ensure the following prerequisites are met: Install MindsDB locally via Docker or Docker Desktop . To connect Pinecone to MindsDB, install the required dependencies following this instruction . Install or ensure access to Pinecone. ​ Implementation This handler uses pinecone-client python library connect to a pinecone environment. The required arguments to establish a connection are: api_key : the API key that can be found in your pinecone account These optional arguments are used with CREATE TABLE statements: dimension : dimensions of the vectors to be stored in the index (default=8) metric : distance metric to be used for similarity search (default=‘cosine’) spec : the spec of the index to be created. This is a dictionary that can contain the following keys: cloud : the cloud provider to use (default=‘aws’) region : the region to use (default=‘us-east-1’) Only the creation of serverless indexes is supported at the moment when running CREATE TABLE statements. ​ Limitations DROP TABLE support Support for namespaces Display score/distance Support for creating/reading sparse values content column is not supported since it does not exist in Pinecone ​ Usage In order to make use of this handler and connect to an environment, use the following syntax: CREATE DATABASE pinecone_dev WITH ENGINE = \"pinecone\" , PARAMETERS = { \"api_key\" : \"...\" } ; You can query pinecone indexes ( temp in the following examples) based on id or search_vector , but not both: SELECT * from pinecone_dev . temp WHERE id = \"abc\" LIMIT 1 SELECT * from pinecone_dev . temp WHERE search_vector = \"[1,2,3,4,5,6,7,8]\" If you are using subqueries, make sure that the result is only a single row since the use of multiple search vectors is not allowed SELECT * from pinecone_database . temp WHERE search_vector = ( SELECT embeddings FROM sqlitetesterdb . test WHERE id = 10 ) Optionally, you can filter based on metadata too: SELECT * from pinecone_dev . temp WHERE id = \"abc\" AND metadata . hello < 100 You can delete records using id or metadata like so: DELETE FROM pinecone_dev . temp WHERE id = \"abc\" Note that deletion through metadata is not supported in starter tier DELETE FROM pinecone_dev . temp WHERE metadata . tbd = true You can insert data into a new collection like so: CREATE TABLE pinecone_dev . temp ( SELECT * FROM mysql_demo_db . temp LIMIT 10 ) ; To update records, you can use insert statement. When there is a conflicting ID in pinecone index, the record is updated with new values. It might take a bit to see it reflected. INSERT INTO pinecone_test . testtable ( id , content , metadata , embeddings ) VALUES ( 'id1' , 'this is a test' , '{\"test\": \"test\"}' , '[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]' ) ; Was this page helpful? Yes No Suggest edits Raise issue PGVector Weaviate github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Implementation Limitations Usage"}
{"file_name": "csv-xlsx-xls.html", "content": "Upload CSV, XLSX, XLS files to MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Files Upload CSV, XLSX, XLS files to MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files CSV, XLSX, XLS JSON TXT PDF Parquet AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Files Upload CSV, XLSX, XLS files to MindsDB You can upload CSV, XLSX, and XLS files of any size to MindsDB that runs locally via Docker or pip . CSV, XLSX, XLS files are stored in the form of a table inside MindsDB. ​ Upload files Follow the steps below to upload a file: Click on the Add dropdown and choose Upload file . Upload a file and provide a name used to access it within MindsDB. Alternatively, upload a file as a link and provide a name used to access it within MindsDB. ​ Query files The CSV, XLSX, and XLS files may contain one or more sheets. Here is how to query data within MindsDB. Query for the list of available sheets in the file uploaded under the name my_file . SELECT * FROM files . my_file ; Query for the content of one of the sheets listed with the command above. SELECT * FROM files . my_file . my_sheet ; Was this page helpful? Yes No Suggest edits Raise issue Weaviate JSON github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Upload files Query files"}
{"file_name": "json.html", "content": "Upload JSON files to MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Files Upload JSON files to MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files CSV, XLSX, XLS JSON TXT PDF Parquet AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Files Upload JSON files to MindsDB You can upload JSON files of any size to MindsDB that runs locally via Docker or pip . JSON files are converted into a table, if the JSON file structure allows for it. Otherwise, JSON files are stored similarly to text files. ​ Upload files Follow the steps below to upload a file: Click on the Add dropdown and choose Upload file . Upload a file and provide a name used to access it within MindsDB. Alternatively, upload a file as a link and provide a name used to access it within MindsDB. ​ Query files Here is how to query data within MindsDB. Query for the content of the file uploaded under the name my_file . SELECT * FROM files . my_file ; Was this page helpful? Yes No Suggest edits Raise issue CSV, XLSX, XLS TXT github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Upload files Query files"}
{"file_name": "txt.html", "content": "Upload TXT files to MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Files Upload TXT files to MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files CSV, XLSX, XLS JSON TXT PDF Parquet AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Files Upload TXT files to MindsDB You can upload TXT files of any size to MindsDB that runs locally via Docker or pip . TXT files are divided into chunks and stored in multiple table cells. MindsDB uses the TextLoader from LangChain to load TXT files. ​ Upload files Follow the steps below to upload a file: Click on the Add dropdown and choose Upload file . Upload a file and provide a name used to access it within MindsDB. ​ Query files Here is how to query data within MindsDB. Query for the content of the file uploaded under the name my_file . SELECT * FROM files . my_file ; Was this page helpful? Yes No Suggest edits Raise issue JSON PDF github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Upload files Query files"}
{"file_name": "parquet.html", "content": "Upload Parquet files to MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Files Upload Parquet files to MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files CSV, XLSX, XLS JSON TXT PDF Parquet AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Files Upload Parquet files to MindsDB You can upload Parquet files of any size to MindsDB that runs locally via Docker or pip . Parquet files are stored in the form of a table inside MindsDB. ​ Upload files Follow the steps below to upload a file: Click on the Add dropdown and choose Upload file . Upload a file and provide a name used to access it within MindsDB. Alternatively, upload a file as a link and provide a name used to access it within MindsDB. ​ Query files Here is how to query data within MindsDB. Query for the content of the file uploaded under the name my_file . SELECT * FROM files . my_file ; Was this page helpful? Yes No Suggest edits Raise issue PDF Overview github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Upload files Query files"}
{"file_name": "pdf.html", "content": "Upload PDF files to MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Files Upload PDF files to MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Integrations Overview Supported Integrations Data Sources Overview Sample Database Applications Databases Vector Stores Files CSV, XLSX, XLS JSON TXT PDF Parquet AI Engines Overview Large Language Models Bring Your Own Models Anomaly Detection AutoML Time Series Models Recommender Models Multi-Media Models Files Upload PDF files to MindsDB You can upload PDF files of any size to MindsDB that runs locally via Docker or pip . Note that MindsDB supports only searchable PDFs, as opposed to scanned PDFs. These are stored in the form of a table inside MindsDB. ​ Upload files Follow the steps below to upload a file: Click on the Add dropdown and choose Upload file . Upload a file and provide a name used to access it within MindsDB. ​ Query files Here is how to query data within MindsDB. Query for the content of the file uploaded under the name my_file . SELECT * FROM files . my_file ; Was this page helpful? Yes No Suggest edits Raise issue TXT Parquet github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Upload files Query files"}
{"file_name": "benefits.html", "content": "Benefits of MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation FAQs Benefits of MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website FAQs Benefits of MindsDB Persisting Predictions MindsDB and PHP Disposable Email Domains and OpenAI Missing required CPU features FAQs Benefits of MindsDB MindsDB facilitates development of AI-powered apps by bridging the gap between data and AI. Thanks to its numerous integrations with data sources (including databases, vector stores, and applications) and AI frameworks (including LLMs and AutoML), you can mix and match between the available integrations to create custom AI workflows with MindsDB. Here are some prominent benefits of using MindsDB: Unified AI Deployment and Management MindsDB integrates directly with the database, warehouse, or stream. This eliminates the need to build and maintain custom, complex data pipelines or separate systems for AI/ML deployment. Automated AI Workflows MindsDB automates the entire AI workflow to execute on time-based or event-based triggers. No need to build custom automation logic to get predictions, move data, or (re)train models. Turn every developer into an AI Engineer MindsDB enables developers to leverage their existing SQL skills, accelerating the adoption of AI across teams and departments, turning every developer into an AI Engineer. Enhanced Scalability and Performance Whether in your private cloud or using MindsDB’s managed service, MindsDB enables you to handle large-scale AI/ML workloads efficiently. MindsDB can scale to meet the demands of your use case, ensuring optimal performance and responsiveness. Was this page helpful? Yes No Suggest edits Raise issue Persisting Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "persist-predictions.html", "content": "How to Persist Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation FAQs How to Persist Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website FAQs Benefits of MindsDB Persisting Predictions MindsDB and PHP Disposable Email Domains and OpenAI Missing required CPU features FAQs How to Persist Predictions MindsDB provides a range of options for persisting predictions and forecasts. Let’s explore all possibilities to save the prediction results. Reasons to Save Predictions Every time you want to get predictions, you need to query the model, usually joined with an input data table, like this: SELECT input . product_name , input . review , output . sentiment FROM mysql_demo_db . amazon_reviews AS input JOIN sentiment_classifier AS output ; However, querying the model returns the result set that is not persistent by default. For future use, it is recommended to persist the result set instead of querying the model again with the same data. MindsDB enables you to save predictions into a view or a table or download as a CSV file. ​ Creating a View After creating the model, you can save the prediction results into a view. CREATE VIEW review_sentiment ( -- querying for predictions SELECT input . product_name , input . review , output . sentiment FROM mysql_demo_db . amazon_reviews AS input JOIN sentiment_classifier AS output LIMIT 10 ) ; Now the review_sentiment view stores sentiment predictions made for all customer reviews. Here is a comprehensive tutorial on how to predict sentiment of customer reviews using OpenAI. ​ Creating a Table After creating the model, you can save predictions into a database table. CREATE TABLE local_postgres . question_answers ( -- querying for predictions SELECT input . article_title , input . question , output . answer FROM mysql_demo_db . questions AS input JOIN question_answering_model AS output LIMIT 10 ) ; Here, the local_postgres database is a PostgreSQL database connected to MindsDB with a user that has the write access. Now the question_answers table stores all prediction results. Here is a comprehensive tutorial on how to answer questions using OpenAI. ​ Downloading a CSV File After executing the SELECT statement, you can download the output as a CSV file. Click the Export button and choose the CSV option. Was this page helpful? Yes No Suggest edits Raise issue Benefits of MindsDB MindsDB and PHP github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Creating a View Creating a Table Downloading a CSV File"}
{"file_name": "disposable-email-doman-and-openai.html", "content": "Disposable Email Domains and OpenAI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation FAQs Disposable Email Domains and OpenAI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website FAQs Benefits of MindsDB Persisting Predictions MindsDB and PHP Disposable Email Domains and OpenAI Missing required CPU features FAQs Disposable Email Domains and OpenAI Disposable email domains can’t make use of OpenAI, therefore users will encounter errors with using MindsDB’s integration with OpenAI. To check if your email domain is disposable, you can verify it on QuickEmailVerification or VerifyEmail.IO . Was this page helpful? Yes No Suggest edits Raise issue MindsDB and PHP Missing required CPU features github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "mindsdb-with-php.html", "content": "How to Interact with MindsDB from PHP - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation FAQs How to Interact with MindsDB from PHP Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website FAQs Benefits of MindsDB Persisting Predictions MindsDB and PHP Disposable Email Domains and OpenAI Missing required CPU features FAQs How to Interact with MindsDB from PHP To get started with MindsDB, you need to install MindsDB locally via Docker or Docker Desktop . There are a few ways you can interact with MindsDB from the PHP code. You can connect to MindsDB using the PHP Data Objects and execute statements directly on MindsDB with the PDO::query method. You can use the REST API endpoints to interact with MindsDB directly from PHP. Was this page helpful? Yes No Suggest edits Raise issue Persisting Predictions Disposable Email Domains and OpenAI github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "whitelist-ips.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "missing-required-cpu-features.html", "content": "Missing required CPU features - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation FAQs Missing required CPU features Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website FAQs Benefits of MindsDB Persisting Predictions MindsDB and PHP Disposable Email Domains and OpenAI Missing required CPU features FAQs Missing required CPU features Depending on the operating system and its setup, you may encounter this runtime warning when starting MindsDB: RuntimeWarning: Missing required CPU features. The following required CPU features were not detected: avx2, fma, bmi1, bmi2, lzcnt The solution is to install the polars-lts-cpu package in the environment where MindsDB runs. If you are on an Apple ARM machine (e.g. M1), this warning is likely due to running Python under Rosetta. To troubleshoot it, install a native version of Python that does not run under Rosetta x86-64 emulation. Was this page helpful? Yes No Suggest edits Raise issue Disposable Email Domains and OpenAI github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "text-summarization-inside-mongodb-with-openai.html", "content": "Text Summarization with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Text Summarization with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Text Summarization with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to summarize an article. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.articles.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d01398bbca62e9c7774ab8' , article: \"Video footage has emerged of a law enforcement officer…\" , highlights: 'The 53 -second video features…\" } { _id: '63d01398bbca62e9c7774ab9' , article: \"A new restaurant is offering a five-course…\" , highlights: \"The Curious Canine Kitchen is…\" } { _id: '63d01398bbca62e9c7774aba' , article: 'Mother-of-two Anna Tilley survived after spending four days…' , highlights: 'Experts have warned hospitals not using standard treatment…' } Let’s create a model collection to summarize all articles from the input dataset: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'text_summarization' , predict: 'highlights' , training_options: { engine: 'openai_engine' , prompt_template: 'provide an informative summary of the text text:{{article}} using full sentences' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called text_summarization that uses the OpenAI integration to predict a field named highlights . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'text_summarization' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.text_summarization.find ( { article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\" } ) Here is the output data: { highlights: \"Apple's Watch hits stores this Friday, and employees will be able to pre-order the\" , article: \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device.\" } Or by joining with a collection for batch predictions: mindsdb > db.text_summarization.find ( { 'collection' : 'mongo_demo_db.articles' } , { 'text_summarization.highlights' : 'highlights' , 'articles.article' : 'article' } ) .limit ( 3 ) Here is the output data: { highlights: 'A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was' , article: \"Video footage has emerged of a law enforcement officer...\" } { highlights: 'A new restaurant in London is offering a five-course drink-paired menu for dogs' , article: \"A new restaurant is offering a five-course...\" } { highlights: \"Sepsis is a potentially life-threatening condition that occurs when the body's response to an\" , article: 'Mother-of-two Anna Tilley survived after spending four days...' } The articles collection is used to make batch predictions. Upon joining the text_summarization model with the articles collection, the model uses all values from the article field. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Text Summarization using SQL Sentiment Analysis using SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "nlp-extended-examples.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "question-answering-inside-mysql-with-openai.html", "content": "Question Answering with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Question Answering with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Question Answering with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to answer questions in a specified domain. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . questions LIMIT 3 ; Here is the output: + ------------------+--------------------------------------------------------+-------------+ | article_title | question | true_answer | + ------------------+--------------------------------------------------------+-------------+ | Alessandro_Volta | Was Volta an Italian physicist? | yes | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | no | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | yes | + ------------------+--------------------------------------------------------+-------------+ Let’s create a model table to answer all questions from the input dataset: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL question_answering_model PREDICT answer USING engine = 'openai_engine' , prompt_template = 'answer the question of text:{{question}} about text:{{article_title}}' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called question_answering_model that uses the OpenAI integration to predict a column named answer . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE question_answering_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT article_title , question , answer FROM question_answering_model WHERE question = 'Was Abraham Lincoln the sixteenth President of the United States?' AND article_title = 'Abraham_Lincoln' ; Here is the output data: + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ | article_title | question | answer | + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ | Abraham_Lincoln | Was Abraham Lincoln the sixteenth President of the United States? | Yes , Abraham Lincoln was the sixteenth President of the United States . | + ------------------+-------------------------------------------------------------------+------------------------------------------------------------------------+ Or by joining with another table for batch predictions: SELECT input . article_title , input . question , output . answer FROM mysql_demo_db . questions AS input JOIN question_answering_model AS output LIMIT 3 ; Here is the output data: + ------------------+--------------------------------------------------------+--------------------------------------------------------+ | article_title | question | answer | + ------------------+--------------------------------------------------------+--------------------------------------------------------+ | Alessandro_Volta | Was Volta an Italian physicist? | Yes , Volta was an Italian physicist . | | Alessandro_Volta | Is Volta buried in the city of Pittsburgh? | No , Volta is not buried in the city of Pittsburgh . | | Alessandro_Volta | Did Volta have a passion for the study of electricity? | Yes , Volta had a passion for the study of electricity . | + ------------------+--------------------------------------------------------+--------------------------------------------------------+ The questions table is used to make batch predictions. Upon joining the question_answering_model model with the questions table, the model uses all values from the article_title and question columns. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Sentiment Analysis in MySQL Text Summarization in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Sentiment Analysis using MQL Question Answering using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "nlp-mindsdb-openai.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "sentiment-analysis-inside-mongodb-with-openai.html", "content": "Sentiment Analysis with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.amazon_reviews.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d013b5bbca62e9c7774b1d' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).' } { _id: '63d013b5bbca62e9c7774b1e' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\" } { _id: '63d013b5bbca62e9c7774b1f' , product_name: 'All-New Fire HD 8 Tablet, 8 HD Display, Wi-Fi, 16 GB - Includes Special Offers, Magenta' , review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.' } Let’s create a model collection to identify sentiment for all reviews: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'sentiment_classifier' , predict: 'sentiment' , training_options: { engine: 'openai_engine' , prompt_template: 'describe the sentiment of the reviews strictly as \"positive\", \"neutral\", or \"negative\". \"I love the product\":positive \"It is a scam\":negative \"{{review}}.\":' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called sentiment_classifier that uses the OpenAI integration to predict a field named sentiment . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'sentiment_classifier' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.sentiment_classifier.find ( { review: 'It is ok.' } ) Here is the output data: { sentiment: 'neutral' , review: 'It is ok.' } Or by joining with a collection for batch predictions: mindsdb > db.sentiment_classifier.find ( { 'collection' : 'mongo_demo_db.amazon_reviews' } , { 'sentiment_classifier.sentiment' : 'sentiment' , 'amazon_reviews.review' : 'review' } ) .limit ( 3 ) Here is the output data: { sentiment: 'positive' , review: 'Late gift for my grandson. He is very happy with it. Easy for him (9yo ).' } { sentiment: 'positive' , review: \"I'm not super thrilled with the proprietary OS on this unit, but it does work okay and does what I need it to do. Appearance is very nice, price is very good and I can't complain too much - just wish it were easier (or at least more obvious) to port new apps onto it. For now, it helps me see things that are too small on my phone while I'm traveling. I'm a happy buyer.\" } { sentiment: 'positive' , review: 'I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren. They basically use it to play Amazon games that you download.' } The amazon_reviews collection is used to make batch predictions. Upon joining the sentiment_classifier model with the amazon_reviews collection, the model uses all values from the review field. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Sentiment Analysis using SQL Question Answering using SQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "sentiment-analysis-inside-mysql-with-openai.html", "content": "Sentiment Analysis with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Sentiment Analysis with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. This example is a sentiment analysis where we infer emotions behind a text. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to infer emotions behind a text, a task also known as sentiment analysis. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . amazon_reviews LIMIT 3 ; Here is the output: + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | product_name | review | + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | Late gift for my grandson . He is very happy with it . Easy for him ( 9 yo ) . | | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | I'm not super thrilled with the proprietary OS on this unit , but it does work okay and does what I n | | All - New Fire HD 8 Tablet , 8 HD Display , Wi - Fi , 16 GB - Includes Special Offers , Magenta | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren . They basic | + -----------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+ Let’s create a model table to identify sentiment for all reviews: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL sentiment_classifier_model PREDICT sentiment USING engine = 'openai_engine' , prompt_template = ' describe the sentiment of the reviews strictly as \"positive\" , \"neutral\" , or \"negative\" . \"I love the product\" :positive \"It is a scam\" :negative \"{{review}}.\" :' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called sentiment_classifier_model that uses the OpenAI integration to predict a column named sentiment . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE sentiment_classifier_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT review , sentiment FROM sentiment_classifier_model WHERE review = 'It is ok.' ; Here is the output data: + -----------+-----------+ | review | sentiment | + -----------+-----------+ | It is ok . | neutral | + -----------+-----------+ Or by joining with another table for batch predictions: SELECT input . review , output . sentiment FROM mysql_demo_db . amazon_reviews AS input JOIN sentiment_classifier_model AS output LIMIT 3 ; Here is the output data: + ------------------------------------------------------------------------------------------------------+-----------+ | review | sentiment | + ------------------------------------------------------------------------------------------------------+-----------+ | Late gift for my grandson . He is very happy with it . Easy for him ( 9 yo ) . | positive | | I'm not super thrilled with the proprietary OS on this unit , but it does work okay and does what I n | positive | | I purchased this Kindle Fire HD 8 was purchased for use by 5 and 8 yer old grandchildren . They basic | positive | + ------------------------------------------------------------------------------------------------------+-----------+ The amazon_reviews table is used to make batch predictions. Upon joining the sentiment_classifier_model model with the amazon_reviews table, the model uses all values from the review column. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Question Answering in MySQL Text Summarization in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Text Summarization using MQL Sentiment Analysis using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "text-summarization-inside-mysql-with-openai.html", "content": "Text Summarization with MindsDB and OpenAI using SQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Text Summarization with MindsDB and OpenAI using SQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Text Summarization with MindsDB and OpenAI using SQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a model to provide a summary of a text. The input data is taken from our sample MySQL database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ Tutorial In this tutorial, we create a predictive model to summarize an article. We use a table from our MySQL public demo database, so let’s start by connecting MindsDB to it: CREATE DATABASE mysql_demo_db WITH ENGINE = 'mysql' , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: SELECT * FROM mysql_demo_db . articles LIMIT 3 ; Here is the output: + ----------------------------------------------------------------+--------------------------------------------------------------+ | article | highlights | + ----------------------------------------------------------------+--------------------------------------------------------------+ | Video footage has emerged of a law enforcement officer… | The 53 - second video features… | | A new restaurant is offering a five - course drink - paired menu… | The Curious Canine Kitchen is … | | Mother - of - two Anna Tilley survived after spending four days… | Experts have warned hospitals not using standard treatment… | + ----------------------------------------------------------------+--------------------------------------------------------------+ Let’s create a model table to summarize all articles from the input dataset: Before creating an OpenAI model, please create an engine, providing your OpenAI API key: CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL text_summarization_model PREDICT highlights USING engine = 'openai_engine' , prompt_template = 'provide an informative summary of the text text:{{article}} using full sentences' ; In practice, the CREATE MODEL statement triggers MindsDB to generate an AI table called text_summarization_model that uses the OpenAI integration to predict a column named highlights . The model lives inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The USING clause specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the CREATE MODEL statement has started execution, we can check the status of the creation process with the following query: DESCRIBE text_summarization_model ; It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI table – you can query it either by specifying synthetic data in the actual query: SELECT article , highlights FROM text_summarization_model WHERE article = \"Apple's Watch hits stores this Friday when customers and employees alike will be able to pre - order the timepiece . And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device . \" ; Here is the output data: + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ | article | highlights | + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ | Apple 's Watch hits stores this Friday when customers and employees alike will be able to pre-order the timepiece. And boss Tim Cook is rewarding his staff by offering them a 50 per cent discount on the device. | Apple' s Watch hits stores this Friday , and employees will be able to pre - order the | + --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+ Or by joining with another table for batch predictions: SELECT input . article , output . highlights FROM mysql_demo_db . articles AS input JOIN text_summarization_model AS output LIMIT 3 ; Here is the output data: + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ | article | highlights | + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ | Video footage has emerged of a law enforcement officer… | A video has emerged of a law enforcement officer grabbing a cell phone from a woman who was | | A new restaurant is offering a five - course drink - paired menu… | A new restaurant in London is offering a five - course drink - paired menu for dogs | | Mother - of - two Anna Tilley survived after spending four days… | Sepsis is a potentially life - threatening condition that occurs when the body's response to an | + ----------------------------------------------------------------+------------------------------------------------------------------------------------------------+ The articles table is used to make batch predictions. Upon joining the text_summarization_model model with the articles table, the model uses all values from the article column. ​ Leverage the NLP Capabilities with MindsDB By integrating databases and OpenAI using MindsDB, developers can easily extract insights from text data with just a few SQL commands. These powerful natural language processing (NLP) models are capable of answering questions with or without context and completing general prompts. Furthermore, these models are powered by large pre-trained language models from OpenAI, so there is no need for manual development work. Ultimately, this provides developers with an easy way to incorporate powerful NLP capabilities into their applications while saving time and resources compared to traditional ML development pipelines and methods. All in all, MindsDB makes it possible for developers to harness the power of OpenAI efficiently! MindsDB is now the fastest-growing open-source applied machine-learning platform in the world. Its community continues to contribute to more than 70 data-source and ML-framework integrations. Stay tuned for the upcoming features - including more control over the interface parameters and fine-tuning models directly from MindsDB! Experiment with OpenAI models within MindsDB and unlock the ML capability over your data in minutes. Finally, if MindsDB’s vision to democratize ML sounds exciting, head to our community Slack , where you can get help and find people to chat about using other available data sources, ML frameworks, or writing a handler to bring your own! Follow our introduction to MindsDB’s OpenAI integration here . Also, we’ve got a variety of tutorials that use MySQL and MongoDB: Sentiment Analysis in MySQL Question Answering in MySQL Sentiment Analysis in MongoDB Question Answering in MongoDB Text Summarization in MongoDB ​ What’s Next? Have fun while trying it out yourself! Bookmark MindsDB repository on GitHub . Engage with the MindsDB community on Slack or GitHub to ask questions and share your ideas and thoughts. If this tutorial was helpful, please give us a GitHub star here . Was this page helpful? Yes No Suggest edits Raise issue Extract JSON Text Summarization using MQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites Tutorial Leverage the NLP Capabilities with MindsDB What’s Next?"}
{"file_name": "question-answering-inside-mongodb-with-openai.html", "content": "Question Answering with MindsDB and OpenAI using MQL - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Enrichment Question Answering with MindsDB and OpenAI using MQL Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Use Cases Overview Automated Fine-Tuning AI Agents AI-Powered Data Retrieval Data Enrichment Overview Image Generator Extract JSON Text Summarization using SQL Text Summarization using MQL Sentiment Analysis using SQL Sentiment Analysis using MQL Question Answering using SQL Question Answering using MQL Text Sentiment with Hugging Face Hugging Face Models Hugging Face Inference API Predictive Analytics In-Database Machine Learning AI Workflow Automation Data Enrichment Question Answering with MindsDB and OpenAI using MQL ​ Introduction In this blog post, we present how to create OpenAI models within MindsDB. In this example, we ask a question to a model and get an answer. The input data is taken from our sample MongoDB database. ​ Prerequisites To follow along, install MindsDB locally via Docker or Docker Desktop . ​ How to Connect MindsDB to a Database We use a collection from our MongoDB public demo database, so let’s start by connecting MindsDB to it. You can use Mongo Compass or Mongo Shell to connect our sample database like this: test > use mindsdb mindsdb > db.databases.insertOne ( { 'name' : 'mongo_demo_db' , 'engine' : 'mongodb' , 'connection_args' : { \"host\" : \"mongodb+srv://user:MindsDBUser123!@demo-data-mdb.trzfwvb.mongodb.net/\" , \"database\" : \"public\" } } ) ​ Tutorial In this tutorial, we create a predictive model to answer questions in a specified domain. Now that we’ve connected our database to MindsDB, let’s query the data to be used in the example: mindsdb > use mongo_demo_db mongo_demo_db > db.questions.find ( { } ) .limit ( 3 ) Here is the output: { _id: '63d01350bbca62e9c77732c0' , article_title: 'Alessandro_Volta' , question: 'Was Volta an Italian physicist?' , true_answer: 'yes' } { _id: '63d01350bbca62e9c77732c1' , article_title: 'Alessandro_Volta' , question: 'Is Volta buried in the city of Pittsburgh?' , true_answer: 'no' } { _id: '63d01350bbca62e9c77732c2' , article_title: 'Alessandro_Volta' , question: 'Did Volta have a passion for the study of electricity?' , true_answer: 'yes' } Let’s create a model collection to answer all questions from the input dataset: Note that you need to create an OpenAI engine first before deploying the OpenAI model within MindsDB. Here is how to create this engine: mongo_demo_db > use mindsdb mindsdb > db.ml_engines.insertOne ( { \"name\" : \"openai_engine\" , \"handler\" : \"openai\" , \"params\" : { \"openai_api_key\" : \"your-openai-api-key\" } } ) mongo_demo_db > use mindsdb mindsdb > db.models.insertOne ( { name: 'question_answering' , predict: 'answer' , training_options: { engine: 'openai_engine' , prompt_template: 'answer the question of text:{{question}} about text:{{article_title}}' } } ) In practice, the insertOne method triggers MindsDB to generate an AI collection called question_answering that uses the OpenAI integration to predict a field named answer . The model is created inside the default mindsdb project. In MindsDB, projects are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . The training_options key specifies the parameters that this handler requires. The engine parameter defines that we use the openai engine. The prompt_template parameter conveys the structure of a message that is to be completed with additional text generated by the model. Follow this instruction to set up the OpenAI integration in MindsDB. Once the insertOne method has started execution, we can check the status of the creation process with the following query: mindsdb > db.models.find ( { 'name' : 'question_answering' } ) It may take a while to register as complete depending on the internet connection. Once the creation is complete, the behavior is the same as with any other AI collection – you can query it either by specifying synthetic data in the actual query: mindsdb > db.question_answering.find ( { question: 'Was Abraham Lincoln the sixteenth President of the United States?' , article_title: 'Abraham_Lincoln' } ) Here is the output data: { answer: 'Yes, Abraham Lincoln was the sixteenth President of the United States.' , question: 'Was Abraham Lincoln the sixteenth President of the United States?' , article_title: 'Abraham_Lincoln' } Or by joining with a collection for batch predictions: mindsdb > db.question_answering.find ( { 'collection' : 'mongo_demo_db.questions' } , { 'question_answering.answer' : 'answer' , 'questions.question' : 'question' , 'questions.article_title' : 'article_title' } ) .limit ( 3 ) Here is the output data: { answer: 'Yes, Volta was an Italian physicist.' , question: 'Was Volta an Italian physicist?' , article_title: 'Alessandro_Volta' } { answer: 'No, Volta is not buried in the city of Pittsburgh.' , question: 'Is Volta buried in the city of Pittsburgh?' , article_title: 'Alessandro_Volta' } { answer: 'Yes, Volta had a passion for the study of electricity. He was fascinated by the' , question: 'Did Volta have a passion for the study of electricity?' , article_title: 'Alessandro_Volta' } The questions collection is used to make batch predictions. Upon joining the question_answering model with the questions collection, the model uses all values from the article_title and question fields. Check out this blog post on time series forecasting with Nixtla and MindsDB using MongoDB-QL . Was this page helpful? Yes No Suggest edits Raise issue Question Answering using SQL Text Sentiment with Hugging Face github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Introduction Prerequisites How to Connect MindsDB to a Database Tutorial"}
{"file_name": "syntax.html", "content": "MindsDB SQL Syntax - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation SQL API MindsDB SQL Syntax Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support SQL API MindsDB SQL Syntax In general, MindsDB SQL attempts to follow the syntax conventions of MySQL and PostgreSQL. The following sections describe some common conventions of MindsDB SQL. ​ Notation We adopt the following notation to describe our commands: Reserved words are capitalized. For example: SELECT Square brackets [] indicate optional clauses Curly brackets {} indicate logical or choices with the options separated by | . For examples {x | y | z} Red denotes required clauses, choices, or identifiers. For readability, statements enclosed in red square or curly brackets are black. Square brackets with a separator followed by three ellipses denote repetition of the previous item, separated by the separator. For example, identifier [, ...] denotes a comma separated list of identifiers. Parentheses () are reserved characters of the MindsDB SQL and are not part of the notation. Where present parentheses are required unless explicitly wrapped in square brackets. ​ Single/Double Quotes & Backticks Identifiers (databases, tables, and column names) with special characters or reserved words must be enclosed with backticks ”`”: SELECT * FROM ` select ` WHERE ` select ` . id > 100 ; SELECT * FROM ` select-DATABASE ` WHERE ` select-DATABASE ` . id > 100 ; String values are represented by single and double quotes: SELECT * FROM table_name WHERE table_name . column_name = 'string' ; SELECT * FROM table_name WHERE table_name . column_name = \"string\" ; ​ Parentheses SQL statements can be nested with parentheses: SELECT * FROM ( SELECT * FROM table_name WHERE table_name . column_name = 'string' ) ; Was this page helpful? Yes No Suggest edits Raise issue Overview MindsDB SQL Editor github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Notation Single/Double Quotes & Backticks Parentheses"}
{"file_name": "overview.html", "content": "SQL API - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation SQL API SQL API Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support SQL API SQL API MindsDB enhances standard SQL by providing AI building blocks. This section introduces custom SQL syntax provided by MindsDB to bring data and AI together. Follow these steps to get started: 1 Connect a data source Use CREATE DATABASE to connect your data source to MindsDB. Explore all available data sources here . 2 Configure an AI engine Use CREATE ML_ENGINE to configure an engine of your choice. Explore all available AI engines here . 3 Create and deploy an AI/ML model Use CREATE MODEL to create, train, and deploy AI/ML models within MindsDB. 4 Query for predictions Query for a single prediction or batch predictions by joining data with models. 5 Automate customized workflows Use JOB , TRIGGER , or AGENT to automate workflows. Was this page helpful? Yes No Suggest edits Raise issue MindsDB SQL Syntax github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "custom_functions.html", "content": "Upload Custom Functions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Functions Upload Custom Functions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Custom Functions The LLM() Function Standard SQL Support Functions Upload Custom Functions Custom functions provide advanced means of manipulating data. Users can upload custom functions written in Python to MindsDB and apply them to data. ​ How It Works You can upload your custom functions via the MindsDB editor by clicking Add and Upload custom functions , like this: Here is the form that needs to be filled out in order to bring your custom functions to MindsDB: Let’s briefly go over the files that need to be uploaded: The Python file stores an implementation of your custom functions. Here is the sample format: def function_name_1 ( a : type , b : type ) - > type : < implementation goes here > return x def function_name_2 ( a : type , b : type , c : type ) - > type : < implementation goes here > return x Note that if the input and output types are not set, then str is used by default. Example def add_integers ( a : int , b : int ) - > int : return a + b The optional requirements file, or requirements.txt , stores all dependencies along with their versions. Here is the sample format: dependency_package_1 = = version dependency_package_2 >= version dependency_package_3 >= verion , < version . . . Example pandas scikit - learn Once you upload the above files, please provide the name for a storage collection. Let’s look at an example. ​ Example We upload the custom functions, as below: Here we upload the functions.py file that stores an implementation of the functions and the requirements.txt file that stores all the dependencies. We named the storage collection as custom_functions . Now we can use the functions as below: SELECT functions . add_integers ( sqft , 1 ) AS added_one , sqft FROM example_db . home_rentals LIMIT 1 ; Here is the output: + -----------+------+ | added_one | sqft | + -----------+------+ | 918 | 917 | + -----------+------+ Was this page helpful? Yes No Suggest edits Raise issue Knowledge Base The LLM() Function github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How It Works Example"}
{"file_name": "llm_function.html", "content": "The LLM() Function - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Functions The LLM() Function Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Custom Functions The LLM() Function Standard SQL Support Functions The LLM() Function MindsDB provides the LLM() function that lets users incorporate the LLM-generated output directly into the data queries. ​ Prerequisites To use the LLM() function with MindsDB, choosing one of the available model providers and define the following environment variables. OpenAI Here are the environment variables for the OpenAI provider: LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_MAX_RETRIES LLM_FUNCTION_MAX_TOKENS LLM_FUNCTION_BASE_URL OPENAI_API_KEY LLM_FUNCTION_API_ORGANIZATION LLM_FUNCTION_REQUEST_TIMEOUT Note that the values stored in the environment variables are specific for each provider. Anthropic Here are the environment variables for the Anthropic provider: LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_MAX_TOKENS LLM_FUNCTION_TOP_P LLM_FUNCTION_TOP_K LLM_FUNCTION_DEFAULT_REQUEST_TIMEOUT LLM_FUNCTION_API_KEY LLM_FUNCTION_BASE_URL Note that the values stored in the environment variables are specific for each provider. Anyscale Here are the environment variables for the Anyscale provider: LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_MAX_RETRIES LLM_FUNCTION_MAX_TOKENS LLM_FUNCTION_BASE_URL LLM_FUNCTION_API_KEY LLM_FUNCTION_PROXY LLM_FUNCTION_REQUEST_TIMEOUT Note that the values stored in the environment variables are specific for each provider. LiteLLM Here are the environment variables for the LiteLLM provider: LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_API_BASE LLM_FUNCTION_MAX_RETRIES LLM_FUNCTION_MAX_TOKENS LLM_FUNCTION_TOP_P LLM_FUNCTION_TOP_K Note that the values stored in the environment variables are specific for each provider. Ollama Here are the environment variables for the Ollama provider: LLM_FUNCTION_BASE_URL LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_TOP_P LLM_FUNCTION_TOP_K LLM_FUNCTION_REQUEST_TIMEOUT LLM_FUNCTION_FORMAT LLM_FUNCTION_HEADERS LLM_FUNCTION_NUM_PREDICT LLM_FUNCTION_NUM_CTX LLM_FUNCTION_NUM_GPU LLM_FUNCTION_REPEAT_PENALTY LLM_FUNCTION_STOP LLM_FUNCTION_TEMPLATE Note that the values stored in the environment variables are specific for each provider. Nvidia NIMs Here are the environment variables for the Nvidia NIMs provider: LLM_FUNCTION_BASE_URL LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_TEMPERATURE LLM_FUNCTION_TOP_P LLM_FUNCTION_REQUEST_TIMEOUT LLM_FUNCTION_FORMAT LLM_FUNCTION_HEADERS LLM_FUNCTION_NUM_PREDICT LLM_FUNCTION_NUM_CTX LLM_FUNCTION_NUM_GPU LLM_FUNCTION_REPEAT_PENALTY LLM_FUNCTION_STOP LLM_FUNCTION_TEMPLATE LLM_FUNCTION_NVIDIA_API_KEY Note that the values stored in the environment variables are specific for each provider. MindsDB Here are the environment variables for the MindsDB provider: LLM_FUNCTION_MODEL_NAME LLM_FUNCTION_PROJECT_NAME To use MindsDB as a provider, create a model in a project within MindsDB and use its name in the LLM_FUNCTION_MODEL_NAME environment variable and the project name in the LLM_FUNCTION_PROJECT_NAME environment variable. ​ Usage You can use the LLM() function to simply ask a question and get an answer. SELECT LLM ( 'How many planets are there in the solar system?' ) ; Here is the output: + ------------------------------------------+ | llm | + ------------------------------------------+ | There are 8 planets in the solar system . | + ------------------------------------------+ Moreover, you can the LLM() function with your data to swiftly complete tasks such as text generation or summarization. SELECT comment , LLM ( 'Describe the comment''s category in one word: ' || comment ) AS category FROM example_db . user_comments ; Here is the output: + --------------------------+----------+ | comment | category | + --------------------------+----------+ | I hate tacos | Dislike | | I want to dance | Desire | | Baking is not a big deal | Opinion | + --------------------------+----------+ Was this page helpful? Yes No Suggest edits Raise issue Custom Functions CTEs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Prerequisites Usage"}
{"file_name": "window-functions.html", "content": "SQL Window Functions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Standard SQL Support SQL Window Functions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support CTEs CASE WHEN Aggregate Functions Window Functions Standard SQL Support SQL Window Functions MindsDB supports standard SQL syntax, including the SQL window functions. Window functions in SQL perform calculations across a set of table rows related to the current row, without collapsing rows into a single result like aggregate functions do. These functions are useful for ranking, calculating running totals, and working with moving averages. Common window functions include ROW_NUMBER() , RANK() , DENSE_RANK() , NTILE(n) , LAG() , LEAD() , SUM() , and AVG() . SELECT a + b * ( c - d ) as a , AVG ( a ) OVER ( PARTITION BY b ) FROM table_name ; Was this page helpful? Yes No Suggest edits Raise issue Aggregate Functions github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "case-when.html", "content": "The CASE WHEN Statement - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Standard SQL Support The CASE WHEN Statement Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support CTEs CASE WHEN Aggregate Functions Window Functions Standard SQL Support The CASE WHEN Statement MindsDB supports standard SQL syntax, including the CASE WHEN statement. The CASE WHEN statement is used for conditional logic within queries. It evaluates conditions and returns specific values based on whether each condition is true or false, allowing for conditional output within SELECT , WHERE , and other clauses. SELECT CASE WHEN a = 1 THEN a + b WHEN 1 + 2 = b * 2 THEN 0 WHEN ( a + b > 2 or b < c + 3 ) AND a > b THEN b ELSE c END FROM table_name ; Was this page helpful? Yes No Suggest edits Raise issue CTEs Aggregate Functions github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "cte.html", "content": "Common Table Expressions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Standard SQL Support Common Table Expressions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support CTEs CASE WHEN Aggregate Functions Window Functions Standard SQL Support Common Table Expressions MindsDB supports standard SQL syntax, including Common Table Expressions (CTEs). CTEs are used to create temporary, named result sets that simplify complex queries, enhance readability, and allow for modular query design by breaking down large queries into manageable parts. WITH table_name1 AS ( SELECT columns FROM table1 t1 JOIN table2 t2 ON t1 . col = t2 . col ) , table_name2 AS ( SELECT columns FROM table1 t1 JOIN table2 t2 ON t1 . col = t2 . col ) SELECT columns FROM table_name1 t1 JOIN table_name2 t2 ON t1 . col - t2 . col ; Was this page helpful? Yes No Suggest edits Raise issue The LLM() Function CASE WHEN github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "aggregate-functions.html", "content": "SQL Aggregate Functions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Standard SQL Support SQL Aggregate Functions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support CTEs CASE WHEN Aggregate Functions Window Functions Standard SQL Support SQL Aggregate Functions MindsDB supports standard SQL syntax, including the SQL aggregate functions. SQL aggregate functions perform calculations on a set of values and return a single result, making them useful for summarizing or analyzing data across multiple rows. Common aggregate functions include COUNT() , SUM() , AVG() , MIN() , and MAX() . These functions are used with GROUP BY to organize results by specific categories. SELECT year , SUM ( salary ) AS annual_salary FROM salaries GROUP BY year ; Was this page helpful? Yes No Suggest edits Raise issue CASE WHEN Window Functions github facebook twitter slack linkedin youtube medium Powered by Mintlify"}
{"file_name": "knowledge-bases.html", "content": "Knowledge Base - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Knowledge Base Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Agent Chatbot Knowledge Base Functions Standard SQL Support AI Agents Knowledge Base A knowledge base is a batteries-included RAG system that you can create and insert data into, as well as query as if it were a table. Internally, knowledge bases use a vector store and an embedding model. By default, it uses the ChromaDB vector store and the OpenAI embedding model, which requires an OpenAI API key set as an evironment variable. You can define a vector store and an embedding model as in the examples below. ​ Syntax Here is a general syntax for creating a knowledge base: CREATE KNOWLEDGE_BASE my_kb USING model = embedding_model , -- optional storage = vector_database . storage_table , -- optional metadata_columns = [ 'date' , 'creator' , . . . ] , -- optional content_columns = [ 'review' , 'content' , . . . ] , -- optional id_column = 'index' ; -- optional Where all the parameters are optional: model : This is an embedding model created within MindsDB with CREATE MODEL embedding_model . If you do not provide the model parameter, then the OpenAI’s text-embedding-ada-002 model is used by default, provided that the OPENAI_API_KEY environment variable is defined. storage : This is a vector database that stores embedded data. It defaults to ChromaDB, or you can connect your vector store to MindsDB with CREATE DATABASE vector_database . metadata_columns : The list of column names that will be stored in the metadata column of the knowledge base. If not set, the metadata column is not used. content_columns : The list of column names that will be stored in the content column of the knowledge base. If not set, all columns are stored in the content column. id_column : The column name that will be stored in the id column of the knowledge base to uniquely identitify the data. Each knowledge base comprises the id , content , and metadata columns that store data defined in the parameters. ​ Examples This section presents examples of how to create knowledge bases and insert data for storage in the form of embeddings. ​ Knowledge Base with OpenAI Embedding Model Note that using OpenAI’s embedding model requires OpenAI API key. First, create an engine through which the model is accessed. CREATE ML_ENGINE embedding FROM langchain_embedding ; Next, create an embedding model, providing an OpenAI API key. CREATE MODEL embedding_model PREDICT embeddings USING engine = \"embedding\" , class = \"openai\" , model = \"text-embedding-ada-002\" , openai_api_key = \"sk-xxx\" , input_columns = [ \"content\" ] ; Analyze the data that you want to insert into a knowledge base: SELECT * FROM example_db . home_rentals ; Here is the output: + -------------------+-------------------+------+----------+----------------+----------------+--------------+----------------------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | neighborhood | rental_price | created_at | + -------------------+-------------------+------+----------+----------------+----------------+--------------+----------------------------+ | 2 | 1 | 917 | great | 13 | berkeley_hills | 3901 | 2024 - 02 - 24 02 : 28 : 21.746167 | | 0 | 1 | 194 | great | 10 | berkeley_hills | 2042 | 2024 - 02 - 19 06 : 10 : 59.693052 | | 1 | 1 | 543 | poor | 18 | westbrae | 1871 | 2024 - 02 - 12 07 : 53 : 45.914146 | + -------------------+-------------------+------+----------+----------------+----------------+--------------+----------------------------+ Decide which columns to use as content and which ones as metadata . For example, we use the days_on_market and neighborhood columns as metadata and the location and rental_price columns as content . Now that you have an embedding model, create a knowledge base, passing this embedding model and defining the content and metadata columns. CREATE KNOWLEDGE_BASE kb_custom_model USING model = embedding_model , metadata_columns = [ 'days_on_market' , 'neighborhood' ] , content_columns = [ 'location' , 'rental_price' ] ; After successful creation of a knowledge base, insert data to store it in the form of embeddings. INSERT INTO kb_custom_model ( SELECT * FROM example_db . home_rentals ) ; Finally, you can verify that the data has been inserted into the knowledge base by querying it. SELECT * FROM kb_custom_model ; Here is the output: + ----+------------------------------+----------------------------------------+ | id | content | metadata | + ----+------------------------------+----------------------------------------+ | 1 | days_on_market: 13 | { \"location\" :great , \"rental_price\" : 3901 } | | | neighborhood: berkeley_hills | | | 2 | days_on_market: 10 | { \"location\" :great , \"rental_price\" : 2042 } | | | neighborhood: berkeley_hills | | | 3 | days_on_market: 18 | { \"location\" :poor , \"rental_price\" : 1871 } | | | neighborhood: westbrae | | + ----+------------------------------+----------------------------------------+ ​ Knowledge Base with Hugging Face Embedding Model This example uses an open source embedding model. First, create an engine through which the model is accessed. CREATE ML_ENGINE embedding FROM langchain_embedding ; Next, create an embedding model. CREATE MODEL embedding_model PREDICT embedding USING engine = 'langchain_embedding' , class = 'HuggingFaceEmbeddings' , model_name = \"sentence-transformers/all-mpnet-base-v2\" ; Now that you have an embedding model, create a knowledge base, passing this embedding model. CREATE KNOWLEDGE_BASE kb_custom_model USING model = embedding_model ; After successful creation of a knowledge base, insert data to store it in the form of embeddings. INSERT INTO kb_custom_model ( SELECT review as content FROM example_db . amazon_reviews ) ; Finally, you can verify that the data has been inserted into the knowledge base by querying it. SELECT * FROM kb_custom_model ; ​ Knowledge Base with Custom Vector Store This example shows how to create a knowledge base with custom vector database. First, connect to your vector database. Here, it is ChromaDB. CREATE DATABASE chroma_dev_local WITH ENGINE = \"chromadb\" , PARAMETERS = { \"persist_directory\" : \"persist path here\" } ; Create an index in the vector store and insert one example point. CREATE TABLE chroma_dev_local . world_news_index ( SELECT content , embeddings FROM embedding_model WHERE content = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Tristique sollicitudin nibh sit amet commodo nulla. Risus sed vulputate odio ut enim blandit volutpat. Suspendisse ultrices gravida dictum fusce ut placerat orci. Eget nulla facilisi etiam dignissim diam. Aenean euismod elementum nisi quis eleifend quam. Ac placerat vestibulum lectus mauris ultrices eros in. Sed turpis tincidunt id aliquet risus feugiat in ante metus. Pellentesque habitant morbi tristique senectus et netus. Imperdiet massa tincidunt nunc pulvinar sapien et ligula. Leo in vitae turpis massa sed elementum tempus egestas. Aliquam malesuada bibendum arcu vitae elementum curabitur. Sit amet tellus cras adipiscing. Enim ut tellus elementum sagittis vitae et leo. Donec pretium vulputate sapien nec.\" ) ; Next, create a knowledge base, passing this vector database connection. CREATE KNOWLEDGE_BASE world_news_kb USING storage = chroma_dev_local . world_news_index ; ​ Automate Data Sync with Knowledge Base This example shows how to set up a job that inserts newly available data from your database into a knowledge base. Here is how you can automate adding content to the knowledge base every time new data becomes available: CREATE JOB keep_knowledge_base_up_to_date AS ( INSERT INTO world_news_kb ( SELECT id , text AS content FROM world_news_with_ids WHERE id > LAST ) ; ) EVERY second ; The LAST keyword enables the quey to select only the newly added data. Learn more about the LAST keyword here . Query the knowledge base as below. SELECT * FROM world_news_kb ; SELECT * FROM world_news_kb WHERE content = \"YouTube\" LIMIT 1 ; SELECT * FROM world_news_kb WHERE content = \"Canada and Google\" LIMIT 1 ; Was this page helpful? Yes No Suggest edits Raise issue Chatbot Custom Functions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Syntax Examples Knowledge Base with OpenAI Embedding Model Knowledge Base with Hugging Face Embedding Model Knowledge Base with Custom Vector Store Automate Data Sync with Knowledge Base"}
{"file_name": "chatbot.html", "content": "Chatbot - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Chatbot Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Agent Chatbot Knowledge Base Functions Standard SQL Support AI Agents Chatbot Within MindsDB, chatbots are agents connected to a chat interface. Creating a chatbot requires either an AI agent or an LLM, and a connection to a chat app, like Slack or MS Teams . ​ Chatbot with an AI agent AI Agents are customized AI models that can answer questions over your data. You can connect your data to agents in the form of skills . Here is how to create a chatbot that integrates an AI Agent and can be connected to a chat interface to have a conversation with your data. CREATE CHATBOT my_chatbot USING database = 'my_slack' , -- created with CREATE DATABASE my_slack agent = 'my_agent' , -- created with CREATE AGENT my_agent is_running = true ; -- default is true The parameters include the following: database stores connection to a chat app (like Slack or MS Teams ) that should be created with the CREATE DATABASE statement. agent is an AI agent created with the CREATE AGENT command. It consists of an AI model and a set of skills, that is, defined data sets provided at inference time via RAG-based techniques. is_running indicates whether or not to start the chatbot upon creation. Here are some tips for using the Slack integration: If you want to use Slack in the CREATE CHATBOT syntax, use this method of connecting Slack to MindsDB . If you want to connect the chatbot to multiple Slack channels, open your Slack application and add the App/Bot to one or more channels: Go to the channel where you want to use the bot. Right-click on the channel and select View Channel Details . Select Integrations . Click on Add an App . ​ Chatbot with an LLM ALternatively, you can create a chatbot that is equivalent to embedding an LLM of your choice into a chat app, like Slack or MS Teams . Here is how to create a chatbot that integrates an LLM and can be connected to a chat interface. CREATE CHATBOT my_chatbot USING database = 'my_slack' , -- created with CREATE DATABASE my_slack model = 'my_model' , -- created with CREATE MODEL my_model is_running = true ; -- default is true The parameters include the following: database stores connection to a chat app (like Slack or MS Teams ) that should be created with the CREATE DATABASE statement. model is a conversational model created with the CREATE MODEL command using the LangChain engine . is_running indicates whether or not to start the chatbot upon creation. Here is how to delete a chatbot: DROP CHATBOT my_chatbot ; And here is how to query all chatbots: SHOW CHATBOTS ; SELECT * FROM chatbots ; ​ Example Following the example from here , let’s create a chatbot utilizing the already created agent. Start by connecting a chat app to MindsDB: Follow this instruction to connect Slack to MindsDB. Follow this instruction to connect MS Teams to MindsDB. Next, create a chatbot. CREATE CHATBOT text_to_sql_chatbot USING database = 'my_slack' , -- this must be created with CREATE DATABASE agent = 'text_to_sql_agent' , -- this must be created with CREATE AGENT is_running = true ; Follow this tutorial to build your own chatbot. Was this page helpful? Yes No Suggest edits Raise issue Agent Knowledge Base github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Chatbot with an AI agent Chatbot with an LLM Example"}
{"file_name": "agent.html", "content": "Agent - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI Agents Agent Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Agent Chatbot Knowledge Base Functions Standard SQL Support AI Agents Agent With MindsDB, you can create and deploy AI agents that comprise AI models and customizable skills such as knowledge bases and text-to-SQL. AI agents use a conversational model (like OpenAI) from LangChain utilizing tools as skills to respond to user input. Users can customize AI agents with their own prompts to fit their use cases. A chatbot can be thought of as an agent connected to some messaging interface. ​ How to work with AI agents ​ Create skills Start by setting up the skills. Here is how you can create and manage skills using SQL API. Creating, inserting into, updating, and deleting a knowledge base: CREATE KNOWLEDGE BASE my_knowledge_base USING model = embedding_model_name , -- this parameter is optional; if not provided, a suitable embedding model is chosen for the task storage = vector_database . storage_table ; -- this parameter is optional; if not provided, the default ChromaDB is used for storage -- inserts new data rows and generates id for each row if id is not provided INSERT INTO my_knowledge_base SELECT text AS content FROM datasource . data_table ; -- inserts new data rows and updates existing ones if id value matches INSERT INTO my_knowledge_base SELECT id , text AS content FROM datasource . data_table ; -- view content of a knowledge base (for example, to look up the generated id values) SELECT * FROM my_knowledge_base ; DROP KNOWLEDGE BASE my_knowledge_base ; Creating, updating, and deleting a knowledge_base skill: CREATE SKILL kb_skill USING type = 'knowledge_base' , source = 'my_knowledge_base' , -- this must be created with CREATE KNOWLEDGE BASE description = 'My data' ; -- data description to help the agent know when to use the knowledge base UPDATE SKILL kb_skill SET source = 'new_knowledge_base' ; -- this must be created with CREATE KNOWLEDGE BASE DROP SKILL kb_skill ; Creating, updating, and deleting a text2sql skill: CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , -- this must be created with CREATE DATABASE tables = [ 'sales_data' ] , -- this is a list of tables passed to this skill description = \"this is sales data\" ; UPDATE SKILL text_to_sql_skill SET database = 'new_example_db' , -- this must be created with CREATE DATABASE tables = [ 'sales_data' ] ; -- this is a list of tables passed to this skill DROP SKILL text_to_sql_skill ; You can query all skills using this command: SHOW KNOWLEDGE_BASES ; SHOW SKILLS ; ​ Create an agent An agent can be created, deleted, queried, and updated. Here is how you can do that using SQL API. Creating an AI agent: CREATE AGENT my_agent USING model = 'chatbot_agent' , -- this must be a conversational model created with CREATE MODEL (as in the Example section) skills = [ 'test_skill' ] ; -- this must be created with CREATE SKILL Alternatively, you can create an agent and define the model to be used by an agent at the agent creation time based on the model providers defined here . CREATE AGENT my_agent USING skills = [ 'text_to_sql_skill' ] , -- this must be created with CREATE SKILL provider = 'openai' , -- choose one of the available model providers (openai, anthropic, anyscale, ollama, litellm, mindsdb) model = 'gpt-4' , -- define the model from the provider prompt_template = 'Answer the user input in a helpful way using tools' , -- provide instruction to the model verbose = True , max_tokens = 100 ; Updating an AI agent: UPDATE AGENT my_agent SET model = 'new_chatbot_agent' , -- this must be a conversational model created with CREATE MODEL skills_to_remove = [ 'test_skill' ] , skills_to_add = [ 'production_skill' ] ; -- this must be created with CREATE SKILL Querying an AI agent: SELECT * FROM my_agent WHERE question = \"insert your question\" ; -- this is the user_column parameter as defined when creating a conversational model for the agent (as in the Example section) Deleting an AI agent: DROP AGENT my_agent ; You can query all agents using this command: SHOW AGENTS [ FROM project_name ] ; SELECT * FROM agents ; ​ Example ​ Agents with Text-to-SQL Skills Start by creating a conversational large language model to be used by an agent. CREATE MODEL my_model PREDICT answer USING engine = 'langchain' , openai_api_key = 'your-model-api-key' , model_name = 'gpt-4' , mode = 'conversational' , user_column = 'question' , assistant_column = 'answer' , max_tokens = 100 , temperature = 0 , verbose = True , prompt_template = 'Answer the user input in a helpful way' ; Agents access models via the LangChain integration with MindsDB . Check out the link to find out available models. Then, connect a data source to be used for creating a skill. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; SELECT * FROM example_db . sales_data ; Create a skill using one or more tables from a connected data source. CREATE SKILL text_to_sql_skill USING type = 'text2sql' , database = 'example_db' , tables = [ 'car_sales' ] , description = \"this is car sales data\" ; Now that we have a model and a skill, let’s create an agent. CREATE AGENT text_to_sql_agent USING model = 'my_model' , skills = [ 'text_to_sql_skill' ] ; Query the agent as below: SELECT * FROM text_to_sql_agent WHERE question = \"how many cars were sold in 2017?\" ; -- this column is defined in the user_column parameter in CREATE MODEL The next step would be to connect a chat app, like Slack, to MindsDB and create a chatbot utilizing this agent. Learn about chatbots here . ​ Agents with Knowledge Bases as Skills In this example, let’s create an embedding model (using OpenAI or LangChain as an engine) for the knowledge base. Note that this step is optional, as knowledge bases provide default embedding model. CREATE ML_ENGINE openai_engine FROM openai USING openai_api_key = 'your-openai-api-key' ; CREATE MODEL embedding_model PREDICT embeddings USING engine = 'openai_engine' , mode = 'embedding' , model_name = 'text-embedding-ada-002' , question_column = 'content' ; Now let’s create a knowledge base that uses this embedding model and the default storage vector database (that is, ChromaDB). CREATE KNOWLEDGE BASE my_knowledge_base USING model = embedding_model ; -- this is optional This is how you can insert data into the knowledge base and select it. INSERT INTO my_knowledge_base ( content ) VALUES ( 'I drink tea.' ) ; SELECT * FROM my_knowledge_base ; Use this knowledge base to create a skill for an agent: CREATE SKILL kb_skill USING type = 'knowledge_base' , source = 'my_knowledge_base' , -- this must be created with CREATE KNOWLEDGE BASE description = 'My data' ; -- data description to help the agent know when to use the knowledge base Now you can assign this skill to the agent ( that was created in the example above) and query it again: UPDATE AGENT text_to_sql_agent SET skills_to_add = [ 'kb_skill' ] ; SELECT * FROM text_to_sql_agent WHERE questions = \"what is your data?\" ; Was this page helpful? Yes No Suggest edits Raise issue Query Triggers Chatbot github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to work with AI agents Create skills Create an agent Example Agents with Text-to-SQL Skills Agents with Knowledge Bases as Skills"}
{"file_name": "finetune.html", "content": "Minds[DB] - AGI's Query Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Get Started Minds[DB] - AGI's Query Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website Get Started Introduction Quickstart Installation Demo Features Model Management AI Integrations Data Integrations Automation Learn more Get Started Minds[DB] - AGI's Query Engine MindsDB is the world’s most effective solution for building AI applications that talk to messy enterprise data sources. Think of it as a librarian Marie Kondo. A federated query engine that tidies up your data-sprawl chaos while meticulously answering every single question you throw at it. From structured to unstructured data, whether it’s scattered across SaaS applications, databases, or… hibernating in data warehouses like that $100 bill in your tuxedo pocket from prom night, lost, waiting to be discovered. ​ Install MindsDB Server MindsDB is an open-source server that can be deployed anywhere - from your laptop to the cloud, and everywhere in between. And yes, you can customize it to your heart’s content. Using Docker Desktop . This is the fastest and recommended way to get started and have it all running. Using Docker . This is also simple, but gives you more flexibility on how to further customize your server. Using PyPI . This option enables you to contribute to MindsDB. ​ Connect Your Data You can connect to hundreds of data sources (learn more) . This is just an example of a Postgres database. -- Connect to demo postgres DB CREATE DATABASE demo_postgres_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" , \"schema\" : \"demo_data\" } ; Once you’ve connected your data sources, you can combine , slice it, dice it , and transform it however your heart desires using good ol’ standard SQL (learn more) . After you’ve whipped your data into shape, it’s time to build AI that actually learns! ​ Build AI Knowledge Our Knowledge Bases are state-of-the-art autonomous RAG systems that can digest data from any source MindsDB supports. Whether your data is structured and neater than a Swiss watch factory or unstructured and messy as a teenager’s bedroom, our Knowledge Base engine will figure out how to find the relevant information. In this example we will create a knowledge base that knows everything about amazon reviews. -- first create a knowledge base CREATE KNOWLEDGE_BASE mindsdb . reviews_kb ; -- now insert everything from the amazon reviews table into it, so it can learn it INSERT INTO mindsdb . reviews_kb ( SELECT review as content FROM demo_pg_db . amazon_reviews ) ; -- check the status of your loads here SELECT * FROM information_schema . knowledge_bases ; -- query the content of the knowledge base SELECT * FROM mindsdb . reviews_kb ; For the tinkerers and optimization enthusiasts out there, you can dive as deep as you want. (Learn more about knowledge Bases) Want to hand-pick your embedding model? Go for it ! Have strong opinions about vector databases? We’re here for it! . But if you’d rather spend your time on other things (like finally building that billion-dollar AI App), that’s perfectly fine too. By default, it’s all handled automatically - you don’t need to worry about the nitty-gritty details like data embedding, chunking, vector optimization, etc. ​ Search Now that your knowledge base is loaded and ready. Let’s hunt for some juicy info! ​ Via SQL -- Find the reviews that about Iphone in beast of lights SELECT * FROM mindsdb . reviews_kb WHERE content LIKE 'what are the best kindle reviews' LIMIT 10 ; ​ Via Python SDK Install MindsDB SDK pip install mindsdb_sdk You can call this AI knowledge base from your app with the following code: import mindsdb_sdk # connects to the specified host and port server = mindsdb_sdk . connect ( 'http://127.0.0.1:47334' ) wiki_kb = server . knowledge_bases . get ( 'mindsdb.reviews_kb' ) ; df = my_kb . find ( 'what are the best kindle reviews' ) . fetch ( ) Was this page helpful? Yes No Suggest edits Raise issue Quickstart github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Install MindsDB Server Connect Your Data Build AI Knowledge Search Via SQL Via Python SDK"}
{"file_name": "list-projects.html", "content": "List Projects - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects List Projects Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Projects List Projects ​ Description The SHOW DATABASES command lists all available data sources and projects. The WHERE clause filters all projects. ​ Syntax Here is the syntax: SHOW DATABASES WHERE type = 'project' ; Alternatively, you can use the FULL keyword to get more information: SHOW FULL DATABASES WHERE type = 'project' ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Project Create, Train, and Deploy a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "show-ml-engines.html", "content": "List ML Engines - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Engines Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support AI/ML Engines List ML Engines ​ Description The SHOW ML_ENGINES statement lists all created ML engines that can be used to create models. ​ Syntax Here is how to list all created ML engines: SHOW ML_ENGINES ; Was this page helpful? Yes No Suggest edits Raise issue Remove an ML Engine Create a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "query-triggers.html", "content": "Query Triggers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Triggers Query Triggers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers Create a Trigger Remove a Trigger Query Triggers AI Agents Functions Standard SQL Support Triggers Query Triggers ​ Description Triggers enable users to define event-based actions. For example, if a table is updated, then run a query to update predictions. Currently, you can create triggers on the following data sources: MongoDB , Slack , Solace . ​ Syntax Here is the syntax for querying all triggers: SHOW TRIGGERS ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Trigger Agent github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "list-data-handlers.html", "content": "List Data Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources List Data Handlers ​ Description The SHOW HANDLERS command lists all available handlers. The WHERE clause filter handlers by the type (data or ML). ​ Syntax Here is the syntax: SHOW HANDLERS WHERE type = 'data' ; Was this page helpful? Yes No Suggest edits Raise issue Grafana Connect a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "get-single-prediction.html", "content": "Get a Single Prediction - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get a Single Prediction Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Join Models with Tables Evaluate Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Predictions Get a Single Prediction ​ Description The SELECT statement fetches predictions from the model table. The data is returned on the fly and the result set is not persisted. But there are ways to save predictions data! You can save your predictions as a view using the CREATE VIEW statement. Please note that a view is a saved query and does not store data like a table. Another way is to create a table using the CREATE TABLE statement or insert your predictions into an existing table using the INSERT INTO statement. ​ Syntax Here is the syntax for fetching a single prediction from the model table: SELECT target_name , target_name_explain FROM mindsdb . predictor_name WHERE column_name = value AND column_name = value ; Grammar Matters Here are some points to keep in mind while writing queries in MindsDB: 1. The column_name = value pairs may be joined by AND or OR keywords. 2. Do not use any quotations for numerical values. 3. Use single quotes for strings. 4. The tables and column names are case sensitive. On execution, we get: + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | target_name | target_name_explain | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | predicted_value | { \"predicted_value\" : 4394 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Where: Name Description target_name Name of the column to be predicted. target_name_explain Object of the JSON type that contains the predicted_value and additional information such as confidence , anomaly , truth , confidence_lower_bound , confidence_upper_bound . predictor_name Name of the model used to make the prediction. WHERE column_name = value AND ... WHERE clause used to pass the data values for which the prediction is made. ​ Example Let’s predict the rental_price value using the home_rentals_model model for the property having sqft=823 , location='good' , neighborhood='downtown' , and days_on_market=10 . SELECT sqft , location , neighborhood , days_on_market , rental_price , rental_price_explain FROM mindsdb . home_rentals_model1 WHERE sqft = 823 AND location = 'good' AND neighborhood = 'downtown' AND days_on_market = 10 ; On execution, we get: + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | sqft | location | neighborhood | days_on_market | rental_price | rental_price_explain | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ | 823 | good | downtown | 10 | 4394 | { \"predicted_value\" : 4394 , \"confidence\" : 0.99 , \"anomaly\" : null , \"truth\" : null , \"confidence_lower_bound\" : 4313 , \"confidence_upper_bound\" : 4475 } | + -------+----------+--------------+----------------+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------+ Was this page helpful? Yes No Suggest edits Raise issue Manage Model Versions Get Batch Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "query-jobs.html", "content": "Query Jobs - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Query Jobs Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Triggers AI Agents Functions Standard SQL Support Jobs Query Jobs ​ Querying Jobs Here is how we can view all jobs in a project: SHOW JOBS WHERE project = 'project-name' ; SELECT * FROM project - name . jobs ; On execution, we get: + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | RUN_START | RUN_END | NEXT_RUN_AT | SCHEDULE_STR | QUERY | + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | drop_model | mindsdb | 2023 - 04 - 01 00 : 00 : 00.000000 | [ NULL ] | 2023 - 04 - 01 00 : 00 : 00.000000 | [ NULL ] | DROP MODEL mindsdb . home_rentals_model | | retrain_model_and_save_predictions | mindsdb | 2023 - 02 - 15 19 : 19 : 43.210122 | 2023 - 04 - 01 00 : 00 : 00.000000 | 2023 - 02 - 15 19 : 19 : 43.210122 | every 2 days | RETRAIN mindsdb . home_rentals_model USING join_learn_process = true ; INSERT INTO my_integration . rentals ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) | | save_predictions | mindsdb | 2023 - 02 - 15 19 : 19 : 48.545580 | [ NULL ] | 2023 - 02 - 15 19 : 19 : 48.545580 | every hour | CREATE TABLE my_integration . ` result_{{START_DATETIME}} ` ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) | + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+---------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Or from all projects at once: SHOW JOBS ; SELECT * FROM information_schema . jobs ; ​ Querying Jobs History You can query the history of jobs similar to querying for jobs. Here you can find information about an error if the job didn’t execute successfully. Here is how we can view all jobs history in the current project: SELECT * FROM log . jobs_history WHERE project = 'mindsdb' ; On execution, we get: + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | NAME | PROJECT | RUN_START | RUN_END | NEXT_RUN_AT | ERROR | QUERY | + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | retrain_model_and_save_predictions | mindsdb | 2023 - 02 - 15 19 : 19 : 43.210122 | 2023 - 04 - 01 00 : 00 : 00.000000 | 2023 - 02 - 15 19 : 19 : 43.210122 | [ NULL ] | RETRAIN mindsdb . home_rentals_model USING join_learn_process = true ; INSERT INTO my_integration . rentals ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) | | save_predictions | mindsdb | 2023 - 02 - 15 19 : 19 : 48.545580 | [ NULL ] | 2023 - 02 - 15 19 : 19 : 48.545580 | [ NULL ] | CREATE TABLE my_integration . ` result_{{START_DATETIME}} ` ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) | + ------------------------------------+---------+----------------------------+----------------------------+----------------------------+--------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Please note that the drop_model job is not in the jobs_history table because it didn’t start yet. Was this page helpful? Yes No Suggest edits Raise issue Remove a Job Create a Trigger github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Querying Jobs Querying Jobs History"}
{"file_name": "get-batch-predictions.html", "content": "Get Batch Predictions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Get Batch Predictions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Join Models with Tables Evaluate Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Predictions Get Batch Predictions ​ Description The SELECT statement fetches predictions from the model table. The data is returned on the fly and the result set is not persisted. But there are ways to save predictions data! You can save your predictions as a view using the CREATE VIEW statement. Please note that a view is a saved query and does not store data like a table. Another way is to create a table using the CREATE TABLE statement or insert your predictions into an existing table using the INSERT INTO statement. ​ Syntax Here is the syntax for making batch predictions by joining one or more data source tables with one or more model tables: SELECT t1 . column , t2 . column , m1 . target , m2 . target FROM integration_name . table_name1 AS t1 JOIN integration_name . table_name2 AS t2 ON t1 . column = t2 . column JOIN . . . JOIN mindsdb . model_name1 AS m1 JOIN mindsdb . model_name2 AS m2 JOIN . . . [ ON t1 . input_data = m1 . expected_argument ] WHERE m1 . parameter = 'value' AND m2 . parameter = 'value' ; Where: There are the data tables that provide input to the models: integration_name.table_name1 , integration_name.table_name2 . These are the AI tables: mindsdb.model_name1 , mindsdb.model_name2 . Note that you can provide input to the models from the data tables and also in the WHERE clause. When querying for predictions, you can specify the partition_size parameter to split data into partitions and run prediction on different workers. Note that the ML task queue needs to be enabled to use this parameter. To use the partition_size parameter, provide it in the USING clause, specifying the partition size, like this: ... USING partition_size=100 Follow this doc page to learn more about AI Tables. ​ Example Let’s make bulk predictions to predict the rental_price value using the home_rentals_model model joined with the data source table. SELECT t . sqft , t . location , t . neighborhood , t . days_on_market , t . rental_price AS real_price , m . rental_price AS predicted_rental_price FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 5 ; On execution, we get: + -------+----------+-----------------+----------------+--------------+-----------------------------+ | sqft | location | neighborhood | days_on_market | real_price | predicted_rental_price | + -------+----------+-----------------+----------------+--------------+-----------------------------+ | 917 | great | berkeley_hills | 13 | 3901 | 3886 | | 194 | great | berkeley_hills | 10 | 2042 | 2007 | | 543 | poor | westbrae | 18 | 1871 | 1865 | | 503 | good | downtown | 10 | 3026 | 3020 | | 1066 | good | thowsand_oaks | 13 | 4774 | 4748 | + -------+----------+-----------------+----------------+--------------+-----------------------------+ Follow this doc page to see examples of joining multiple data table with multiple models. Was this page helpful? Yes No Suggest edits Raise issue Get a Single Prediction Join Models with Tables github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "show-databases.html", "content": "List Data Sources - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources List Data Sources Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources List Data Sources ​ Description The SHOW DATABASES statement lists all connected data sources that MindsDB can access. ​ Syntax Here is how to list all connected data sources: SHOW DATABASES ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Data Source Use a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "native-queries.html", "content": "Native Queries - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Native Queries Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Native Queries The underlying database engine of MindsDB is MySQL. However, you can run queries native to your database engine within MindsDB. ​ Connect your Database to MindsDB To run queries native to your database, you must first connect your database to MindsDB using the CREATE DATABASE statement. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; Here we connect the example_db database, which is a PostgreSQL database. ​ Run Queries Native to your Database Once we have our PostgreSQL database connected, we can run PostgreSQL-native queries. ​ Querying To run PostgreSQL-native code, we must nest it within the SELECT statement like this: SELECT * FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST ( ( mpg / 2.3521458 ) AS numeric ) , 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ; On execution, we get: + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | model | year | price | transmission | mileage | fueltype | mpg | kml | years_old | units_to_sell | tax_div_price | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ | A1 | 2010 | 9990 | Automatic | 38000 | Petrol | 53.3 | 22.7 | 12 | 1 | 0.013 | | A1 | 2011 | 6995 | Manual | 65000 | Petrol | 53.3 | 22.7 | 11 | 5 | 0.018 | | A1 | 2011 | 6295 | Manual | 107000 | Petrol | 53.3 | 22.7 | 11 | 5 | 0.02 | | A1 | 2011 | 4250 | Manual | 116000 | Diesel | 70.6 | 30 | 11 | 5 | 0.005 | | A1 | 2011 | 6475 | Manual | 45000 | Diesel | 70.6 | 30 | 11 | 5 | 0 | + -----+----+-----+------------+-------+--------+----+----+---------+-------------+-------------+ The first line ( SELECT * FROM example_db ) informs MindsDB that we select from a PostgreSQL database. After that, we nest a PostgreSQL code within brackets. ​ Creating Views We can create a view based on a native query. CREATE VIEW cars FROM example_db ( SELECT model , year , price , transmission , mileage , fueltype , mpg , -- miles per galon ROUND ( CAST ( ( mpg / 2.3521458 ) AS numeric ) , 1 ) AS kml , -- kilometers per liter ( date_part ( 'year' , CURRENT_DATE ) - year ) AS years_old , -- age of a car COUNT ( * ) OVER ( PARTITION BY model , year ) AS units_to_sell , -- how many units of a certain model are sold in a year ROUND ( ( CAST ( tax AS decimal ) / price ) , 3 ) AS tax_div_price -- value of tax divided by price of a car FROM demo_data . used_car_price ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Was this page helpful? Yes No Suggest edits Raise issue Query a Table Update a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Connect your Database to MindsDB Run Queries Native to your Database Querying Creating Views"}
{"file_name": "list-ml-handlers.html", "content": "List ML Handlers - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines List ML Handlers Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support AI/ML Engines List ML Handlers ​ Description The SHOW HANDLERS command lists all available handlers. The WHERE clause filter handlers by the type (data or ML). ​ Syntax Here is the syntax: SHOW HANDLERS WHERE type = 'ml' ; Was this page helpful? Yes No Suggest edits Raise issue Use a Data Source Configure an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "show-models.html", "content": "List Models - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models List Models Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models List Models ​ Description The SHOW MODELS statement lists all models created in MindsDB. ​ Syntax Here is how to list all models: SHOW MODELS ; Alternatively, you can query the models table. SELECT * FROM project_name . models ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Model Describe a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "table.html", "content": "Remove a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Remove a Table ​ Description The DROP TABLE statement deletes a table or a file. Please note that this feature is not yet implemented for tables from connected data sources. ​ Syntax Here is the syntax: DROP TABLE table_name ; And for files: DROP TABLE files . file_name ; On execution, we get: Query successfully completed Please note that the uploaded files are tables as well. So to remove an uploaded file, use this DROP TABLE statement. Was this page helpful? Yes No Suggest edits Raise issue Create a Table Query a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "trigger.html", "content": "Remove a Trigger - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Triggers Remove a Trigger Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers Create a Trigger Remove a Trigger Query Triggers AI Agents Functions Standard SQL Support Triggers Remove a Trigger ​ Description Triggers enable users to define event-based actions. For example, if a table is updated, then run a query to update predictions. Currently, you can create triggers on the following data sources: MongoDB , Slack , Solace . ​ Syntax Here is the syntax for removing a trigger: DROP TRIGGER trigger_name ; Was this page helpful? Yes No Suggest edits Raise issue Create a Trigger Query Triggers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "ml-engine.html", "content": "Remove an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Remove an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support AI/ML Engines Remove an ML Engine ​ Description The DROP ML_ENGINE statement deletes the ML engine. ​ Syntax Here is the syntax: DROP ML_ENGINE [ IF EXISTS ] ml_engine_name ; On execution, we get: Query successfully completed Was this page helpful? Yes No Suggest edits Raise issue Configure an ML Engine List ML Engines github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "project.html", "content": "Remove a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Remove a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Projects Remove a Project ​ Description The DROP PROJECT statement deletes the project. ​ Syntax Here is the syntax: DROP PROJECT [ IF EXISTS ] project_name ; On execution, we get: Query successfully completed Was this page helpful? Yes No Suggest edits Raise issue Create a Project List Projects github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "view.html", "content": "Remove a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Remove a View ​ Description The DROP VIEW statement deletes the view. ​ Syntax Here is the syntax: DROP VIEW [ IF EXISTS ] view_name ; On execution, we get: Query successfully completed Was this page helpful? Yes No Suggest edits Raise issue Create a View Query a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "database.html", "content": "Remove a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Remove a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources Remove a Data Source ​ Description The DROP DATABASE statement deletes the database. ​ Syntax Here is the syntax: DROP DATABASE [ IF EXISTS ] database_name ; On execution, we get: Query successfully completed Was this page helpful? Yes No Suggest edits Raise issue Connect a Data Source List Data Sources github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "jobs.html", "content": "Remove a Job - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs Remove a Job Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Triggers AI Agents Functions Standard SQL Support Jobs Remove a Job ​ Description The DROP JOB statement deletes the job. ​ Syntax Here is the syntax for deleting a job: DROP JOB [ IF EXISTS ] [ project_name . ] job_name ; The project_name value is optional. The job_name value indicates the job to be deleted. Let’s look at some examples: DROP JOB my_project . retrain_and_save_job ; Here we drop the retrain_and_save_job that resides in the my_project project. And another example: DROP JOB create_table_job ; Here we drop the create_table_job job that resides in the current project. To learn more about projects in MindsDB, visit our docs here . Was this page helpful? Yes No Suggest edits Raise issue Create a Job Query Jobs github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "file.html", "content": "Remove a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Remove a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Remove a File ​ Description The DROP TABLE statement is also used to delete a file. ​ Syntax Here is the syntax: DROP TABLE files . file_name ; On execution, we get: Query successfully completed Please note that the uploaded files are tables as well. So to remove an uploaded file, use this DROP TABLE statement. Was this page helpful? Yes No Suggest edits Raise issue Upload a File Query a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "model.html", "content": "Remove a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Remove a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Remove a Model ​ Description The DROP MODEL statement deletes the model table. ​ Syntax Here is the syntax: DROP MODEL [ IF EXISTS ] predictor_name ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) Where: Name Description predictor_name Name of the model to be deleted. ​ Example Let’s list all the available predictor tables. SHOW MODELS ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | | home_rentals_model | + ---------------------+ Now we delete the home_rentals_model table. DROP MODEL home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) We can check if the deletion was successful by querying the mindsdb.models table again. SHOW MODELS ; On execution, we get: + ---------------------+ | name | + ---------------------+ | other_model | + ---------------------+ Was this page helpful? Yes No Suggest edits Raise issue Create, Train, and Deploy a Model List Models github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "table.html", "content": "Create a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Create a Table ​ Description The CREATE TABLE statement creates a table and optionally fills it with data from provided query. It may be used to materialize prediction results as tables. ​ Syntax You can use the CREATE TABLE statement to create an empty table: CREATE TABLE integration_name . table_name ( column_name data_type , . . . ) ; You can use the CREATE TABLE statement to create a table and fill it with data: CREATE TABLE integration_name . table_name ( SELECT . . . ) ; Or the CREATE OR REPLACE TABLE statement: CREATE OR REPLACE TABLE integration_name . table_name ( SELECT . . . ) ; Note that the integration_name connection must be created with the CREATE DATABASE statement and the user with write access. Here are the steps followed by the syntax: It executes a subselect query to get the output data. In the case of the CREATE OR REPLACE TABLE statement, the integration_name.table_name table is dropped before recreating it. It (re)creates the integration_name.table_name table inside the integration_name integration. It uses the INSERT INTO statement to insert the output of the (SELECT ...) query into the integration_name.table_name . On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) ​ Example We want to save the prediction results into the int1.tbl1 table. Here is the schema structure used throughout this example: int1 └── tbl1 mindsdb └── predictor_name int2 └── tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let’s execute the query. CREATE OR REPLACE TABLE int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Was this page helpful? Yes No Suggest edits Raise issue Evaluate Predictions Remove a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "trigger.html", "content": "Create a Trigger - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Triggers Create a Trigger Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers Create a Trigger Remove a Trigger Query Triggers AI Agents Functions Standard SQL Support Triggers Create a Trigger ​ Description Triggers enable users to define event-based actions. For example, if a table is updated, then run a query to update predictions. Currently, you can create triggers on the following data sources: MongoDB (available for MongoDB Atlas Database), Slack , Solace , PostgreSQL (requires write access). ​ Syntax Here is the syntax for creating a trigger: CREATE TRIGGER trigger_name ON integration_name . table_name [ COLUMNS column_name1 , column_name2 , . . . ] ( sql_code ) By creating a trigger on a data source, every time this data source is updated or new data is inserted, the sql_code provided in the statement will be executed. You can create a trigger either on a table… CREATE TRIGGER trigger_name ON integration_name . table_name ( sql_code ) …or on one or more columns of a table. CREATE TRIGGER trigger_name ON integration_name . table_name COLUMNS column_name1 , column_name2 ( sql_code ) ​ Example Firstly, connect Slack to MindsDB following this instruction and connect the Slack app to a channel. CREATE DATABASE mindsdb_slack WITH ENGINE = 'slack' , PARAMETERS = { \"token\" : \"xoxb-...\" , \"app_token\" : \"xapp-...\" } ; Create a model that will be used to answer chat questions every time new messages arrive. Here we use the OpenAI engine , but you can use any other LLM . CREATE MODEL chatbot_model PREDICT answer USING engine = 'openai_engine' , prompt_template = 'answer the question: {{text}}' ; Here is how to generate answers to Slack messages using the model: SELECT s . text AS question , m . answer FROM chatbot_model m JOIN mindsdb_slack . messages s WHERE s . channel_id = 'slack-bot-channel-id' AND s . user != 'U07J30KPAUF' AND s . created_at > LAST ; Let’s analyze this query: We select the question from the Slack connection and the answer generated by the model. We join the model with the messages table. In the WHERE clause: We provide the channel name where the app/bot is integrated. We exclude the messages sent by the app/bot. You can find the user ID of the app/bot by querying the mindsdb_slack.users table. We use the LAST keyword to ensure that the model generates answers only to the newly sent messages. Finally, create a trigger that will insert an answer generated by the model every time when new messages are sent to the channel. CREATE TRIGGER slack_trigger ON mindsdb_slack . messages ( INSERT INTO mindsdb_slack . messages ( channel_id , text ) SELECT 'slack-bot-channel-id' AS channel_id , answer AS text FROM chatbot_model m JOIN TABLE_DELTA s WHERE s . user != 'slack-bot-id' # this is to prevent the bot from replying to its own messages AND s . channel_id = 'slack-bot-channel-id' ) ; Let’s analyze this statement: We create a trigger named slack_trigger . The trigger is created on the mindsdb_slack.messages table. Therefore, every time when data is added or updated, the trigger will execute its code. We provide the code to be executed by the trigger every time the triggering event takes place. We insert an answer generated by the model into the messages table. The TABLE_DELTA stands for the table on which the trigger has been created. We exclude the messages sent by the app/bot. You can find the user ID of the app/bot by querying the mindsdb_slack.users table. Was this page helpful? Yes No Suggest edits Raise issue Query Jobs Remove a Trigger github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "ml-engine.html", "content": "Configure an ML Engine - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation AI/ML Engines Configure an ML Engine Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines List ML Handlers Configure an ML Engine Remove an ML Engine List ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support AI/ML Engines Configure an ML Engine You can create machine learning (ML) engines based on the ML handlers available in MindsDB. If you can’t find the ML handler of your interest, you can always contribute by building a new ML handler . ​ Description The CREATE ML_ENGINE command creates an ML engine that uses one of the available ML handlers. ​ Syntax Before creating an ML engine, make sure that the ML handler of your interest is available by querying for the ML handlers. SELECT * FROM information_schema . handlers ; -- or SHOW HANDLERS ; If you can’t find the ML handler of your interest, you can contribute by building a new ML handler . Please note that in the process of contributing new ML engines, ML engines and/or their tests will only run correctly if all dependencies listed in the requirements.txt file are installed beforehand. If you find the ML handler of your interest, then you can create an ML engine using this command: CREATE ML_ENGINE [ IF NOT EXISTS ] ml_engine_name FROM handler_name [ USING argument_key = argument_value ] ; Please replace ml_engine_name , handler_name , and optionally, argument_key and argument_value with the real values. Please do not use the same ml_engine_name as the handler_name to avoid issue while dropping the ML engine. To verify that your ML engine was successfully created, run the command below: SELECT * FROM information_schema . ml_engines ; -- or SHOW ML_ENGINES ; If you want to drop an ML engine, run the command below: DROP ML_ENGINE ml_engine_name ; ​ Example Let’s check what ML handlers are currently available: SHOW HANDLERS ; On execution, we get: + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ | NAME | TITLE | DESCRIPTION | VERSION | CONNECTION_ARGS | IMPORT_SUCCESS | IMPORT_ERROR | + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ | \"ray_serve\" | \"RayServe\" | \"MindsDB handler for Ray Serve\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"neuralforecast\" | \"NeuralForecast\" | \"MindsDB handler for Nixtla's NeuralForecast package\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"autosklearn\" | \"Auto-Sklearn\" | \"MindsDB handler for Auto-Sklearn\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'autosklearn'\" | | \"mlflow\" | \"MLFlow\" | \"MindsDB handler for MLflow\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'mlflow'\" | | \"openai\" | \"OpenAI\" | \"MindsDB handler for OpenAI\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"merlion\" | \"Merlion\" | \"MindsDB handler for Merlion\" | \"0.0.1\" | \"[NULL]\" | \"false\" | \"object.__init__() takes exactly one argument (the instance to initialize)\" | | \"byom\" | \"BYOM\" | \"MindsDB handler for BYOM\" | \"0.0.1\" | \"{'code': {'type': 'path', 'description': 'The path to model code'}, 'modules': {'type': 'path', 'description': 'The path to model requirements'}}\" | \"true\" | \"[NULL]\" | | \"ludwig\" | \"Ludwig\" | \"MindsDB handler for Ludwig AutoML\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'dask'\" | | \"lightwood\" | \"Lightwood\" | \"[NULL]\" | \"1.0.0\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"huggingface_api\" | \"Hugging Face API\" | \"MindsDB handler for Auto-Sklearn\" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'hugging_py_face'\" | | \"statsforecast\" | \"StatsForecast\" | \"MindsDB handler for Nixtla's StatsForecast package\" | \"0.0.0\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"huggingface\" | \"Hugging Face\" | \"MindsDB handler for Higging Face\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"TPOT\" | \"Tpot\" | \"MindsDB handler for TPOT \" | \"0.0.2\" | \"[NULL]\" | \"false\" | \"No module named 'tpot'\" | | \"langchain\" | \"LangChain\" | \"MindsDB handler for LangChain\" | \"0.0.1\" | \"[NULL]\" | \"true\" | \"[NULL]\" | | \"autokeras\" | \"Autokeras\" | \"MindsDB handler for Autokeras AutoML\" | \"0.0.1\" | \"[NULL]\" | \"false\" | \"No module named 'autokeras'\" | + -------------------+--------------------+-------------------------------------------------------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------+----------------+-----------------------------------------------------------------------------+ Here we create an ML engine using the OpenAI handler and providing an OpenAI API key in the USING clause. CREATE ML_ENGINE my_openai_engine FROM openai USING openai_api_key = '<your opanai api key>' ; On execution, we get: Query successfully completed Now let’s verify that our ML engine exists. SHOW ML_ENGINES ; On execution, we get: + -------------------+------------+------------------------------------------------------+ | NAME | HANDLER | CONNECTION_DATA | + -------------------+------------+------------------------------------------------------+ | lightwood | lightwood | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | huggingface | huggingface | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | openai | openai | { \"key\" : [ \"password\" ] , \"value\" : [ \"\" ] } | | my_openai_engine | openai | { \"key\" : [ \"openai_api_key\" , \"password\" ] , \"value\" : [ \"\" , \"\" ] } | + -------------------+------------+------------------------------------------------------+ Please note that the USING clause is optional, as it depends on the ML handler whether it requires some arguments or not. Here, we created an OpenAI engine and provided own API key. After creating your ML engine, you can create a model like this: CREATE MODEL my_model PREDICT answer USING engine = 'my_openai_engine' , prompt_template = 'ask a question to a model' The USING clause specifies the ML engine to be used for creating a new model. Was this page helpful? Yes No Suggest edits Raise issue List ML Handlers Remove an ML Engine github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "project.html", "content": "Create a Project - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Projects Create a Project Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Create a Project Remove a Project List Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Projects Create a Project ​ Description MindsDB introduces projects that are a natural way to keep artifacts, such as models or views, separate according to what predictive task they solve. You can learn more about MindsDB projects here . ​ Syntax Here is the syntax for creating a project: CREATE PROJECT [ IF NOT EXISTS ] project_name ; Was this page helpful? Yes No Suggest edits Raise issue List ML Engines Remove a Project github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "view.html", "content": "Create a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Create a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Create a View ​ Description The CREATE VIEW statement creates a view, which is a great way to do data preparation in MindsDB. A VIEW is a saved SELECT statement, which is executed every time we call this view. ​ Syntax Here is the syntax: CREATE VIEW [ IF NOT EXISTS ] project_name . view_name AS ( SELECT a . column_name , . . . , p . model_column AS model_column FROM integration_name . table_name AS a JOIN mindsdb . predictor_name AS p ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description project_name Name of the project to store the view. view_name Name of the view. a.column_name, ... Columns of the data source table that are the input for the model to make predictions. p.model_column Name of the target column to be predicted. integration_name.table_name Data source table name along with the integration where it resides. predictor_name Name of the model. ​ Example Below is the query that creates and trains the home_rentals_model model to predict the rental_price value. The inner SELECT statement provides all real estate listing data used to train the model. CREATE MODEL mindsdb . home_rentals_model FROM integration ( SELECT * FROM house_rentals_data ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Now, we can JOIN the home_rentals_data table with the home_rentals_model model to make predictions. By creating a view (using the CREATE VIEW statement) that is based on the SELECT statement joining the data and model tables, we create an AI Table. Here, the SELECT statement joins the data source table and the model table. The input data for making predictions consists of the sqft , number_of_bathrooms , and location columns. These are joined with the rental_price column that stores predicted values. CREATE VIEW mindsdb . home_rentals_predictions AS ( SELECT a . sqft , a . number_of_bathrooms , a . location , p . rental_price AS price FROM integration . home_rentals_data AS a JOIN mindsdb . home_rentals_model AS p ) ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Dataset for Training and Dataset for Joining In this example, we used the same dataset ( integration.home_rentals_data ) for training the model (see the CREATE MODEL statement above) and for joining with the model to make predictions (see the CREATE VIEW statement above). It doesn’t happen like that in real-world scenarios. Normally, you use the old data to train the model, and then you join the new data with this model to make predictions. Consider the old_data dataset that stores data from the years 2019-2021 and the new_data dataset that stores data from the year 2022. We train the model with the old_data dataset like this: CREATE MODEL mindsdb . data_model FROM integration ( SELECT * FROM old_data ) PREDICT column ; Now, having the data_model model trained using the old_data dataset, we can join this model with the new_data dataset to make predictions like this: CREATE VIEW mindsdb . data_predictions AS ( SELECT a . column1 , a . column2 , a . column3 , p . column AS predicted_column FROM integration . new_data AS a JOIN mindsdb . data_model AS p ) ; ​ USING VIEW Examples to use view Complex select on view (it is grouping in this example). SELECT type , last ( bedrooms ) FROM mindsdb . house_v GROUP BY 1 Creating predictor from view CREATE MODEL house_sales_model FROM mindsdb ( SELECT * FROM house_v ) PREDICT ma ORDER BY saledate GROUP BY bedrooms , type WINDOW 1 HORIZON 4 Using predictor with view SELECT * FROM mindsdb . house_v JOIN mindsdb . house_sales_model WHERE house_v . saledate > latest From Our Community Check out the article created by our community: Article on Creating Views with MindsDB by Rutam Prita Mishra Was this page helpful? Yes No Suggest edits Raise issue Delete From a Table Remove a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example USING VIEW"}
{"file_name": "database.html", "content": "Connect a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Connect a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources Connect a Data Source ​ Description MindsDB lets you connect to your favorite databases, data warehouses, data lakes, etc., via the CREATE DATABASE command. The MindsDB SQL API supports creating connections to integrations by passing the connection parameters specific per integration. You can find more in the Supported Integrations chapter. MindsDB doesn’t store or copy your data. Instead, it fetches data directly from your connected sources each time you make a query, ensuring that any changes to the data are instantly reflected. This means your data remains in its original location, and MindsDB always works with the most up-to-date information. ​ Syntax Let’s review the syntax for the CREATE DATABASE command. CREATE DATABASE [ IF NOT EXISTS ] datasource_name [ WITH ] [ ENGINE [ = ] engine_name ] [ , ] [ PARAMETERS [ = ] { \"key\" : \"value\" , . . . } ] ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Where: Name Description datasource_name Identifier for the data source to be created. engine_name Engine to be selected depending on the database connection. PARAMETERS {\"key\": \"value\"} object with the connection parameters specific for each engine. SQL Commands Resulting in the Same Output Please note that the keywords/statements enclosed within square brackets are optional. Also, by default, the engine is mindsdb if not provided otherwise. That yields the following SQL commands to result in the same output. CREATE DATABASE db ; CREATE DATABASE db ENGINE 'mindsdb' ; CREATE DATABASE db ENGINE = 'mindsdb' ; CREATE DATABASE db WITH ENGINE 'mindsdb' ; CREATE DATABASE db USING ENGINE = 'mindsdb' ; ​ What’s available on your installation Here is how you can query for all the available data handlers used to create database connections. SELECT * FROM information_schema . handlers WHERE type = 'data' ; Or, alternatively: SHOW HANDLERS WHERE type = 'data' ; And here is how you can query for all the connected databases: SELECT * FROM information_schema . databases ; Or, alternatively: SHOW DATABASES ; SHOW FULL DATABASES ; ​ Example ​ Connecting a Data Source Here is an example of how to connect to a MySQL database. CREATE DATABASE mysql_datasource WITH ENGINE = 'mariadb' , PARAMETERS = { \"user\" : \"root\" , \"port\" : 3307 , \"password\" : \"password\" , \"host\" : \"127.0.0.1\" , \"database\" : \"my_database\" } ; On execution, we get: Query OK , 0 rows affected ( 8.878 sec ) ​ Listing Linked Databases You can list all the linked databases using the command below. SHOW DATABASES ; On execution, we get: + --------------------+ | Database | + --------------------+ | information_schema | | mindsdb | | files | | mysql_datasource | + --------------------+ ​ Making your Local Database Available to MindsDB When connecting your local database to MindsDB Cloud, you should expose the local database server to be publicly accessible. It is easy to accomplish using Ngrok Tunnel . The free tier offers all you need to get started. The installation instructions are easy to follow. Head over to the downloads page and choose your operating system. Follow the instructions for installation. Then create a free account at Ngrok to get an auth token that you can use to configure your Ngrok instance. Once installed and configured, run the following command to obtain the host and port for your localhost at port-number . ngrok tcp port-number Here is an example. Assuming that you run a PostgreSQL database at localhost:5432 , use the following command: ngrok tcp 5432 On execution, we get: Session Status online Account myaccount ( Plan: Free ) Version 2.3 .40 Region United States ( us ) Web Interface http://127.0.0.1:4040 Forwarding tcp://4.tcp.ngrok.io:15093 - > localhost 5432 Now you can access your local database at 4.tcp.ngrok.io:15093 instead of localhost:5432 . So to connect your local database to the MindsDB GUI, use the Forwarding information. The host is 4.tcp.ngrok.io , and the port is 15093 . Proceed to create a database connection in the MindsDB GUI by executing the CREATE DATABASE statement with the host and port number obtained from Ngrok. CREATE DATABASE psql_datasource WITH ENGINE = 'postgres' , PARAMETERS = { \"user\" : \"postgres\" , \"port\" : 15093 , \"password\" : \"password\" , \"host\" : \"4.tcp.ngrok.io\" , \"database\" : \"postgres\" } ; Please note that the Ngrok tunnel loses connection when stopped or canceled. To reconnect your local database to MindsDB, you should create an Ngrok tunnel again. In the free tier, Ngrok changes the host and port values each time you launch the program, so you need to reconnect your database in the MindsDB Cloud by passing the new host and port values obtained from Ngrok. Before resetting the database connection, drop the previously connected data source using the DROP DATABASE statement. DROP DATABASE psql_datasource ; After dropping the data source and reconnecting your local database, you can use the predictors that you trained using the previously connected data source. However, if you have to RETRAIN your predictors, please ensure the database connection has the same name you used when creating the predictor to avoid failing to retrain. ​ Supported Integrations The list of databases supported by MindsDB keeps growing. Check out all our database integrations here . Was this page helpful? Yes No Suggest edits Raise issue List Data Handlers Remove a Data Source github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax What’s available on your installation Example Connecting a Data Source Listing Linked Databases Making your Local Database Available to MindsDB Supported Integrations"}
{"file_name": "jobs.html", "content": "JOBS - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Jobs JOBS Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Create a Job Remove a Job Query Jobs Triggers AI Agents Functions Standard SQL Support Jobs JOBS MindsDB enables you to automate any pipeline using JOBS, which grant you the power to schedule any query at any frequency. Additionally, it introduces the keyword LAST , offering the capability for a JOB to act solely on new data, essentially treating any data source as a stream. ​ Description The CREATE JOB statement lets you schedule the execution of queries by providing relevant parameters, such as start date, end date, or repetition frequency. ​ Syntax ​ CREATE JOB Here is the syntax: CREATE JOB [ IF NOT EXISTS ] [ project_name . ] job_name [ AS ] ( < statement_1 > [ ; < statement_2 > ] [ ; . . . ] ) [ START < date > ] [ END < date > ] [ EVERY [ number ] < period > ] [ IF ( < statement_1 > [ ; < statement_2 > ] [ ; . . . ] ) ] ; Where: Expression Description [project_name.]job_name Name of the job preceded by an optional project name where the job is to be created. If you do not provide the project_name value, then the job is created in the default mindsdb project. <statement_1>[; <statement_2>][; ...] One or more statements separated by ; to be executed by the job. [START <date>] Optional. The date when the job starts its periodical or one-time execution. If not set, it is the current system date. [END <date>] Optional. The date when the job ends its periodical or one-time execution. If it is not set (and the repetition rules are set), then the job repeats forever. [EVERY [number] <period>] Optional. The repetition rules for the job. If not set, the job runs once, not considering the end date value. If the number value is not set, it defaults to 1. [IF (<statement_1>[; <statement_2>][; ...])] Optional. If the last statement returns one or more rows, only then the job will execute. Available <date> formats Here are the supported <date> formats: '%Y-%m-%d %H:%M:%S' '%Y-%m-%d' Please note that the default time zone is UTC. Available <period> values And the supported <period> values: minute / minutes / min hour / hours day / days week / weeks month / months Further, you can query all jobs and their execution history like this: SHOW JOBS ; SELECT * FROM [ project_name . ] jobs WHERE name = 'job_name' ; SELECT * FROM log . jobs_history WHERE project = 'mindsdb' AND name = 'job_name' ; ​ LAST MindsDB provides a custom LAST keyword that enables you to fetch data inserted after the last time you queried for it. It is a convenient way to select only the newly added data rows when running a job. Imagine you have the fruit_data table that contains the following: + -------+-----------+ | id | name | + -------+-----------+ | 1 | apple | | 2 | orange | + -------+-----------+ When you run the SELECT query with the LAST keyword for the first time, it’ll give an empty output. SELECT id , name FROM fruit_data WHERE id > LAST ; This query returns: + -------+-----------+ | id | name | + -------+-----------+ | null | null | + -------+-----------+ If you want to specify a concrete value for LAST in the first execution of such a query, use the COALESCE(LAST, <value>) function. SELECT id , name FROM fruit_data WHERE id > COALESCE ( LAST , 1 ) ; It will result in executing the following query in the first run: SELECT id , name FROM fruit_data WHERE id > 1 ; And the below query at each subsequent run: SELECT id , name FROM fruit_data WHERE id > LAST ; Now imagine you inserted a new record into the fruit_data table: + -------+-----------+ | id | name | + -------+-----------+ | 1 | apple | | 2 | orange | | 3 | pear | + -------+-----------+ When you run the SELECT query with the LAST keyword again, you’ll get only the newly added record as output. SELECT id , name FROM fruit_data WHERE id > LAST ; This query returns: + -------+-----------+ | id | name | + -------+-----------+ | 3 | pear | + -------+-----------+ From this point on, whenever you add new records into the fruit_data table, it’ll be returned by the next run of the SELECT query with the LAST keyword. And, if you do not add any new records between the query runs, then the output will be null. If you want to clear context for the LAST keyword in the editor, then run set context = 0 or set context = null . ​ Conditional Jobs Here is how you can create a conditional job that will execute periodically only if there is new data available: CREATE JOB conditional_job ( FINETUNE MODEL model_name FROM ( SELECT * FROM datasource . table_name WHERE incremental_column > LAST ) ) EVERY 1 min IF ( SELECT * FROM datasource . table_name WHERE incremental_column > LAST ) ; The above job will be triggered every minute, but it will execute its task (i.e. finetuning the model) only if there is new data available. ​ Examples ​ Example 1: Retrain a Model In this example, we create a job in the current project to retrain the home_rentals_model model and insert predictions into the rentals table. CREATE JOB retrain_model_and_save_predictions ( RETRAIN mindsdb . home_rentals_model USING join_learn_process = true ; INSERT INTO my_integration . rentals ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) ) END '2023-04-01 00:00:00' EVERY 2 days ; Please note that the join_learn_process parameter in the USING clause of the RETRAIN statement ensures that the retraining process completes before inserting predictions into a table. In general, this parameter is used to prevent several retrain processes from running simultaneously. The retrain_model_and_save_predictions job starts its execution on the current system date and ends on the 1st of April 2023. The job is executed every 2 days. ​ Example 2: Save Predictions In this example, the job creates a table named as result_{{START_DATETIME}} and inserts predictions into it. CREATE JOB save_predictions ( CREATE TABLE my_integration . ` result_{{START_DATETIME}} ` ( SELECT m . rental_price , m . rental_price_explain FROM mindsdb . home_rentals_model AS m JOIN example_db . demo_data . home_rentals AS d ) ) EVERY hour ; Please note that the uniqueness of the created table name is ensured here by using the {{START_DATETIME}} variable that is replaced at runtime by the date and time of the current run. You can use the following variables for this purpose: PREVIOUS_START_DATETIME is replaced by date and time of the previous run of this job. START_DATETIME is replaced by date and time of the current job run. START_DATE is replaced by date of the current job run. The save_predictions job starts its execution on the current system date and repeats every 2 hours until it is manually disabled. ​ Example 3: Drop a Model In this example, we create a job to drop the home_rentals_model model scheduled on the 1st of April 2023. CREATE JOB drop_model ( DROP MODEL mindsdb . home_rentals_model ) START '2023-04-01' ; This job runs once on the 1st of April 2023. ​ Example 4: Twitter Chatbot You can easily create a chatbot to respond to tweets using jobs. But before you get to it, you should connect your Twitter account to MindsDB following the instructions here . Follow the tutorial on how to create a Twitter chatbot to learn the details. Let’s create a job that runs every hour, checks for new tweets, and responds using the OpenAI model. CREATE JOB mindsdb . gpt4_twitter_job AS ( -- insert into tweets the output of joining model and new tweets INSERT INTO my_twitter_v2 . tweets ( in_reply_to_tweet_id , text ) SELECT t . id AS in_reply_to_tweet_id , r . response AS text FROM my_twitter . tweets t JOIN mindsdb . snoopstein_model r WHERE t . query = '(@snoopstein OR @snoop_stein OR #snoopstein OR #snoop_stein) -is:retweet -from:snoop_stein' AND t . created_at > LAST LIMIT 10 ) EVERY hour ; The SELECT statement joins the data table with the model table to get responses for newly posted tweets, thanks to the LAST keyword. Then, the INSERT INTO statement writes these responses to the tweets table of the my_twitter integration. To learn more about OpenAI integration with MindsDB, visit our docs here . ​ Additional Configuration Here is the template of the config.json file that you can pass as a parameter when starting your local MindsDB instance: \"jobs\" : { \"disable\" : true, \"check_interval\" : 30 } The disable parameter defines whether the scheduler is active ( true ) or not ( false ). By default, in the MindsDB Editor, the scheduler is active. The check_interval parameter defines the interval in seconds between consecutive checks of the scheduler table. By default, in the MindsDB Editor, it is 30 seconds. You can modify the default configuration in your local MindsDB installation by creating a config.json file and starting MindsDB with this file as a parameter. You can find detailed instructions here . Was this page helpful? Yes No Suggest edits Raise issue Query a File Remove a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax CREATE JOB LAST Conditional Jobs Examples Example 1: Retrain a Model Example 2: Save Predictions Example 3: Drop a Model Example 4: Twitter Chatbot Additional Configuration"}
{"file_name": "file.html", "content": "Upload a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Upload a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Upload a File Follow the steps below to upload a file to MindsDB. Note that the trailing whitespaces on column names are erased upon uploading a file to MindsDB. Access the MindsDB Editor. Navigate to Add data section by clicking the Add data button located in the top right corner. Choose the Files tab. Choose the Import File option. Upload a file (here it is house_sales.csv ), name a table used to store the file data (here it is house_sales ), and click the Save and Continue button. ​ Configuring URL File Upload for Specific Domains The File Uploader can be configured to interact only with specific domains by using the file_upload_domains setting in the config.json file. This feature allows you to restrict the handler to upoad and process files only from the domains you specify, enhancing security and control over web interactions. To configure this, simply list the allowed domains under the file_upload_domains key in config.json . For example: \"file_upload_domains\" : [ \"s3.amazonaws.com\" , \"drive.google.com\" ] ​ What’s Next? Now, you are ready to create a predictor from a file. Make sure to check out this guide on how to do that. Was this page helpful? Yes No Suggest edits Raise issue Query a View Remove a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Configuring URL File Upload for Specific Domains What’s Next?"}
{"file_name": "model.html", "content": "Create, Train, and Deploy a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Create, Train, and Deploy a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Create, Train, and Deploy a Model ​ Description The CREATE MODEL statement creates and trains a machine learning (ML) model. Please note that the CREATE MODEL statement is equivalent to the CREATE MODEL statement. We are transitioning to the CREATE MODEL statement, but the CREATE MODEL statement still works. ​ Syntax ​ Overview Here is the full syntax: CREATE [ OR REPLACE ] MODEL [ IF NOT EXISTS ] project_name . predictor_name [ FROM [ integration_name | project_name ] ( SELECT [ sequential_column , ] [ partition_column , ] column_name , . . . FROM [ integration_name . | project_name . ] table_name [ JOIN model_name ] ) ] PREDICT target_column [ ORDER BY sequential_column ] [ GROUP BY partition_column ] [ WINDOW int ] [ HORIZON int ] [ USING engine = 'engine_name' , tag = 'tag_name' , . . . ] ; Where: Expressions Description project_name Name of the project where the model is created. By default, the mindsdb project is used. predictor_name Name of the model to be created. integration_name Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT column_name, ... FROM table_name) Selecting data to be used for training and validation. target_column Column to be predicted. ORDER BY sequential_column Used in time series models. The column by which time series is ordered. It can be a date or anything that defines the sequence of events. GROUP BY partition_column Used in time series models. It is optional. The column by which rows that make a partition are grouped. For example, if you want to forecast the inventory for all items in the store, you can partition the data by product_id , so each distinct product_id has its own time series. WINDOW int Used in time series models. The number of rows to look back at when making a prediction. It comes after the rows are ordered by the column defined in ORDER BY and split into groups by the column(s) defined in GROUP BY . The WINDOW 10 syntax could be interpreted as “Always use the previous 10 rows”. HORIZON int Used in time series models. It is optional. It defines the number of future predictions (it is 1 by default). However, the HORIZON parameter, besides defining the number of predictions, has an impact on the training procedure when using the Lightwood ML backend. For example, different mixers are selected depending on whether the HORIZON value is one or greater than one. engine_name You can optionally provide an ML engine, based on which the model is created. tag_name You can optionally provide a tag that is visible in the training_options column of the mindsdb.models table. ​ Regression Models Here is the syntax for regression models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT column_name , . . . FROM table_name ) PREDICT target_column [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. The target_column that will be predicted is a numerical value. The prediction values are not limited to a defined set of values, but can be any number from the given range of numbers. ​ Classification Models Here is the syntax for classification models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT column_name , . . . FROM table_name ) PREDICT target_column [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. The target_column that will be predicted is a string value. The prediction values are limited to a defined set of values, such as Yes and No . ​ Time Series Models Here is the syntax for time series models: CREATE MODEL project_name . predictor_name FROM integration_name ( SELECT sequential_column , partition_column , other_column , target_column FROM table_name ) PREDICT target_column ORDER BY sequential_column [ GROUP BY partition_column ] WINDOW int [ HORIZON int ] [ USING engine = 'engine_name' , tag = 'tag_name' ] ; Please note that the FROM clause is mandatory here. Due to the nature of time series forecasting, you need to use the JOIN statement and join the data table with the model table to get predictions. ​ NLP Models Here is the syntax for using external models within MindsDB: CREATE MODEL project_name . model_name PREDICT PRED USING engine = 'engine_name' , task = 'task_name' , model_name = 'hub_model_name' , input_column = 'input_column_name' , labels = [ 'label1' , 'label2' ] ; Please note that you don’t need to define the FROM clause here. Instead, the input_column is defined in the USING clause. It allows you to bring an external model, for example, from the Hugging Face model hub, and use it within MindsDB. ​ Large Language Models (LLM) MindsDB integrates with numerous LLM providers listed here . Commonly, LLMs support the prompt_template parameter that stores the message/instruction to the model. CREATE MODEL project_name . model_name PREDICT answer USING engine = 'llm_engine_name' , prompt_template = 'answer users questions in a helpful way: {{questions}}' ; The prompt_template parameter instructs the model what output should be generated. It can include variables enclosed in double curly braces, which will be replaced with data values upon joining the model with the input data. ​ Example ​ Regression Models Here is an example for regression models that uses data from a database: CREATE MODEL mindsdb . home_rentals_model FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING engine = 'lightwood' , tag = 'my home rentals model' ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on regression models to see the full example. ​ Classification Models Here is an example for classification models that uses data from a file: CREATE MODEL mindsdb . customer_churn_predictor FROM files ( SELECT * FROM churn ) PREDICT Churn USING engine = 'lightwood' , tag = 'my customers model' ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on classification models to see the full example. ​ Time Series Models Here is an example for time series models that uses data from a file: CREATE MODEL mindsdb . house_sales_predictor FROM files ( SELECT * FROM house_sales ) PREDICT MA ORDER BY saledate GROUP BY bedrooms -- the target column to be predicted stores one row per quarter WINDOW 8 -- using data from the last two years to make forecasts (last 8 rows) HORIZON 4 ; -- making forecasts for the next year (next 4 rows) On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Visit our tutorial on time series models to see the full example. ​ NLP Models Here is an example for the Hugging Face model: CREATE MODEL mindsdb . spam_classifier PREDICT PRED USING engine = 'huggingface' , task = 'text-classification' , model_name = 'mrm8488/bert-tiny-finetuned-sms-spam-detection' , input_column = 'text_spammy' , labels = [ 'ham' , 'spam' ] ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) ​ Large Language Models (LLM) Here is an example using the OpenAI engine: CREATE MODEL sentiment_classifier PREDICT sentiment USING engine = 'openai_engine' , prompt_template = 'analyze customer reviews and assign sentiment as positive or negative or neutral: {{review}}' ; Note that the prompt_template parameter stores instructions that the model will follow to generate output. Visit our page no how to bring Hugging Face models into MindsDB for more details. Checking Model Status After you run the CREATE MODEL statement, you can check the status of the training process by querying the mindsdb.models table. DESCRIBE predictor_name ; On execution, we get: + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | name | status | accuracy | predict | update_status | mindsdb_version | error | select_data_query | training_options | + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ | predictor_name | generating or training or complete | number depending on the accuracy metric | column_to_be_predicted | up_to_date | 22.7 .5 .0 | | | | + ---------------+-----------------------------------+----------------------------------------+-----------------------+-------------+---------------+-----+-----------------+----------------+ Was this page helpful? Yes No Suggest edits Raise issue List Projects Remove a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Overview Regression Models Classification Models Time Series Models NLP Models Large Language Models (LLM) Example Regression Models Classification Models Time Series Models NLP Models Large Language Models (LLM)"}
{"file_name": "evaluate.html", "content": "EVALUATE - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions EVALUATE Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Join Models with Tables Evaluate Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Predictions EVALUATE ​ Description The EVALUATE statement evaluates predictions based on available metrics . ​ Syntax Here is the syntax: EVALUATE metric_name FROM ( SELECT real_value AS actual , predicted_value AS prediction FROM table_name ) ; Where: Name Description metric_name It is the name of the metric to be evaluated chosen from here . real_value It is the real value that will be compared with the predicted value. predicted_value It is the value predicted by the model. table_name It is the table that stores corresponding real and predicted values. ​ Example This example calculates the mean absolute error. EVALUATE mean_absolute_error FROM ( SELECT column_name_that_stores_real_value AS actual , column_name_that_stores_predicted_value AS prediction FROM table ) ; Was this page helpful? Yes No Suggest edits Raise issue Join Models with Tables Create a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "select.html", "content": "Query a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Query a Table ​ Description The SELECT statement fetches data from a table and predictions from a model. Here we go over example of selecting data from tables of connected data sources. To learn how to select predictions from a model, visit this page . ​ Syntax ​ Simple SELECT FROM an integration In this example, query contains only tables from one integration. This query will be executed on this integration database (where integration name will be cut from the table name). SELECT location , max ( sqft ) FROM example_db . demo_data . home_rentals GROUP BY location LIMIT 5 ; ​ Raw SELECT FROM an integration It is also possible to send native queries to integration that use syntax native to a given integration. It is useful when a query can not be parsed as SQL. SELECT . . . FROM integration_name ( native query goes here ) ; Here is an example of selecting from a Mongo integration using Mongo-QL syntax: SELECT * FROM mongo ( db . house_sales2 . find ( ) . limit ( 1 ) ) ; ​ Complex queries Subselect on data from integration. It can be useful in cases when integration engine doesn’t support some functions, for example, grouping, as shown below. In this case, all data from raw select are passed to MindsDB and then subselect performs operations on them inside MindsDB. SELECT type , max ( bedrooms ) , last ( MA ) FROM mongo ( db . house_sales2 . find ( ) . limit ( 300 ) ) GROUP BY 1 Unions It is possible to use UNION and UNION ALL operators. It this case, every subselect from union will be fetched and merged to one result-set on MindsDB side. SELECT data . time as date , data . target FROM datasource . table_name as data UNION ALL SELECT model . time as date , model . target as target FROM mindsdb . model as model JOIN datasource . table_name as t WHERE t . time > LATEST AND t . group = 'value' ; Was this page helpful? Yes No Suggest edits Raise issue Remove a Table Native Queries github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Simple SELECT FROM an integration Raw SELECT FROM an integration Complex queries"}
{"file_name": "select-files.html", "content": "Query a File - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a File Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Query a File ​ Description The SELECT * FROM files.file_name statement is used to select data from a file. First, you upload a file to the MindsDB Editor by following this guide . And then, you can CREATE MODEL from the uploaded file. ​ Syntax Here is the syntax: SELECT * FROM files . file_name ; On execution, we get: + --------+--------+--------+--------+ | column | column | column | column | + --------+--------+--------+--------+ | value | value | value | value | + --------+--------+--------+--------+ Where: Name Description file_name Name of the file uploaded to the MindsDB Editor by following this guide . column Name of the column from the file. ​ Example Once you uploaded your file by following this guide , you can query it like a table. SELECT * FROM files . home_rentals LIMIT 10 ; On execution, we get: + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | initial_price | neighborhood | rental_price | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ | 0 | 1 | 484 , 8 | great | 10 | 2271 | south_side | 2271 | | 1 | 1 | 674 | good | 1 | 2167 | downtown | 2167 | | 1 | 1 | 554 | poor | 19 | 1883 | westbrae | 1883 | | 0 | 1 | 529 | great | 3 | 2431 | south_side | 2431 | | 3 | 2 | 1219 | great | 3 | 5510 | south_side | 5510 | | 1 | 1 | 398 | great | 11 | 2272 | south_side | 2272 | | 3 | 2 | 1190 | poor | 58 | 4463 | westbrae | 4123.812 | | 1 | 1 | 730 | good | 0 | 2224 | downtown | 2224 | | 0 | 1 | 298 | great | 9 | 2104 | south_side | 2104 | | 2 | 1 | 878 | great | 8 | 3861 | south_side | 3861 | + -----------------+---------------------+-------+----------+----------------+---------------+--------------+--------------+ Now let’s create a predictor using the uploaded file. You can learn more about the CREATE MODEL command here . CREATE MODEL mindsdb . home_rentals_model FROM files ( SELECT * from home_rentals ) PREDICT rental_price ; On execution, we get: Query OK , 0 rows affected ( x . xxx sec ) Was this page helpful? Yes No Suggest edits Raise issue Remove a File Create a Job github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "describe.html", "content": "Describe a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Describe a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Describe a Model ​ Description The DESCRIBE statement is used to display the attributes of an existing model. The available options to describe a model depend on the underlying engine. ​ Syntax Here is how to retrieve general information on the model: DESCRIBE model_name ; Or: DESCRIBE MODEL model_name ; This command is similar to the below command: SELECT * FROM models WHERE name = 'model_name' ; One difference between these two commands is that DESCRIBE outputs an additional column that stores all available options to describe a model, depending on the underlying engine. ​ Examples ​ Lightwood Models MindsDB uses the Lightwood engine by default. Let’s see how to describe such models. DESCRIBE [ MODEL ] home_rentals_model ; On execution we get: + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ | [ \"info\" , \"features\" , \"model\" , \"jsonai\" ] | home_rentals_model | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 23.4 .4 .0 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' } | [ NULL ] | + --------------------------------------+--------------------+-----------+---------+--------+---------+----------+----------+--------------+---------------+-----------------+--------+--------------------------------------+----------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE info DESCRIBE features DESCRIBE model DESCRIBE jsonai DESCRIBE [ MODEL ] home_rentals_model . info ; The above command returns the following output columns: Name Description accuracies It lists the accuracy function used to evaluate the model and the achieved score. column_importances It lists all feature-type columns and assigns importance values. outputs The target column. inputs All the feature columns. ​ NLP Models MindsDB offers NLP models that utilize either Hugging Face or OpenAI engines. Let’s see how to describe such models. DESCRIBE [ MODEL ] sentiment_classifier ; On execution we get: + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | [ \"args\" , \"metadata\" ] | sentiment_classifier | openai | mindsdb | true | 1 | complete | [ NULL ] | sentiment | up_to_date | 23.1 .3 .2 | [ NULL ] | [ NULL ] | { 'target' : 'sentiment' , 'using' : { 'prompt_template' : 'describe the sentiment of the reviews\\n strictly as \"positive\", \"neutral\", or \"negative\".\\n \"I love the product\":positive\\n \"It is a scam\":negative\\n \"{{review}}.\":' }} | [ NULL ] | + ---------------------+----------------------+--------+---------+--------+---------+----------+----------+-----------+---------------+-----------------+--------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE args DESCRIBE metadata DESCRIBE [ MODEL ] sentiment_classifier . args ; The above command returns the following output columns: Name Description key It stores parameters, such as prompt_template and target . value It stores parameter values. ​ Nixtla Models MindsDB integrates Nixtla engines, such as StatsForecast, NeuralForecast, and HierarchicalForecast. Let’s see how to describe models based on Nixtla engines. DESCRIBE [ MODEL ] quarterly_expenditure_forecaster ; On execution we get: + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | tables | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | TAG | + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ | [ \"info\" , \"features\" , \"model\" ] | quarterly_expenditure_forecaster | statsforecast | mindsdb | true | 1 | complete | [ NULL ] | expenditure | up_to_date | 23.4 .4 .0 | [ NULL ] | SELECT * FROM historical_expenditures | { 'target' : 'expenditure' , 'using' : {} , 'timeseries_settings' : { 'is_timeseries' : True , 'order_by' : 'month' , 'horizon' : 3 , 'group_by' : [ 'category' ] }} | [ NULL ] | + -----------------------------+----------------------------------+---------------+---------+--------+---------+----------+----------+-------------+---------------+-----------------+--------+---------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+--------+ The tables output column lists all available options to describe a model. DESCRIBE info DESCRIBE features DESCRIBE model DESCRIBE [ MODEL ] quarterly_expenditure_forecaster . info ; The above command returns the following output columns: Name Description accuracies It lists the chosen model name and its accuracy score. outputs The target column. inputs All the feature columns. ​ Other Models Models that utlize LangChain or are brought to MindsDB via MLflow can be described as follows: DESCRIBE [ MODEL ] other_model ; The above command returs [\"info\"] in its first output column. DESCRIBE [ MODEL ] other_model . info ; The above command lists basic model information. If you need more information on how to DESCRIBE [MODEL] or understand the results, feel free to ask us on the community Slack workspace . Was this page helpful? Yes No Suggest edits Raise issue List Models Retrain a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Examples Lightwood Models NLP Models Nixtla Models Other Models"}
{"file_name": "finetune.html", "content": "Finetune a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Finetune a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Finetune a Model ​ Description The FINETUNE statement lets you retrain a model with additional training data. Imagine you have a model that was trained with a certain dataset. Now there is more training data available and you wish to retrain this model with a new dataset. The FINETUNE statement lets you partially retrain the model, so it takes less time and resources than the RETRAIN statement. In the machine learning literature, this is also referred to as fine-tuning a model. ​ Syntax Here is the syntax: FINETUNE [ MODEL ] project_name . model_name FROM [ integration_name | project_name ] ( SELECT column_name , . . . FROM [ integration_name . | project_name . ] table_name [ WHERE incremental_column > LAST ] ) [ USING key = value , . . . ] ; Where: Expressions Description project_name Name of the project where the model resides. model_name Name of the model to be retrained. integration_name Name of the integration created using the CREATE DATABASE statement or file upload. (SELECT column_name, ... FROM table_name) Selecting additional data to be used for retraining. WHERE incremental_column > LAST Selecting only newly added data to be used to finetune the model. Learn more about the LAST keyword here . USING key = value Optional. The USING clause lets you pass multiple parameters to the FINETUNE statement. Model Versions Every time the model is finetuned or retrained, its new version is created with an incremented version number. Unless overridden, the most recent version becomes active when training completes. You can query for all model versions like this: SELECT * FROM project_name . models ; For more information on managing model versions, check out our docs here . While the model is being generated and trained, it is not active. The model becomes active only after it completes generating and training. ​ Examples ​ Example 1: OpenAI Model Fine-Tuning ​ Example 2: Llama2 Model Fine-Tuning ​ Example 3: Regression Model Fine-Tuning ​ Example 4: Classification Model Fine-Tuning Was this page helpful? Yes No Suggest edits Raise issue Retrain a Model Manage Model Versions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Examples Example 1: OpenAI Model Fine-Tuning Example 2: Llama2 Model Fine-Tuning Example 3: Regression Model Fine-Tuning Example 4: Classification Model Fine-Tuning"}
{"file_name": "join.html", "content": "Join Models with Tables - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Predictions Join Models with Tables Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Get a Single Prediction Get Batch Predictions Join Models with Tables Evaluate Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Predictions Join Models with Tables ​ Description The JOIN clause combines rows from the database table and the model table on a column defined in its implementation. It is used to make batch predictions, as shown in the examples. ​ Syntax Here is the syntax that lets you join multiple data tables with multiple models to get all predictions at once. SELECT d1 . column_name , d2 . column_name , m1 . column_name , m2 . column_name , . . . FROM integration_name . table_name_1 [ AS ] d1 [ JOIN integration_name . table_name_2 [ AS ] d2 ON . . . ] [ JOIN . . . ] JOIN project_name . model_name_1 [ AS ] m1 [ JOIN project_name . model_name_2 [ AS ] m2 ] [ JOIN . . . ] [ ON d1 . input_data = m1 . expected_argument ] ; Where: Name Description integration_name.table_name_1 Name of the data source table used as input for making predictions. integration_name.table_name_2 Optionally, you can join arbitrary number of data source tables. project_name.model_name_1 Name of the model table used to make predictions. project_name.model_name_2 Optionally, you can join arbitrary number of models. ​ Mapping input data to model arguments If the input data contains a column named question and the model requires an argument named input , you can map these columns, as explained below. We have a model that expects to receive input : CREATE MODEL model_name PREDICT answer USING engine = 'openai' , prompt_template = 'provide answers to an input from a user: {{input}}' ; We have an input data table that has the following columns: + ----+-------------------------------------------+ | id | question | + ----+-------------------------------------------+ | 1 | How many planets are in the solar system? | | 2 | How many stars are in the solar system? | + ----+-------------------------------------------+ Now if you want to get answers to these questions using the model, you need to join the input data table with the model and map the question column onto the input argument. SELECT * FROM input_table AS d JOIN model_name AS m ON d . question = m . input ; ​ Example 1 Let’s join the home_rentals table with the home_rentals_model model using this statement: SELECT t . rental_price AS real_price , m . rental_price AS predicted_price , t . number_of_rooms , t . number_of_bathrooms , t . sqft , t . location , t . days_on_market FROM example_db . demo_data . home_rentals AS t JOIN mindsdb . home_rentals_model AS m LIMIT 20 ; On execution, we get: + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | real_price | predicted_price | number_of_rooms | number_of_bathrooms | sqft | location | days_on_market | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ | 3901 | 3886 | 2 | 1 | 917 | great | 13 | | 2042 | 2007 | 0 | 1 | 194 | great | 10 | | 1871 | 1865 | 1 | 1 | 543 | poor | 18 | | 3026 | 3020 | 2 | 1 | 503 | good | 10 | | 4774 | 4748 | 3 | 2 | 1066 | good | 13 | | 4382 | 4388 | 3 | 2 | 816 | poor | 25 | | 2269 | 2272 | 0 | 1 | 461 | great | 6 | | 2284 | 2272 | 1 | 1 | 333 | great | 6 | | 5420 | 5437 | 3 | 2 | 1124 | great | 9 | | 5016 | 4998 | 3 | 2 | 1204 | good | 7 | | 1421 | 1427 | 0 | 1 | 538 | poor | 43 | | 3476 | 3466 | 2 | 1 | 890 | good | 6 | | 5271 | 5255 | 3 | 2 | 975 | great | 6 | | 3001 | 2993 | 2 | 1 | 564 | good | 13 | | 4682 | 4692 | 3 | 2 | 953 | good | 10 | | 1783 | 1738 | 1 | 1 | 493 | poor | 24 | | 1548 | 1543 | 1 | 1 | 601 | poor | 47 | | 1492 | 1491 | 0 | 1 | 191 | good | 12 | | 2431 | 2419 | 0 | 1 | 511 | great | 1 | | 4237 | 4257 | 3 | 2 | 916 | poor | 36 | + ------------+-----------------+-----------------+---------------------+------+----------+----------------+ ​ Example 2 Let’s query a time series model using this statement: SELECT m . saledate as date , m . ma AS forecast FROM mindsdb . house_sales_model AS m JOIN example_db . demo_data . house_sales AS t WHERE t . saledate > LATEST AND t . type = 'house' LIMIT 4 ; On execution, we get: + ----------+------------------+ | date | forecast | + ----------+------------------+ | 2019 - 12 - 31 | 517506.31349071994 | | 2019 - 12 - 31 | 627822.6592658638 | | 2019 - 12 - 31 | 953426.9545788583 | | 2019 - 12 - 31 | 767252.4205039773 | + ----------+------------------+ Follow this doc page to see examples of joining multiple data table with multiple models. Was this page helpful? Yes No Suggest edits Raise issue Get Batch Predictions Evaluate Predictions github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Mapping input data to model arguments Example 1 Example 2"}
{"file_name": "insert.html", "content": "Insert Into a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Insert Into a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Insert Into a Table ​ Description The INSERT INTO statement inserts data into a table. The data comes from a subselect query. It is commonly used to input prediction results into a database table. ​ Syntax Here is the syntax: INSERT INTO integration_name . table_name ( SELECT . . . ) ; Please note that the destination table ( integration_name.table_name ) must exist and contain all the columns where the data is to be inserted. And the steps followed by the syntax: It executes a subselect query to get the output dataset. It uses the INSERT INTO statement to insert the output of the (SELECT ...) query into the integration_name.table_name table. On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs ​ Example We want to save the prediction results into the int1.tbl1 table. Here is the schema structure used throughout this example: int1 └── tbl1 mindsdb └── predictor_name int2 └── tbl2 Where: Name Description int1 Integration where the table that stores prediction results resides. tbl1 Table that stores prediction results. predictor_name Name of the model. int2 Integration where the data source table used in the inner SELECT statement resides. tbl2 Data source table used in the inner SELECT statement. Let’s execute the query. INSERT INTO int1 . tbl1 ( SELECT * FROM int2 . tbl2 AS ta JOIN mindsdb . predictor_name AS tb WHERE ta . date > '2015-12-31' ) ; On execution, we get: Query OK , 0 row ( s ) updated - x . xxxs Was this page helpful? Yes No Suggest edits Raise issue Update a Table Join Tables On github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example"}
{"file_name": "manage-models-versions.html", "content": "Manage Model Versions - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Manage Model Versions Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Manage Model Versions ​ Creating a Model To create a model, use the CREATE MODEL statement. CREATE MODEL mindsdb . home_rentals FROM example_db ( SELECT * FROM demo_data . home_rentals ) PREDICT rental_price USING engine = 'lightwood' , tag = 'my model' ; Now, your model has one version. You can verify by querying the models table. DESCRIBE MODEL home_rentals ; On execution, we get: + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | CURRENT_TRAINING_PHASE | TOTAL_TRAINING_PHASES | TAG | CREATED_AT | TRAINING_TIME | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | home_rentals | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 16 : 01 : 04.990958 | 19.946 | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ ​ Retraining a Model To retrain a model, use the RETRAIN statement. RETRAIN mindsdb . home_rentals ; Let’s query for the model versions again. DESCRIBE MODEL home_rentals ; On execution, we get: + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | NAME | ENGINE | PROJECT | ACTIVE | VERSION | STATUS | ACCURACY | PREDICT | UPDATE_STATUS | MINDSDB_VERSION | ERROR | SELECT_DATA_QUERY | TRAINING_OPTIONS | CURRENT_TRAINING_PHASE | TOTAL_TRAINING_PHASES | TAG | CREATED_AT | TRAINING_TIME | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ | home_rentals | lightwood | mindsdb | true | 1 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 16 : 01 : 04.990958 | 19.946 | | home_rentals | lightwood | mindsdb | true | 2 | complete | 0.999 | rental_price | up_to_date | 22.11 .3 .2 | [ NULL ] | SELECT * FROM demo_data . home_rentals | { 'target' : 'rental_price' , 'using' : {}} | 5 | 5 | [ NULL ] | 2024 - 02 - 07 17 : 01 : 04.990958 | 21.923 | + ------------+---------+-------+------+-------+--------+--------+------------+-------------+---------------+------+------------------------------------+---------------------------------------+----------------------+---------------------+------+--------------------------+-------------+ ​ Using Active Model Version To use the currently active model version, run this query: SELECT * FROM mindsdb . home_rentals AS p JOIN example_db . demo_data . home_rentals AS d ; ​ Using Specific Model Version To use a specific model version, even if it is set to inactive, run this query: SELECT * FROM mindsdb . home_rentals . 1 AS p JOIN example_db . demo_data . home_rentals AS d ; ​ Setting Model Version as Active To set a specific model version as active, run the below statement: SET model_active = home_rentals . 1 ; ​ Deleting Specific Model Version To delete a specific model version, run the below statement: DROP MODEL home_rentals . 2 Please note that you cannot delete the version that is active. ​ Deleting All Model Versions To delete all models version, run the DROP MODEL statement: DROP MODEL mindsdb . home_rentals ; Was this page helpful? Yes No Suggest edits Raise issue Finetune a Model Get a Single Prediction github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Creating a Model Retraining a Model Using Active Model Version Using Specific Model Version Setting Model Version as Active Deleting Specific Model Version Deleting All Model Versions"}
{"file_name": "delete.html", "content": "Delete From a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Delete From a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Delete From a Table ​ Description The DELETE statement removes rows that fulfill the WHERE clause criteria. ​ Syntax Here is the syntax: DELETE FROM integration_name . table_name WHERE column_name = column_value_to_be_removed ; This statement removes all rows from the table_name table (that belongs to the integration_name integration) wherever the column_name column value is equal to column_value_to_be_removed . And here is another way to filter the rows using a subquery: DELETE FROM integration_name . table_name WHERE column_name IN ( SELECT column_value_to_be_removed FROM some_integration . some_table WHERE some_column = some_value ) ; This statement removes all rows from the table_name table (that belongs to the integration_name integration) wherever the column_name column value is equal to one of the values returned by the subquery. Was this page helpful? Yes No Suggest edits Raise issue Join Tables On Create a View github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "join-on.html", "content": "Join Tables On - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Join Tables On Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Join Tables On ​ Description The JOIN statement combines two or more tables based ON a specified column(s). It functions as a standard JOIN in SQL while offering the added capability of combining data from multiple data sources , allowing users to join data from one or more data sources seamlessly. ​ Syntax Here is the syntax: SELECT t1 . column_name , t2 . column_name , t3 . column_name FROM datasource1 . table1 [ AS ] t1 JOIN datasource2 . table2 [ AS ] t2 ON t1 . column_name = t2 . column_name JOIN datasource3 . table3 [ AS ] t3 ON t1 . column_name = t3 . column_name ; This query joins data from three different datasources - datasource1 , datasource2 , and datasource3 - allowing users to execute federated queries accross multiple data sources. Nested JOINs MindsDB provides you with two categories of JOINs . One is the JOIN statement which combines the data table with the model table in order to fetch bulk predictions. Another is the regular JOIN used throughout SQL, which requires the ON clause. You can nest these types of JOINs as follows: SELECT * FROM ( SELECT * FROM project_name . model_table AS m JOIN datasource_name . data_table AS d ; ) AS t1 JOIN ( SELECT * FROM project_name . model_table AS m JOIN datasource_name . data_table AS d ; ) AS t2 ON t1 . column_name = t2 . column_name ; ​ Example 1 Let’s use the following data to see how the different types of JOINs work. The pets table that stores pets: + ------+-------+ | pet_id | name | + ------+-------+ | 1 | Moon | | 2 | Ripley | | 3 | Bonkers | | 4 | Star | | 5 | Luna | | 6 | Lake | + ------+-------+ And the owners table that stores pets’ owners: + --------+-------+------+ | owner_id | name | pet_id | + --------+-------+------+ | 1 | Amy | 4 | | 2 | Bob | 1 | | 3 | Harry | 5 | | 4 | Julia | 2 | | 5 | Larry | 3 | | 6 | Henry | 0 | + --------+-------+------+ ​ JOIN or INNER JOIN The JOIN or INNER JOIN command joins the rows of the owners and pets tables wherever there is a match. For example, a pet named Lake does not have an owner, so it’ll be left out. SELECT * FROM files . owners o [ INNER ] JOIN files . pets p ON o . pet_id = p . pet_id ; On execution, we get: + --------+-------+------+------+-------+ | owner_id | name | pet_id | pet_id | name | + --------+-------+------+------+-------+ | 1 | Amy | 4 | 4 | Star | | 2 | Bob | 1 | 1 | Moon | | 3 | Harry | 5 | 5 | Luna | | 4 | Julia | 2 | 2 | Ripley | | 5 | Larry | 3 | 3 | Bonkers | + --------+-------+------+------+-------+ As in standard SQL, you can use the WHERE clause to filter the output data. SELECT * FROM files . owners o [ INNER ] JOIN files . pets p ON o . pet_id = p . pet_id WHERE o . name = 'Amy' OR o . name = 'Bob' ; On execution, we get: + --------+-------+------+------+-------+ | owner_id | name | pet_id | pet_id | name | + --------+-------+------+------+-------+ | 1 | Amy | 4 | 4 | Star | | 2 | Bob | 1 | 1 | Moon | + --------+-------+------+------+-------+ ​ LEFT JOIN The LEFT JOIN command joins the rows of two tables such that all rows from the left table, even the ones with no match, show up. Here, the left table is the owners table. SELECT * FROM files . owners o LEFT JOIN files . pets p ON o . pet_id = p . pet_id ; On execution, we get: + --------+-------+------+------+-------+ | owner_id | name | pet_id | pet_id | name | + --------+-------+------+------+-------+ | 1 | Amy | 4 | 4 | Star | | 2 | Bob | 1 | 1 | Moon | | 3 | Harry | 5 | 5 | Luna | | 4 | Julia | 2 | 2 | Ripley | | 5 | Larry | 3 | 3 | Bonkers | | 6 | Henry | 0 | [ NULL ] | [ NULL ] | + --------+-------+------+------+-------+ ​ RIGHT JOIN The RIGHT JOIN command joins the rows of two tables such that all rows from the right table, even the ones with no match, show up. Here, the right table is the pets table. SELECT * FROM files . owners o RIGHT JOIN files . pets p ON o . pet_id = p . pet_id ; On execution, we get: + --------+-------+------+------+-------+ | owner_id | name | pet_id | pet_id | name | + --------+-------+------+------+-------+ | 2 | Bob | 1 | 1 | Moon | | 4 | Julia | 2 | 2 | Ripley | | 5 | Larry | 3 | 3 | Bonkers | | 1 | Amy | 4 | 4 | Star | | 3 | Harry | 5 | 5 | Luna | | [ NULL ] | [ NULL ] | [ NULL ] | 6 | Lake | + --------+-------+------+------+-------+ ​ FULL JOIN or FULL OUTER JOIN The FULL [OUTER] JOIN command joins the rows of two tables such that all rows from both tables, even the ones with no match, show up. SELECT * FROM files . owners o FULL [ OUTER ] JOIN files . pets p ON o . pet_id = p . pet_id ; On execution, we get: + --------+------+------+------+-------+---------+ | owner_id | name | pet_id | pet_id | name | animal_id | + --------+------+------+------+-------+---------+ | 1 | Amy | 4 | 4 | Star | 2 | | 2 | Bob | 1 | 1 | Moon | 1 | | 3 | Harry | 5 | 5 | Luna | 2 | | 4 | Julia | 2 | 2 | Ripley | 1 | | 5 | Larry | 3 | 3 | Bonkers | 3 | | 6 | Henry | 0 | [ NULL ] | [ NULL ] | [ NULL ] | | [ NULL ] | [ NULL ] | [ NULL ] | 6 | Lake | 4 | + --------+------+------+------+-------+---------+ ​ Example 2 More than two tables can be joined subsequently. Let’s use another table called animals : + ---------+-------+ | animal_id | name | + ---------+-------+ | 1 | Dog | | 2 | Cat | | 3 | Hamster | | 4 | Fish | + ---------+-------+ Now we can join all three tables. SELECT * FROM files . owners o RIGHT JOIN files . pets p ON o . pet_id = p . pet_id JOIN files . animals a ON p . animal_id = a . animal_id ; On execution, we get: + --------+-------+------+------+-------+---------+---------+-------+ | owner_id | name | pet_id | pet_id | name | animal_id | animal_id | name | + --------+-------+------+------+-------+---------+---------+-------+ | 2 | Bob | 1 | 1 | Moon | 1 | 1 | Dog | | 4 | Julia | 2 | 2 | Ripley | 1 | 1 | Dog | | 5 | Larry | 3 | 3 | Bonkers | 3 | 3 | Hamster | | 1 | Amy | 4 | 4 | Star | 2 | 2 | Cat | | 3 | Harry | 5 | 5 | Luna | 2 | 2 | Cat | | [ NULL ] | [ NULL ] | [ NULL ] | 6 | Lake | 4 | 4 | Fish | + --------+-------+------+------+-------+---------+---------+-------+ Was this page helpful? Yes No Suggest edits Raise issue Insert Into a Table Delete From a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax Example 1 JOIN or INNER JOIN LEFT JOIN RIGHT JOIN FULL JOIN or FULL OUTER JOIN Example 2"}
{"file_name": "retrain.html", "content": "Retrain a Model - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Models Retrain a Model Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Create, Train, and Deploy a Model Remove a Model List Models Describe a Model Retrain a Model Finetune a Model Manage Model Versions Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Models Retrain a Model ​ Description The RETRAIN statement is used to retrain the already trained predictors with the new data. The predictor is updated to leverage the new data in optimizing its predictive capabilities. Retraining takes at least as much time as the training process of the predictor did because now the dataset used to retrain has new or updated data in addition to the old data. ​ Syntax Here is the syntax: RETRAIN [ MODEL ] project_name . predictor_name [ FROM [ integration_name | project_name ] ( SELECT column_name , . . . FROM [ integration_name . | project_name . ] table_name ) PREDICT target_name USING engine = 'engine_name' , tag = 'tag_name' , active = 0 / 1 ] ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) Where: Expressions Description project_name Name of the project where the model resides. predictor_name Name of the model to be retrained. integration_name Optional. Name of the integration created using the CREATE DATABASE statement or file upload . (SELECT column_name, ... FROM table_name) Optional. Selecting data to be used for training and validation. target_column Optional. Column to be predicted. engine_name You can optionally provide an ML engine, based on which the model is retrained. tag_name You can optionally provide a tag that is visible in the training_options column of the mindsdb.models table. active Optional. Setting it to 0 causes the retrained version to be inactive. And setting it to 1 causes the retrained version to be active. Model Versions Every time the model is retrained, its new version is created with the incremented version number. You can query for all model versions like this: SELECT * FROM project_name . models ; For more information on managing model versions, check out our docs here . ​ When to RETRAIN the Model? It is advised to RETRAIN the predictor whenever the update_status column value from the mindsdb.models table is set to available . Here is when the update_status column value is set to available : When the new version of MindsDB is available that causes the predictor to become obsolete. When the new data is available in the table that was used to train the predictor. To find out whether you need to retrain your model, query the mindsdb.models table and look for the update_status column. Here are the possible values of the update_status column: Name Description available It indicates that the model should be updated. updating It indicates that the retraining process of the model takes place. up_to_date It indicates that your model is up to date and does not need to be retrained. Let’s run the query. SELECT name , update_status FROM mindsdb . models WHERE name = 'predictor_name' ; On execution, we get: + ------------------+---------------+ | name | update_status | + ------------------+---------------+ | predictor_name | up_to_date | + ------------------+---------------+ Where: Name Description predictor_name Name of the model to be retrained. update_status Column informing whether the model needs to be retrained. Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL predictor_name ; ​ Example Let’s look at an example using the home_rentals_model model. First, we check the status of the predictor. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | available | + --------------------+---------------+ Alternatively, use the DESCRIBE command as below: DESCRIBE MODEL home_rentals_model ; The available value of the update_status column informs us that we should retrain the model. RETRAIN mindsdb . home_rentals_model ; On execution, we get: Query OK , 0 rows affected ( 0.058 sec ) Now, let’s check the status again. SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | updating | + --------------------+---------------+ And after the retraining process is completed: SELECT name , update_status FROM mindsdb . models WHERE name = 'home_rentals_model' ; On execution, we get: + --------------------+---------------+ | name | update_status | + --------------------+---------------+ | home_rentals_model | up_to_date | + --------------------+---------------+ Was this page helpful? Yes No Suggest edits Raise issue Describe a Model Finetune a Model github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax When to RETRAIN the Model? Example"}
{"file_name": "select-view.html", "content": "Query a View - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Query a View Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Query a View ​ Description The SELECT statement fetches data from a view that resides inside a project. ​ Syntax Here is the syntax: SELECT * FROM project_name . view_name ; Was this page helpful? Yes No Suggest edits Raise issue Remove a View Upload a File github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "update.html", "content": "Update a Table - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Tables, Views, and Files Update a Table Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Create a Table Remove a Table Query a Table Native Queries Update a Table Insert Into a Table Join Tables On Delete From a Table Create a View Remove a View Query a View Upload a File Remove a File Query a File Jobs Triggers AI Agents Functions Standard SQL Support Tables, Views, and Files Update a Table ​ Description MindsDB provides two ways of using the UPDATE statement: The regular UPDATE statement updates specific column values in an existing table. The UPDATE FROM SELECT statement updates data in an existing table from a subselect query. It can be used as an alternative to CREATE TABLE or INSERT INTO to store predictions. ​ Syntax Here is an example of the regular UPDATE statement: UPDATE integration_name . table_name SET column_name = new_value WHERE column_name = old_value Please replace the placeholders as follows: integration_name is the name of the connected data source. table_name is the table name within that data source. column_name is the column name within that table. And here is an example of the UPDATE FROM SELECT statement that updates a table with predictions made within MindsDB: UPDATE integration_to_be_updated . table_to_be_updated SET column_to_be_updated = prediction_data . predicted_value_column , FROM ( SELECT p . predicted_value_column , p . column1 , p . column2 FROM integration_name . table_name as t JOIN model_name as p ) AS prediction_data WHERE column1 = prediction_data . column1 AND column2 = prediction_data . column2 Below is an alternative for the UPDATE FROM SELECT statement that updates a table with predictions. This syntax is easier to write. UPDATE integration_to_be_updated . table_to_be_updated ON column1 , column2 FROM ( SELECT p . predicted_value_column as column_to_be_updated , p . column1 , p . column2 FROM integration_name . table_name as t JOIN model_name as p ) The steps followed by the syntax: It executes query from the FROM clause to get the output data. In our example, we query for predictions, but it could be a simple select from another table. Please note that it is aliased as prediction_data . It updates all rows from the table_to_be_updated table (that belongs to the integration_to_be_updated integration) that match the WHERE clause criteria. The rows are updated with values as defined in the SET clause. It is recommended to use the primary key column(s) in the WHERE clause (here, column1 and column2 ), as the primary key column(s) uniquely identify each row. Otherwise, the UPDATE statement may lead to unexpected results by altering rows that you didn’t want to affect. Was this page helpful? Yes No Suggest edits Raise issue Native Queries Insert Into a Table github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "use.html", "content": "Use a Data Source - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Data Sources Use a Data Source Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect Data Sources List Data Handlers Connect a Data Source Remove a Data Source List Data Sources Use a Data Source AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Data Sources Use a Data Source ​ Description The USE integration_name statement provides an option to use the connected datasources and SELECT from the database tables. Even if you are connecting to MindsDB as MySQL database, you will still be able to SELECT from your database. ​ Syntax To connect to your database USE the created datasource: USE integration_name ; Then, simply SELECT from the tables: SELECT * FROM table_name ; Was this page helpful? Yes No Suggest edits Raise issue List Data Sources List ML Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Description Syntax"}
{"file_name": "mindsdb_editor.html", "content": "MindsDB SQL Editor - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB SQL Editor Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB SQL Editor MindsDB provides a SQL Editor, so you don’t need to download additional SQL clients to connect to MindsDB. ​ How to Use the MindsDB SQL Editor There are two ways you can use the Editor, as below. Self-Hosted Local Deployment After setting up the MindsDB using Docker , or pip on Linux / Windows / MacOS , or pip via source code , go to your terminal and execute the following: python -m mindsdb On execution, we get: .. . 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ .. . Immediately after, your browser automatically opens the MindsDB SQL Editor. In case if it doesn’t, visit the URL http://127.0.0.1:47334/ in your browser of preference. Here is a sneak peek of the MindsDB SQL Editor: ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MindsDB SQL Syntax Postgres CLI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Use the MindsDB SQL Editor What’s Next?"}
{"file_name": "sql-alchemy.html", "content": "MindsDB and SQL Alchemy - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and SQL Alchemy Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and SQL Alchemy SQL Alchemy is a Python SQL toolkit, that provides object-relational mapping features for the Python programming language. SQL Alchemy facilitates working with databases and Python. You can download it here or run a pip install sqlalchemy . You can consider an option to interact with MindsDB directly from MySQL CLI or Postgres CLI . ​ How to Connect Please follow the instructions below to connect your MindsDB to SQL Alchemy. Local MindsDB You can use the Python code below to connect your MindsDB database to SQL Alchemy. Make sure you have the pymysql module installed before executing the Python code. To install it, run the pip install pymysql command. from sqlalchemy import create_engine user = 'mindsdb' password = '' host = '127.0.0.1' port = 47335 database = '' def get_connection ( ) : return create_engine ( url = \"mysql+pymysql://{0}:{1}@{2}:{3}/{4}\" . format ( user , password , host , port , database ) ) if __name__ == '__main__' : try : engine = get_connection ( ) engine . connect ( ) print ( f\"Connection to the { host } for user { user } created successfully.\" ) except Exception as ex : print ( \"Connection could not be made due to the following error: \\n\" , ex ) Please note that we use the following connection details: Username is mindsdb Password is left empty Host is 127.0.0.1 Port is 47335 Database name is left empty To create a database connection, execute the code above. On success, the following output is expected: Connection to the 127.0 .0.1 for user mindsdb created successfully. The Sqlachemy create_engine is lazy. This implies any human error when entering the connection details would be undetectable until an action becomes necessary, such as when calling the execute method to execute SQL commands. ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MariaDB SkySQL Deepnote github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect What’s Next?"}
{"file_name": "deepnote.html", "content": "MindsDB and Deepnote - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Deepnote Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Deepnote We have worked with the team at Deepnote, and built native integration to Deepnote notebooks. Please check: Deepnote Demo Guide Deepnote Integration Docs ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue SQL Alchemy Metabase github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page What’s Next?"}
{"file_name": "jupysql.html", "content": "MindsDB and Jupyter Notebooks - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Jupyter Notebooks Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Jupyter Notebooks Jupysql - full SQL client on Jupyter. It allows you to run SQL and plot large datasets in Jupyter via a %sql and %%sql magics. It also allows users to plot the data directly from the DB ( via %sqlplot magics). Jupysql facilitates working with databases and Jupyter. You can download it here or run a pip install jupysql . You can consider an option to interact with MindsDB directly from MySQL CLI or Postgres CLI . ​ How to Connect ​ Pre-requisite: Make sure you have jupysql installed: To install it, run pip install jupysql Make sure you have pymysql installed: To install it, run pip install pymysql You can easily verify the installation of jupysql by running this code: % load_ext sql This command loads the package and allows you to run cell magics on top of Jupyter. And for pymysql, validate by running this command: import pymysql Please follow the instructions below to connect to your MindsDB via Jupysql and Jupyter. Local MindsDB You can use the Python code below to connect your Jupyter notebook (or lab) to Local MindsDB database (via Jupysql). Load the extension: % load_ext sql Connect to your DB: % sql mysql + pymysql : // mindsdb : @ 127.0 .0 .1 : 47335 / mindsdb Testing connection by listing the existing tables (pure SQL): % sql show tables Please note that we use the following connection details: Username is mindsdb Password is left empty Host is 127.0.0.1 Port is 47335 Database name is mindsdb Docker - connecting to docker might have a different port. Create a database connection and execute the code above. On success, only the last command which lists the tables will output. The expected output is: * mysql+pymysql://mindsdb:***@127.0.0.1:47335/mindsdb 2 rows affected. Tables_in_mindsdb models ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue Tableau Grafana github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect Pre-requisite: What’s Next?"}
{"file_name": "tableau.html", "content": "MindsDB and Tableau - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Tableau Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Tableau Tableau lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Tableau and see the forecasts. ​ How to Connect Follow the steps below to connect your MindsDB to Tableau. First, create a new workbook in Tableau and open the Connectors tab in the Connect to Data window. Next, choose MySQL and provide the details of your MindsDB connection, such as the IP, port, and database name. Optionally, you can provide a username and password. Then, click Sign In . Here are the connection parameters specific to your MindsDB installation type: Local MindsDB Host: `localhost` Port: `47335` Database name: `mindsdb` Username: `mindsdb` Password: *leave it empty* Now you’re connected! ​ Overview of MindsDB in Tableau The content of your MindsDB is visible in the right-side pane. All the predictors are listed under the Table section. You can also switch between the integrations, such as mindsdb or files , in the Database section using the drop-down. Now, let’s run some examples! ​ Examples ​ Example 1 Previewing one of the tables from the mysql integration: ​ Example 2 There is one technical limitation. Namely, we cannot join tables from different databases/integrations in Tableau. To overcome this challenge, you can use either views or custom SQL queries. Previewing a view that joins a data table with a predictor table: Using a custom SQL query by clicking the New Custom SQL button in the right-side pane: ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. From Our Community Check out the articles and video guides created by our community: Article on Predicting & Visualizing Hourly Electricity Demand in the US with MindsDB and Tableau by Teslim Odumuyiwa Article on Predicting & Visualizing Petroleum Production with MindsDB and Tableau by Teslim Odumuyiwa Article on Predicting & Visualizing Gas Prices with MindsDB and Tableau by Teslim Odumuyiwa Article on How To Visualize MindsDB Predictions with Tableau by Ephraimx Video guide on Connecting MindsDB to Tableau by Alissa Troiano Video guide on Visualizing prediction result in Tableau by Teslim Odumuyiwa Have fun! Was this page helpful? Yes No Suggest edits Raise issue Metabase Jupyter Notebooks github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect Overview of MindsDB in Tableau Examples Example 1 Example 2 What’s Next?"}
{"file_name": "connect-mariadb-skysql.html", "content": "MariaDB SkySQL Setup Guide with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MariaDB SkySQL Setup Guide with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MariaDB SkySQL Setup Guide with MindsDB Find more information on MariaDB Sky SQL here ​ 1. Select your service for MindsDB If you haven’t already, identify the service to be enabled with MindsDB and make sure it is running. Otherwise, skip to step 2. ​ 2. Add MindsDB to your service Allowlist Access to MariaDB SkySQL services is restricted on a per-service basis . Add the following IP addresses to allow MindsDB to connect to your MariaDB service, do this by clicking on the cog icon and navigating to Security Access. In the dialog, input as prompted – one by one – the following IPs: 18.220.205.95 3.19.152.46 52.14.91.162 ​ 3. Download your service .pem file A certificate authority chain (.pem file) must be provided for proper TLS certificate validation. From your selected service, click on the world globe icon (Connect to service). In the Login Credentials section, click Download. The aws_skysql_chain.pem file will download onto your machine. ​ 4. Publically Expose your service .pem File Select secure storage for the aws_skysql_chain.pem file that allows a working public URL or localpath. ​ 5. Link MindsDB to your MariaDB SkySQL Service To print the query template, select Add Data in either the top or side navigation and choose MariaDB SkySQL from the list. Fill in the values and run a query to complete the setup. Here are the codes: Template Example for MariaDB SkySQL Service CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'MariaDB' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : True / False , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue DBeaver SQL Alchemy github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page 1. Select your service for MindsDB 2. Add MindsDB to your service Allowlist 3. Download your service .pem file 4. Publically Expose your service .pem File 5. Link MindsDB to your MariaDB SkySQL Service What’s Next?"}
{"file_name": "grafana.html", "content": "MindsDB and Grafana - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Grafana Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Grafana Grafana is an open-source analytics and interactive visualization web application that allows users to ingest data from various sources, query this data, and display it on customizable charts for easy analysis. ​ How to Connect To begin, set up Grafana by following one of the methods outlined in the Grafana Installation Documentation . Once Grafana is successfully set up in your environment, navigate to the Connections section, click on Add new connection, and select the MySQL plugin , as shown below. Now it’s time to fill in the connection details. There are three options, as below. Local MindsDB You can connect to your local MindsDB. To do that, please use the connection details below: Host: `127.0.0.1:47335` Username: `mindsdb` Password: <leave it empty> Database: <leave it empty> Now we are ready to Save & test the connection. ​ Testing the Connection Click on the Save & test button to check if all the provided data allows you to connect to MindsDB. On success, you should see the message, as below. ​ Examples ​ Querying To verify the functionality of our MindsDB database connection, you can query data in the Explore view. Use the text edit mode to compose your queries. SHOW FULL DATABASES ; On execution, we get: ​ Visual Query Builder Now you can build a dashboard with a MindsDB database connection. Example query : CREATE DATABASE mysql_demo_db WITH ENGINE = \"mysql\" , PARAMETERS = { \"user\" : \"user\" , \"password\" : \"MindsDBUser123!\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"3306\" , \"database\" : \"public\" } ; SELECT * FROM mysql_demo_db . air_passengers ; On execution, we get: How to whitelist MindsDB Cloud IP address ? ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue Jupyter Notebooks List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect Testing the Connection Examples Querying Visual Query Builder What’s Next?"}
{"file_name": "mysql-client.html", "content": "MindsDB and MySQL CLI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MySQL CLI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and MySQL CLI MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command Line Client. Please note that connecting to MindsDB’s MySQL API is the same as connecting to a MySQL database. Find more information on MySQL CLI here . By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ How to Connect To connect MindsDB in MySQL, use the mysql client program: mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here is the command that allows you to connect to MindsDB. mysql -h 127.0 .0.1 --port 47335 -u mindsdb On execution, we get: Welcome to the MariaDB monitor. Commands end with \";\" or \"\\g\" . Server version: 5.7 .1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [ ( none ) ] > ​ What’s Next? Now that you are all set, we recommend you check out our Use Cases section, where you’ll find various examples of regression, classification, time series, and NLP predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the MindsDB SQL section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue Postgres CLI DBeaver github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect What’s Next?"}
{"file_name": "dbeaver.html", "content": "MindsDB and DBeaver - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and DBeaver Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and DBeaver DBeaver is a database tool that allows you to connect to and work with various database engines. You can download it here . ​ Data Setup First, create a new database connection in DBeaver by clicking the icon, as shown below. Next, choose the MySQL database engine and click the Next button. If you have multiple MySQL options, choose the Driver for MySQL8 and later . Now it’s time to fill in the connection details. There are three options, as below. Local MindsDB You can connect to your local MindsDB. To do that, please use the connection details below: Hostname: `127.0.0.1` Port: `47334` Username: `mindsdb` Password: <leave it empty> Database: <leave it empty> Now we are ready to test the connection. ​ Testing the Connection Click on the Test Connection... button to check if all the provided data allows you to connect to MindsDB. On success, you should see the message, as below. ​ Let’s Run Some Queries To finally make sure that our MindsDB database connection works, let’s run some queries. SHOW FULL DATABASES ; On execution, we get: + ----------------------+---------+--------+ | Database | TYPE | ENGINE | + ----------------------+---------+--------+ | information_schema | system | [ NULL ] | | mindsdb | project | [ NULL ] | | files | data | files | + ----------------------+---------+--------+ Here is how it looks in DBeaver: How to whitelist MindsDB Cloud IP address ? ​ What’s Next? Now that you are all set, we recommend you to check out our Tutorials section where you’ll find various examples of regression, classification, and time series predictions with MindsDB or Community Tutorials list. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MySQL CLI MariaDB SkySQL github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Data Setup Testing the Connection Let’s Run Some Queries What’s Next?"}
{"file_name": "metabase.html", "content": "MindsDB and Metabase - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Metabase Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Metabase Metabase is open-source software that facilitates data analysis. It lets you visualize your data easily and intuitively. Now that MindsDB supports the MySQL binary protocol, you can connect it to Metabase and see the forecasts by creating and training the models. For more information, visit Metabase . ​ Setup ​ MindsDB Install MindsDB locally via Docker or Docker Desktop . ​ Metabase Now, let’s set up the Metabase by following one of the approaches presented on the Metabase Open Source Edition page . Here, we use the .jar approach for Metabase. ​ How to Connect Follow the steps below to connect your MindsDB to Metabase. Open your Metabase and navigate to the Admin settings by clicking the cog in the bottom left corner. Once there, click on Databases in the top navigation bar. Click on Add database in the top right corner. Fill in the form using the following data: Local MindsDB Database type: `MySQL` Display name: `MindsDB` Host: `localhost` Port: `47335` Database name: `mindsdb` Username: `mindsdb` Password: *leave it empty* Click on Save . Now you’re connected! ​ Example Now that the connection between MindsDB and Metabase is established, let’s do some examples. Most of the SQL statements that you usually run in your MindsDB SQL Editor can be run in Metabase as well. Let’s start with something easy. On your Metabase’s home page, click on New > SQL query in the top right corner and then, select your MindsDB database. Let’s execute the following command in the editor. SHOW TABLES ; On execution, we get: Please note that creating a database connection using the CREATE DATABASE statement fails because of the curly braces ( {} ) being used by JDBC as the escape sequences. CREATE DATABASE example_db WITH ENGINE = \"postgres\" , PARAMETERS = { \"user\" : \"demo_user\" , \"password\" : \"demo_password\" , \"host\" : \"samples.mindsdb.com\" , \"port\" : \"5432\" , \"database\" : \"demo\" } ; On execution, we get: You can overcome this issue using the MindsDB SQL Editor to create a database. Now, getting back to the Metabase, let’s run some queries on the database created with the help of the MindsDB SQL Editor . SELECT * FROM example_db . demo_data . home_rentals LIMIT 10 ; On execution, we get: ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue Deepnote Tableau github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Setup MindsDB Metabase How to Connect Example What’s Next?"}
{"file_name": "postgres-client.html", "content": "MindsDB and Postgres CLI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Postgres CLI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Postgres CLI The Postgres API enables users to connect to MindsDB using the PostgreSQL protocol. By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ How to Connect To connect MindsDB in PostgreSQL, use the psql client program: psql -h localhost -p 55432 -d mindsdb -U mindsdb We use the local MindsDB installation with the following parameters: host: localhost port: 55432 database: mindsdb user: mindsdb Here is the command that allows you to connect to MindsDB. psql -h 127.0 .0.1 -p 47335 -d mindsdb -U mindsdb Please see this page for implementation details. ​ What’s Next? Now that you are all set, we recommend you check out our Use Cases section, where you’ll find various examples of regression, classification, time series, and NLP predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the MindsDB SQL section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MindsDB SQL Editor MySQL CLI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect What’s Next?"}
{"file_name": "mindsdb_editor.html", "content": "MindsDB SQL Editor - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB SQL Editor Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB SQL Editor MindsDB provides a SQL Editor, so you don’t need to download additional SQL clients to connect to MindsDB. ​ How to Use the MindsDB SQL Editor There are two ways you can use the Editor, as below. Self-Hosted Local Deployment After setting up the MindsDB using Docker , or pip on Linux / Windows / MacOS , or pip via source code , go to your terminal and execute the following: python -m mindsdb On execution, we get: .. . 2022 -05-06 14 :07:04,599 - INFO - - GUI available at http://127.0.0.1:47334/ .. . Immediately after, your browser automatically opens the MindsDB SQL Editor. In case if it doesn’t, visit the URL http://127.0.0.1:47334/ in your browser of preference. Here is a sneak peek of the MindsDB SQL Editor: ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MindsDB SQL Syntax Postgres CLI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Use the MindsDB SQL Editor What’s Next?"}
{"file_name": "connect-mariadb-skysql.html", "content": "MariaDB SkySQL Setup Guide with MindsDB - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MariaDB SkySQL Setup Guide with MindsDB Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MariaDB SkySQL Setup Guide with MindsDB Find more information on MariaDB Sky SQL here ​ 1. Select your service for MindsDB If you haven’t already, identify the service to be enabled with MindsDB and make sure it is running. Otherwise, skip to step 2. ​ 2. Add MindsDB to your service Allowlist Access to MariaDB SkySQL services is restricted on a per-service basis . Add the following IP addresses to allow MindsDB to connect to your MariaDB service, do this by clicking on the cog icon and navigating to Security Access. In the dialog, input as prompted – one by one – the following IPs: 18.220.205.95 3.19.152.46 52.14.91.162 ​ 3. Download your service .pem file A certificate authority chain (.pem file) must be provided for proper TLS certificate validation. From your selected service, click on the world globe icon (Connect to service). In the Login Credentials section, click Download. The aws_skysql_chain.pem file will download onto your machine. ​ 4. Publically Expose your service .pem File Select secure storage for the aws_skysql_chain.pem file that allows a working public URL or localpath. ​ 5. Link MindsDB to your MariaDB SkySQL Service To print the query template, select Add Data in either the top or side navigation and choose MariaDB SkySQL from the list. Fill in the values and run a query to complete the setup. Here are the codes: Template Example for MariaDB SkySQL Service CREATE DATABASE maria_datasource --- display name for the database WITH ENGINE = 'MariaDB' , --- name of the MindsDB handler PARAMETERS = { \"host\" : \" \" , --- host IP address or URL \"port\" : , --- port used to make TCP/IP connection \"database\" : \" \" , --- database name \"user\" : \" \" , --- database user \"password\" : \" \" , --- database password \"ssl\" : True / False , --- optional, the `ssl` parameter value indicates whether SSL is enabled (`True`) or disabled (`False`) \"ssl_ca\" : { --- optional, SSL Certificate Authority \"path\" : \" \" --- either \"path\" or \"url\" } , \"ssl_cert\" : { --- optional, SSL certificates \"url\" : \" \" --- either \"path\" or \"url\" } , \"ssl_key\" : { --- optional, SSL keys \"path\" : \" \" --- either \"path\" or \"url\" } } ; ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials and Community Tutorials sections, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the SQL API section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue DBeaver SQL Alchemy github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page 1. Select your service for MindsDB 2. Add MindsDB to your service Allowlist 3. Download your service .pem file 4. Publically Expose your service .pem File 5. Link MindsDB to your MariaDB SkySQL Service What’s Next?"}
{"file_name": "mongo-compass.html", "content": "MindsDB and MongoDB Compass - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MongoDB Compass Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect MongoDB Compass MongoDB Shell Data Sources AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Connect MindsDB and MongoDB Compass MongoDB Compass is a graphical user interface (GUI) for MongoDB. It provides detailed schema visualizations, real-time performance metrics, sophisticated querying abilities, and more. You can download MongoDB Compass here . ​ Overview Here is an overview of the connection between MindsDB and MongoDB Compass: Let’s go through the steps presented above: We connect MongoDB Compass to MindsDB. It is discussed in the following content. We connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details. Having completed steps 1 and 2, you can access the connected database from MongoDB Compass via MindsDB. ​ How to Connect Here is how to connect MongoDB Compass to MindsDB using either MindsDB Cloud or local installation. Please add the MindsDB Cloud Public IPs to the access list of your Mongo database. Local MindsDB First, create a new connection in MongoDB Compass by clicking the New Connection button in the left navigation panel. The host value is 127.0.0.1 and the port value is 47336 . Input it in the Host field, as below. ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials section, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don’t miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples. Have fun! From Our Community Check out the video guides created by our community: Video guide on How to connect Mongo Compass to MindsDB by HellFire Video guide on Integrating your MindsDB instance into MongoDB by Syed Zubeen Was this page helpful? Yes No Suggest edits Raise issue Overview MongoDB Shell github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Overview How to Connect What’s Next?"}
{"file_name": "mysql-client.html", "content": "MindsDB and MySQL CLI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MySQL CLI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and MySQL CLI MindsDB provides a powerful MySQL API that allows users to connect to it using the MySQL Command Line Client. Please note that connecting to MindsDB’s MySQL API is the same as connecting to a MySQL database. Find more information on MySQL CLI here . By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ How to Connect To connect MindsDB in MySQL, use the mysql client program: mysql -h [ hostname ] --port [ TCP/IP port number ] -u [ user ] -p [ password ] Here is the command that allows you to connect to MindsDB. mysql -h 127.0 .0.1 --port 47335 -u mindsdb On execution, we get: Welcome to the MariaDB monitor. Commands end with \";\" or \"\\g\" . Server version: 5.7 .1-MindsDB-1.0 ( MindsDB ) Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. MySQL [ ( none ) ] > ​ What’s Next? Now that you are all set, we recommend you check out our Use Cases section, where you’ll find various examples of regression, classification, time series, and NLP predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the MindsDB SQL section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue Postgres CLI DBeaver github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect What’s Next?"}
{"file_name": "postgres-client.html", "content": "MindsDB and Postgres CLI - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and Postgres CLI Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SQL API Overview MindsDB SQL Syntax Connect MindsDB SQL Editor Postgres CLI MySQL CLI DBeaver MariaDB SkySQL SQL Alchemy Deepnote Metabase Tableau Jupyter Notebooks Grafana Data Sources AI/ML Engines Projects Models Predictions Tables, Views, and Files Jobs Triggers AI Agents Functions Standard SQL Support Connect MindsDB and Postgres CLI The Postgres API enables users to connect to MindsDB using the PostgreSQL protocol. By default, MindsDB starts the http and mysql APIs. You can define which APIs to start using the api flag as below. python -m mindsdb --api http,mysql,postgres,mongodb If you want to start MindsDB without the graphical user interface (GUI), use the --no_studio flag as below. python -m mindsdb --no_studio ​ How to Connect To connect MindsDB in PostgreSQL, use the psql client program: psql -h localhost -p 55432 -d mindsdb -U mindsdb We use the local MindsDB installation with the following parameters: host: localhost port: 55432 database: mindsdb user: mindsdb Here is the command that allows you to connect to MindsDB. psql -h 127.0 .0.1 -p 47335 -d mindsdb -U mindsdb Please see this page for implementation details. ​ What’s Next? Now that you are all set, we recommend you check out our Use Cases section, where you’ll find various examples of regression, classification, time series, and NLP predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB database structure . Also, don’t miss out on the remaining pages from the MindsDB SQL section, as they explain a common SQL syntax with examples. Have fun! Was this page helpful? Yes No Suggest edits Raise issue MindsDB SQL Editor MySQL CLI github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page How to Connect What’s Next?"}
{"file_name": "mongo-shell.html", "content": "MindsDB and MongoDB Shell - MindsDB MindsDB home page Search or ask... Blog Events News Try Demo Try Demo Search... Navigation Connect MindsDB and MongoDB Shell Documentation Use Cases SQL API SDKs REST API Integrations Contribute FAQs Join our Slack GitHub Website SDKs Overview MongoDB API Overview Connect MongoDB Compass MongoDB Shell Data Sources AI/ML Engines Projects Models Predictions Jobs Python SDK JavaScript SDK Connect MindsDB and MongoDB Shell MongoDB Shell is the quickest way to connect and work with MongoDB. MindsDB provides a powerful MongoDB API, allowing users to connect MindsDB to the MongoDB Shell . Please note that connection to MongoDB API provided by MindsDB is the same as connection to a MongoDB database. You can download MongoDB Shell here . ​ Overview Here is an overview of the connection between MindsDB and MongoDB Shell: Let’s go through the steps presented above: We connect MongoDB Shell to MindsDB. It is discussed in the following content. We connect MindsDB to a database. You can use the CREATE DATABASE statement and run it from MindsDB, passing all required database connection details. Having completed steps 1 and 2, you can access the connected database from MongoDB Shell via MindsDB. ​ How to Connect Here is how to connect MongoDB Shell to MindsDB using either MindsDB Cloud or local installation. Please add the MindsDB Cloud Public IPs to the access list of your Mongo database. Upon opening the MongoDB Shell, you see the following message: Let’s look at the connection strings for both MindsDB Cloud and local installation. Local MindsDB Provide your local MindsDB connection string to connect to a local MindsDB installation. You can copy the connection string from the MongoDB Compass if you have already created a connection there. Here is a connection string to connect to a local MindsDB installation: mongodb://127.0.0.1:47336/ ​ What’s Next? Now that you are all set, we recommend you check out our Tutorials section, where you’ll find various examples of regression, classification, and time series predictions with MindsDB. To learn more about MindsDB itself, follow the guide on MindsDB collection structure . Also, don’t miss out on the remaining pages from the Mongo API section, as these explain common MQL syntax with examples. Have fun! From Our Community Check out the video guide created by our community: Video guide on Easily connect to MindsDB Cloud from MongoShell by @akhilcoder Was this page helpful? Yes No Suggest edits Raise issue MongoDB Compass List Data Handlers github facebook twitter slack linkedin youtube medium Powered by Mintlify On this page Overview How to Connect What’s Next?"}
