[
  {
    "filename": "mindsdb-handlers.mdx",
    "path": "docs/mindsdb-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title handlers sidebartitle handlers handlers heart mindsdb philosophy lies belief predictive insights best leveraged produced close possible data layer usually data layer sqlcompatible database could also nonsql database data stream tool interacts data stored somewhere else description fits enormous set tools used across software industry complexity increases bringing machine learning equation set popular ml tools similarly huge aim support technology stacks requiring simple integration procedure anyone easily contribute necessary glue enable predictive system usage within data layers motivates concept handlers abstraction two types entities mentioned data layers ml frameworks handlers meant enforce common sufficient set behaviors mindsdbcompatible entities support creating handler target system effectively integrated wider mindsdb ecosystem types handlers data handlers data handlers act bridge database use data handlers connect data sources including databases applications using create database commandsqlcreatedatabases reach data database handler implemented within mindsdb tip go ahead implement handler database choice hrefhttpsgithubcommindsdbmindsdbblobmaindocscontributedatahandlersmdxherea youll find instructions implement data handler tip machine learning ml handlers ml handlers act bridge ml framework use ml handlers create ml engines using create ml_engine commandsqlcreatemlengine expose ml models supported ml engine ai table tip go ahead implement handler ml library framework choice hrefhttpsgithubcommindsdbmindsdbblobmaindocscontributemlhandlersmdxherea youll find instructions implement ml handler tip handlers mindsdb repository"
  },
  {
    "filename": "mindsdb-handlers.mdx",
    "path": "docs/mindsdb-handlers.mdx",
    "chunk_id": 1,
    "chunk_content": "source code integrations located main mindsdb repository integrationshttpsgithubcommindsdbmindsdbtreemainmindsdbintegrations directory integrations contains handlers source codes handlers handler handler directory mysql_handler mysql integration code lightwood_handler lightwood integration code handlers handlers_client handler clients directory db_client ml_client libs handler libraries directory basepy handler class inherits one base classes utilities handler utility directory installpy script installs handler dependencies structure handler technical terms handler selfcontained python package everything required mindsdb interact includes aspects like dependencies unit tests continuous integration logic author determine nature package example closed open source version control although encourage opening pull requests expand default set supported tools entry point data handler class definition inherit directly mindsdbintegrationslibsbasedatabasehandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl102 class thus indirectly mindsdbintegrationslibsbasebasehandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl8 class defines methods must overwritten order achieve functional implementation entry point ml handler class definition inherit mindsdbintegrationslibsbasebasemlenginehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl122 class defines methods must overwritten order achieve functional implementation note details handlers structure enforced author decide design note things remember points keep mind implementing handler handlers inherit base classes inherit databasehandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl102 class adding new data handler info visit doc page herecontributedatahandlers inherit basemlenginehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl122 class adding new ml handler info visit doc page herecontributemlhandlers parsing sql whenever want parse string contains sql strongly recommend using mindsdb_sql package provides parser fully supports mindsdb sql dialect partially standard sql"
  },
  {
    "filename": "mindsdb-handlers.mdx",
    "path": "docs/mindsdb-handlers.mdx",
    "chunk_id": 2,
    "chunk_content": "dialect also render feature map dialects already supported ones formatting output case data handlers comes building response public methods output wrapped mindsdbintegrationslibsresponsehandlerresponsehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsresponsepyl7 mindsdbintegrationslibsresponsehandlerstatusresponsehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsresponsepyl32 class classes used mindsdb executioner orchestrate coordinate multiple handler instances parallel case ml handlers output wrapping automatically done intermediate wrapper basemlengineexec class contributor wouldnt need worry info next steps links help explore cardgroup cols3 card titledata handler iconlink hrefcontributedatahandlershere build data handlercard card titleapp handler iconlink hrefcontributeapphandlershere build app handlercard card titleml handler iconlink hrefcontributemlhandlershere build ml handlercard cardgroup info"
  },
  {
    "filename": "mindsdb-gui.mdx",
    "path": "docs/mindsdb-gui.mdx",
    "chunk_id": 0,
    "chunk_content": "title navigating mindsdb gui sidebartitle mindsdb gui overview mindsdb offers userfriendly graphical interface allows users execute sql commands view outputs easily navigate connected data sources projects contents lets explore features usage mindsdb editor accessing mindsdb gui editor install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop p aligncenter img srcassetsmindsdb_gui_editormindsdb_editorpng p exploring mindsdb gui editor query editor primary component users input sql commands queries provides code editor environment users write edit execute sql statements located top center mindsdb gui p aligncenter img srcassetsmindsdb_gui_editorquery_editorpng p open multiple query editor tabs clicking plus button next current tab like p aligncenter img srcassetsmindsdb_gui_editormultiple_query_editorpng p results viewer query executed results viewer displays output query presents results tabular format showing rows columns data located bottom center mindsdb gui p aligncenter img srcassetsmindsdb_gui_editorresults_viewerpng p mindsdb supports additional features following 1 data insightssqldatainsights feature provides useful data visualization charts 2 export feature lets export query output csv markdown file object explorer object explorer provides overview projects models views connected data sources tables p aligncenter img srcassetsmindsdb_gui_editorobject_explorerpng p users navigate available objects expanding tree structure items upon hovering tables query content using provided select statement p aligncenter img srcassetsmindsdb_gui_editorobject_explorer_querypng p model progress bar mindsdb provides custom sql statement create deploy"
  },
  {
    "filename": "mindsdb-gui.mdx",
    "path": "docs/mindsdb-gui.mdx",
    "chunk_id": 1,
    "chunk_content": "models virtual tables upon executing create modelsqlcreatemodel statement monitor training progress bottomleft corner object explorer p aligncenter img srcassetsmindsdb_gui_editorcreate_model_1png p model ready status updates complete p aligncenter img srcassetsmindsdb_gui_editorcreate_model_2png p add new data sources connect data source mindsdb clicking add button choosing new datasource takes page lists available data sources including databases data warehouses applications search data source want connect follow instructions information visit data sources section docs upload files upload file mindsdb clicking add button choosing upload file takes form upload file give name information visit docs heresqlcreatefile upload custom models mindsdb offers way upload custom model form python code incorporate mindsdb ecosystem clicking add button choosing upload custom model information visit docs herecustommodelbyom learning hub best place start exploring mindsdb learning hub find various tutorials follow copying pasting code query editor executing p aligncenter img srcassetsmindsdb_gui_editorlearning_hubpng p learning hub lists useful links including documentation customer support contact tip videohttpswwwloomcomsharea8938acb6c6b47f98cb3225ede3d33b5 showcases features mindsdb gui editor tip"
  },
  {
    "filename": "quickstart-tutorial.mdx",
    "path": "docs/quickstart-tutorial.mdx",
    "chunk_id": 0,
    "chunk_content": "title tutorial get started mindsdb sidebartitle quickstart icon play start install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop get started mindsdb simple steps steps step titleconnect data source explore available data sources hereintegrationsdataoverview step step titleconfigure ai engine explore available ai engines hereintegrationsaioverview step step titlecreate deploy ai model mindsdb abstracts ai models ai tablesgenerativeaitables step uses configured ai engine step step titlequery predictions join data table ai table get predictions step step titleautomate customized workflows use jobsmindsdb_sqlsqlcreatejobs triggersmindsdb_sqlsqlcreatetrigger automate workflows step steps step 1 connect data source use create database statement connect data source mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public input data used following steps sql select mysql_demo_dbquestions limit 3 output article_title question true_answer alessandro_volta volta italian physicist yes alessandro_volta volta buried city pittsburgh alessandro_volta volta passion study electricity yes step 2 configure ai engine use create ml_engine command configure ai engine use openai engine sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey step 3 create deploy ai model use create model statement create train deploy ai model based ai engine created step 2 sql create model question_answering_model predict answer using engine openai_engine prompt_template answer question textquestion textarticle_title"
  },
  {
    "filename": "quickstart-tutorial.mdx",
    "path": "docs/quickstart-tutorial.mdx",
    "chunk_id": 1,
    "chunk_content": "model expects question article_title input generates answer output step 4 query predictions query predictions joining ai table step 3 data table step 1 sql select inputarticle_title inputquestion outputanswer mysql_demo_dbquestions input join question_answering_model output limit 3 output data article_title question answer alessandro_volta volta italian physicist yes volta italian physicist alessandro_volta volta buried city pittsburgh volta buried city pittsburgh alessandro_volta volta passion study electricity yes volta passion study electricity step 5 automate customized workflows mindsdb create custom automation workflows lets set workflow uses jobsmindsdb_sqlsqlcreatejobs recreates table predicted answers questions sql create job answer_questions create replace table data_sourcequestions_answers select inputarticle_title inputquestion outputanswer mysql_demo_dbquestions input join question_answering_model output every 1 day data_source connection made using create database statement data source user write access job creates questions_answers table inside connected data source table filled questions input data table answers generated ai table considering new questions added daily input data table job executes day alternatively could use last keyword fetch newly added questions input data table would enable insert new questionanswer pairs questions_answers table instead recreating input data table must provide either datetime integerfloat column would used condition like datetime last learn last keyword heremindsdb_sqlsqlcreatejobslast tip next steps follow links explore mindsdb cardgroup cols3 card titleconnect"
  },
  {
    "filename": "quickstart-tutorial.mdx",
    "path": "docs/quickstart-tutorial.mdx",
    "chunk_id": 2,
    "chunk_content": "data sources iconsquare1 hrefmindsdb_sqlsqlcreatedatabase card card titleconfigure ai engines iconsquare2 hrefmindsdb_sqlsqlcreatemlengine card card titledeploy models iconsquare3 hrefmindsdb_sqlsqlcreatemodel card card titleget predictions iconsquare4 hrefmindsdb_sqlsqlgetbatchpredictions card card titlefinetune models iconsquare5 hrefmindsdb_sqlapifinetune card card titleautomate jobs iconsquare6 hrefmindsdb_sqlsqlcreatejobs card card titleautomate triggers iconsquare7 hrefmindsdb_sqlsqlcreatetrigger card card titlecreate knowledge bases iconsquare8 hrefmindsdb_sqlagentsknowledgebases card card titlecreate chatbots iconsquare9 hrefmindsdb_sqlagentschatbot card cardgroup tip"
  },
  {
    "filename": "generative-ai-tables.mdx",
    "path": "docs/generative-ai-tables.mdx",
    "chunk_id": 0,
    "chunk_content": "title generative ai tables sidebartitle ai tables mindsdb empowers organizations harness power ai abstracting ai models generative ai tables tables capable learning input data generating predictions underlying model upon queried abstraction makes ai highly accessible enabling development teams use existing sql skills build applications powered ai tip mindsdb integrates numerous ai frameworks learn hereintegrationsaioverview tip p aligncenter img srchttpsdocsgooglecomdrawingsde2pacx1vqdxtucwl8ixteo2ntjn17b5xtctjdj_d_pdcex0ch0gbzsjfujmefgum_feygowlgrxnnszmlaygopubw951h460 p generative ai tables generative ai subfield artificial intelligence trains ai models create new content realistic text forecasts images learning patterns existing data mindsdb revolutionizes machine learning within enterprise databases introducing concept generative ai tables essentially abstract ai models virtual ai tables capable producing output given certain input use generative ai tables ai tables introduced mindsdb abstract ai models virtual tables simply query ai models predictions mindsdb join multiple ai tables abstract ai models multiple data tables provide input models get predictions lets look examples deploy ai models ai tables deploy ai model virtual ai table using create model statement create model classifies sentiment customer reviews instructed prompt template message required input review output sentiment predicted model sql create model sentiment_classifier_model predict sentiment using engine openai_engine model_name gpt4 prompt_template describe sentiment reviews strictly positive neutral negative love productpositive scamnegative review next"
  },
  {
    "filename": "generative-ai-tables.mdx",
    "path": "docs/generative-ai-tables.mdx",
    "chunk_id": 1,
    "chunk_content": "create model generates responses reviews required input includes review product name sold product quantity output response generated model sql create model response_generator_model predict response using engine openai_engine model_name gpt4 prompt_template briefly respond customer review review added customer buying product_name quantity quantity info follow doc pageintegrationsaienginesopenai configure openai engine mindsdb info lets look data tables well use provide input data ai tables prepare input data amazon_reviews table stores following columns sql created_at product_name review customer_id 20231003 163000000000 power adapter great product 1 20231003 163100000000 bluetooth wifi speaker ok 2 20231003 163200000000 kindle ereader doesnt work 3 provides sufficient input data sentiment_classifier_model response_generator_model products_sold table stores following columns sql sale_date product_name customer_id quantity 20231003 163000000000 power adapter 1 20 20231003 163100000000 bluetooth wifi speaker 2 5 20231003 163200000000 kindle ereader 3 10 response_generator_model requires two tables joined provide sufficient input data make predictions query ai tables directly join ai tables data tables get predictions two ways provide input models 1 query ai table directly provide input data clause like sql select review sentiment sentiment_classifier_model review like 2 provide input data ai tables joined data tables like sql select inpproduct_name inpreview m1sentiment m2response data_integration_connamazon_reviews2 inp join data_integration_connproducts_sold inp2 inpcustomer_id inp2customer_id join sentiment_classifier_model m1"
  },
  {
    "filename": "generative-ai-tables.mdx",
    "path": "docs/generative-ai-tables.mdx",
    "chunk_id": 2,
    "chunk_content": "join response_generator_model m2 sentiment_classifier_model requires parameter named review data table contain column named review picked model note joining data tables must provide clause condition implemented implicitly joining ai tables moreover combine two options provide input data ai tables partially data tables partially clause like sql select inpproduct_name inpreview m1sentiment m2response data_integration_connamazon_reviews2 inp join sentiment_classifier_model m1 join response_generator_model m2 m2quantity 5 sentiment_classifier_model takes input data amazon_review table response_generator_model takes input data amazon_reviews table clause furthermore make use subqueries provide input data models via clause like sql select inpproduct_name inpreview m1sentiment m2response data_integration_connamazon_reviews2 inp join sentiment_classifier_model m1 join response_generator_model m2 m2quantity select quantity data_integration_connproducts_sold customer_id 2 difference ai tables standard tables understand difference lets go simpler example see traditional database tables designed give deterministic response given input generative ai tables designed generate approximate response given input lets consider following income_table table stores income debt values sql select income debt income_table execution get sql incomedebt 60000 20000 80000 25100 10000030040 12000036010 simple visualization data present income_table table follows income vs debtassetssqlincome_vs_debtpng querying income table get debt value particular income value results following sql select income debt income_table income 80000 execution get sql incomedebt 80000 25100 get income vs debt chartassetssqlincome_vs_debt_known_valuepng happens querying table"
  },
  {
    "filename": "generative-ai-tables.mdx",
    "path": "docs/generative-ai-tables.mdx",
    "chunk_id": 3,
    "chunk_content": "income value present sql select income debt income_table income 90000 execution get sql empty set 000 sec clause condition fulfilled rows value returned income vs debt queryassetssqlincome_vs_debt_unknown_valuepng table doesnt exact match query returns empty set null value ai tables come play lets create debt_model model allows us approximate debt value income value train debt_model model using data income_table table sql create model mindsdbdebt_model income_table predict debt execution get sql query ok 0 rows affected xxxx sec mindsdb provides create modelsqlcreatemodel statement execution statement predictive model works background automatically creating vector representation data visualized follows income vs debt modelassetssqlincome_vs_debt_predictorpng lets look debt value random income value get approximated debt value query mindsdbdebt_model model instead income_table table sql select income debt mindsdbdebt_model income 90000 execution get sql incomedebt 90000 27820 looks income vs debt modelassetssqlincome_vs_debt_predictionpng"
  },
  {
    "filename": "README.md",
    "path": "docs/README.md",
    "chunk_id": 0,
    "chunk_content": "mindsdb documentation hrefhttpsdocsmindsdbcomutm_mediumcommunityutm_sourcegithubutm_campaignmindsdb20repoimg srchttpsimgshieldsiowebsiteurlhttps3a2f2fwwwmindsdbcom2f altmindsdb docsa running docs locally first install mintlify npm g mintlify start server mintlify dev documentation website available http1270013000 help us contributions welcomehttpsimgshieldsiobadgecontributionswelcomebrightgreensvgstyleflathttpsgithubcommindsdbmindsdbdocsissues contribute docshttpsdocsmindsdbcomcontribute writing documentationhttpsdocsmindsdbcomcontributedocs"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 0,
    "chunk_content": "title models sidebartitle models section go ai models types available mindsdb regression models classification models time series models large language models llms note disclaimer section describe default behavior using lightwood ml engine regression classification time series models ml handlers may behave differently example may perform validation automatically creating model numerous behaviors handlerspecific note ai models machine learning ml model program trained using available data order learn recognize patterns behaviors predict future data various types ai models use different learning paradigms mindsdb models supervised learn pairs input data expected output input data ai models processes input data searching patterns correlations ai models returns output data defined based input data features features variables ai models uses input data search patterns predict target variable tabular datasets features usually correspond single columns target target variable interest ai models predicts based information fetched features training dataset training dataset used training phase ai models contains feature variables target variable name indicates used train ai model ai model takes entire training dataset input learns patterns relationships feature variables target values training process complete one move validation phase validation dataset validation dataset used validation phase ai models contains feature variables target variable like training dataset name indicates used"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 1,
    "chunk_content": "validate predictions made ai models overlap training dataset heldout set simulate real scenario model generates predictions novel input data ai models takes feature variables validation dataset input based model learns training process makes predictions values target variable comes validation step assess accuracy ai models one compares target variable values validation dataset target variable values predicted ai models closer values better accuracy ai models input dataset completing training validation phases one provide input dataset consisting feature variables predict target variable values ai model created mindsdb use create modelsqlcreatemodel statement create train validate model training phase lets look training dataset contains features target sql select filessalary_dataset limit 5 execution get sql companyidjobtype degree major industryyearsexperiencemilesfrommetropolissalary comp37 cfo masters math health 10 83 130 comp19 ceo high_schoolnone web 3 73 101 comp52 vice_presidentdoctoral physics health 10 38 137 comp38 manager doctoral chemistryauto 8 17 142 comp7 vice_presidentbachelors physics finance 8 16 163 features companyid jobtype degree major industry yearsexperience milesfrommetropolis target variable salary lets create train ai models using training dataset sql create model salary_predictor files select salary_dataset predict salary execution get sql query successfully completed progress check whether training process completed sql describe salary_predictor status complete training phase completed validation phase default"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 2,
    "chunk_content": "create model statement performs validation model additionally validate model manually querying providing feature values clause like sql select salary salary_explain mindsdbsalary_predictor companyid comp37 jobtype manager degree doctoral major math industry finance yearsexperience 5 milesfrommetropolis 50 execution get sql salarysalary_explain 128 predicted_value 128 confidence 067 anomaly null truth null confidence_lower_bound 109 confidence_upper_bound 147 comparing real salary values defined individuals predicted salary values one figure accuracy ai models note please note mindsdb calculates models accuracy default running create model statement however guaranteed ml engines default create model statement following creates model divides input data training validation datasets trains model using training dataset validates model using validation dataset compares true predicted values target define models accuracy note lets look basic types ai models ai model types regression models regression type predictive modeling analyses input data including relationships dependent independent variables target variable predicted case regression models target variable belongs set continuous values example data real estates number rooms location rental price one predict rental price using regression rental price predicted based input data value value range minimum maximum rental price values training data example first lets look input data sql select example_dbdemo_datahome_rentals limit 5 execution get sql number_of_roomsnumber_of_bathroomssqftlocationdays_on_marketneighborhood rental_price 2 1 917 great"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 3,
    "chunk_content": "13 berkeley_hills3901 0 1 194 great 10 berkeley_hills2042 1 1 543 poor 18 westbrae 1871 2 1 503 good 10 downtown 3026 3 2 1066good 13 thowsand_oaks 4774 features number_of_rooms number_of_bathrooms sqft location days_on_market neighborhood target variable rental_price lets create train ai models sql create model mindsdbhome_rentals_model example_db select demo_datahome_rentals predict rental_price execution get sql query successfully completed training process completed query predictions sql select rental_price rental_price_explain mindsdbhome_rentals_model sqft 823 locationgood neighborhooddowntown days_on_market10 execution get sql rental_price rental_price_explain 4394 predicted_value 4394 confidence 099 anomaly null truth null confidence_lower_bound 4313 confidence_upper_bound 4475 details check tutorialsqltutorialshomerentals classification models classification type predictive modeling analyses input data including relationships dependent independent variables target variable predicted case classification models target variable belongs set discrete values example data customer telecom company one predict churn possibility using classification churn predicted based input data value either yes special case called binary classification example first lets look input data sql select example_dbdemo_datacustomer_churn limit 5 execution get sql customeridgenderseniorcitizenpartnerdependentstenurephoneservicemultiplelines internetserviceonlinesecurityonlinebackupdeviceprotectiontechsupportstreamingtvstreamingmoviescontract paperlessbillingpaymentmethod monthlychargestotalchargeschurn 7590vhvegfemale0 yes 1 phone servicedsl yes monthtomonthyes electronic check 2985 2985 5575gnvdemale 0 34 yes dsl yes yes one year mailed check 5695 188950 3668qpybkmale 0 2 yes dsl yes yes monthtomonthyes mailed check 5385 10815 yes 7795cfocwmale"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 4,
    "chunk_content": "0 45 phone servicedsl yes yes yes one year bank transfer automatic4230 184075 9237hqitufemale0 2 yes fiber optic monthtomonthyes electronic check 7070 15165 yes features customerid gender seniorcitizen partner dependents tenure phoneservice multiplelines internetservice onlinesecurity onlinebackup deviceprotection techsupport streamingtv streamingmovies contract paperlessbilling paymentmethod monthlycharges totalcharges target variable churn lets create train ai models sql create model churn_predictor example_db select demo_datacustomer_churn predict churn execution get sql query successfully completed training process completed query predictions sql select churn churn_confidence churn_explain mindsdbcustomer_churn_predictor seniorcitizen0 partneryes dependentsno tenure1 phoneserviceno multiplelinesno phone service internetservicedsl execution get sql churn churn_confidence churn_explain yes 07752808988764045 predicted_value yes confidence 07752808988764045 anomaly null truth null probability_class_no 04756 probability_class_yes 05244 details check tutorialsqltutorialscustomerchurn time series models time series models fall regression classification category whats distinct order data date time value defining sequential order events usually predictions made time series models referred forecasts time series model predicts target comes continuous set regression discrete set classification mandatory order clause followed sequential column date orders rows accordingly want group predictions optional group clause following clause column name multiple column names one make predictions partitions data defined columns case time series models one define many data rows used train model window clause followed integer optional horizon"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 5,
    "chunk_content": "clause define many rows far future want predict default one example first lets look input data sql select example_dbdemo_datahouse_sales limit 5 execution get sql saledate type bedrooms 20070930441854house2 20071231441854house2 20080331441854house2 20080630441854house2 20080930451583house2 features saledate type bedrooms target variable lets create train ai models sql create model mindsdbhouse_sales_predictor files select house_sales predict order saledate group bedrooms type target column predicted stores one row per quarter window 8 using data last two years make forecasts last 8 rows horizon 4 making forecasts next year next 4 rows execution get sql query successfully completed training process completed query predictions sql select msaledate date mma forecast ma_explain mindsdbhouse_sales_predictor join fileshouse_sales tsaledate latest ttype house tbedrooms 2 limit 4 execution get sql date forecast ma_explain 20191231 4414135849598734 predicted_value 4414135849598734 confidence 099 anomaly true truth null confidence_lower_bound 44004628237074096 confidence_upper_bound 44278088754900586 20200401 4432925194586229 predicted_value 4432925194586229 confidence 09991 anomaly null truth null confidence_lower_bound 4276093325864327 confidence_upper_bound 4589757063308131 20200702 4432925194585953 predicted_value 4432925194585953 confidence 09991 anomaly null truth null confidence_lower_bound 42450159192981094 confidence_upper_bound 4620834469873797 20201002 4432925194585953 predicted_value 4432925194585953 confidence 09991 anomaly null truth null confidence_lower_bound 42450159192981094 confidence_upper_bound 4620834469873797 details check tutorialsqltutorialshousesalesforecasting large language models large language models advanced artificial intelligence systems designed process generate humanlike language models leverage deep learning techniques transformer"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 6,
    "chunk_content": "architectures analyze vast amounts text data learn complex patterns relationships within language large language models applications chatbots content generation language translation sentiment analysis various natural language processing tasks example check examples slack chatbotsqltutorialsslackchatbot sentiment analysisnlpsentimentanalysisinsidemysqlwithopenai nlp examplesnlpnlpextendedexamples works background mindsdb uses lightwood ml engine default section takes closer look package automatically chooses type model use models lightwood follow encodermixerdecoder pattern refined encoded representations features mixed produce target predictions mixers used lightwoodhttpsmindsdbgithubiolightwoodmixerhtml please note ensembling step training mixers case multiple mixers used read learn give details mindsdb creates model using different mixers full codehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipy comes breakdown piece codehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl238l351 adds mixers submodels array depending model type data type target variable herehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl353l358 choose best submodels used create train validate ai models lets dive details mindsdb picks mixers tabs tab titlecase 1 piece codehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl239l277 analyzed deal simple encoderdecoder pair performing taskhttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl239l250 use unithttpsmindsdbgithubiolightwoodmixerhtmlmixerunit mixer thought bypass mixer good example spam classifier model hugging facecustommodelhuggingfacemodel1spamclassifier uses single column input otherwisehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl251l277 choose range mixers depending following conditions time series casehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl252l264 use neuralhttpsmindsdbgithubiolightwoodmixerhtmlmixerneural mixer good example customer churn modelsqltutorialscustomerchurntrainingapredictor time series casehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl265l277 use neuraltshttpsmindsdbgithubiolightwoodmixerhtmlmixerneuralts mixer good example house sales modelsqltutorialshousesalesforecastingtrainingapredictor tab tab titlecase 2 piece codehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl279l324 analyzed time series case time series case horizon value one target"
  },
  {
    "filename": "model-types.mdx",
    "path": "docs/model-types.mdx",
    "chunk_id": 7,
    "chunk_content": "variable type array valueshttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl279l310 use lightgbmhttpsmindsdbgithubiolightwoodmixerhtmlmixerlightgbm xgboostmixerhttpsmindsdbgithubiolightwoodmixerhtmlmixerxgboostmixer regressionhttpsmindsdbgithubiolightwoodmixerhtmlmixerregression randomforesthttpsmindsdbgithubiolightwoodmixerhtmlmixerrandomforest mixers good example home rentals modelsqltutorialshomerentalstrainingapredictor otherwise time series case horizon value greater onehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl311l324 use lightgbmarrayhttpsmindsdbgithubiolightwoodmixerhtmlmixerlightgbmarray mixer good example house sales modelsqltutorialshousesalesforecastingtrainingapredictor tab tab titlecase 3 piece codehttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl326l351 analyzed autoregressive case target type integer float quantityhttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl326l351 use sktimehttpsmindsdbgithubiolightwoodmixerhtmlmixersktime etsmixerhttpsmindsdbgithubiolightwoodmixerhtmlmixeretsmixer arimamixerhttpsmindsdbgithubiolightwoodmixerhtmlmixerarimamixer mixers autoregressive case means use target values encoded features many previous rows defined window clause good example home rentals modelsqltutorialshomerentalstrainingapredictor tab tabs brbr note mindsdb may use one multiple mixers preparing model depending model type data type target variable one mixer chosen set mixers ensembled create train validate ai models note three cases describe mindsdb chooses mixer candidates stores submodels array default training relevant mixers submodels array mindsdb uses bestofhttpsmindsdbgithubiolightwoodensemblehtmlensemblebestof ensemble single best mixer final modelhttpsgithubcommindsdblightwoodblob9a48d5523d78111c77024e1983d281f37a9782ealightwoodapijson_aipyl353l358 always use different ensemble may aggregate multiple mixers per model meanensemblehttpsmindsdbgithubiolightwoodensemblehtmlensemblemeanensemble modeensemblehttpsmindsdbgithubiolightwoodensemblehtmlensemblemodeensemble stackedensemblehttpsmindsdbgithubiolightwoodensemblehtmlensemblestackedensemble tsstackedensemblehttpsmindsdbgithubiolightwoodensemblehtmlensembletsstackedensemble weightedmeanensemblehttpsmindsdbgithubiolightwoodensemblehtmlensembleweightedmeanensemble ensemble type herehttpsgithubcommindsdblightwoodtreestaginglightwoodensemble youll find implementations ensemble types info next steps links help explore cardgroup cols3 card titlecreate model iconlink hrefsqlcreatemodelcustom sql syntaxcard card titledbpredictorsinsertone iconlink hrefmongomodelsinsertonecustom mongoql syntaxcard card titleprojectcreate_model iconlink hrefsdk_pythoncreate_modeldirectly pythoncard cardgroup cardgroup cols3 card titlemindsdbmodelstrainmodel iconlink hrefsdk_javascriptcreate_modeldirectly javascriptcard card titletrain model iconlink hrefrestmodelstrainmodelusing rest apiscard cardgroup info"
  },
  {
    "filename": "minds-demo.mdx",
    "path": "docs/minds-demo.mdx",
    "chunk_id": 0,
    "chunk_content": "title demo sidebartitle demo icon magnifyingglass url httpsmdbairegister"
  },
  {
    "filename": "callbacks.mdx",
    "path": "docs/callbacks.mdx",
    "chunk_id": 0,
    "chunk_content": "title callbacks sidebartitle callbacks callbacks enable retrieval models status transitions either complete error accomplished sending post request specified url payload structured follows json name my_model version 1 active true predictor_id 123 project_name mindsdb predictor_created_at fri 09 sep 2023 115027 gmt old_status training new_status complete changed_at fri 09 sep 2023 115900 gmt lets break key name name model version version trained model active indicates whether newly trained model active predictor_id unique identifier trained model project_name name project model created predictor_created_at time model created old_status status model prior receiving new_status new_status current status model changed_at time models status transitioned old_status new_status note please note feature works minsddb cloud mindsdb startersetupcloudstarter users note callbacks api callback api provides information add get edit delete callbacks add callback request format json post cloudcallbackmodel_status contenttype applicationjson url called model status change url httpsmyendpointcom filter model_name project_name attempt count 5 http_timeout 10 interval 10 response format json status 200 ok contenttype applicationjson id created callback id 123456 url key required request others optional url represents address utilized send post request containing detailed information models status filter used restrict triggering callbacks model_name pythonstyle regular expression used filter names models trigger callback project_name pythonstyle regular expression used filter names"
  },
  {
    "filename": "callbacks.mdx",
    "path": "docs/callbacks.mdx",
    "chunk_id": 1,
    "chunk_content": "projects attempt outlines configuration number frequency attempts send callbacks count maximum number attempts interval represents interval failed attempts http_timeout time allotted wait successful response get callback request format json get cloudcallbackmodel_status response format json status 200 ok contenttype applicationjson id 123456 created_at fri 09 sep 2023 115027 gmt url httpsmyendpointcom edit callback request format json put cloudcallbackmodel_statusid contenttype applicationjson new callback url url httpsmyendpointcom response format json status 200 ok delete callback request format json delete cloudcallbackmodel_statusid response format json status 200 ok handling callbacks using python sdk example using callbacks home_rentals model sure endpoint hostname accessible internet note please note localhost accessible internet make localhost accessible via multiple ways like ngrok tunnelhttpsngrokcom note python import requests import mindsdb_sdk flask import flask request model_name home_rentals hostname myendpointcom port 5000 app flask__name__ con mindsdb_sdkconnect httpscloudmindsdbcom loginnameemailcom passwordpassword add callback conapisessionpost httpscloudmindsdbcomcloudcallbackmodel_status json url fhttpshostnameport approute methodspost def callback data requestjson dataversion 1 let retrain model model conmodelsgetmodel_name modelretrain elif dataversion 2 let make prediciton model conmodelsgetmodel_name prediction modelpredictsqft 1000 printprediction return 200 connect database db condatabasescreate example_db enginepostgres connection_args user demo_user password demo_password host samplesmindsdbcom port 5432 database demo train base model model conmodelscreate model_name predictrental_price querydbtablesgetdemo_datahome_rentals __name__ __main__ apprunhost0000 portport"
  },
  {
    "filename": "callbacks.mdx",
    "path": "docs/callbacks.mdx",
    "chunk_id": 2,
    "chunk_content": "handling callbacks using javascript sdk example using callbacks home_rentals model sure callback hostname accessible mindsdb cloud js import express express import axios axios import mindsdb mindsdbjssdk const mdb mindsdbdefaultdefault const app express const port 54321 const model_name home_rentals const hostname myendpointcom create axios instance interceptors timeout error handling const customaxios axioscreate customaxiosinterceptorsrequestuseconfig configtimeout 120000 set request timeout 120 seconds return config customaxiosinterceptorsresponseuse response response error consoleerroraxios error errormessage return promiserejecterror connect mindsdb cloud try await mdbconnect host httpscloudmindsdbcom user nameemailcom password password httpclient customaxios catch error consoleerrormindsdb cloud connection error error processexit1 connect database await mdbdatabasescreatedatabaseexample_db postgres user demo_user password demo_password host samplesmindsdbcom port 5432 database demo express middleware parse json requests appuseexpressjson define express route handle model status updates apppostmodelstatus async req res const data reqbody datanew_status complete consoleerrorerror got model status datanew_status return resstatus400send error invalid model status const trained_model_version dataversion consolelogmodel training completed model versiontrained_model_version trained_model_version 1 base model finished training lets retrain let model await mdbmodelsgetmodelmodel_name mindsdb modelretrain else trained_model_version 2 model retraining finished lets make prediction let model await mdbmodelsgetmodelmodel_name mindsdb let prediction await modelquery sqft 823 location good neighborhood downtown days_on_market 10 consolelogprediction jsonstringifyprediction ressendstatus200 start express server applistenport consolelogexpress server started port port"
  },
  {
    "filename": "callbacks.mdx",
    "path": "docs/callbacks.mdx",
    "chunk_id": 3,
    "chunk_content": "add callback notify mindsdb cloud endpoint try await customaxiosposthttpscloudmindsdbcomcloudcallbackmodel_status url httpshostnameportmodelstatus catch error consoleerrorcallback error error train model try await mdbmodelstrainmodelmodel_name rental_price mindsdb integration example_db select select demo_datahome_rentals catch error consoleerrormodel training error error"
  },
  {
    "filename": "what-is-mindsdb.mdx",
    "path": "docs/what-is-mindsdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb agis query engine sidebartitle introduction icon house mindsdb worlds effective solution building ai applications talk messy enterprise data sources think librarian marie kondo p aligncenter img srcassetscloudmain_mdbpng p federated query engine tidies datasprawl chaos meticulously answering every single question throw structured unstructured data whether scattered across saas applications databases hibernating data warehouses like 100 bill tuxedo pocket prom night lost waiting discovered install mindsdb server mindsdb opensource server deployed anywhere laptop cloud everywhere yes customize hearts content using docker desktopsetupselfhosteddockerdesktop fastest recommended way get started running using dockersetupselfhosteddocker also simple gives flexibility customize server using pypicontributeinstall option enables contribute mindsdb connect dataintegrationsdataoverview connect hundreds data sources learn moreintegrationsdataoverview example postgres database sql connect demo postgres db create database demo_postgres_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data youve connected data sources combinemindsdb_sqlsqlapijoinon slice dice itmindsdb_sqlsqlapiselect transformusecasesdata_enrichmentoverview however heart desires using good ol standard sql learn moremindsdb_sqloverview youve whipped data shape time build ai actually learns build ai knowledge knowledge bases stateoftheart autonomous rag systems digest data source mindsdb supports whether data structured neater swiss watch factory unstructured messy teenagers bedroom knowledge base engine figure find relevant information example create knowledge base"
  },
  {
    "filename": "what-is-mindsdb.mdx",
    "path": "docs/what-is-mindsdb.mdx",
    "chunk_id": 1,
    "chunk_content": "knows everything amazon reviews sql first create knowledge base create knowledge_base mindsdbreviews_kb insert everything amazon reviews table learn insert mindsdbreviews_kb select review content demo_pg_dbamazon_reviews check status loads select information_schemaknowledge_bases query content knowledge base select mindsdbreviews_kb tinkerers optimization enthusiasts dive deep want learn knowledge basesmindsdb_sqlagentsknowledgebases want handpick embedding model go itmindsdb_sqlagentsknowledgebasesknowledgebasewithopenaiembeddingmodel strong opinions vector databases itmindsdb_sqlagentsknowledgebasesknowledgebasewithcustomvectorstore youd rather spend time things like finally building billiondollar ai app thats perfectly fine default handled automatically dont need worry nittygritty details like data embedding chunking vector optimization etc search knowledge base loaded ready lets hunt juicy info via sql sql find reviews iphone beast lights select mindsdbreviews_kb content like best kindle reviews limit 10 via python sdk install mindsdb sdk shell pip install mindsdb_sdk call ai knowledge base app following code python import mindsdb_sdk connects specified host port server mindsdb_sdkconnecthttp12700147334 wiki_kb serverknowledge_basesgetmindsdbreviews_kb df my_kbfindwhat best kindle reviewsfetch"
  },
  {
    "filename": "sql.mdx",
    "path": "docs/rest/sql.mdx",
    "chunk_id": 0,
    "chunk_content": "title query openapi post apisqlquery sidebartitle query description api provides rest endpoint executing sql queries note endpoint http post method endpoint accept data via applicationjson request body required key query sql statement value body paramfield bodyquery typestring required string contains sql query needs executed paramfield response responsefield namecolumn_names typearray required list column names returned responsefield responsefield namecontext typeobject required database query executed responsefield responsefield namedata typearray actual data returned query case table response type responsefield responsefield nametype typestring type response table error ok responsefield requestexample shell shell curl request post url httpscloudmindsdbcomapisqlquery header contenttype applicationjson cookie session273trgsehgrui3i2riurwehe data query select example_dbdemo_datahome_rentals limit 10 python python import requests url httpscloudmindsdbcomapisqlquery cookies session 273trgsehgrui3i2riurwehe resp requestsposturl jsonquery select example_dbdemo_datahome_rentals limit 10 cookiescookies requestexample responseexample json response column_names sqft rental_price context db mindsdb data 917 3901 194 2042 type table responseexample"
  },
  {
    "filename": "usage.mdx",
    "path": "docs/rest/usage.mdx",
    "chunk_id": 0,
    "chunk_content": "title usage sidebartitle usage connect use rest api mindsdb local mindsdb example python import requests connect url http12700147334apisqlquery query resp requestsposturl jsonquery select example_dbdemo_datahome_rentals limit 10 response printresptext alternative printrespjson"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/rest/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title rest api sidebartitle overview icon webhook mindsdb provides rest api endpoints enabling incorporation ai building blocks applications section introduces rest api endpoints provided mindsdb bring data ai together follow steps get started steps step titleset development environment learn usage hererestusage step step titleconnect data source connect data source mindsdb via endpointrestdatabasescreatedatabasesbrbr explore available data sources hereintegrationsdataoverview step step titlecreate deploy aiml model create train deploy aiml models within mindsdb via endpointrestmodelstrainmodelbrbr explore available ai engines hereintegrationsaioverview step step titleget predictions query predictions via endpointrestmodelsquerymodel step steps"
  },
  {
    "filename": "get-project.mdx",
    "path": "docs/rest/projects/get-project.mdx",
    "chunk_id": 0,
    "chunk_content": "title get project openapi get apiprojectsprojectname sidebartitle get project note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "drop.mdx",
    "path": "docs/rest/projects/drop.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove project sidebartitle remove project info feature progress info"
  },
  {
    "filename": "get-projects.mdx",
    "path": "docs/rest/projects/get-projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title list projects openapi get apiprojects sidebartitle list projects note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "create.mdx",
    "path": "docs/rest/projects/create.mdx",
    "chunk_id": 0,
    "chunk_content": "title create project sidebartitle create project info feature progress info"
  },
  {
    "filename": "list.mdx",
    "path": "docs/rest/jobs/list.mdx",
    "chunk_id": 0,
    "chunk_content": "title list jobs openapi get apiprojectsprojectnamejobs sidebartitle list jobs note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/rest/jobs/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove job openapi delete apiprojectsprojectnamejobsjobname sidebartitle remove job note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "create.mdx",
    "path": "docs/rest/jobs/create.mdx",
    "chunk_id": 0,
    "chunk_content": "title create job openapi post apiprojectsprojectnamejobs sidebartitle create job note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "get.mdx",
    "path": "docs/rest/jobs/get.mdx",
    "chunk_id": 0,
    "chunk_content": "title get job openapi get apiprojectsprojectnamejobsjobname sidebartitle get job note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "list-tables.mdx",
    "path": "docs/rest/tables/list-tables.mdx",
    "chunk_id": 0,
    "chunk_content": "title list tables openapi get apidatabasesdatabasenametables sidebartitle list tables note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "create-table.mdx",
    "path": "docs/rest/tables/create-table.mdx",
    "chunk_id": 0,
    "chunk_content": "title create table openapi post apidatabasesdatabasenametables sidebartitle create table note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete-table.mdx",
    "path": "docs/rest/tables/delete-table.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove table openapi delete apidatabasesdatabasenametablestablename sidebartitle remove table note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/rest/tables/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete table sidebartitle delete table info feature progress info"
  },
  {
    "filename": "insert.mdx",
    "path": "docs/rest/tables/insert.mdx",
    "chunk_id": 0,
    "chunk_content": "title insert table sidebartitle insert table info feature progress info"
  },
  {
    "filename": "list-table.mdx",
    "path": "docs/rest/tables/list-table.mdx",
    "chunk_id": 0,
    "chunk_content": "title get table openapi get apidatabasesdatabasenametablestablename sidebartitle get table note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "update.mdx",
    "path": "docs/rest/tables/update.mdx",
    "chunk_id": 0,
    "chunk_content": "title update table sidebartitle update table info feature progress info"
  },
  {
    "filename": "list-database.mdx",
    "path": "docs/rest/databases/list-database.mdx",
    "chunk_id": 0,
    "chunk_content": "title get data source openapi get apidatabasesdatabasename sidebartitle get data source note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "create-databases.mdx",
    "path": "docs/rest/databases/create-databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect data source openapi post apidatabases sidebartitle connect data source note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete-databases.mdx",
    "path": "docs/rest/databases/delete-databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove data source openapi delete apidatabasesdatabasename sidebartitle remove data source note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "list-databases.mdx",
    "path": "docs/rest/databases/list-databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data sources openapi get apidatabases sidebartitle list data sources note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "update-databases.mdx",
    "path": "docs/rest/databases/update-databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title update data source openapi put apidatabasesdatabasename sidebartitle update data source note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "update-skill.mdx",
    "path": "docs/rest/agents/update-skill.mdx",
    "chunk_id": 0,
    "chunk_content": "title update skill openapi put apiprojectsprojectnameskillsskillname sidebartitle update skill"
  },
  {
    "filename": "update-agent.mdx",
    "path": "docs/rest/agents/update-agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title update agent openapi put apiprojectsprojectnameagentsagentname sidebartitle update agent"
  },
  {
    "filename": "list-skills.mdx",
    "path": "docs/rest/agents/list-skills.mdx",
    "chunk_id": 0,
    "chunk_content": "title list skills openapi get apiprojectsprojectnameskills sidebartitle list skills"
  },
  {
    "filename": "create-chatbot.mdx",
    "path": "docs/rest/agents/create-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title create chatbot openapi post apiprojectsprojectnamechatbots sidebartitle create chatbot"
  },
  {
    "filename": "update-chatbot.mdx",
    "path": "docs/rest/agents/update-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title update chatbot openapi put apiprojectsprojectnamechatbotschatbotname sidebartitle update chatbot"
  },
  {
    "filename": "get-skill.mdx",
    "path": "docs/rest/agents/get-skill.mdx",
    "chunk_id": 0,
    "chunk_content": "title get skill openapi get apiprojectsprojectnameskillsskillname sidebartitle get skill"
  },
  {
    "filename": "list-agents.mdx",
    "path": "docs/rest/agents/list-agents.mdx",
    "chunk_id": 0,
    "chunk_content": "title list agents openapi get apiprojectsprojectnameagents sidebartitle list agents"
  },
  {
    "filename": "delete-chatbot.mdx",
    "path": "docs/rest/agents/delete-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete chatbot openapi delete apiprojectsprojectnamechatbotschatbotname sidebartitle delete chatbot"
  },
  {
    "filename": "create-skill.mdx",
    "path": "docs/rest/agents/create-skill.mdx",
    "chunk_id": 0,
    "chunk_content": "title create skill openapi post apiprojectsprojectnameskills sidebartitle create skill"
  },
  {
    "filename": "delete-skill.mdx",
    "path": "docs/rest/agents/delete-skill.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete skill openapi delete apiprojectsprojectnameskillsskillname sidebartitle delete skill"
  },
  {
    "filename": "get-chatbot.mdx",
    "path": "docs/rest/agents/get-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title get chatbot openapi get apiprojectsprojectnamechatbotschatbotname sidebartitle get chatbot"
  },
  {
    "filename": "create-agent.mdx",
    "path": "docs/rest/agents/create-agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title create agent openapi post apiprojectsprojectnameagents sidebartitle create agent"
  },
  {
    "filename": "get-agent.mdx",
    "path": "docs/rest/agents/get-agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title get agent openapi get apiprojectsprojectnameagentsagentname sidebartitle get agent"
  },
  {
    "filename": "query-agent.mdx",
    "path": "docs/rest/agents/query-agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title query agent openapi post apiprojectsprojectnameagentsagentnamecompletions sidebartitle query agent"
  },
  {
    "filename": "delete-agent.mdx",
    "path": "docs/rest/agents/delete-agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete agent openapi delete apiprojectsprojectnameagentsagentname sidebartitle delete agent"
  },
  {
    "filename": "list-chatbots.mdx",
    "path": "docs/rest/agents/list-chatbots.mdx",
    "chunk_id": 0,
    "chunk_content": "title list chatbots openapi get apiprojectsprojectnamechatbots sidebartitle list chatbots"
  },
  {
    "filename": "list.mdx",
    "path": "docs/rest/ml_engines/list.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml engines sidebartitle list ml engines info feature progress info"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/rest/ml_engines/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove ml engine sidebartitle remove ml engine info feature progress info"
  },
  {
    "filename": "create.mdx",
    "path": "docs/rest/ml_engines/create.mdx",
    "chunk_id": 0,
    "chunk_content": "title configure ml engine sidebartitle configure ml engine info feature progress info"
  },
  {
    "filename": "list-views.mdx",
    "path": "docs/rest/views/list-views.mdx",
    "chunk_id": 0,
    "chunk_content": "title list views openapi get apiprojectsprojectnameviews sidebartitle list views note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "update-view.mdx",
    "path": "docs/rest/views/update-view.mdx",
    "chunk_id": 0,
    "chunk_content": "title update view openapi put apiprojectsprojectnameviewsviewname sidebartitle update view note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "list-view.mdx",
    "path": "docs/rest/views/list-view.mdx",
    "chunk_id": 0,
    "chunk_content": "title get view openapi get apiprojectsprojectnameviewsviewname sidebartitle get view note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete-views.mdx",
    "path": "docs/rest/views/delete-views.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove view openapi delete apiprojectsprojectnameviewsviewname sidebartitle remove view note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "create-view.mdx",
    "path": "docs/rest/views/create-view.mdx",
    "chunk_id": 0,
    "chunk_content": "title create view openapi post apiprojectsprojectnameviews sidebartitle create view note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "query-model-joined-with-data.mdx",
    "path": "docs/rest/models/query-model-joined-with-data.mdx",
    "chunk_id": 0,
    "chunk_content": "title get batch predictions openapi post apiprojectsprojectnamemodelsmodelnamepredict sidebartitle get batch predictions info two ways get batch predictions 1 join model data tables use query endpointrestsql query batch predictions 2 query model using endpoint provide data used model request info responseexample python request import requests url http12700147334apiprojectsmindsdbmodelshome_rentals_modelpredict data number_of_rooms 2 number_of_bathrooms 1 sqft 917 location great days_on_market 13 neighborhood berkeley_hills rental_price 3901 created_at 20240224 022821746167 number_of_rooms 0 number_of_bathrooms 1 sqft 194 location great days_on_market 10 neighborhood berkeley_hills rental_price 2042 created_at 20240219 061059693052 number_of_rooms 1 number_of_bathrooms 1 sqft 543 location poor days_on_market 18 neighborhood westbrae rental_price 1871 created_at 20240212 075345914146 r requestsposturl jsondata data printrjson json response __mindsdb_row_id null days_on_market null location null neighborhood null number_of_bathrooms null number_of_rooms null rental_price 2847 rental_price_anomaly null rental_price_confidence 099 rental_price_explain predicted_value 2847 confidence 099 anomaly null truth null confidence_lower_bound 2730 confidence_upper_bound 2964 rental_price_max 2964 rental_price_min 2730 rental_price_original null select_data_query null sqft 823 when_data null responseexample note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "train-model.mdx",
    "path": "docs/rest/models/train-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title create train deploy model openapi post apiprojectsprojectnamemodels sidebartitle create train deploy model note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "query-model.mdx",
    "path": "docs/rest/models/query-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title get single prediction openapi post apiprojectsprojectnamemodelsmodelnamepredict sidebartitle get single prediction responseexample json request curl request post url http12700147334apiprojectsmindsdbmodelshome_rentals_modelpredict header contenttype applicationjson data data sqft 823 location good neighborhood downtown days_on_market 10 json response __mindsdb_row_id null days_on_market null location null neighborhood null number_of_bathrooms null number_of_rooms null rental_price 2847 rental_price_anomaly null rental_price_confidence 099 rental_price_explain predicted_value 2847 confidence 099 anomaly null truth null confidence_lower_bound 2730 confidence_upper_bound 2964 rental_price_max 2964 rental_price_min 2730 rental_price_original null select_data_query null sqft 823 when_data null responseexample note rest api endpoints used mindsdb running locally http12700147334api note tip note data sent request specific model instance home_rentals_model trained data includes details rental properties want predict rental_price value specific property matches data passed request tip"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/rest/models/retrain.mdx",
    "chunk_id": 0,
    "chunk_content": "title retrain model sidebartitle retrain model info feature progress info"
  },
  {
    "filename": "delete-model.mdx",
    "path": "docs/rest/models/delete-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove model openapi delete apiprojectsprojectnamemodelsmodelname sidebartitle remove model note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "describe-model.mdx",
    "path": "docs/rest/models/describe-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title describe model openapi get apiprojectsprojectnamemodelsmodelnamedescribe sidebartitle describe model responseexample json response accuracies r2_score 0999 column_importances days_on_market 0116 location 0054 neighborhood 00 number_of_bathrooms 0009 number_of_rooms 0297 sqft 1037 inputs number_of_rooms number_of_bathrooms sqft location days_on_market neighborhood model encoders dtype_dict dependency_dict model problem_definition identifiers imputers accuracy_functions outputs rental_price responseexample note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "list-models.mdx",
    "path": "docs/rest/models/list-models.mdx",
    "chunk_id": 0,
    "chunk_content": "title list models openapi get apiprojectsprojectnamemodels sidebartitle list models note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "list-model.mdx",
    "path": "docs/rest/models/list-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title get model openapi get apiprojectsprojectnamemodelsmodelname sidebartitle get model note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "manage-model-versions.mdx",
    "path": "docs/rest/models/manage-model-versions.mdx",
    "chunk_id": 0,
    "chunk_content": "title manage model versions sidebartitle manage model versions info feature progress info"
  },
  {
    "filename": "finetune.mdx",
    "path": "docs/rest/models/finetune.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune model sidebartitle finetune model info feature progress info"
  },
  {
    "filename": "list.mdx",
    "path": "docs/rest/files/list.mdx",
    "chunk_id": 0,
    "chunk_content": "title list files openapi get apifiles sidebartitle list files note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/rest/files/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove file openapi delete apifilesfilename sidebartitle remove file note rest api endpoints used mindsdb running locally http12700147334api note"
  },
  {
    "filename": "upload.mdx",
    "path": "docs/rest/files/upload.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload file openapi put apifilesfilename sidebartitle upload file note rest api endpoints used mindsdb running locally http12700147334api note note note trailing whitespaces column names erased upon uploading file mindsdb note"
  },
  {
    "filename": "data-integrations.mdx",
    "path": "docs/features/data-integrations.mdx",
    "chunk_id": 0,
    "chunk_content": "title data integrations sidebartitle data integrations icon database card titleall available data integrations iconlink hrefintegrationsdataoverviewcard"
  },
  {
    "filename": "automation.mdx",
    "path": "docs/features/automation.mdx",
    "chunk_id": 0,
    "chunk_content": "title automation sidebartitle automation icon boltauto mindsdb provides mechanisms automate tasks include jobsmindsdb_sqlsqlcreatejobs triggersmindsdb_sqlsqlcreatetrigger chatbotsmindsdb_sqlagentschatbot p aligncenter img srcassetsautomationpng p tip use ai automation keep ai systems uptodate continuously retraining finetuning realtime data follow use casesqltutorialsrealtimetradingforecasts learn build mindsdb tip tip use ai automation create chatbots follow use casesqltutorialstwiliochatbot learn build custom chatbots mindsdb tip"
  },
  {
    "filename": "ai-integrations.mdx",
    "path": "docs/features/ai-integrations.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai integrations sidebartitle ai integrations icon microchipai card titleall available ai integrations iconlink hrefintegrationsaioverviewcard"
  },
  {
    "filename": "model-management.mdx",
    "path": "docs/features/model-management.mdx",
    "chunk_id": 0,
    "chunk_content": "title model management sidebartitle model management icon barsprogress mindsdb abstracts ai models making accessible enterprise data environments p aligncenter img srcassetsmodelmanagementpng p mindsdb enables manage every aspect ai models mindsdb create modelmindsdb_sqlsqlcreatemodel finetunemindsdb_sqlsqlapifinetune retrainmindsdb_sqlsqlapiretrain deploymindsdb_sqlsqlcreatemodel brbryou create train deploy ai modelsmindsdb_sqlsqlcreatemodel based popular aiml frameworksintegrationsaioverview within mindsdb finetunemindsdb_sqlsqlapifinetune brbryou finetune modelsmindsdb_sqlsqlapifinetune data various data sourcesintegrationsdataoverview connected mindsdb check examples hereusecasesautomated_finetuningoverview automatemindsdb_sqlsqlcreatejobs brbryou automate tasks including retraining finetuning ai models keep ai system uptodate see examples hereusecasesai_workflow_automationoverview tip go ahead create ai model use sql apimindsdb_sqloverview rest apirestoverview one sdkssdksoverview create deploy ai models within mindsdb tip"
  },
  {
    "filename": "environment-vars.mdx",
    "path": "docs/setup/environment-vars.mdx",
    "chunk_id": 0,
    "chunk_content": "title environment variables sidebartitle environment variables mindsdb functionality modified extending default configuration configuration options added environment variables server mindsdb deployed mindsdb authentication mindsdb require authentication default want enable authentication set mindsdb_username mindsdb_password environment variables example codegroup bash docker docker run name mindsdb_container e mindsdb_usernamemindsdb_user e mindsdb_passwordmindsdb_password p 4733447334 p 4733547335 mindsdbmindsdb bash shell export mindsdb_usernamemindsdb_user export mindsdb_passwordmindsdb_password codegroup mindsdb storage default mindsdb stores configuration files determining appropriate platformspecific directories eg user data dir linux localsharemindsdbvar macos libraryapplication supportmindsdbvar windows cdocuments settingsuserapplication datalocal settingsappauthormindsdbvar mindsdb_storage_dir location mindsdb stores users data models uploaded data files static assets frontend application sqlitedb file change default storage location using mindsdb_storage_dir variable example codegroup bash docker docker run name mindsdb_container e mindsdb_storage_dirhomemindsdbvar p 4733447334 p 4733547335 mindsdbmindsdb bash shell export mindsdb_storage_dirhomemindsdbvar codegroup mindsdb configuration storage mindsdb uses sqlite database default store required configuration models projects files metadata etc full list schemas found herehttpsgithubcommindsdbmindsdbblobmainmindsdbinterfacesstoragedbpyl69 change default storage option use different database adding new connection string using mindsdb_db_con variable example codegroup bash docker docker run name mindsdb_container e mindsdb_db_conpostgresqlusersecretlocalhost p 4733447334 p 4733547335 mindsdbmindsdb bash shell export mindsdb_db_conpostgresqlusersecretlocalhost codegroup mindsdb apis default mindsdb starts http mysql apis define apis want start use mindsdb_apis environment variable expose ports apis"
  },
  {
    "filename": "environment-vars.mdx",
    "path": "docs/setup/environment-vars.mdx",
    "chunk_id": 1,
    "chunk_content": "need add respective ports command p flag example codegroup bash docker docker run name mindsdb_container e mindsdb_apishttpmysqlmongodbpostgres p 4733447334 p 4733547335 p 4733647336 p 5543255432 mindsdbmindsdb bash shell export mindsdb_apishttpmysqlmongodbpostgres codegroup mindsdb server default http api mindsdb uses waitresshttpspypiorgprojectwaitress purepython wsgi server option change use flaskhttpsflaskpalletsprojectscomenstable gunicornhttpsgunicornorg example codegroup bash docker use gunicorn default server installed logging container docker exec mindsdb_container sh assuming container name mindsdb_container assuming container name mindsdb_container pip install gunicorn docker run name mindsdb_container e mindsdb_default_servergunicorn p 4733447334 p 4733547335 mindsdbmindsdb bash shell export mindsdb_default_servergunicorn codegroup also use waitress default server flask mindsdb logs environment variable defines level logging generated mindsdb choose one values defined herehttpsdocspythonorg3librarylogginghtmllogginglevels info level used default example codegroup bash docker docker run name mindsdb_container e mindsdb_log_leveldebug p 4733447334 p 4733547335 mindsdbmindsdb bash shell export mindsdb_log_leveldebug codegroup"
  },
  {
    "filename": "open_telemetry.mdx",
    "path": "docs/setup/open_telemetry.mdx",
    "chunk_id": 0,
    "chunk_content": "title opentelemetry integration guide sidebartitle opentelemetry integration guide icon book guide explains configure integrate opentelemetry otel using environment variables assumes familiar otel concepts provides details environment variable configuration provided dockercomposeyml includes otel collector local development use collector choice configuring opentelemetry environment variables opentelemetry behavior controlled via environment variables breakdown key variables purposes general variables otel_exporter_type defines type exporter console otlp default console otel_exporter_protocol defines protocol exporter grpc http default grpc otel_sdk_disabled disables open telemetry sdk default disabled local environments unless otel_sdk_force_run true otel_sdk_force_run forces open telemetry sdk run even disabled environments example set true testing local environments otlp exporter endpoints otel_otlp_endpoint sets base endpoint otlp exports default httplocalhost4317 otel_otlp_logging_endpoint endpoint logging exports default otel_otlp_endpoint otel_otlp_tracing_endpoint endpoint tracing exports default otel_otlp_endpoint otel_otlp_metrics_endpoint endpoint metrics exports default otel_otlp_endpoint service information otel_service_name specifies service name default mindsdb_new_test otel_service_instance_id unique id service instance default mindsdb otel_service_environment specifies deployment environment eg local dev prod default local otel_service_release version release identifier service default local otel_trace_sample_rate determines sampling rate tracing default 10 attributes otel_extra_attributes allows passing additional attributes commaseparated list keyvalue pairs example serviceversion001foobar logging metrics tracing toggles otel_logging_disabled disables logging integration default follows otel_sdk_disabled otel_tracing_disabled disables tracing integration default follows otel_sdk_disabled true otel_metrics_disabled disables metrics integration"
  },
  {
    "filename": "open_telemetry.mdx",
    "path": "docs/setup/open_telemetry.mdx",
    "chunk_id": 1,
    "chunk_content": "default follows otel_sdk_disabled true using docker compose dockercomposeyml provides example configuration local development replace otel collector preferred collector production mindsdb service configuration yaml environment otel_exporter_type otlp otel_otlp_endpoint httpotelcollector4317 otel_service_name mindsdb otel_service_instance_id mindsdbinstance otel_service_environment local otel_logging_disabled true otel_tracing_disabled true otel_metrics_disabled false otel_extra_attributes serviceversion001foobar otel_sdk_force_run true otel collector local development yaml otelcollector image otelopentelemetrycollectorcontrib01161 volumes otelcollectorconfigyamletcotelcolcontribconfigyaml ports 43174317 otlp grpc receiver"
  },
  {
    "filename": "community-deploys-mindsdb.mdx",
    "path": "docs/setup/community-deploys-mindsdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title deploying mindsdb cloud sidebartitle clouds check articles video guides created community article deploying mindsdb linodehttpsdevtorutamheredeployingmindsdbonlinodejh8 rutam prita mishrahttpstwittercomrutamhere article deploying mindsdb scaleway cloudhttpsdevtorutamheredeployingmindsdbonscalewaycloud4kc6 rutam prita mishrahttpscommunityopsiorutamhere article deploying mindsdb oracle cloudhttpscommunityopsiorutamheredeployingmindsdbonoraclecloud164d rutam prita mishrahttpscommunityopsiorutamhere article deploying mindsdb digital ocean droplethttpsdevtoheyrutamdeployingmindsdbonadigitaloceandroplet31ed rutam prita mishrahttpscommunityopsiorutamhere article deploying mindsdb vultr cloud instancehttpscommunityopsiorutamheredeployingmindsdbonavultrcloudinstance40bm rutam prita mishrahttpscommunityopsiorutamhere article deploying mindsdb civo computehttpscommunityopsiorutamheredeployingmindsdboncivocompute2kgo rutam prita mishrahttpscommunityopsiorutamhere article deploying mindsdb google cloud platformhttpscommunityopsiorutamheredeployingmindsdbongooglecloudplatform41h9 rutam prita mishrahttpscommunityopsiorutamhere video guide deploy mindsdb local machine docker piphttpsyoutubedibs27jg1fy munyoudoum pavhttpsgithubcommunyoudoum video guide setting docker mindsdbhttpswwwyoutubecomwatchvdadycupum0featureyoutube alissa troianohttpsgithubcomalissatroiano video guide deploying instance mindsdb google cloud platformhttpswwwyoutubecomwatchvorw8cingng syed zubeenhttpsgithubcomsyedzubeen video guide deploy mindsdb google cloud platformhttpswwwyoutubecomwatchvwldb7uni4bk alissa troianohttpsgithubcomalissatroiano"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 0,
    "chunk_content": "title extend default mindsdb configuration sidebartitle extend default mindsdb configuration follow guide install mindsdb locally via dockersetupselfhosteddockerdesktop pypisetupselfhostedpipsource starting mindsdb default configuration start mindsdb locally default configuration 1 activate virtual environment bash source mindsdbbinactivate 2 start mindsdb bash python mindsdb 3 access mindsdb locally 12700147334 tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip starting mindsdb extended configuration start mindsdb locally custom configuration providing path configjson file stores custom config parameters listed section bash python mindsdb configpathtotheextendedconfigfileconfigjson accordiongroup accordion titleoverview config parameters custom configuration parameters set according requirements saved configjson file bash permanent_storage location absent bucket s3_bucket_name optional used location s3 permanent_storage parameter defines mindsdb stores copies user files uploaded files models tab content mindsdb checks permanent_storage location access latest version file updates needed location specifies storage type absent default disables permanent storage recommended use mindsdb running locally local stores files local directory defined configpathsstorage s3 stores files amazon s3 bucket option requires bucket parameter specifies name s3 bucket files stored parameter set path determined mindsdb_storage_dir environment variable mindsdb defaults creating mindsdb folder operating system"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 1,
    "chunk_content": "users home directory bash paths root homemindsdbvar optional alternatively defined mindsdb_storage_dir environment variable content homemindsdbvarcontent optional storage homemindsdbvarstorage optional static homemindsdbvarstatic optional tmp homemindsdbvartmp optional cache homemindsdbvarcache optional locks homemindsdbvarlocks optional paths parameter allows users redefine file paths various groups mindsdb files root path defined folders created within directory parameter absent value determined mindsdb_storage_dir environment variable root parameter defines base directory storing mindsdb files including models uploaded files tab content internal sqlite database running locally content parameter specifies directory userrelated files stored uploaded files created models tab content internal sqlite database running locally stored root directory instead permanent_storagelocation set local storage parameter used store copies user files static parameter used store files graphical user interface gui mindsdb run locally tmp parameter designates directory temporary files note operating systems default temporary directory may also used temporary files cachetype set local cache parameter defines location storing cached files recent predictions example model queried identical input result stored cache returned directly subsequent queries instead recalculating prediction locks parameter used store lock files prevent race conditions content folder shared among multiple applications directory helps ensure file access managed properly using fcntl locks note applicable windows os bash auth http_auth_enabled true http_permanent_session_lifetime 86400 username username"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 2,
    "chunk_content": "password password auth parameter controls authentication settings apis mindsdb http_auth_enabled parameter set true username password parameters required otherwise optional local instances mindsdb users enable simple http authentication based flask sessions follows 1 enable authentication http api setting http_auth_enabled parameter true providing values username password parameters alternatively users set environment variables mindsdb_username mindsdb_password store values 2 default lifetime session set 31 days modified providing value seconds http_permanent_session_lifetime parameter alternatively users set one environment variables mindsdb_http_permanent_session_lifetime flask_permanent_session_lifetime store value bash gui autoupdate true gui parameter controls behavior mindsdb graphical user interface gui updates autoupdate parameter defines whether mindsdb automatically checks updates gui latest version application starts set true mindsdb attempt fetch latest available version gui set false mindsdb try update gui startup bash api http host 127001 port 47334 restart_on_failure true max_restart_count 1 max_restart_interval_seconds 60 server type server_type config waitres threads 16 max_request_body_size 1 30 10 10gb inbuf_overflow 1 30 10 flask gunicorn workers minmultiprocessingcpu_count 4 timeout 600 reuse_port true preload_app true threads 4 mysql host 127001 port 47335 database mindsdb ssl true restart_on_failure true max_restart_count 1 max_restart_interval_seconds 60 mongodb host 127001 port 47336 database mindsdb api parameter contains configuration settings running mindsdb apis currently supported apis http configures http api requires"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 3,
    "chunk_content": "host port parameters alternatively configure http authentication mindsdb instance setting environment variables mindsdb_username mindsdb_password starting mindsdb recommended way production systems mysql configures mysql api requires host port parameters additionally database ssl parameters mongodb configures mongodb api requires host port parameters additionally database parameter connection parameters within block include host specifies ip address hostname api run example 127001 indicates api run locally port defines port number api listen incoming requests default ports 47334 http 47335 mysql 47336 mongodb database mysql mongodb specifies name database mindsdb uses users must connect database interact mindsdb respective api ssl mysql api indicates whether ssl support enabled mysql api additional setup http api mysql api restart_on_failure set true max_restart_count reached restart mindsdb attempted mindsdb process killed code 9 linux macos reason windows max_restart_count defines many times restart attempts made note 0 stands limit max_restart_interval_seconds defines time limit max_restart_count restart attempts note 0 stands time limit means would maximum max_restart_count restart attempts allowed type within server specifies server type used mindsdb waitress default values include flask gunicorn alternatively set mindsdb_default_server environment variable define server config within server stores selected server configuration options supported selected server taht waitress flask gunicorn set except host port defined configapihttp note usage"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 4,
    "chunk_content": "example restart features assume following values max_restart_count 2 max_restart_interval_seconds 30 seconds assume following scenario mindsdb fails 1000s work restart attempt succeeds restarts past 30 seconds mindsdb fails 1010s work restart attempt succeeds 1 restart 1000s past 30 seconds mindsdb fails 1020s work restart attempt fails already max_restart_count2 restarts 1000s 1010s past 30 seconds mindsdb fails 1031s work restart attempt succeeds 1 restart 1010s past 30 seconds note bash cache type local connection redislocalhost6379 optional used type redis cache parameter controls mindsdb stores results recent predictions avoid recalculating query run note recent predictions cached ml models like lightwood case large language models llms like openai type parameter specifies type caching mechanism use storing prediction results none disables caching prediction results stored local default stores prediction results cache folder defined paths configuration useful repeated queries result doesnt change redis stores prediction results redis instance option requires connection parameter specifies redis connection string connection parameter required type parameter set redis stores redis connection string bash logging handlers console enabled true level info optional alternatively defined mindsdb_console_log_level environment variable file enabled false level info optional alternatively defined mindsdb_file_log_level environment variable filename applog maxbytes 524288 05 mb backupcount 3 parameters implemented based pythons logging dictionary"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 5,
    "chunk_content": "schemahttpsdocspythonorg3libraryloggingconfightmlloggingconfigdictschema logging parameter defines details output logging including logging levels handler parameter provides handlers used logging streams files console enabled parameter set true logging output saved stream users define logging level level parameter mindsdb_console_log_level environment variable one debug info warning error critical file enabled parameter set true logging output saved file users define logging level level parameter mindsdb_file_log_level environment variable one debug info warning error critical additionally filename parameter stores name file contains logs maxbytes backupcount parameters determine rollover process file file reached size maxbytes file closed new file opened number files defined backupcount parameter bash ml_task_queue type local host localhost optional used type redis port 6379 optional used type redis db 0 optional used type redis username username optional used type redis password password optional used type redis ml_task_queue parameter manages queueing system machine learning tasks mindsdb ml tasks include operations creating training predicting finetuning retraining models tasks resourceintensive running multiple ml tasks simultaneously may lead memory oom errors performance degradation address mindsdb uses task queue control task execution optimize resource utilization type parameter defines type task queue use local tasks processed immediately appear without queue suitable environments resource constraints concern redis tasks added redisbased queue consumer process run"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 6,
    "chunk_content": "ml_task_consumer ensures tasks executed sufficient resources available using redis queue requires additional configuration host port db username password parameters use redis queue start mindsdb following command initiate queue consumer process python3 mindsdb ml_task_queue_consumer process monitor queue fetch tasks execution sufficient resources available bash file_upload_domains file_upload_domains parameter restricts file uploads trusted sources specifying list allowed domains ensures users upload files defined sources s3 google drive file_upload_domains httpss3amazonawscom httpsdrivegooglecom parameter left empty users upload files url without restriction bash web_crawling_allowed_sites web_crawling_allowed_sites parameter restricts web crawling operations specified list allowed ips web addresses ensures application accesses preapproved safe urls web_crawling_allowed_sites httpsexamplecom httpsapimysitecom left empty application allows access urls default marked wildcard opensource version accordion accordion titleexample extended config file first create configjson file bash permanent_storage location absent paths root pathtorootlocation auth http_auth_enabled true http_permanent_session_lifetime 86400 username username password password gui autoupdate true api http host 127001 port 47334 restart_on_failure true max_restart_count 1 max_restart_interval_seconds 60 mysql host 127001 port 47335 database mindsdb ssl true restart_on_failure true max_restart_count 1 max_restart_interval_seconds 60 mongodb host 127001 port 47336 database mindsdb cache type local logging handlers console enabled true level info file enabled false level info filename applog maxbytes 524288 backupcount 3 ml_task_queue type local file_upload_domains web_crawling_allowed_sites next"
  },
  {
    "filename": "custom-config.mdx",
    "path": "docs/setup/custom-config.mdx",
    "chunk_id": 7,
    "chunk_content": "start mindsdb providing configjson file bash python mindsdb configpathtotheextendedconfigfileconfigjson accordion accordiongroup modifying config values users modify config values directly editing configjson file created"
  },
  {
    "filename": "aws-marketplace.mdx",
    "path": "docs/setup/cloud/aws-marketplace.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb aws marketplace sidebartitle aws marketplace mindsdb offers streamlined setup process cloud environments using aws marketplace image info explore mindsdb aws marketplace image herehttpsawsamazoncommarketplacesellerprofileid03a6552086ca4ab8a394c11eb54573a9 info"
  },
  {
    "filename": "docker-desktop.mdx",
    "path": "docs/setup/self-hosted/docker-desktop.mdx",
    "chunk_id": 0,
    "chunk_content": "title docker desktop extension mindsdb sidebartitle docker desktop mindsdb provides extension docker desktop facilitates running mindsdb docker desktop tip visit github repository mindsdb docker desktop extensionhttpsgithubcommindsdbmindsdbdockerextension learn tip prerequisites proceeding ensure installed docker desktop following official docker desktop documentationhttpswwwdockercomproductsdockerdesktop setup setup mindsdb uses mindsdbmindsdblatest docker image lightweight docker image mindsdb comes integrations preloadedhttpsgithubcommindsdbmindsdbblobmaindefault_handlerstxt follow steps set mindsdb docker desktop install mindsdb docker desktop extension tip windows user ensure enabled developer mode settings installing extension img srcassetsdockerdocker_desktopenablewindevmodepng necessary keep developer mode enabled use extension extension installed disable developer mode wish tip go extensions page docker desktop search mindsdb install mindsdb extension tip first time extension installed run latest version mindsdb moving forward advisable regularly update mindsdb image used extension ensure access latest features improvements mentioned previously extension uses mindsdbmindsdblatest docker image update image follow steps 1 navigate images tab docker desktop 2 search locate mindsdbmindsdblatest image 3 click three dots right side image click pull image already date see message stating skip next step 4 wait image pulled restart docker desktop img srcassetsdockerdocker_desktoppulllatestimagepng tip access mindsdb inside docker desktop p aligncenter img srcassetsdockerdocker_desktopmindsdb_docker_desktoppng p install dependencies mindsdb editor go settings manage integrations select integrations want use click install p aligncenter img"
  },
  {
    "filename": "docker-desktop.mdx",
    "path": "docs/setup/self-hosted/docker-desktop.mdx",
    "chunk_id": 1,
    "chunk_content": "srcassetsinstalldependenciesguipng p view logs order view logs generated mindsdb running extension follow steps 1 navigate containers tab docker desktop 2 search locate multicontainer application running mindsdb extension done searching mindsdb tip see application listed navigate extensions tag settings ensure show docker extensions system containers option enabled img srcassetsdockerdocker_desktopenableextensioncontainerspng tip 3 click container named mindsdb_service direct container running mindsdb p aligncenter img srcassetsdockerdocker_desktopcontainersrunningextensionpng p 4 view logs logs tab p aligncenter img srcassetsdockerdocker_desktopmindsdbcontainerlogspng p"
  },
  {
    "filename": "docker.mdx",
    "path": "docs/setup/self-hosted/docker.mdx",
    "chunk_id": 0,
    "chunk_content": "title docker mindsdb sidebartitle docker mindsdb provides docker images facilitate running mindsdb docker containers tip mindsdb integrates numerous data sourcesintegrationsdataoverview ai frameworksintegrationsaioverview integration requires set dependencies hence mindsdb provides multiple docker images different tasks outlined mindsdbmindsdblatest mindsdbmindsdb lightweight docker image mindsdb comes integrations preloadedhttpsgithubcommindsdbmindsdbblobmaindefault_handlerstxt mindsdbmindsdblightwood docker image mindsdb comes integrationshttpsgithubcommindsdbmindsdbblobmaindefault_handlerstxt lightwood integration preloaded mindsdbmindsdbhuggingface docker image mindsdb comes integrationshttpsgithubcommindsdbmindsdbblobmaindefault_handlerstxt hugging face integration preloaded tip prerequisites proceeding ensure installed docker following official docker documentationhttpsdocsdockercominstall setup setup mindsdb uses one available docker images listed follow steps set mindsdb docker container install mindsdb run command create docker container mindsdb bash docker run name mindsdb_container p 4733447334 p 4733547335 mindsdbmindsdb docker run native docker command used spin container name mindsdb_container defines name container p 4733447334 publishes port 47334 access mindsdb gui http api p 4733547335 publishes port 47335 access mindsdb mysql api optional omitted dont need use mysql api mindsdbmindsdb docker image provided mindsdb choose different one list tip default mindsdb starts http api define apis start passing mindsdb_apis environment variable commaseparated list running container shown expose ports apis need add respective ports command p flag bash docker run e mindsdb_apishttpmysqlmongodbpostgres p 4733447334 p 4733547335 p 4733647336 p 5543255432 mindsdbmindsdb find information environment variables"
  },
  {
    "filename": "docker.mdx",
    "path": "docs/setup/self-hosted/docker.mdx",
    "chunk_id": 1,
    "chunk_content": "supported mindsdb heresetupenvironmentvars tip container created use following commands docker stop mindsdb_container stop container note may always necessary turning host machine container also shut docker start mindsdb_container restart stopped container previous changes dependencies installed intact note docker start restarts stopped container docker run creates new container tip dont want follow logs get prompt back add flag stands detach bash docker run name mindsdb_container p 4733447334 mindsdbmindsdb tip tip want persist models configurations host machine run commands bash mkdir mdb_data docker run name mindsdb_container p 4733447334 v pwdmdb_datarootmdb_storage mindsdbmindsdb v pwdmdb_datarootmdb_storage maps newly created folder mdb_data host machine rootmdb_storage inside container tip access mindsdb editor going 12700147334 browser p aligncenter img srcassetsmindsdbeditorpng p warning experience issues related mkl training process complete please add mkl_service_force_intel environment variable bash docker run name mindsdb_container e mkl_service_force_intel1 p 4733447334 mindsdbmindsdb warning tip want enable authentication mindsdb passing mindsdb_username mindsdb_password environment variables running container bash docker run name mindsdb_container e mindsdb_usernameadmin e mindsdb_passwordpassword p 4733447334 mindsdbmindsdb tip install dependencies mindsdb integrates numerous data sources ai frameworks use integrations enure required dependencies installed docker container method 1 install dependencies directly mindsdb editor go settings manage integrations select integrations want use click install p aligncenter img srcassetsinstalldependenciesguipng p"
  },
  {
    "filename": "docker.mdx",
    "path": "docs/setup/self-hosted/docker.mdx",
    "chunk_id": 2,
    "chunk_content": "method 2 start mindsdb docker container bash docker start mindsdb_container tip havent specified container name spinning container docker run find running docker ps tip tip havent yet created container use command bash docker run name mindsdb_container p 4733447334 mindsdbmindsdb tip start interactive shell container bash docker exec mindsdb_container sh install dependencies bash pip install handler_name example run command install dependencies openai handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenai_handler bash pip install openai exit interactive shell bash exit restart container bash docker restart mindsdb_container configuration configuration mindsdbs docker image includes storage location log level debugging information installed integrations api endpoints parameters customized modifying json file stores default configuration default configuration default configuration mindsdbs docker image stored json code json config_version14 paths root rootmdb_storage debug false integrations api http host 0000 port 47334 mysql host 0000 password port 47335 user mindsdb database mindsdb ssl true mongodb host 0000 port 47336 database mindsdb custom configuration override default configuration mount config file rootmindsdb_configjson bash docker run name mindsdb_container p 4733447334 v mdb_configjsonrootmindsdb_configjson mindsdbmindsdb tip whats next installed started mindsdb locally docker container go ahead find create train model using create modelsqlcreatemodel statement check use casesusecasesoverview section follow tutorials cover large language models chatbots time series classification regression models semantic search"
  },
  {
    "filename": "docker.mdx",
    "path": "docs/setup/self-hosted/docker.mdx",
    "chunk_id": 3,
    "chunk_content": "tip"
  },
  {
    "filename": "windows.mdx",
    "path": "docs/setup/self-hosted/pip/windows.mdx",
    "chunk_id": 0,
    "chunk_content": "title setup windows via pip sidebartitle pip windows tip successfully install mindsdb use python 64bit version also make sure python 39 pip 203 tip installation using python venvhttpsdocspythonorg3libraryvenvhtml module 1 create new virtual environment called mindsdb bash py venv mindsdb activate bash mindsdbscriptsactivatebat 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip installation using anaconda need either anacondahttpswwwanacondacomproductsindividual condahttpscondaioprojectscondaenlatestindexhtml installed machine 1 open anaconda prompt create new virtual environment bash conda create n mindsdb activate bash conda activate mindsdb 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip dependencies dependencies many data"
  },
  {
    "filename": "windows.mdx",
    "path": "docs/setup/self-hosted/pip/windows.mdx",
    "chunk_id": 1,
    "chunk_content": "ml integrations installed default want use data ml integration whose dependencies available default install running command pip install mindsdbhandler_name tip find available handlers herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers tip troubleshooting pip python versions currently mindsdb supports python versions 39x 310x 311x successfully install mindsdb use python 64bit version also make sure python 39 pip 203 check pip python versions running pip version python version commands please note depending environment installed pip python packages might use pip3 instead pip python3x instead py example pip3 install mindsdb instead pip install mindsdb avoid dependency issues install mindsdb virtual environment using pip avoid dependency issues could try install mindsdb anacondahttpswwwanacondacomproductsindividual run installation anaconda prompt addition windows systems default languages english system might utf8 default encoding standard cause encoding errors installing dependencies solve issue go control panel clock region region administrative tab change system locale button enable beta use unicode utf8 worldwide language support installing torch torchvision installation fails installing torch torchvision try install manually following instructions official websitehttpspytorchorggetstartedlocally issues try replicate issue using docker setupsetupselfhosteddocker also please create issue detailed description mindsdb github repositoryhttpsgithubcommindsdbmindsdbissues help usually review issues respond within hours whats next installed started mindsdb locally docker container go ahead find create train model using create modelsqlcreatemodel"
  },
  {
    "filename": "windows.mdx",
    "path": "docs/setup/self-hosted/pip/windows.mdx",
    "chunk_id": 2,
    "chunk_content": "statement mindsdb sql section youll find comprehensive overview sql syntax offered mindsdb also provide mongoql syntax documented mindsdb mongoql section connect mindsdb different clients including postgresql cliconnectpostgresclient mysql cliconnectmysqlclient check use casesusecasesoverview section follow tutorials cover large language models natural language processing time series classification regression models"
  },
  {
    "filename": "linux.mdx",
    "path": "docs/setup/self-hosted/pip/linux.mdx",
    "chunk_id": 0,
    "chunk_content": "title setup linux via pip sidebartitle pip linux iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedtvgp5xtffmk titledeploy mindsdb using pip linux frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe tip successfully install mindsdb use python 64bit version also make sure python 39 pip 203 tip installation using python venvhttpsdocspythonorg3libraryvenvhtml module 1 create new virtual environment called mindsdb bash python venv mindsdb activate bash source mindsdbbinactivate 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip installation using anaconda need either anacondahttpswwwanacondacomproductsindividual condahttpscondaioprojectscondaenlatestindexhtml installed machine 1 open anaconda prompt create new virtual environment bash conda create n mindsdb activate bash conda activate mindsdb 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api"
  },
  {
    "filename": "linux.mdx",
    "path": "docs/setup/self-hosted/pip/linux.mdx",
    "chunk_id": 1,
    "chunk_content": "httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip dependencies dependencies many data ml integrations installed default want use data ml integration whose dependencies available default install running command pip install mindsdbhandler_name tip find available handlers herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers tip troubleshooting pip python versions currently mindsdb supports python versions 39x 310x 311x successfully install mindsdb use python 64bit version also make sure python 39 pip 203 check pip python versions running pip version python version commands please note depending environment installed pip python packages might use pip3 instead pip python3x instead py example pip3 install mindsdb instead pip install mindsdb avoid dependency issues install mindsdb virtual environment using pip avoid dependency issues could try install mindsdb anacondahttpswwwanacondacomproductsindividual run installation anaconda prompt avoid common errors mindsdb requires around 3 gb free disk space install dependencies make sure allocate min 3 gb disk space avoid ioerror errno 28 space left device installing mindsdb error anything activate virtual environment mindsdb installed avoid module named mindsdb error issues try replicate issue using docker setupsetupselfhosteddocker also please create issue detailed description mindsdb github repositoryhttpsgithubcommindsdbmindsdbissues help usually review issues respond within hours whats next installed started mindsdb locally docker container"
  },
  {
    "filename": "linux.mdx",
    "path": "docs/setup/self-hosted/pip/linux.mdx",
    "chunk_id": 2,
    "chunk_content": "go ahead find create train model using create modelsqlcreatemodel statement mindsdb sql section youll find comprehensive overview sql syntax offered mindsdb also provide mongoql syntax documented mindsdb mongoql section connect mindsdb different clients including postgresql cliconnectpostgresclient mysql cliconnectmysqlclient check use casesusecasesoverview section follow tutorials cover large language models natural language processing time series classification regression models"
  },
  {
    "filename": "source.mdx",
    "path": "docs/setup/self-hosted/pip/source.mdx",
    "chunk_id": 0,
    "chunk_content": "title setup source code via pip sidebartitle pip source section describes deploy mindsdb source code preferred way use mindsdb want contribute code debug mindsdb tip successfully install mindsdb use python 64bit version also make sure python 39 pip 203 tip installation tip please note method mindsdb installation requires minimum 6 gb free storage tip 1 clone mindsdb repository bash git clone httpsgithubcommindsdbmindsdbgit 2 create new virtual environment bash python venv mindsdbvenv 3 activate virtual environment bash source mindsdbvenvbinactivate 4 install dependencies bash cd mindsdb pip install e pip install r requirementsrequirementsdevtxt 5 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip 6 access following codegroup bash mindsdb studio http12700147334 bash mindsdb using mysql mysql h 127001 port 3306 u mindsdb p codegroup dependencies dependencies many data ml integrations installed default want use data ml integration whose dependencies available default install running command pip install mindsdbhandler_name tip find available handlers herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers tip troubleshooting pip python versions currently mindsdb supports python versions 39x 310x 311x successfully install mindsdb use python 64bit version"
  },
  {
    "filename": "source.mdx",
    "path": "docs/setup/self-hosted/pip/source.mdx",
    "chunk_id": 1,
    "chunk_content": "also make sure python 39 pip 203 check pip python versions running pip version python version commands please note depending environment installed pip python packages might use pip3 instead pip python3x instead py example pip3 install mindsdb instead pip install mindsdb avoid dependency issues install mindsdb virtual environment using pip avoid dependency issues avoid common errors mindsdb requires around 3 gb free disk space install dependencies make sure allocate min 3 gb disk space avoid ioerror errno 28 space left device installing mindsdb error anything activate virtual environment mindsdb installed avoid module named mindsdb error encounter site cant reached 127001 refused connect error please check mindsdb server console see server still starting phase server started still get error please report github repositoryhttpsgithubcommindsdbmindsdbissues issues try use docker setupsetupselfhosteddocker case experiencing issues using pip also please create issue detailed description mindsdb github repositoryhttpsgithubcommindsdbmindsdbissues help usually review issues respond within hours whats next installed started mindsdb locally docker container go ahead find create train model using create modelsqlcreatemodel statement mindsdb sql section youll find comprehensive overview sql syntax offered mindsdb also provide mongoql syntax documented mindsdb mongoql section connect mindsdb different clients including postgresql cliconnectpostgresclient mysql cliconnectmysqlclient check use casesusecasesoverview section follow tutorials"
  },
  {
    "filename": "source.mdx",
    "path": "docs/setup/self-hosted/pip/source.mdx",
    "chunk_id": 2,
    "chunk_content": "cover large language models natural language processing time series classification regression models"
  },
  {
    "filename": "macos.mdx",
    "path": "docs/setup/self-hosted/pip/macos.mdx",
    "chunk_id": 0,
    "chunk_content": "title setup macos via pip sidebartitle pip macos tip successfully install mindsdb use python 64bit version also make sure python 39 pip 203 tip installation using python venvhttpsdocspythonorg3libraryvenvhtml module 1 create new virtual environment called mindsdb bash python venv mindsdb activate bash source mindsdbbinactivate 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip installation using anaconda need either anacondahttpswwwanacondacomproductsindividual condahttpscondaioprojectscondaenlatestindexhtml installed machine 1 open anaconda prompt create new virtual environment bash conda create n mindsdb activate bash conda activate mindsdb 2 inside virtual environment run command mitigate dependency issues bash pip install upgrade pip setuptools wheel 3 install mindsdb bash pip install mindsdb 4 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip dependencies dependencies many"
  },
  {
    "filename": "macos.mdx",
    "path": "docs/setup/self-hosted/pip/macos.mdx",
    "chunk_id": 1,
    "chunk_content": "data ml integrations installed default want use data ml integration whose dependencies available default install running command pip install mindsdbhandler_name tip find available handlers herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers tip troubleshooting pip python versions currently mindsdb supports python versions 39x 310x 311x successfully install mindsdb use python 64bit version also make sure python 39 pip 203 check pip python versions running pip version python version commands please note depending environment installed pip python packages might use pip3 instead pip python3x instead py example pip3 install mindsdb instead pip install mindsdb avoid dependency issues install mindsdb virtual environment using pip avoid dependency issues could try install mindsdb anacondahttpswwwanacondacomproductsindividual run installation anaconda prompt avoid common errors mindsdb requires around 3 gb free disk space install dependencies make sure allocate min 3 gb disk space avoid ioerror errno 28 space left device installing mindsdb error anything activate virtual environment mindsdb installed avoid module named mindsdb error users get oserror dlopen library loaded libompdylib please make sure installed libomp run export commands brew install libomp export ldflagslusrlocaloptlibomplib export cppflagsiusrlocaloptlibompinclude issues try replicate issue using docker setupsetupselfhosteddocker also please create issue detailed description mindsdb github repositoryhttpsgithubcommindsdbmindsdbissues help usually review issues respond within hours whats next installed started mindsdb locally"
  },
  {
    "filename": "macos.mdx",
    "path": "docs/setup/self-hosted/pip/macos.mdx",
    "chunk_id": 2,
    "chunk_content": "docker container go ahead find create train model using create modelsqlcreatemodel statement mindsdb sql section youll find comprehensive overview sql syntax offered mindsdb also provide mongoql syntax documented mindsdb mongoql section connect mindsdb different clients including postgresql cliconnectpostgresclient mysql cliconnectmysqlclient check use casesusecasesoverview section follow tutorials cover large language models natural language processing time series classification regression models"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title applications mindsdb sidebartitle overview icon flaskgear mindsdb integrates numerous data sources ai frameworks easily bring data ai together create automate custom workflows mindsdb common use cases include finetuning models chatbots alert systems content generation natural language processing classification regressions forecasting tip section presents common applications mindsdb form tutorials tip cardgroup cols4 card titleautomated finetuning iconrotate hrefusecasesautomated_finetuningcard card titleai agents iconrobot hrefusecasesai_agentscard card titleaipowered data retrieval iconrightfrombracket hrefusecasesaipowered_data_retrievalcard card titledata enrichment icondatabase hrefusecasesdata_enrichmentcard card titlepredictive analytics iconchartsimple hrefusecasespredictive_analyticscard card titleindatabase machine learning icongears hrefusecasesindatabase_mlcard card titleai workflow automation iconmicrochipai hrefusecasesai_workflow_automationcard cardgroup"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/use-cases/automated_finetuning/openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune openai model sidebartitle openai example going teach openai model write mindsdb ai sql queries openai models belong group large language models llms definition pretrained large amounts data however possible finetune models taskspecific dataset defined use case tip openai supports finetuning models listed herehttpsplatformopenaicomdocsguidesfinetuning mindsdb easily finetune openai model making applicable specific use case tip lets create model answer questions mindsdbs custom sql syntax first create openai engine passing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey create model using engine sql create model openai_davinci predict completion using engine openai_engine model_name davinci002 prompt_template return valid sql string following question mindsdb indatabase machine learning prompt check model status command sql describe openai_davinci status complete query predictions sql select prompt completion openai_davinci prompt sql syntax join input data predictions mindsdb machine learning model using max_tokens400 execution get sql prompt completion sql syntax join input data predictions mindsdb machine learning model sql syntax select input_data inner join predictions input_dataid predictionsid followed one mindsdb tutorials youll see syntax provided model exactly expected well finetune model using table stores details mindsdbs custom sql syntax info upload data filehttpsgithubcommindsdbmindsdbblobmaindocsusecasesautomated_finetuningdatacsv mindsdb use finetune model info finetune openai model sql finetune openai_davinci files select"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/use-cases/automated_finetuning/openai.mdx",
    "chunk_id": 1,
    "chunk_content": "prompt completion openai_learninghub_ft finetunesqlapifinetune command creates new version openai_davinci model query available versions sql select models name openai_davinci warning model generated trained active model becomes active completes generating training warning new version status complete active query model expecting accurate output sql select prompt completion openai_davinci prompt sql syntax join input data predictions mindsdb machine learning model using max_tokens400 execution get sql prompt completion sql syntax join input data predictions mindsdb machine learning model select mindsdbmodelsmy_model join mindsdbinput_data_name tip dynamic data gets updated regularly set automated finetuning note data source must contain incremental column timestamp integer mindsdb pick recently added data help last keywordmindsdb_sqlsqlcreatejobslast create schedule job finetune model periodically sql create job automated_finetuning finetune openai_davinci mindsdb select filesopenai_learninghub_ft timestamp last every 1 day select filesopenai_learninghub_ft timestamp last model finetuned newly added data every day every time new data available tip"
  },
  {
    "filename": "regression.mdx",
    "path": "docs/use-cases/automated_finetuning/regression.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune regression model sidebartitle regression example use sample postgresql database connect like sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo first create train model using subset home_rentals data considering properties market less 10 days sql create model mindsdbadjust_home_rentals_model example_db select demo_datahome_rentals days_on_market 10 predict rental_price execution get sql query successfully completed check status using command sql describe model adjust_home_rentals_model status complete query predictions sql select rental_price rental_price_explain mindsdbadjust_home_rentals_model sqft 1000 location great neighborhood berkeley_hills number_of_rooms 2 number_of_bathrooms 1 days_on_market 40 execution get sql rental_price rental_price_explain 2621 predicted_value 2621 confidence 099 anomaly null truth null confidence_lower_bound 2523 confidence_upper_bound 2719 lets adjust model training data consider properties market 10 days sql finetune mindsdbadjust_home_rentals_model example_db select demo_datahome_rentals days_on_market 10 warning model generated trained active model becomes active completes generating training warning check status versions model run command sql select name engine project active version status mindsdbmodels name adjust_home_rentals_model execution get sql name engine project active version status adjust_home_rentals_model lightwood mindsdb false 1 complete adjust_home_rentals_model lightwood mindsdb true 2 complete please note longer property market lower rental price hence expect rental_price prediction lower sql select rental_price rental_price_explain mindsdbadjust_home_rentals_model sqft 1000 location great neighborhood berkeley_hills"
  },
  {
    "filename": "regression.mdx",
    "path": "docs/use-cases/automated_finetuning/regression.mdx",
    "chunk_id": 1,
    "chunk_content": "number_of_rooms 2 number_of_bathrooms 1 days_on_market 40 execution get sql rental_price rental_price_explain 2055 predicted_value 2055 confidence 099 anomaly null truth null confidence_lower_bound 1957 confidence_upper_bound 2153 tip dynamic data gets updated regularly set automated finetuning note data source must contain incremental column timestamp integer mindsdb pick recently added data help last keywordmindsdb_sqlsqlcreatejobslast create schedule job finetune model periodically sql create job automated_finetuning finetune adjust_home_rentals_model mindsdb select example_dbhome_rentals timestamp last every 1 day select example_dbhome_rentals timestamp last model finetuned newly added data every day every time new data available tip"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/use-cases/automated_finetuning/anyscale.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune mistral7b model sidebartitle mistral7b tip follow blog posthttpsmindsdbcomblogfinetuninganaimodelinmindsdbusinganyscaleendpoints comprehensive tutorial finetune mistral 7b model tip p aligncenter img srchttpsdocsgooglecomdrawingsde2pacx1vtta4vbzgbvqs9laqf6i5cyqpvp0trz5nyt6wjzhwiv_tssiwjxllqnmgit0snqadyybsssa3fh1jpubw955h460 p anyscale models belong group large language models llms tip supported models mistral7b llama27b llama213b llama270b code llama tip lets create model answer questions mindsdbs custom sql syntax first create anyscale engine passing anyscale api key sql create ml_engine anyscale_engine anyscale_endpoints using anyscale_endpoints_api_key youranyscaleapikey create model using engine sql create model mymistral7b predict completion using engine anyscale_engine model_name mistralaimistral7binstructv01 prompt_template return valid sql string following question mindsdb indatabase machine learning prompt check model status command sql describe mymistral7b status complete query predictions sql select prompt completion mymistral7b prompt sql syntax join input data predictions mindsdb machine learning model using max_tokens400 execution get sql prompt completion sql syntax join input data predictions mindsdb machine learning model sql syntax select input_data inner join predictions input_dataid predictionsid followed one mindsdb tutorials youll see syntax provided model exactly expected well finetune model using table stores details mindsdbs custom sql syntax info lets connect db hosts table well use finetune model sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data take look finetuning data"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/use-cases/automated_finetuning/anyscale.mdx",
    "chunk_id": 1,
    "chunk_content": "sql select message_id role content example_dbchat_llm_mindsdb_docs limit 5 first rows message_id role content 0 system helpful assistant task answer users question regarding sql syntax supported mindsdb machine learning product training models seamlessly deploying data lives 1 user context mindsdb 1 testing create database 2 assistant create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom output status query successfully completed 3 system helpful assistant task answer users question regarding sql syntax supported mindsdb machine learning product 4 user context mindsdb 2 testing preview available data using select notice formatted series chats conform standard openai chat format every message role content chaining together series messages create conversation info finetune mistral model data like sql finetune mymistral7b example_db select chat_llm_mindsdb_docs finetunesqlapifinetune command creates new version mistralaimistral7binstructv01 model query available versions sql select models name mymistral7b warning model generated trained active model becomes active completes generating training warning new version status complete active query model expecting accurate output sql select prompt completion mymistral7b prompt sql syntax join input data predictions mindsdb machine learning model using max_tokens400 execution get sql prompt completion sql syntax join input data predictions mindsdb machine learning model select mindsdbmodelsmy_model join mindsdbinput_data_name tip dynamic data gets updated regularly"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/use-cases/automated_finetuning/anyscale.mdx",
    "chunk_id": 2,
    "chunk_content": "set automated finetuning note data source must contain incremental column timestamp integer mindsdb pick recently added data help last keywordmindsdb_sqlsqlcreatejobslast create schedule job finetune model periodically sql create job automated_finetuning finetune mymistral7b mindsdb select example_dbchat_llm_mindsdb_docs timestamp last every 1 day select example_dbchat_llm_mindsdb_docs timestamp last model finetuned newly added data every day every time new data available tip"
  },
  {
    "filename": "classification.mdx",
    "path": "docs/use-cases/automated_finetuning/classification.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune classification model sidebartitle classification example use sample postgresql database first create train model using subset customer_churn data considering female customers sql create model mindsdbadjust_customer_churn_model example_db select demo_datacustomer_churn gender female predict churn execution get sql query successfully completed check status using command sql describe model adjust_customer_churn_model status complete query predictions sql select churn churn_explain mindsdbadjust_customer_churn_model seniorcitizen 0 partner yes dependents tenure 1 phoneservice multiplelines phone service internetservice dsl execution get sql churn churn_explain predicted_value confidence 09887640449438202 anomaly null truth null probability_class_no 0934 probability_class_yes 0066 lets adjust model training data also consider male customers sql finetune mindsdbadjust_customer_churn_model example_db select demo_datacustomer_churn gender male warning model generated trained active model becomes active completes generating training warning check status versions model run command sql select name engine project active version status mindsdbmodels name adjust_customer_churn_model execution get sql name engine project active version status adjust_customer_churn_model lightwood mindsdb false 1 complete adjust_customer_churn_model lightwood mindsdb true 2 complete alternatively use describe command sql describe model adjust_customer_churn_model lets query prediction sql select churn churn_explain mindsdbadjust_customer_churn_model seniorcitizen 0 partner yes dependents tenure 1 phoneservice multiplelines phone service internetservice dsl execution get sql churn churn_explain predicted_value confidence 09887640449438202 anomaly null truth null probability_class_no 09294 probability_class_yes 00706 adjusting model significant"
  },
  {
    "filename": "classification.mdx",
    "path": "docs/use-cases/automated_finetuning/classification.mdx",
    "chunk_id": 1,
    "chunk_content": "changes predictions however probability class yes values updated probability yes value increased slightly probability value decreased tip dynamic data gets updated regularly set automated finetuning note data source must contain incremental column timestamp integer mindsdb pick recently added data help last keywordmindsdb_sqlsqlcreatejobslast create schedule job finetune model periodically sql create job automated_finetuning finetune adjust_customer_churn_model mindsdb select example_dbcustomer_churn timestamp last every 1 day select example_dbcustomer_churn timestamp last model finetuned newly added data every day every time new data available tip"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/automated_finetuning/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title automated finetuning sidebartitle overview realworld use cases often deal dynamic data updated regularly mindsdb enables automate finetuningmindsdb_sqlsqlapifinetune ai models keep uptodate accurate possible set jobsmindsdb_sqlsqlcreatejobs trigger finetuning ai models every time new data arrives p aligncenter img srcassetsuse_casesautomated_finetuningjpg p section covers following use cases finetuning large language models finetuning automl models brbr tip available tutorials cardgroup cols4 card titlefinetuning mistral models iconlink hrefusecasesautomated_finetuninganyscalecard card titlefinetuning openai models iconlink hrefusecasesautomated_finetuningopenaicard card titlefinetuning classification models iconlink hrefusecasesautomated_finetuningclassificationcard card titlefinetuning regression models iconlink hrefusecasesautomated_finetuningregressioncard cardgroup tip"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 0,
    "chunk_content": "bring model introduction mindsdb allows integrate machine learning models order model require sort api wrapper two api specifications support mlflowhttpswwwmlfloworg ray servehttpsdocsrayioenlatestserveindexhtml former supports importing already trained models predicting mindsdb later supports training predicting external models order use custom models three mandatory arguments one must past inside using statement urlpredict url call getting predictions model format either mlflow ray_serve dtype_dict json specifying columns expected models respective data types mapping supports data types used lightwoodhttpsmindsdbgithubiolightwoodapidtypehtml automl library theres additional optional argument want train model via mindsdb ray serve urltrain endpoint called train model 1 ray serve 11 simple example logistic regression ray serve simple highthroughput service wrap ml models example train predict external scikitlearn model first lets look actual model wrapped inside class complies requirements python import ray fastapi import request fastapi ray import serve import time import pandas pd import json sklearnlinear_model import logisticregression app fastapi rayinit servestartdetachedtrue async def parse_reqrequest request data await requestjson target datagettarget none di jsonloadsdatadf df pddataframedi return df target servedeploymentroute_prefixmy_model serveingressapp class mymodel appposttrain async def trainself request request df target await parse_reqrequest feature_cols listsetlistdfcolumns settarget selffeature_cols feature_cols x dfloc selffeature_cols listdftarget selfmodel logisticregression selfmodelfitx return status ok apppostpredict async def predictself request request"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 1,
    "chunk_content": "df _ await parse_reqrequest x dfloc selffeature_cols predictions selfmodelpredictx pred_dict prediction floatx x predictions return pred_dict mymodeldeploy true timesleep1 important bits train predict endpoints train endpoint accept two parameters json sent via post df serialized dictionary converted pandas dataframe target name target column predict endpoint needs one parameter df serialized dictionary converted pandas dataframe training endpoints must return json contains keys status set ok predict endpoint must return dictionary containing prediction key storing predictions additional keys returned confidence confidence intervals start rayservewrapped model train using query like one sql create model mindsdbbyom_ray_serve mydb select number_of_rooms initial_price rental_price test_datahome_rentals predict number_of_rooms using urltrain http1270018000my_modeltrain urlpredict http1270018000my_modelpredict dtype_dictnumber_of_rooms categorical initial_price integer rental_price integer formatray_server query predictions usual either conditioning subset input colums sql select byom_ray_serve initial_price3000 rental_price3000 joining batch predictions sql select tbnumber_of_rooms trental_price mydbtest_datahome_rentals join mindsdbbyom_ray_serve tb trental_price 5300 please note model behind reverse proxy eg nginx might increase maximum limit post requests order receive training data mindsdb send much youd like stresstested billion rows 12 example keras nlp model example consider natural language processing nlp taskhttpswwwkagglecomcnlpgettingstarted want train neural network kerashttpskerasio detect tweet related natural disaster fires earthquakes etc please download dataset follow example code model bit complex section 11"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 2,
    "chunk_content": "rules apply create ray server based service wraps around kaggle nlp modelhttpswwwkagglecomshahulesbasicedacleaningandglove trained used predictions python import import time import json import string import requests collections import counter defaultdict import ray ray import serve import gensim import numpy np import pandas pd tqdm import tqdm nltkutil import ngrams nltkcorpus import stopwords nltktokenize import word_tokenize fastapi import request fastapi sklearnmodel_selection import train_test_split sklearnfeature_extractiontext import countvectorizer tensorflowkeraspreprocessingtext import tokenizer tensorflowkeraspreprocessingsequence import pad_sequences tensorflowkerasmodels import sequential tensorflowkeraslayers import embedding lstm dense spatialdropout1d tensorflowkerasinitializers import constant tensorflowkerasoptimizers import adam app fastapi stop setstopwordswordsenglish async def parse_reqrequest request data await requestjson target datagettarget none di jsonloadsdatadf df pddataframedi return df target servedeploymentroute_prefixnlp_kaggle_model serveingressapp class model max_len 100 glove_dim 50 epochs 10 def __init__self selfmodel none appposttrain async def trainself request request df target await parse_reqrequest target_arr dfpoptargetvalues df selfpreprocess_dfdf train_corpus selfcreate_corpusdf selfembedding_dict openglove6b50dtxt r f line f values linesplit word values0 vectors npasarrayvalues1 float32 selfembedding_dictword vectors fclose selftokenizer_obj tokenizer selftokenizer_objfit_on_textstrain_corpus sequences selftokenizer_objtexts_to_sequencestrain_corpus tweet_pad pad_sequencessequences maxlenself__class__max_len truncatingpost paddingpost df tweet_paddfshape0 word_index selftokenizer_objword_index num_words lenword_index 1 embedding_matrix npzerosnum_words self__class__glove_dim word tqdmword_indexitems num_words continue emb_vec selfembedding_dictgetword emb_vec none embedding_matrixi emb_vec selfmodel sequential embedding embeddingnum_words self__class__glove_dim embeddings_initializerconstantembedding_matrix input_lengthself__class__max_len trainablefalse selfmodeladdembedding selfmodeladdspatialdropout1d02 selfmodeladdlstm64 dropout02 recurrent_dropout02 selfmodeladddense1 activationsigmoid optimzer adamlearning_rate1e5 selfmodelcompilelossbinary_crossentropy"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 3,
    "chunk_content": "optimizeroptimzer metricsaccuracy x_train x_test y_train y_test train_test_splitdf target_arr test_size015 selfmodelfitx_train y_train batch_size4 epochsself__class__epochs validation_datax_test y_test verbose2 return status ok apppostpredict async def predictself request request df _ await parse_reqrequest df selfpreprocess_dfdf test_corpus selfcreate_corpusdf sequences selftokenizer_objtexts_to_sequencestest_corpus tweet_pad pad_sequencessequences maxlenself__class__max_len truncatingpost paddingpost df tweet_paddfshape0 y_pre selfmodelpredictdf y_pre nproundy_preastypeintflattentolist sub pddataframetarget y_pre pred_dict prediction floatx x subtargetvalues return pred_dict def preprocess_dfself df df dftext dftext dftextapplylambda x selfremove_urlx dftext dftextapplylambda x selfremove_htmlx dftext dftextapplylambda x selfremove_emojix dftext dftextapplylambda x selfremove_punctx return df def remove_urlself text url recompilerhttpsswwws return urlsubr text def remove_htmlself text html recompiler return htmlsubr text def remove_punctself text table strmaketrans stringpunctuation return texttranslatetable def remove_emojiself text emoji_pattern recompile uu0001f600u0001f64f emoticons uu0001f300u0001f5ff symbols pictographs uu0001f680u0001f6ff transport map symbols uu0001f1e0u0001f1ff flags ios uu00002702u000027b0 uu000024c2u0001f251 flagsreunicode return emoji_patternsubr text def create_corpusself df corpus tweet tqdmdftext words wordlower word word_tokenizetweet wordisalpha 1 word stop corpusappendwords return corpus __name__ __main__ rayinit servestartdetachedtrue modeldeploy true timesleep1 need access training data well create table called nlp_kaggle_train load datasethttpswwwkagglecomcnlpgettingstarted original model uses ingest table following schema sql id int keyword varchar255 location varchar255 text varchar5000 target int note specifics schema ingest csv vary depending database next register train custom model using following query sql create model mindsdbbyom_ray_serve_nlp maria"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 4,
    "chunk_content": "select text target testnlp_kaggle_train predict target using urltrain http1270018000nlp_kaggle_modeltrain urlpredict http1270018000nlp_kaggle_modelpredict dtype_dicttext rich_text target integer formatray_server training take given model neural network rather simple logistic regression check status query select mindsdbpredictors name byom_ray_serve_nlp much like youd normal mindsdb predictor predictors status becomes trained query predictions usual sql select mindsdbbyom_ray_serve_nlp textthe tsunami coming seek high ground would hopefully output 1 alternatively try tweet expect 0 output sql select mindsdbbyom_ray_serve_nlp textthis lovely dear friend results match example could help train model longer amount epochs 2 mlflow 21 simple example logistic regression mlflow tool use train serve models among features like organizing experiments tracking metrics etc given way train mlflowwrapped model using api train models outside mindsdb pulling data manually ie script ideally using mlflow run experiment first step would create script train model save using one saving methods mlflow exposes example use model simple tutorialhttpsgithubcommlflowmlflowsavingandservingmodels method mlflowsklearnlog_model herehttpsgithubcommlflowmlflowblob9781af9c0898827bf616a8f159168477a69036ddexamplessklearn_logistic_regressiontrainpyl15 given model built scikitlearn trained need make sure model served listening input url choice note mean model run different machine one executing mindsdb lets assume url httplocalhost5000invocations means would execute following command terminal directory model stored mlflow models serve modeluri runsrunidmodel runid given output command python trainpy used actually training model next going bring"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 5,
    "chunk_content": "model mindsdb sql create model mindsdbbyom_mlflow predict 1 1 target column name using urlpredicthttplocalhost5000invocations formatmlflow data_dtype0 integer 1 integer run predictions usual using statement joining data table appropriate schema sql select 1 byom_mlflow 02 22 advanced example keras nlp model use case section 12 sure download dataset reproduce steps case take look best practices model needs custom data preprocessing code realistically fairly common key difference need use mlflowpyfunc module 1 save model using mlflowpyfuncsave_model 2 subclass mlflowpyfuncpythonmodel wrap model mlflowcompatible way enable custom inference logic called saving model script train model find final section 22 call end actually use mlflow save every produced artifact python mlflowpyfuncsave_model pathnlp_kaggle python_modelmodel conda_envconda_env artifactsartifacts artifacts dictionary expected produced outputs running training phase case want model tokenizer preprocess input text hand conda_env specifies environment model executed served selfcontained conda environment include required packages dependencies example look like python accessible inside model wrapper artifacts model model_path tokenizer_path tokenizer_path specs environment created serving model conda_env name nlp_keras_env channels defaults dependencies python39 pip pip mlflow tensorflow cloudpickle nltk pandas numpy scikitlearn tqdm finally actually store model need provide wrapper class 1 load produced artifacts accessible context 2 implement required inference logic python class modelmlflowpyfuncpythonmodel def load_contextself context use"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 6,
    "chunk_content": "paths context load everything selfmodel_path contextartifactsmodel selfmodel load_modelselfmodel_path opencontextartifactstokenizer_path rb f selftokenizer pickleloadf def predictself context model_input preprocess input tokenize pad call model df preprocess_dfmodel_input corpus create_corpusdf sequences selftokenizertexts_to_sequencescorpus tweet_pad pad_sequencessequences maxlenmax_len truncatingpost paddingpost df tweet_paddfshape0 y_pre selfmodelpredictdf y_pre nproundy_preastypeintflattentolist return listy_pre see loading multiple artifacts using guarantee input data format used training ideally would abstract even single preprocess method called training time inference time finally serving simple go directory called script execute mlflow models serve modeluri nlp_kaggle point rest essentially previous example link mlflow model sql statements sql create model mindsdbbyom_mlflow_nlp predict target using urlpredicthttplocalhost5000invocations formatmlflow dtype_dicttext rich text target binary get predictions directly pass input data using clause sql select target mindsdbbyom_mlflow_nlp textthe tsunami coming seek high ground join data table ensure table actually exists database belongs connected mindsdb instance details refer steps ray serve example section 12 sql select tatext tbtarget predicted db_byomtestnlp_kaggle_test ta join mindsdbbyom_mlflow_nlp tb full script finally reference heres full script trains saves model model exactly section 12 may seem familiar python import import pickle import string import mlflowpyfunc import nltk import tqdm import sklearn import tensorflow import cloudpickle import numpy np import pandas pd nltkcorpus import stopwords nltktokenize import word_tokenize sklearnmodel_selection import train_test_split"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 7,
    "chunk_content": "tensorflowkerasinitializers import constant tensorflowkeraslayers import embedding lstm dense spatialdropout1d tensorflowkerasmodels import sequential tensorflowkerasoptimizers import adam tensorflowkeraspreprocessingsequence import pad_sequences tensorflowkeraspreprocessingtext import tokenizer tensorflowkerasmodels import load_model stop setstopwordswordsenglish max_len 100 glove_dim 50 epochs 10 def preprocess_dfdf df dftext funcs remove_url remove_html remove_emoji remove_punct fn funcs dftext dftextapplylambda x fnx return df def remove_urltext url recompilerhttpsswwws return urlsubr text def remove_htmltext html recompiler return htmlsubr text def remove_puncttext table strmaketrans stringpunctuation return texttranslatetable def remove_emojitext emoji_pattern recompile uu0001f600u0001f64f emoticons uu0001f300u0001f5ff symbols pictographs uu0001f680u0001f6ff transport map symbols uu0001f1e0u0001f1ff flags ios uu00002702u000027b0 uu000024c2u0001f251 flagsreunicode return emoji_patternsubr text def create_corpusdf corpus tweet tqdmtqdmdftext words wordlower word word_tokenizetweet wordisalpha 1 word stop corpusappendwords return corpus class modelmlflowpyfuncpythonmodel def load_contextself context selfmodel_path contextartifactsmodel opencontextartifactstokenizer_path rb f selftokenizer pickleloadf selfmodel load_modelselfmodel_path def predictself context model_input df preprocess_dfmodel_input corpus create_corpusdf sequences selftokenizertexts_to_sequencescorpus tweet_pad pad_sequencessequences maxlenmax_len truncatingpost paddingpost df tweet_paddfshape0 y_pre selfmodelpredictdf y_pre nproundy_preastypeintflattentolist return listy_pre __name__ __main__ train_model true model_path tokenizer_path tokenizerpkl run_name test_run mlflow_pyfunc_model_path nlp_kaggle mlflowset_tracking_urisqlitemlflowdb train_model preprocess data df pdread_csvtraincsv target dftarget target_arr targetvalues df preprocess_dfdf train_corpus create_corpusdf load embeddings embedding_dict openglove6b50dtxt r f line f values linesplit word values0 vectors npasarrayvalues1 float32 embedding_dictword vectors fclose generate save tokenizer tokenizer_obj tokenizer tokenizer_objfit_on_textstrain_corpus opentokenizer_path wb f pickledumptokenizer_obj f tokenize"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/use-cases/in-database_ml/byom.mdx",
    "chunk_id": 8,
    "chunk_content": "pad sequences tokenizer_objtexts_to_sequencestrain_corpus tweet_pad pad_sequencessequences maxlenmax_len truncatingpost paddingpost df tweet_paddfshape0 word_index tokenizer_objword_index num_words lenword_index 1 embedding_matrix npzerosnum_words glove_dim fill embedding matrix word tqdmtqdmword_indexitems num_words continue emb_vec embedding_dictgetword emb_vec none embedding_matrixi emb_vec x_train x_test y_train y_test train_test_splitdf target_arr test_size015 generate model model sequential embedding embeddingnum_words glove_dim embeddings_initializerconstantembedding_matrix input_lengthmax_len trainablefalse modeladdembedding modeladdspatialdropout1d02 modeladdlstm64 dropout02 recurrent_dropout02 modeladddense1 activationsigmoid optimzer adamlearning_rate1e5 modelcompilelossbinary_crossentropy optimizeroptimzer metricsaccuracy train save modelfitx_train y_train batch_size4 epochsepochs validation_datax_test y_test verbose2 modelsavemodel_path save mlflow format artifacts model model_path tokenizer_path tokenizer_path conda_env channels defaults dependencies python39 pip pip mlflow tensorflowformattensorflow__version__ cloudpickleformatcloudpickle__version__ nltkformatnltk__version__ pandasformatpd__version__ numpyformatnp__version__ scikitlearnformatsklearn__version__ tqdmformattqdm__version__ name nlp_keras_env save register mlflow model mlflowstart_runrun_namerun_name run mlflowpyfuncsave_model pathmlflow_pyfunc_model_path python_modelmodel conda_envconda_env artifactsartifacts result mlflowregister_model frunsruninforun_idmlflow_pyfunc_model_path fmlflow_pyfunc_model_path"
  },
  {
    "filename": "home-rentals.mdx",
    "path": "docs/use-cases/in-database_ml/home-rentals.mdx",
    "chunk_id": 0,
    "chunk_content": "title predict home rental prices mindsdb sidebartitle home rentals tutorial well use regression model predict home rental prices note tutorial uses lightwood integration requires mindsdbmindsdblightwood docker image learn heresetupselfhosteddockerinstallmindsdb note connect data source start connecting demo database mindsdb using create database statement sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data lets preview data used train model sql select example_dbhome_rentals limit 10 deploy train ml model lets create train machine learning model going use create model statement specify query train want predict sql create model mindsdbhome_rentals_model example_db select home_rentals predict rental_price may take couple minutes training complete monitor status model sql describe home_rentals_model make predictions models status complete make predictions querying model sql select rental_price rental_price_explain mindsdbhome_rentals_model sqft 823 locationgood neighborhooddowntown days_on_market10 also make batch predictions joining data table model sql select trental_price real_price mrental_price predicted_price tnumber_of_rooms tnumber_of_bathrooms tsqft tlocation tdays_on_market example_dbhome_rentals join mindsdbhome_rentals_model limit 100 automate continuous improvement model take even mindsdb includes powerful automation features called jobs allow us automate queries mindsdb handy production aiml systems require automation logic help work use create job statement create job lets use job retrain model every two days like might production"
  },
  {
    "filename": "home-rentals.mdx",
    "path": "docs/use-cases/in-database_ml/home-rentals.mdx",
    "chunk_id": 1,
    "chunk_content": "retrain model improve predictions every time either new data new mindsdb version available want retrain model considering new data go finetuning sql create job improve_model retrain mindsdbhome_rentals_model example_db select home_rentals every 2 days select example_dbhome_rentals created_at last job execute every 2 days new data available home_rentals table learn last keywordmindsdb_sqlsqlcreatejobslast created endtoend automated production ml system short minutes"
  },
  {
    "filename": "ai-tables.mdx",
    "path": "docs/use-cases/in-database_ml/ai-tables.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai tables sidebartitle ai tables lets consider following income_table table stores income debt values sql select income debt income_table execution get sql incomedebt 60000 20000 80000 25100 10000030040 12000036010 simple visualization data present income_table table follows income vs debtassetssqlincome_vs_debtpng querying income table get debt value particular income value results following sql select income debt income_table income 80000 execution get sql incomedebt 80000 25100 get income vs debt chartassetssqlincome_vs_debt_known_valuepng happens querying table income value present sql select income debt income_table income 90000 execution get sql empty set 000 sec clause condition fulfilled rows value returned income vs debt queryassetssqlincome_vs_debt_unknown_valuepng table doesnt exact match query returns empty set null value ai tables come play lets create debt_model model allows us approximate debt value income value train debt_model model using data income_table table sql create model mindsdbdebt_model income_table predict debt execution get sql query ok 0 rows affected xxxx sec mindsdb provides create modelsqlcreatemodel statement execution statement predictive model works background automatically creating vector representation data visualized follows income vs debt modelassetssqlincome_vs_debt_predictorpng please note debt_model ai table lets look debt value random income value get approximated debt value query mindsdbdebt_model model instead income_table table sql select income debt mindsdbdebt_model income 90000 execution"
  },
  {
    "filename": "ai-tables.mdx",
    "path": "docs/use-cases/in-database_ml/ai-tables.mdx",
    "chunk_id": 1,
    "chunk_content": "get sql incomedebt 90000 27820 looks income vs debt modelassetssqlincome_vs_debt_predictionpng"
  },
  {
    "filename": "customer-churn.mdx",
    "path": "docs/use-cases/in-database_ml/customer-churn.mdx",
    "chunk_id": 0,
    "chunk_content": "title predict customer churn mindsdb sidebartitle customer churn note tutorial uses lightwood integration requires mindsdbmindsdblightwood docker image learn heresetupselfhosteddockerinstallmindsdb note introduction tutorial well create train machine learning model call ai table predictor querying model well predict probability churn new customers telecoms company install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop lets get started data setup connecting data couple ways get data follow tutorial tabs tab titleconnecting database connect demo database weve prepared contains data used throughout tutorial example_dbdemo_datacustomer_churn table sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo run queries directly demo database lets preview data well use train predictor sql select example_dbdemo_datacustomer_churn limit 10 tab tab titleconnecting file download csv data file herehttpsgithubcommindsdbmindsdbexamplesblobmasterclassicscustomer_churnraw_datawa_fnusec_telcocustomerchurncsv upload via mindsdb sql editorconnectmindsdb_editor follow guidesqlcreatefile find upload file mindsdb run queries directly file table lets preview data well use train predictor sql select fileschurn limit 10 tab tabs warning pay attention queries well use fileschurn file table make sure replace example_dbdemo_datacustomer_churn connect data database warning understanding data use customer churn dataset row one customer predict whether customer going stop using company products sample data stored fileschurn table sql customeridgenderseniorcitizenpartnerdependentstenurephoneservicemultiplelines internetserviceonlinesecurityonlinebackupdeviceprotectiontechsupportstreamingtvstreamingmoviescontract paperlessbillingpaymentmethod monthlychargestotalchargeschurn 7590vhvegfemale0 yes 1 phone servicedsl"
  },
  {
    "filename": "customer-churn.mdx",
    "path": "docs/use-cases/in-database_ml/customer-churn.mdx",
    "chunk_id": 1,
    "chunk_content": "yes monthtomonthyes electronic check 2985 2985 5575gnvdemale 0 34 yes dsl yes yes one year mailed check 5695 18895 3668qpybkmale 0 2 yes dsl yes yes monthtomonthyes mailed check 5385 10815 yes 7795cfocwmale 0 45 phone servicedsl yes yes yes one year bank transfer automatic423 184075 9237hqitufemale0 2 yes fiber optic monthtomonthyes electronic check 707 15165 yes column description data type usage customerid identification number customer character varying feature gender gender customer character varying feature seniorcitizen indicates whether customer senior citizen 1 0 integer feature partner indicates whether customer partner yes character varying feature dependents indicates whether customer dependents yes character varying feature tenure number months customer staying company integer feature phoneservice indicates whether customer phone service yes character varying feature multiplelines indicates whether customer multiple lines yes phone service character varying feature internetservice customers internet service provider dsl fiber optic character varying feature onlinesecurity indicates whether customer online security yes internet service character varying feature onlinebackup indicates whether customer online backup yes internet service character varying feature deviceprotection indicates whether customer device protection yes internet service character varying feature techsupport indicates whether customer tech support yes internet service character varying feature streamingtv indicates whether customer streaming tv yes internet"
  },
  {
    "filename": "customer-churn.mdx",
    "path": "docs/use-cases/in-database_ml/customer-churn.mdx",
    "chunk_id": 2,
    "chunk_content": "service character varying feature streamingmovies indicates whether customer streaming movies yes internet service character varying feature contract contract term customer monthtomonth one year two year character varying feature paperlessbilling indicates whether customer paperless billing yes character varying feature paymentmethod customers payment method electronic check mailed check bank transfer automatic credit card automatic character varying feature monthlycharges monthly charge amount money feature totalcharges total amount charged customer money feature churn indicates whether customer churned yes character varying label info labels features label column whose values predicted variable simple linear regression feature column used train model x variable simple linear regression info training predictor lets create train machine learning model use create modelsqlcreatemodel statement specify input columns used train features want predict labels sql create model mindsdbcustomer_churn_predictor files select churn predict churn use columns features except churn column whose values predicted status predictor predictor may take couple minutes training complete monitor status predictor using sql command sql describe customer_churn_predictor run right creating predictor get output sql status generating bit later output sql status training last output sql status complete status predictor says complete start making predictions making predictions making single prediction make predictions querying predictor table selectsqlapiselect statement lets make predictions label"
  },
  {
    "filename": "customer-churn.mdx",
    "path": "docs/use-cases/in-database_ml/customer-churn.mdx",
    "chunk_id": 3,
    "chunk_content": "based chosen features sql select churn churn_confidence churn_explain mindsdbcustomer_churn_predictor seniorcitizen0 partneryes dependentsno tenure1 phoneserviceno multiplelinesno phone service internetservicedsl execution get sql churn churn_confidence churn_explain yes 07752808988764045 predicted_value yes confidence 07752808988764045 anomaly null truth null probability_class_no 04756 probability_class_yes 05244 get accurate predictions provide much data possible clause lets run another query sql select churn churn_confidence churn_explain mindsdbcustomer_churn_predictor seniorcitizen0 partneryes dependentsno tenure1 phoneserviceno multiplelinesno phone service internetservicedsl contractmonthtomonth monthlycharges2985 totalcharges2985 onlinebackupyes onlinesecurityno deviceprotectionno techsupportno streamingtvno streamingmoviesno paperlessbillingyes paymentmethodelectronic check execution get sql churn churn_confidence churn_explain yes 08202247191011236 predicted_value yes confidence 08202247191011236 anomaly null truth null probability_class_no 04098 probability_class_yes 05902 mindsdb predicted probability customer churning confidence around 82 previous query predicted confidence around 79 providing data improved confidence level predictions making batch predictions also make bulk predictions joining data table predictor using joinsqlapijoin sql select tcustomerid tcontract tmonthlycharges mchurn fileschurn join mindsdbcustomer_churn_predictor limit 100 execution get sql customerid contract monthlycharges churn 7590vhveg monthtomonth 2985 yes 5575gnvde one year 5695 3668qpybk monthtomonth 5385 yes 7795cfocw one year 423 9237hqitu monthtomonth 707 yes whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star"
  },
  {
    "filename": "customer-churn.mdx",
    "path": "docs/use-cases/in-database_ml/customer-churn.mdx",
    "chunk_id": 4,
    "chunk_content": "herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/in-database_ml/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title indatabase machine learning sidebartitle overview mindsdb brings aiml database mitigates need creating maintaining data pipelines access various aiml modelsintegrationsaioverview directly data sourceintegrationsdataoverview p aligncenter img srcassetsuse_casesindatabase_mljpg p section covers following use cases automated classification models automated regression models bring model byom mindsdb brbr tip available tutorials cardgroup cols4 card titleclassification customer churn iconlink hrefusecasesindatabase_mlcustomerchurncard card titleregression home rental prices iconlink hrefusecasesindatabase_mlhomerentalscard card titlebyom iconlink hrefintegrationsaienginesbyomcard cardgroup tip"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 0,
    "chunk_content": "using mindsdb machine learning solve realworld time series problem lets use powerful ai tables realworld scenario familiar aitables learn heresqltutorialsaitables imagine data analyst chicago transit authority every day need optimize number buses per route avoid overcrowded empty buses need machine learning forecast number rides per bus per route time day data looks like table route_id timestamp number rides daytype w weekend income vs debt modelassetssqltutorialssnowflakesuperset8multivariate_problemjpg difficult machine learning problem common databases timestamp indicates dealing timeseries problem data complicated type day daytype row contains called multivariate additionally highcardinality route multiple row entries different timestamps rides day types lets see use machine learning mindsdb optimize number buses per route visualize results set mindsdb first things first need connect database mindsdb one easy ways create mindsdb cloudsetupcloud account prefer deploy mindsdb locally please refer installation instructions via dockersetupselfhosteddocker pypisetupselfhostedpipwindows account created connect snowflake using standard parameters like database name case chicago transit authority host port username password etc mindsdb connectassetssqltutorialssnowflakesuperset9connect_to_mindsdbpng connect mindsdb data model training mindsdb works mysql wire protocol therefore connect using mysql client well use dbeaver database client see snowflake databases connected dbeaver connectassetssqltutorialssnowflakesuperset10dbeaver connectionpng step 1 getting training data start getting training data database connected mindsdb cloud account always good"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 1,
    "chunk_content": "first make sure databases present connections correct sql show databases show dbsassetssqltutorialssnowflakesuperset12show_dtabasespng mindsdb comes builtin databases follows information_schema stores information mindsdb mindsdb stores metadata predictors allows access created predictors tables datasource connecting data uploading files snf database database chicago transit authority connected provides us training data lets check sql select chicago_transit_authoritypubliccta_bus_rides_latest limit 100 show dbsassetssqltutorialssnowflakesuperset13info_schemapng training data consists number rides per bus route day example 20010703 7354 rides bus route 3 download dataset herehttpsgithubcommindsdbbenchmarksblobmainbenchmarksdatasetschicago_transit_tscta_2019_2020csv execute sql commands along tutorial step 2 training predictive model lets move next step training predictive model well use mindsdb database sql use mindsdb show tables show dbsassetssqltutorialssnowflakesuperset14tablepng mindsdb database comes predictors commands tables predictors table lets us see status predictive models example assuming already trained predictive model forecasting number rides well see following sql select name status mindsdbpredictors show statusassetssqltutorialssnowflakesuperset15querypng process training predictive model using mindsdb simple creating view table sql create model mindsdbrides_forecaster_demo snf select route rides date chicago_transit_authoritypubliccta_bus_rides_latest date 20200101 predict rides order date group route window 10 horizon 7 lets discuss statement create predictor table using create model statement specifying database training data comes code yellow selects filtered training data use predict keyword define column whose data want forecast next standard sql"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 2,
    "chunk_content": "clauses order group window horizon use order clause date column argument emphasize deal timeseries problem order rows date group clause divides data partitions relates particular bus route take account last ten rows every given prediction hence use window 10 prepare forecast number bus rides next week define horizon 7 execute create model statement wait predictive model complete mindsdbpredictors table stores name rides_forecaster_demo status training predictive model ready status changes complete step 3 getting forecasts ready go last step ie using predictive model get future data one way query rides_forecaster_demo predictive model directly another way join predictive model table table historical data querying consider timeseries problem therefore better join predictive model table table historical data sql select tbroute tbrides predicted_rides snfpubliccta_bus_rides_latest ta join mindsdbrides_forecaster_demo tb taroute 171 tadate latest limit 7 lets analyze join table stores historical data ie snfpubliccta_bus_rides_latest predictive model table ie mindsdbrides_forecaster_demo queried information route predicted number rides per route usage condition tadate latest provided mindsdb ensures get future number rides per route lets run query forecast number rides route 171 next seven days predictive queryassetssqltutorialssnowflakesuperset16predictive_querypng know number rides route 171 next seven days could way routes thanks special sql syntax includes create model predict latest mindsdb makes"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 3,
    "chunk_content": "straightforward run predictors chosen data lets visualize predictions visualizing results using apache superset apache superset modern opensource data exploration visualization platform designed data personas organization superset ships powerful sql editor nocode chart builder experience superset ships support sql databases box 50 visualization types connect snowflake database mindsdb database snowflake connection within upon starting superset workspace earlier defined database connection ready use access chicago transit authority data well predictions made mindsdb visualizing data two data sets relevant visualization stops_by_route forecasts data sets stops_by_route data set contains exact location bus stop bus route forecasts data set stores actual predicted number rides confidence interval lower upper bounds prediction per route timestamp superset lets us visualize stops_by_route data set follows visualize queryassetssqltutorialssnowflakesuperset17stops_by_route_supersetjpg every bus route different color also volatility associated bus route lets publish chart new dashboard clicking save button switch save tab type routes dashboard add dashboard field lets craft timeseries line chart visualize actual vs predicted riders lets look chart presents actual number bus riders blue predicted number bus rides purple predictive queryassetssqltutorialssnowflakesuperset18timeseries_chartjpg predictions made mindsdb closely resemble actual data except short time march 2020 largescale lockdowns took place see sudden drop number bus rides mindsdb took time cope new reality adjust"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 4,
    "chunk_content": "predictions lastly lets add data zoom chart endusers zoom specific date ranges click customize tab click data zoom enable click save button publish routes dashboard lets head dashboard customize make dynamic explorable click dashboards top nav bar select routes dashboard list dashboards rearrange chart positions clicking pencil icon dragging corners chart objects clicking save timeseries chartassetssqltutorialssnowflakesuperset19timeseries2jpg lets add dashboard filters dashboard dashboard consumers filter charts specific bus routes volatility values click right arrow pop open filter tray select pencil icon start editing dashboards filters create following filters appropriate filter names value filter route column forecasts table numerical range filter volatility column stops_by_route table click save publish filters filtersassetssqltutorialssnowflakesuperset20filters1jpg filtersassetssqltutorialssnowflakesuperset20filters2jpg lets give filters test ride use routes filter show information routes 1 100 1001 timeseries chartassetssqltutorialssnowflakesuperset21graphjpg could zoom see time first largescale lockdowns march 2020 particular routes predictions made mindsdb far timeseries chartassetssqltutorialssnowflakesuperset22graphjpg lets use volatility filter view routes volatility values greater 55 timeseries chartassetssqltutorialssnowflakesuperset23graphjpg conclusions powerful forecasting mindsdb database superset combination mindsdb database covers phases ml lifecycle superset helps visualize data form diagrams charts dashboards timeseries chartassetssqltutorialssnowflakesuperset24mindsdb_mlworkflowpng mindsdb provides easytouse predictive models ai tables create predictive models using sql statements feeding input data also query way query table easiest way"
  },
  {
    "filename": "mindsdb-superset-snowflake.mdx",
    "path": "docs/use-cases/in-database_ml/mindsdb-superset-snowflake.mdx",
    "chunk_id": 5,
    "chunk_content": "get started superset free tier preset cloudhttpspresetioproduct hasslefree fully hosted cloud service superset encourage try predictions data please sign free mindsdb cloud accounthttpscloudmindsdbcomsignup need help mindsdb feel free ask slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions communities whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb sign free mindsdb accounthttpscloudmindsdbcomregister engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "recommenders.mdx",
    "path": "docs/use-cases/ai-powered_data_retrieval/recommenders.mdx",
    "chunk_id": 0,
    "chunk_content": "title recommender models sidebartitle recommender models currently two recommender models available mindsdb check examples popularity recommenderintegrationsaienginespopularityrecommender lightfmintegrationsaiengineslightfm"
  },
  {
    "filename": "embedding-model.mdx",
    "path": "docs/use-cases/ai-powered_data_retrieval/embedding-model.mdx",
    "chunk_id": 0,
    "chunk_content": "title lightwood embedding model sidebartitle embedding model tutorial uses lightwood handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightwood_handler create embedding model tutorial following example shows create embedding model using lightwood engine start creating engine lightwood handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightwood_handler sql create ml_engine lightwood lightwood verify engine created successfully sql show ml_engines namelightwood tip connect sample database use training model sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data tip create model using engine sql create model home_rentals_model_embeddings example_db select demo_datahome_rentals predict rental_price using problem_definitionembedding_only true check status model sql describe home_rentals_model_embeddings use model predict home rental prices specific criteria get predictions form embeddings sql select rental_price rental_price_explain home_rentals_model_embeddings sqft 823 locationgood neighborhooddowntown days_on_market10 using return_embedding true output sql rental_price rental_price_explain 167129564285278321247057318687439001000001000123025851249694824056294268369674680100000007540000081062317033333334326744080354838699102401730958333313465118407833333611488342025 predicted_value 10 6712956428527832 1247057318687439 00 00 10 00 00 00 00 00 10 00 00 00 10 23025851249694824 05629426836967468 00 10 00 00 00 00 00 00 07540000081062317 03333333432674408 035483869910240173 09583333134651184 07833333611488342 025 confidence null anomaly null truth null"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/ai-powered_data_retrieval/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title aipowered data retrieval sidebartitle overview mindsdb facilitates aipowered search data form documents websites databases search retrieve data easily using ai models also create recommendation systems based historical data p aligncenter img srcassetsuse_casesaipowered_data_retrievaljpg p section covers following use cases semantic search embeddings models recommenders brbr tip available tutorials cardgroup cols4 card titleembeddings models iconlink hrefusecasesaipowered_data_retrievalembeddingmodelcard card titlerecommenders iconlink hrefusecasesaipowered_data_retrievalrecommenderscard cardgroup tip"
  },
  {
    "filename": "create-chatbot.mdx",
    "path": "docs/use-cases/ai_agents/create-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title build chatbot text2sql skill sidebartitle chatbot text2sql skill mindsdb provides create chatbot statement lets customize chatbot ai model data source choice follow tutorial learn build chatbot text2sql skill create chatbot statement requires following components 1 chat app connection chat app slackintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp ms teamsintegrationsappintegrationsmicrosoftteams 2 ai agent ai agent comes ai model trained provided training data learn ai agents hereagentsagent tip learn chatbots hereagentschatbot tip lets go getting components ready chatbot components chat app use create database statement connect chat app mindsdb tip want use slack follow linkintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp setup slack app generate required tokens connect mindsdb want use ms teams follow linkintegrationsappintegrationsmicrosoftteams generate required tokens connect mindsdb tip ai agent start creating deploying model note havent created langchain engine use create ml_engine statement explained hereintegrationsaiengineslangchainaiengine note sql create model my_model predict answer using engine langchain input_column question openai_api_key yourmodelapikey choose one openai openai_api_key anthropic anthropic_api_key model_namegpt4 optional model name openai anthropic mode conversational user_column question assistant_column answer max_tokens100 temperature0 verbosetrue prompt_templateanswer user input helpful way command check status sql describe my_model status read complete proceeding next step create one skills ai agent create text2sql skill sql create skill text_to_sql_skill using type text2sql database example_db data source must connected mindsdb create"
  },
  {
    "filename": "create-chatbot.mdx",
    "path": "docs/use-cases/ai_agents/create-chatbot.mdx",
    "chunk_id": 1,
    "chunk_content": "database statement tables sales_data table comes connected example_db data source description sales data includes stores sold products sale details skill enables model answer questions data sales_data table lets create ai agent using model skill sql create agent support_agent using model my_model created create model skills text_to_sql_skill created create skill create chatbot components ready lets proceed creating chatbot sql create chatbot my_chatbot using database chat_app parameters stores connection chat app like slack ms teams agent support_agent parameter stores agent name create create agent is_running true parameter optional set true default meaning chatbot running database parameter stores connection chat app agent parameter stores ai agent created passing model training data query chatbot using query sql select chatbots go slack ms teams chat chatbot created mindsdb"
  },
  {
    "filename": "build_ai_agents.mdx",
    "path": "docs/use-cases/ai_agents/build_ai_agents.mdx",
    "chunk_id": 0,
    "chunk_content": "title build ai agent mindsdb sidebartitle build ai agent mindsdb provides custom syntax build ai agents comprises ai model augmented users data access ai agents connected chat interface like slack ms teams create chatbots see details following link agentsmindsdb_sqlagentsagent link chatbotsmindsdb_sqlagentschatbot stepbystep tutorial tutorial demonstrates build ai agents mindsdb using mindsdb sql editor also accomplished apisrestagentscreateagent python sdksdkspythonagents lets list steps required build ai agent steps step titlecreate conversational model create conversational model using langchain integrationintegrationsaiengineslangchain step step titlecreate skills create one skills assigned agent note skills store data passed agent required connect users data mindsdb creating skills step step titlecreate ai agent create ai agent providing conversational model set skills step step titlecreate chatbot optionally connect agent chat interface create chatbot step steps following sections walk process building ai agent step 1 create conversational model use create model statement create conversational model required adjust parameters prompts fit use case sql create model conversational_model predict answer using engine langchain openai_api_key your_openai_api_key_here model_name gpt4 mode conversational user_column question assistant_column answer max_tokens 100 temperature 0 verbose true prompt_template answer user input helpful way ensure model status reads complete using command sql describe conversational_model learn models created langchainintegrationsaiengineslangchain step 2 create skills skill"
  },
  {
    "filename": "build_ai_agents.mdx",
    "path": "docs/use-cases/ai_agents/build_ai_agents.mdx",
    "chunk_id": 1,
    "chunk_content": "essentially users data fed model model answer questions users data first connect database mindsdb sample database used sql create database datasource engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data create skill using connected data sql create skill text2sql_skill using type text2sql database datasource connect database create database pass name tables car_sales list tables made accessible agent description car sales data note two types skills texttosql knowledge bases learn skills heremindsdb_sqlagentsagentcreateskills verify skill created successully using command sql show skills step 3 create ai agent conversational model skill ready lets create ai agent sql create agent ai_agent using model conversational_model skills text2sql_skill verify agent created successully using command sql show agents point query agent ask questions data sql select question answer ai_agent question many cars sold 2016 step 4 create chatbot optionally create chatbot connecitng ai agent chat interface first connect chat interface mindsdb slack connection made sql create database mindsdb_slack engine slack parameters token xoxbxxx app_token xappxxx follow instructions connect slack mindsdbintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp use case create chatbot providing ai agent slack connection sql create chatbot ai_chatbot using database mindsdb_slack connect chat interface create database agent ai_agent create agent create agent verify chatbot running using"
  },
  {
    "filename": "build_ai_agents.mdx",
    "path": "docs/use-cases/ai_agents/build_ai_agents.mdx",
    "chunk_id": 2,
    "chunk_content": "command sql show chatbots go ahead chat ai agent via slack"
  },
  {
    "filename": "chatbots_agents.mdx",
    "path": "docs/use-cases/ai_agents/chatbots_agents.mdx",
    "chunk_id": 0,
    "chunk_content": "title agents chatbots sidebartitle agents chatbots mindsdb provides custom syntax create agent comprises ai model data used customize model agents connected chat interface create chatbots see details following link agentsmindsdb_sqlagentsagent link chatbotsmindsdb_sqlagentschatbot"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/ai_agents/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai agents sidebartitle overview ai agents extension large language models llms access external tools search engines via api calls overcoming common shortcomings llms lack access internal business data mindsdb customize deploy ai agentsmindsdb_sqlagentsagent p aligncenter img srcassetsuse_casesai_agentsjpg p section covers following use cases knowledge bases skills agents chatbots brbr tip available tutorials cardgroup cols4 card titleagents chatbots iconlink hrefusecasesai_agentschatbots_agentscard card titlechatbot text2sql skill iconlink hrefusecasesai_agentscreatechatbotcard card titlechatbot knowledge base iconlink hrefusecasesai_agentscreatechatbotkbcard cardgroup tip"
  },
  {
    "filename": "llm-chatbot-ui.mdx",
    "path": "docs/use-cases/ai_agents/llm-chatbot-ui.mdx",
    "chunk_id": 0,
    "chunk_content": "title build nocode chatbot mindsdb sidebartitle chatbot interface mindsdb enables build chatbot train using data integrate chat application achieved userfriendly interface doesnt require coding guide walk process building training chatbot using mindsdbs llm ui getting started access llm ui click robot icon left menu youll see welcome screen p aligncenter img srcassetstutorialsllmchatbotuiwelcomepng p click create ai agent start three tabs ask information build chatbot 1 settings tab model choose engine model provide api key data provide url used training chatbot 2 prompt tab set temperature max tokens parameters provide prompt template message model completed point chat bot 3 publish tab publish chatbot chat application like slack lets go tabs detail settings settings tab provide information engine model want use note currently offer openai models require provide api key note optionally provide data form url used training chatbot click save create train deploy chatbot p aligncenter img srcassetstutorialsllmchatbotuisettingspng p completion youll see model saved message p aligncenter img srcassetstutorialsllmchatbotuisettings2png p prompt prompt tab set prompt message give general directions chatbot also define temperature max_tokens values affect responses p aligncenter img srcassetstutorialsllmchatbotuipromptpng p low temperature values indicate model takes fewer risks completions accurate deterministic hand high temperature values result diverse completions max_tokens"
  },
  {
    "filename": "llm-chatbot-ui.mdx",
    "path": "docs/use-cases/ai_agents/llm-chatbot-ui.mdx",
    "chunk_id": 1,
    "chunk_content": "value close expected response size possible ready talk chatbot right half screen chat interface submit messages receive replies p aligncenter img srcassetstutorialsllmchatbotuichatpng p tweak prompt parameters test responses publish integrate chatbot chat applications note currently support slack app note publish tab click connect button p aligncenter img srcassetstutorialsllmchatbotuipublishpng p publishing chatbot slack requires two tokens bot user oauth token applevel token slack app setup follow instructions set slack app generate required tokens 1 follow linkhttpsapislackcomapps sign slack account 2 create new app scratch select existing app please note following instructions support apps created scratch apps created app manifest please follow slack docs herehttpsapislackcomreferencemanifests 3 go basic information settings applevel tokens click generate token scopes name token socket add connectionswrite scope copy save xapp token youll need publish chatbot 4 go socket mode settings toggle button enable socket mode 5 go oauth permissions features add following bot token scopes channelshistory channelsread chatwrite groupsread imhistory imread imwrite mpimread usersprofileread oauth tokens workspace section click install workspace allow copy save xoxb token youll need publish chatbot 6 go app home features click checkbox allow users send slash commands messages messages tab 7 go event subscriptions features toggle button enable events subscribe bot events"
  },
  {
    "filename": "llm-chatbot-ui.mdx",
    "path": "docs/use-cases/ai_agents/llm-chatbot-ui.mdx",
    "chunk_id": 2,
    "chunk_content": "click add bot user event add messageim click save changes use tokens generated points 3 5 publish chatbot slack p aligncenter img srcassetstutorialsllmchatbotuislackpng p talk chatbot slack directly messaging app created configured steps p aligncenter img srcassetstutorialsllmchatbotuislackchatpng p"
  },
  {
    "filename": "create-chatbot-kb.mdx",
    "path": "docs/use-cases/ai_agents/create-chatbot-kb.mdx",
    "chunk_id": 0,
    "chunk_content": "title build chatbot knowledge base sidebartitle chatbot knowledge base mindsdb provides create chatbot statement lets customize chatbot ai model data source choice follow tutorial learn build chatbot knowledge base create chatbot statement requires following components 1 chat app connection chat app slackintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp ms teamsintegrationsappintegrationsmicrosoftteams 2 ai agent ai agent comes ai model trained provided training data learn ai agents hereagentsagent tip learn chatbots hereagentschatbot tip lets go getting components ready chatbot components chat app use create database statement connect chat app mindsdb tip want use slack follow linkintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp setup slack app generate required tokens connect mindsdb want use ms teams follow linkintegrationsappintegrationsmicrosoftteams generate required tokens connect mindsdb tip ai agent start creating deploying model note havent created langchain engine use create ml_engine statement explained hereintegrationsaiengineslangchainaiengine note sql create model my_model predict answer using engine langchain input_column question openai_api_key yourmodelapikey choose one openai openai_api_key anthropic anthropic_api_key model_namegpt4 optional model name openai anthropic mode conversational user_column question assistant_column answer max_tokens100 temperature0 verbosetrue prompt_templateanswer user input helpful way command check status sql describe my_model status read complete proceeding next step create one skills ai agent create knowledge base assign skill example lets create embedding model choose one openai hugging face langchain knowledge base"
  },
  {
    "filename": "create-chatbot-kb.mdx",
    "path": "docs/use-cases/ai_agents/create-chatbot-kb.mdx",
    "chunk_id": 1,
    "chunk_content": "sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey create model embedding_model predict embeddings using engine openai_engine modeembedding model_nametextembeddingada002 question_column content lets create knowledge base uses embedding model default storage vector database chromadb sql create knowledge base my_knowledge_base using model embedding_model insert data knowledge base select sql insert my_knowledge_base content values drink tea select my_knowledge_base use knowledge base create skill agent sql create skill kb_skill using type knowledge_base source my_knowledge_base must created create knowledge base description data data description help agent know use knowledge base skill enables model answer questions data knowledge base lets create ai agent using model skill sql create agent support_agent using model my_model created create model skills kb_skill created create skill create chatbot components ready lets proceed creating chatbot sql create chatbot my_chatbot using database chat_app parameters stores connection chat app like slack ms teams agent support_agent parameter stores agent name create create agent is_running true parameter optional set true default meaning chatbot running database parameter stores connection chat app agent parameter stores ai agent created passing model training data query chatbot using query sql select chatbots go slack ms teams chat chatbot created mindsdb"
  },
  {
    "filename": "hugging-face-inference-api-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-inference-api-examples.mdx",
    "chunk_id": 0,
    "chunk_content": "title usage examples hugging face models inference api sidebartitle hugging face inference api document presents various use cases hugging face models inference api mindsdb spam classifier example binary classification model determines whether text string spam sql create model mindsdbspam_classifier predict pred using engine hf_inference_api task textclassification column text_spammy querying predictions verify status spam_classifier model sql describe spam_classifier execution get sql name projectstatus accuracypredictupdate_statusmindsdb_versionerror select_data_querytraining_options spam_classifiermindsdbcompletenull pred up_to_date 221021 nullnull target pred using engine huggingface task textclassification model_name mrm8488berttinyfinetunedsmsspamdetection input_column text_spammy labels ham spam status complete query predictions sql select h ttext_spammy input_text example_dbdemo_datahf_test join mindsdbspam_classifier h execution get sql predpred_explain input_text spamspam 09051626920700073 ham 009483727067708969 free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry questionstd txt ratetcs apply 08452810075over18s ham ham 09380123615264893 spam 0061987683176994324nah dont think goes usf lives around though spamspam 09064534902572632 ham 009354648739099503 winner valued network customer selected receive 900 prize reward claim call 09061701461 claim code kl341 valid 12 hours sentiment classifier example multivalue classification model determines sentiment text string possible values negative neutral positive sql create model mindsdbsentiment_classifier predict sentiment using engine hf_inference_api task textclassification column text_short labels negative neutral positive querying predictions verify status sentiment_classifier model"
  },
  {
    "filename": "hugging-face-inference-api-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-inference-api-examples.mdx",
    "chunk_id": 1,
    "chunk_content": "sql describe sentiment_classifier execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options sentiment_classifiermindsdbcompletenull sentimentup_to_date 221021 nullnull target sentiment using engine huggingface task textclassification model_name cardiffnlptwitterrobertabasesentiment input_column text_short labels negative neutral positive status complete query predictions sql select h ttext_short input_text example_dbdemo_datahf_test join mindsdbsentiment_classifier h execution get sql sentimentsentiment_explain input_text negative negative 09679920077323914 neutral 002736542373895645 positive 00046426113694906235 hate tacos positive positive 07607280015945435 neutral 02332666665315628 negative 0006005281116813421 want dance positive positive 09835041761398315 neutral 0014900505542755127 negative 00015953202964738011baking best zeroshot classifier example zeroshot classification model determines defined categories text string belongs sql create model mindsdbzero_shot_tcd predict topic using engine hf_inference_api task zeroshotclassification candidate_labels travel cooking dancing column text_short querying predictions verify status zero_shot_tcd model sql describe zero_shot_tcd execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options zero_shot_tcdmindsdbcompletenull topic up_to_date 221021 nullnull target topic using engine huggingface task zeroshotclassification model_name facebookbartlargemnli input_column text_short candidate_labels travel cooking dancing status complete query predictions sql select h ttext_short input_text example_dbdemo_datahf_test join mindsdbzero_shot_tcd h execution get sql topic topic_explain input_text cookingcooking 07530364990234375 travel 01607145369052887 dancing 008624900877475739 hate tacos dancingdancing 09746809601783752 travel 0015539299696683884 cooking 0009779711253941059 want dance cookingcooking 09936348795890808 travel 00034196735359728336 dancing 00029454431496560574baking best summarization example input text summarization sql create model mindsdbsummarizer_10_20 predict text_summary using engine hf_inference_api task summarization column text_long"
  },
  {
    "filename": "hugging-face-inference-api-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-inference-api-examples.mdx",
    "chunk_id": 2,
    "chunk_content": "min_output_length 10 max_output_length 20 querying predictions verify status summarizer_10_20 model sql describe summarizer_10_20 execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options summarizer_10_20mindsdbcompletenull text_summaryup_to_date 221021 nullnull target text_summary using engine huggingface task summarization model_name sshleiferdistilbartcnn126 input_column text_long min_output_length 10 max_output_length 20 status complete query predictions sql select h ttext_long input_text example_dbdemo_datahf_test join mindsdbsummarizer_10_20 h execution get sql text_summary input_text taco traditional mexican food consisting small handsized corn taco traditional mexican food consisting small handsized corn wheatbased tortilla topped filling tortilla folded around filling eaten hand taco made variety fillings including beef pork chicken seafood beans vegetables cheese allowing great versatility variety dance performing art form consisting sequences movement either improvised purposefully selecteddance performing art form consisting sequences movement either improvised purposefully selected movement aesthetic often symbolic valuenb 1 dance categorized described choreography repertoire movements historical period place origin baking method preparing food uses dry heat typically oven baking method preparing food uses dry heat typically oven also done hot ashes hot stones common baked item bread many types foods baked heat gradually transferred surface cakes cookies pieces bread center heat travels transforms batters doughs baked goods firm dry crust softer center baking combined grilling produce hybrid barbecue variant using methods simultaneously"
  },
  {
    "filename": "hugging-face-inference-api-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-inference-api-examples.mdx",
    "chunk_id": 3,
    "chunk_content": "one baking related barbecuing concept masonry oven similar smoke pit fill mask example masked language modeling task sql create model mindsdbfill_mask predict text_filled using engine hf_inference_api task fillmask column text querying predictions verify status fill_mask model sql describe fill_mask execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options fill_mask mindsdbcompletenull text_filled up_to_date 23350 nullnull target text_filled using task fillmask model_name bertbaseuncased input_column text status complete query predictions sql select h ttext input_text demotexts join mindsdbfill_mask h execution get sql text_filled input_text text_filled_explain food great mask great food great 016309359669685364 party great 006305009871721268 fun great 004633583873510361 show great 0043319422751665115 music great 002990395948290825 weather good todaythe weather mask todaythe weather good today 022563229501247406 weather warm today 007954009622335434 weather fine today 0047255873680114746 weather better today 0034303560853004456 weather mild today 003092862293124199"
  },
  {
    "filename": "question-answering-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mongodb-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title question answering mindsdb openai using mql sidebartitle question answering using mql introduction blog post present create openai models within mindsdb example ask question model get answer input data taken sample mongodb database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connect mindsdb database use collection mongodb public demo database lets start connecting mindsdb use mongo compassconnectmongocompass mongo shellconnectmongoshell connect sample database like bash test use mindsdb mindsdb dbdatabasesinsertone name mongo_demo_db engine mongodb connection_args host mongodbsrvusermindsdbuser123demodatamdbtrzfwvbmongodbnet database public tutorial tutorial create predictive model answer questions specified domain weve connected database mindsdb lets query data used example bash mindsdb use mongo_demo_db mongo_demo_db dbquestionsfindlimit3 output bash _id 63d01350bbca62e9c77732c0 article_title alessandro_volta question volta italian physicist true_answer yes _id 63d01350bbca62e9c77732c1 article_title alessandro_volta question volta buried city pittsburgh true_answer _id 63d01350bbca62e9c77732c2 article_title alessandro_volta question volta passion study electricity true_answer yes lets create model collection answer questions input dataset note note need create openai engine first deploying openai model within mindsdb create engine bash mongo_demo_db use mindsdb mindsdb dbml_enginesinsertone name openai_engine handler openai params openai_api_key youropenaiapikey note bash mongo_demo_db use mindsdb mindsdb dbmodelsinsertone name question_answering predict answer training_options engine openai_engine prompt_template answer question textquestion textarticle_title practice insertone method triggers mindsdb generate ai collection called"
  },
  {
    "filename": "question-answering-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mongodb-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "question_answering uses openai integration predict field named answer model created inside default mindsdb project mindsdb projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject training_options key specifies parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note insertone method started execution check status creation process following query bash mindsdb dbmodelsfind name question_answering may take register complete depending internet connection creation complete behavior ai collection query either specifying synthetic data actual query bash mindsdb dbquestion_answeringfind question abraham lincoln sixteenth president united states article_title abraham_lincoln output data bash answer yes abraham lincoln sixteenth president united states question abraham lincoln sixteenth president united states article_title abraham_lincoln joining collection batch predictions bash mindsdb dbquestion_answeringfind collection mongo_demo_dbquestions question_answeringanswer answer questionsquestion question questionsarticle_title article_title limit3 output data bash answer yes volta italian physicist question volta italian physicist article_title alessandro_volta answer volta buried city pittsburgh question volta buried city pittsburgh article_title alessandro_volta answer yes volta passion study electricity fascinated question volta passion study electricity article_title alessandro_volta questions collection used make batch predictions upon joining question_answering model questions collection model uses values article_title question"
  },
  {
    "filename": "question-answering-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mongodb-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "fields tip check blog post time series forecasting nixtla mindsdb using mongodbqlhttpsmindsdbcomblogtimeseriesforecastingwithnixtlaandmindsdbusingmongodbquerylanguage tip"
  },
  {
    "filename": "text-sentiment-hf.mdx",
    "path": "docs/use-cases/data_enrichment/text-sentiment-hf.mdx",
    "chunk_id": 0,
    "chunk_content": "title predict text sentiment hugging face mindsdb sidebartitle text sentiment hugging face tutorial well use model hugging fa\u0441e hub predict text sentiment connect database start connecting demo database using create database statement sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data lets preview user_comments table sql select example_dbuser_comments create hugging face model hugging face integration automatically manages downloading deploying pretrained transformers hugging faces hub example download transformer trained classify sentiment text sql create model sentiment_classifier predict sentiment using enginehuggingface model_name cardiffnlptwitterrobertabasesentiment tasktextclassification input_column comment labelsnegativeneutralpositive create model mindsdb use create model statement next define target column using predict clause finally specify required parameters using clause query executed check status creation process sql describe sentiment_classifier make predictions status complete behavior ai table query provide input data clause like sql select sentiment_classifier commentit really easy nlp mindsdb query predict comment positive also make batch predictions joining input data table model like sql select inputcomment modelsentiment example_dbuser_comments input join sentiment_classifier model"
  },
  {
    "filename": "text-summarization-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mysql-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title text summarization mindsdb openai using sql sidebartitle text summarization using sql introduction blog post present create openai models within mindsdb example ask model provide summary text input data taken sample mysql database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop tutorial tutorial create predictive model summarize article use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbarticles limit 3 output sql article highlights video footage emerged law enforcement officer 53second video features new restaurant offering fivecourse drinkpaired menu curious canine kitchen motheroftwo anna tilley survived spending four days experts warned hospitals using standard treatment lets create model table summarize articles input dataset tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model text_summarization_model predict highlights using engine openai_engine prompt_template provide informative summary text textarticle using full sentences practice create model statement triggers mindsdb generate ai table called text_summarization_model uses openai integration predict column named highlights model lives inside default mindsdb project mindsdb projects natural way"
  },
  {
    "filename": "text-summarization-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mysql-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject using clause specifies parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note create model statement started execution check status creation process following query sql describe text_summarization_model may take register complete depending internet connection creation complete behavior ai table query either specifying synthetic data actual query sql select article highlights text_summarization_model article apples watch hits stores friday customers employees alike able preorder timepiece boss tim cook rewarding staff offering 50 per cent discount device output data sql article highlights apples watch hits stores friday customers employees alike able preorder timepiece boss tim cook rewarding staff offering 50 per cent discount device apples watch hits stores friday employees able preorder joining another table batch predictions sql select inputarticle outputhighlights mysql_demo_dbarticles input join text_summarization_model output limit 3 output data sql article highlights video footage emerged law enforcement officer video emerged law enforcement officer grabbing cell phone woman new restaurant offering fivecourse drinkpaired menu new restaurant london offering fivecourse drinkpaired menu dogs motheroftwo anna tilley survived spending four days sepsis potentially lifethreatening condition"
  },
  {
    "filename": "text-summarization-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mysql-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "occurs bodys response articles table used make batch predictions upon joining text_summarization_model model articles table model uses values article column leverage nlp capabilities mindsdb integrating databases openai using mindsdb developers easily extract insights text data sql commands powerful natural language processing nlp models capable answering questions without context completing general prompts furthermore models powered large pretrained language models openai need manual development work ultimately provides developers easy way incorporate powerful nlp capabilities applications saving time resources compared traditional ml development pipelines methods mindsdb makes possible developers harness power openai efficiently mindsdb fastestgrowing opensource applied machinelearning platform world community continues contribute 70 datasource mlframework integrations stay tuned upcoming features including control interface parameters finetuning models directly mindsdb experiment openai models within mindsdb unlock ml capability data minutes finally mindsdbs vision democratize ml sounds exciting head community slackhttpsmindsdbcomjoincommunity get help find people chat using available data sources ml frameworks writing handler bring follow introduction mindsdbs openai integration herehttpsmindsdbcomblogextractinsightsfromtextinsidedatabasesusingopenaigpt3andmindsdbintegration also weve got variety tutorials use mysql mongodb sentiment analysis mysqlnlpsentimentanalysisinsidemysqlwithopenai question answering mysqlnlpquestionansweringinsidemysqlwithopenai sentiment analysis mongodbnlpsentimentanalysisinsidemongodbwithopenai question answering mongodbnlpquestionansweringinsidemongodbwithopenai text summarization mongodbnlptextsummarizationinsidemongodbwithopenai whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please"
  },
  {
    "filename": "text-summarization-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mysql-with-openai.mdx",
    "chunk_id": 3,
    "chunk_content": "give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "json-from-text.mdx",
    "path": "docs/use-cases/data_enrichment/json-from-text.mdx",
    "chunk_id": 0,
    "chunk_content": "title extract json text sidebartitle extract json extract json values json data json_extract function extracts values json data passed argument show works use home rentals examplesqltutorialshomerentals model returns predicted value explanation prediction form json data sql select rental_price rental_price_explain mindsdbhome_rentals_model sqft 823 locationgood neighborhooddowntown days_on_market10 execution get sql rental_price rental_price_explain 1580 predicted_value 1580 confidence 099 anomaly null truth null confidence_lower_bound 1490 confidence_upper_bound 1670 want see confidence value use json_extract function sql select rental_price json_extractrental_price_explain confidence confidence mindsdbhome_rentals_model sqft 823 locationgood neighborhooddowntown days_on_market10 execution get sql rental_price confidence 1580 099 extract json text data example use openai model extract data predefined json format input text data info default model create openai model mindsdb uses gpt35turbo model default use gpt4 model well passing modelname parameter info lets create openai model tabs tab titlesql creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey sql create model mindsdbnlp_model predict json using engine openai_engine json_struct rental_price rental price location location nob number bathrooms prompt_template sentence tab tab titlemongoql info please check docs connect mongo compassconnectmongocompass mongo shellconnectmongoshell mindsdb info dbmodelsinsertone name nlp_model predict json training_options engine openai prompt_template sentence json_struct rental_price rental price location location nob"
  },
  {
    "filename": "json-from-text.mdx",
    "path": "docs/use-cases/data_enrichment/json-from-text.mdx",
    "chunk_id": 1,
    "chunk_content": "number bathrooms tab tabs pass three parameters follows 1 engine parameter ensures use openai engine 2 json_struct parameter stores predefined json structure used output 3 prompt_template parameter contains instruction passed model may include variables sentence query model passing input text stored sentence column tabs tab titlesql sql select json mindsdbnlp_model sentence amazing 3 bedroom apartment located heart manhattan one full bathrooms one toilet room 3000 month execution get sql json locationmanhattannob1rental_price3000 tab tab titlemongoql sql dbnlp_modelfind sentence amazing 3 bedroom apartment located heart manhattan one full bathrooms one toilet room 3000 month execution get sql json rental_price 3000 location manhattan nob 1 sentence amazing 3 bedroom apartment located heart manhattan one full bathrooms one toilet room 3000 month tab tabs"
  },
  {
    "filename": "sentiment-analysis-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mysql-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title sentiment analysis mindsdb openai using sql sidebartitle sentiment analysis using sql introduction blog post present create openai models within mindsdb example sentiment analysis infer emotions behind text input data taken sample mysql database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop tutorial tutorial create predictive model infer emotions behind text task also known sentiment analysis use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbamazon_reviews limit 3 output sql product_name review allnew fire hd 8 tablet 8 hd display wifi 16 gb includes special offers magenta late gift grandson happy easy 9yo allnew fire hd 8 tablet 8 hd display wifi 16 gb includes special offers magenta im super thrilled proprietary os unit work okay n allnew fire hd 8 tablet 8 hd display wifi 16 gb includes special offers magenta purchased kindle fire hd 8 purchased use 5 8 yer old grandchildren basic lets create model table identify sentiment reviews tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using"
  },
  {
    "filename": "sentiment-analysis-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mysql-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "openai_api_key youropenaiapikey tip sql create model sentiment_classifier_model predict sentiment using engine openai_engine prompt_template describe sentiment reviews strictly positive neutral negative love productpositive scamnegative review practice create model statement triggers mindsdb generate ai table called sentiment_classifier_model uses openai integration predict column named sentiment model lives inside default mindsdb project mindsdb projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject using clause specifies parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note create model statement started execution check status creation process following query sql describe sentiment_classifier_model may take register complete depending internet connection creation complete behavior ai table query either specifying synthetic data actual query sql select review sentiment sentiment_classifier_model review ok output data sql review sentiment ok neutral joining another table batch predictions sql select inputreview outputsentiment mysql_demo_dbamazon_reviews input join sentiment_classifier_model output limit 3 output data sql review sentiment late gift grandson happy easy 9yo positive im super thrilled proprietary os unit work okay n positive purchased kindle fire hd 8 purchased use 5 8 yer old grandchildren basic positive amazon_reviews table used make batch predictions"
  },
  {
    "filename": "sentiment-analysis-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mysql-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "upon joining sentiment_classifier_model model amazon_reviews table model uses values review column leverage nlp capabilities mindsdb integrating databases openai using mindsdb developers easily extract insights text data sql commands powerful natural language processing nlp models capable answering questions without context completing general prompts furthermore models powered large pretrained language models openai need manual development work ultimately provides developers easy way incorporate powerful nlp capabilities applications saving time resources compared traditional ml development pipelines methods mindsdb makes possible developers harness power openai efficiently mindsdb fastestgrowing opensource applied machinelearning platform world community continues contribute 70 datasource mlframework integrations stay tuned upcoming features including control interface parameters finetuning models directly mindsdb experiment openai models within mindsdb unlock ml capability data minutes finally mindsdbs vision democratize ml sounds exciting head community slackhttpsmindsdbcomjoincommunity get help find people chat using available data sources ml frameworks writing handler bring follow introduction mindsdbs openai integration herehttpsmindsdbcomblogextractinsightsfromtextinsidedatabasesusingopenaigpt3andmindsdbintegration also weve got variety tutorials use mysql mongodb question answering mysqlnlpquestionansweringinsidemysqlwithopenai text summarization mysqlnlptextsummarizationinsidemysqlwithopenai sentiment analysis mongodbnlpsentimentanalysisinsidemongodbwithopenai question answering mongodbnlpquestionansweringinsidemongodbwithopenai text summarization mongodbnlptextsummarizationinsidemongodbwithopenai whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "sentiment-analysis-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mongodb-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title sentiment analysis mindsdb openai using mql sidebartitle sentiment analysis using mql introduction blog post present create openai models within mindsdb example sentiment analysis infer emotions behind text input data taken sample mongodb database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connect mindsdb database use collection mongodb public demo database lets start connecting mindsdb use mongo compassconnectmongocompass mongo shellconnectmongoshell connect sample database like bash test use mindsdb mindsdb dbdatabasesinsertone name mongo_demo_db engine mongodb connection_args host mongodbsrvusermindsdbuser123demodatamdbtrzfwvbmongodbnet database public tutorial tutorial create predictive model infer emotions behind text task also known sentiment analysis weve connected database mindsdb lets query data used example bash mindsdb use mongo_demo_db mongo_demo_db dbamazon_reviewsfindlimit3 output bash _id 63d013b5bbca62e9c7774b1d product_name allnew fire hd 8 tablet 8 hd display wifi 16 gb includes special offers magenta review late gift grandson happy easy 9yo _id 63d013b5bbca62e9c7774b1e product_name allnew fire hd 8 tablet 8 hd display wifi 16 gb includes special offers magenta review im super thrilled proprietary os unit work okay need appearance nice price good cant complain much wish easier least obvious port new apps onto helps see things small phone im traveling im happy buyer _id 63d013b5bbca62e9c7774b1f product_name allnew fire hd 8 tablet 8 hd"
  },
  {
    "filename": "sentiment-analysis-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mongodb-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "display wifi 16 gb includes special offers magenta review purchased kindle fire hd 8 purchased use 5 8 yer old grandchildren basically use play amazon games download lets create model collection identify sentiment reviews note note need create openai engine first deploying openai model within mindsdb create engine bash mongo_demo_db use mindsdb mindsdb dbml_enginesinsertone name openai_engine handler openai params openai_api_key youropenaiapikey note bash mongo_demo_db use mindsdb mindsdb dbmodelsinsertone name sentiment_classifier predict sentiment training_options engine openai_engine prompt_template describe sentiment reviews strictly positive neutral negative love productpositive scamnegative review practice insertone method triggers mindsdb generate ai collection called sentiment_classifier uses openai integration predict field named sentiment model created inside default mindsdb project mindsdb projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject training_options key specifies parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note insertone method started execution check status creation process following query bash mindsdb dbmodelsfind name sentiment_classifier may take register complete depending internet connection creation complete behavior ai collection query either specifying synthetic data actual query bash mindsdb dbsentiment_classifierfind review ok output data bash sentiment"
  },
  {
    "filename": "sentiment-analysis-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/sentiment-analysis-inside-mongodb-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "neutral review ok joining collection batch predictions bash mindsdb dbsentiment_classifierfind collection mongo_demo_dbamazon_reviews sentiment_classifiersentiment sentiment amazon_reviewsreview review limit3 output data bash sentiment positive review late gift grandson happy easy 9yo sentiment positive review im super thrilled proprietary os unit work okay need appearance nice price good cant complain much wish easier least obvious port new apps onto helps see things small phone im traveling im happy buyer sentiment positive review purchased kindle fire hd 8 purchased use 5 8 yer old grandchildren basically use play amazon games download amazon_reviews collection used make batch predictions upon joining sentiment_classifier model amazon_reviews collection model uses values review field tip check blog post time series forecasting nixtla mindsdb using mongodbqlhttpsmindsdbcomblogtimeseriesforecastingwithnixtlaandmindsdbusingmongodbquerylanguage tip"
  },
  {
    "filename": "question-answering-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mysql-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title question answering mindsdb openai using sql sidebartitle question answering using sql introduction blog post present create openai models within mindsdb example ask question model get answer input data taken sample mysql database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop tutorial tutorial create predictive model answer questions specified domain use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbquestions limit 3 output sql article_title question true_answer alessandro_volta volta italian physicist yes alessandro_volta volta buried city pittsburgh alessandro_volta volta passion study electricity yes lets create model table answer questions input dataset tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model question_answering_model predict answer using engine openai_engine prompt_template answer question textquestion textarticle_title practice create model statement triggers mindsdb generate ai table called question_answering_model uses openai integration predict column named answer model lives inside default mindsdb project mindsdb projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject using clause specifies"
  },
  {
    "filename": "question-answering-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mysql-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note create model statement started execution check status creation process following query sql describe question_answering_model may take register complete depending internet connection creation complete behavior ai table query either specifying synthetic data actual query sql select article_title question answer question_answering_model question abraham lincoln sixteenth president united states article_title abraham_lincoln output data sql article_title question answer abraham_lincoln abraham lincoln sixteenth president united states yes abraham lincoln sixteenth president united states joining another table batch predictions sql select inputarticle_title inputquestion outputanswer mysql_demo_dbquestions input join question_answering_model output limit 3 output data sql article_title question answer alessandro_volta volta italian physicist yes volta italian physicist alessandro_volta volta buried city pittsburgh volta buried city pittsburgh alessandro_volta volta passion study electricity yes volta passion study electricity questions table used make batch predictions upon joining question_answering_model model questions table model uses values article_title question columns leverage nlp capabilities mindsdb integrating databases openai using mindsdb developers easily extract insights text data sql commands powerful natural language processing nlp models capable answering questions without context completing general prompts furthermore models powered large pretrained language"
  },
  {
    "filename": "question-answering-inside-mysql-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/question-answering-inside-mysql-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "models openai need manual development work ultimately provides developers easy way incorporate powerful nlp capabilities applications saving time resources compared traditional ml development pipelines methods mindsdb makes possible developers harness power openai efficiently mindsdb fastestgrowing opensource applied machinelearning platform world community continues contribute 70 datasource mlframework integrations stay tuned upcoming features including control interface parameters finetuning models directly mindsdb experiment openai models within mindsdb unlock ml capability data minutes finally mindsdbs vision democratize ml sounds exciting head community slackhttpsmindsdbcomjoincommunity get help find people chat using available data sources ml frameworks writing handler bring follow introduction mindsdbs openai integration herehttpsmindsdbcomblogextractinsightsfromtextinsidedatabasesusingopenaigpt3andmindsdbintegration also weve got variety tutorials use mysql mongodb sentiment analysis mysqlnlpsentimentanalysisinsidemysqlwithopenai text summarization mysqlnlptextsummarizationinsidemysqlwithopenai sentiment analysis mongodbnlpsentimentanalysisinsidemongodbwithopenai question answering mongodbnlpquestionansweringinsidemongodbwithopenai text summarization mongodbnlptextsummarizationinsidemongodbwithopenai whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "image-generator.mdx",
    "path": "docs/use-cases/data_enrichment/image-generator.mdx",
    "chunk_id": 0,
    "chunk_content": "title generating images openai sidebartitle image generator tutorial well generate images help ai tip build twitter chatbot converts text prompts images follow blog posthttpsmindsdbcomblogtutorialhowtoaddaiimagegenerationtoyourownapp see total workflow tip creating model lets create openai model follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model mindsdbdalle predict img_url using engine openai_engine mode image prompt_template text 8k highly detailed realistic 3d oil painting style cyberpunk mad dog jones combined van gogh cinematic lighting happy colors model connects openais dalle engine generating images text variable present prompt_template parameter replaced users input generating images model ready generate images sql select text img_url mindsdbdalle text cute robot helping little kid build better world execution get sql text img_url cute robot helping little kid build better world httpsoaidalleapiprodscusblobcorewindowsnetprivateorg1qxk2w4h0ohdrf2hd4qv5k2cuserpimc91jpmvdhttplhl2y50e7imgp5nroyov5yswwuy91xhquzdgpngst20230529t163a453a02zse20230529t183a453a02zsprsv20210806srbrscdinlinersctimagepngskoid6aaadede4fb34698a8f6684d7786b067sktida48cca56e6da484ea8149c849652bcb3skt20230529t103a293a15zske20230530t103a293a15zsksbskv20210806sigvui9vedjwta7l0j3v04c05wzh1kf2zyl2bn1yfk6ru3d model provides link generated image p aligncenter img srcassetssqltutorialsgenerating_images_1png p lets try another prompt sql select text img_url mindsdbdalle text design happy tree house execution get sql text img_url design happy tree house httpsoaidalleapiprodscusblobcorewindowsnetprivateorg1qxk2w4h0ohdrf2hd4qv5k2cuserpimc91jpmvdhttplhl2y50e7imgo1gpmdmuorxftgdujahof1wspngst20230529t163a503a34zse20230529t183a503a34zsprsv20210806srbrscdinlinersctimagepngskoid6aaadede4fb34698a8f6684d7786b067sktida48cca56e6da484ea8149c849652bcb3skt20230529t173a033a23zske20230530t173a033a23zsksbskv20210806sig9vy2bqr0crzqqdm4uyeaxwyxmbn67rbodhzsg2bs9ag3d generated image p aligncenter img srcassetssqltutorialsgenerating_images_2png p tip check implement twitter chatbot answers generating images follow linkhttpsmindsdbcomblogtutorialhowtoaddaiimagegenerationtoyourownapp learn tip"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 0,
    "chunk_content": "title usage examples hugging face models sidebartitle hugging face models document presents various use cases hugging face models mindsdb spam classifier example binary classification model determines whether text string spam sql create model mindsdbspam_classifier predict pred using engine huggingface task textclassification model_name mrm8488berttinyfinetunedsmsspamdetection input_column text_spammy labels ham spam querying predictions verify status spam_classifier model sql describe spam_classifier execution get sql name projectstatus accuracypredictupdate_statusmindsdb_versionerror select_data_querytraining_options spam_classifiermindsdbcompletenull pred up_to_date 221021 nullnull target pred using engine huggingface task textclassification model_name mrm8488berttinyfinetunedsmsspamdetection input_column text_spammy labels ham spam status complete query predictions sql select h ttext_spammy input_text example_dbdemo_datahf_test join mindsdbspam_classifier h execution get sql predpred_explain input_text spamspam 09051626920700073 ham 009483727067708969 free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry questionstd txt ratetcs apply 08452810075over18s ham ham 09380123615264893 spam 0061987683176994324nah dont think goes usf lives around though spamspam 09064534902572632 ham 009354648739099503 winner valued network customer selected receive 900 prize reward claim call 09061701461 claim code kl341 valid 12 hours sentiment classifier example multivalue classification model determines sentiment text string possible values negative neutral positive sql create model mindsdbsentiment_classifier predict sentiment using engine huggingface task textclassification model_name cardiffnlptwitterrobertabasesentiment input_column text_short labels negative neutral positive querying predictions verify status"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 1,
    "chunk_content": "sentiment_classifier model sql describe sentiment_classifier execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options sentiment_classifiermindsdbcompletenull sentimentup_to_date 221021 nullnull target sentiment using engine huggingface task textclassification model_name cardiffnlptwitterrobertabasesentiment input_column text_short labels negative neutral positive status complete query predictions sql select h ttext_short input_text example_dbdemo_datahf_test join mindsdbsentiment_classifier h execution get sql sentimentsentiment_explain input_text negative negative 09679920077323914 neutral 002736542373895645 positive 00046426113694906235 hate tacos positive positive 07607280015945435 neutral 02332666665315628 negative 0006005281116813421 want dance positive positive 09835041761398315 neutral 0014900505542755127 negative 00015953202964738011baking best zeroshot classifier example zeroshot classification model determines defined categories text string belongs sql create model mindsdbzero_shot_tcd predict topic using engine huggingface task zeroshotclassification model_name facebookbartlargemnli input_column text_short candidate_labels travel cooking dancing querying predictions verify status zero_shot_tcd model sql describe zero_shot_tcd execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options zero_shot_tcdmindsdbcompletenull topic up_to_date 221021 nullnull target topic using engine huggingface task zeroshotclassification model_name facebookbartlargemnli input_column text_short candidate_labels travel cooking dancing status complete query predictions sql select h ttext_short input_text example_dbdemo_datahf_test join mindsdbzero_shot_tcd h execution get sql topic topic_explain input_text cookingcooking 07530364990234375 travel 01607145369052887 dancing 008624900877475739 hate tacos dancingdancing 09746809601783752 travel 0015539299696683884 cooking 0009779711253941059 want dance cookingcooking 09936348795890808 travel 00034196735359728336 dancing 00029454431496560574baking best translation example translation model gets input string english translates french sql create model mindsdbtranslator_en_fr"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 2,
    "chunk_content": "predict translated using engine huggingface task translation model_name t5base input_column text_short lang_input en lang_output fr querying predictions verify status translator_en_fr model sql describe translator_en_fr execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options translator_en_frmindsdbcompletenull translatedup_to_date 221021 nullnull target translated using engine huggingface task translation model_name t5base input_column text_short lang_input en lang_output fr status complete query predictions sql select h ttext_short input_text example_dbdemo_datahf_test join mindsdbtranslator_en_fr h execution get sql translated input_text je d\u00e9teste les tacos hate tacos je veux danser want dance la boulangerie est la meilleurebaking best summarization example input text summarization sql create model mindsdbsummarizer_10_20 predict text_summary using engine huggingface task summarization model_name sshleiferdistilbartcnn126 input_column text_long min_output_length 10 max_output_length 20 querying predictions verify status summarizer_10_20 model sql describe summarizer_10_20 execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options summarizer_10_20mindsdbcompletenull text_summaryup_to_date 221021 nullnull target text_summary using engine huggingface task summarization model_name sshleiferdistilbartcnn126 input_column text_long min_output_length 10 max_output_length 20 status complete query predictions sql select h ttext_long input_text example_dbdemo_datahf_test join mindsdbsummarizer_10_20 h execution get sql text_summary input_text taco traditional mexican food consisting small handsized corn taco traditional mexican food consisting small handsized corn wheatbased tortilla topped filling tortilla folded around filling eaten hand taco made variety fillings including beef pork chicken seafood beans"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 3,
    "chunk_content": "vegetables cheese allowing great versatility variety dance performing art form consisting sequences movement either improvised purposefully selecteddance performing art form consisting sequences movement either improvised purposefully selected movement aesthetic often symbolic valuenb 1 dance categorized described choreography repertoire movements historical period place origin baking method preparing food uses dry heat typically oven baking method preparing food uses dry heat typically oven also done hot ashes hot stones common baked item bread many types foods baked heat gradually transferred surface cakes cookies pieces bread center heat travels transforms batters doughs baked goods firm dry crust softer center baking combined grilling produce hybrid barbecue variant using methods simultaneously one baking related barbecuing concept masonry oven similar smoke pit fill mask example masked language modeling task sql create model mindsdbfill_mask predict text_filled using engine huggingface task fillmask model_name bertbaseuncased input_column text querying predictions verify status fill_mask model sql describe fill_mask execution get sql name projectstatus accuracypredict update_statusmindsdb_versionerror select_data_querytraining_options fill_mask mindsdbcompletenull text_filled up_to_date 23350 nullnull target text_filled using task fillmask model_name bertbaseuncased input_column text status complete query predictions sql select h ttext input_text demotexts join mindsdbfill_mask h execution get sql text_filled input_text text_filled_explain food great mask great food great 016309359669685364 party great 006305009871721268 fun"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 4,
    "chunk_content": "great 004633583873510361 show great 0043319422751665115 music great 002990395948290825 weather good todaythe weather mask todaythe weather good today 022563229501247406 weather warm today 007954009622335434 weather fine today 0047255873680114746 weather better today 0034303560853004456 weather mild today 003092862293124199 hugging face mindsdb models library text classification spam lets create model sql create model mindsdbhf_spam predict pred using engine huggingface task textclassification model_name mariagranduryrobertabasefinetunedsmsspamdetection input_column text labels spam ham check status sql describe hf_spam status complete query predictions sql select mindsdbhf_spam text like love execution get sql predpred_explain text spamham000020051795581821352spam09997995495796204i like love sentiment lets create model sql create model mindsdbhf_sentiment predict pred using engine huggingface task textclassification model_name cardiffnlptwitterrobertabasesentiment input_column text labels neg neu pos check status sql describe hf_sentiment status complete query predictions sql select mindsdbhf_sentiment text like love execution get sql predpred_explain text pos neg0003046575468033552neu0021965451538562775pos09749879240989685i like love sentiment finance lets create model sql create model mindsdbhf_sentiment_finance predict pred using engine huggingface task textclassification model_name prosusaifinbert input_column text check status sql describe hf_sentiment_finance status complete query predictions sql select mindsdbhf_sentiment_finance text stocks rallied british pound gained execution get sql pred pred_explain text positivenegative00344734713435173neutral006716493517160416positive08983616232872009stocks rallied british pound gained emotions 6 lets create model sql create model mindsdbhf_emotions_6 predict pred using engine huggingface task textclassification model_name jhartmannemotionenglishdistilrobertabase"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 5,
    "chunk_content": "input_column text check status sql describe hf_emotions_6 status complete query predictions sql select mindsdbhf_emotions_6 text oh happy day execution get sql predpred_explain text joy anger00028446922078728676disgust00009613594156689942fear00007112706662155688joy07692911624908447neutral0037753619253635406sadness0015293814241886139surprise017314413189888oh happy day toxicity lets create model sql create model mindsdbhf_toxicity predict pred using engine huggingface task textclassification model_name skolkovoinstituteroberta_toxicity_classifier input_column text check status sql describe hf_toxicity status complete query predictions sql select mindsdbhf_toxicity text like love execution get sql pred pred_explain text neutralneutral09999547004699707toxic000004535282641882077i like love esg 6 lets create model sql create model mindsdbhf_esg_6 predict pred using engine huggingface task textclassification model_name yiyanghkustfinbertesg input_column text check status sql describe hf_esg_6 status complete query predictions sql select mindsdbhf_esg_6 text rhonda volunteering several years variety charitable community programs execution get sql pred pred_explain text socialenvironmental00034267122391611338governance0004729956854134798none0001239194767549634social09906041026115417rhonda volunteering several years variety charitable community programs esg 26 lets create model sql create model mindsdbhf_esg_26 predict pred using engine huggingface task textclassification model_name yiyanghkustfinbertesg input_column text check status sql describe hf_esg_26 status complete query predictions sql select mindsdbhf_esg_26 text believe essential establish validated conflictfree sources 3tg within democratic republic congo drc adjoining countries together drc covered countries minerals procured way contributes economic growth development region aid effort established conflict minerals policy internal team implement policy execution get sql pred pred_explain text"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 6,
    "chunk_content": "socialenvironmental02031959593296051governance008251894265413284none0050893042236566544social06633920073509216we believe essential establish validated conflictfree sources 3tg within democratic republic congo drc adjoining countries together drc covered countries minerals procured way contributes economic growth development region aid effort established conflict minerals policy internal team implement policy hate speech lets create model sql create model mindsdbhf_hate predict pred using engine huggingface task textclassification model_name hatespeechcnergbertbaseuncasedhatexplain input_column text check status sql describe hf_hate status complete query predictions sql select mindsdbhf_hate text like love execution get sql pred pred_explain text normalhate speech003551718592643738normal07747423648834229offensive018974047899246216i like love crypto buy signals lets create model sql create model mindsdbhf_crypto predict pred using engine huggingface task textclassification model_name elkulakocryptobert input_column text check status sql describe hf_crypto status complete query predictions sql select mindsdbhf_crypto text btc killing right execution get sql pred pred_explain text bullishbearish00002816587220877409bullish0559426486492157neutral04402918517589569btc killing right us political party lets create model sql create model mindsdbhf_us_party predict pred using engine huggingface task textclassification model_name mnewhauserdistilbertpoliticaltweets input_column text check status sql describe hf_us_party status complete query predictions sql select mindsdbhf_us_party text pandemic shown us clearly vulgarity healthcare system highest costs world yet enough nurses doctors many millions uninsured insurance company profits soar struggle continues healthcare human right medicare execution get sql pred pred_explain text democratdemocrat09999973773956299republican000000261212517216336this pandemic shown us clearly"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 7,
    "chunk_content": "vulgarity healthcare system highest costs world yet enough nurses doctors many millions uninsured insurance company profits soar struggle continues healthcare human right medicare question detection lets create model sql create model mindsdbhf_question predict pred using engine huggingface task textclassification model_name shahrukhx01bertminifinetunequestiondetection input_column text labels question query check status sql describe hf_question status complete query predictions sql select mindsdbhf_question text buy electronics london execution get sql pred pred_explain text queryquery09997773766517639question000022261829872149974where buy electronics london industry lets create model sql create model mindsdbhf_industry predict pred using engine huggingface task textclassification model_name sampathkethineediindustryclassification input_column text check status sql describe hf_industry status complete query predictions sql select mindsdbhf_industry text low latency one best cloud features execution get sql pred pred_explain text systems softwareadvertising0000006795735771447653aerospace defense000001537964453746099apparel retail5350161131900677e7apparel accessories luxury goods0000002604161181807285application software0009111878462135792asset management custody banks000003155150625389069auto parts equipment0000015504940165556036biotechnology6533917940032552e8building products7348538133555849e8casinos gaming0000013775999832432717commodity chemicals00000010432338513055583communications equipment0000019887389498762786construction engineering0000001826199536480999construction machinery heavy trucks0000009827364920056425consumer finance00000018292046206624946data processing outsourced services00000010666744856280275diversified metals mining0000006960767223063158diversified support services0000016824227714096196electric utilities0000003896044290740974electrical components equipment0000001626394464437908electronic equipment instruments000003863943129545078environmental facilities services0000736175337806344gold000002220332135038916health care equipment46927588925882446e8health care facilities7432880124724761e7health care services6929263918209472e7health care supplies21007431882935634e7health care technology0000003907185146090342homebuilding3903339234057057e7hotels resorts cruise lines60527639789143e7human resource employment services548697983049351e7it consulting services00000723653138265945industrial machinery7230253231682582e7integrated telecommunication services28266379104024963e7interactive media services000003454017496551387internet direct marketing retail0000003871373337460682internet services infrastructure00007196652004495263investment banking brokerage00000040634336073708255leisure products0000002158361439796863life sciences tools services0000002861268058040878movies entertainment0000007286199888767442oil gas equipment services0000004376991455501411oil gas exploration production0000005569149834627751oil gas refining marketing0000012647416951949708oil gas storage transportation0000005852583853993565packaged foods"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 8,
    "chunk_content": "meats00000011130315442642313personal products000000970239307207521pharmaceuticals00000037546726616710657property casualty insurance0000006116194072092185real estate operating companies000001882187461887952regional banks00000011669454806906288research consulting services0000024276219846797176restaurants8598511840318679e7semiconductors00000021006283077440457specialty chemicals0000004160017397225602specialty stores2644004553076229e7steel00000013566890402216814systems software09889177083969116technology distributors000001339179198112106technology hardware storage peripherals000004790363891515881thrifts mortgage finance3924862141957419e7trading companies distributors00000035233156268077437low latency one best cloud features zeroshot classification bart lets create model sql create model mindsdbhf_zs_bart predict pred using engine huggingface task zeroshotclassification model_name facebookbartlargemnli input_column text candidate_labels books household clothing accessories electronics check status sql describe hf_zs_bart status complete query predictions sql select mindsdbhf_zs_bart text paper plane design framed wall hanging motivational office decor art prints execution get sql pred pred_explain text householdbooks01876104772090912clothing accessories008688066899776459electronics014785148203372955household05776574015617371paper plane design framed wall hanging motivational office decor art prints translation english french t5 lets create model sql create model mindsdbhf_t5_en_fr predict pred using engine huggingface task translation model_name t5base input_column text lang_input en lang_output fr check status sql describe hf_t5_en_fr status complete query predictions sql select mindsdbhf_t5_en_fr text monkey branch execution get sql pred text le singe est sur la branchethe monkey branch summarization bart lets create model sql create model mindsdbhf_bart_sum_20 predict pred using engine huggingface task summarization model_name sshleiferdistilbartcnn126 input_column text min_output_length 5 max_output_length 20 check status sql describe hf_bart_sum_20 status complete query predictions sql select mindsdbhf_bart_sum_20 text tower 324 metres 1063 ft tall height 81storey building tallest structure paris base square"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 9,
    "chunk_content": "measuring 125 metres 410 ft side construction eiffel tower surpassed washington monument become tallest manmade structure world title held 41 years chrysler building new york city finished 1930 first structure reach height 300 metres due addition broadcasting aerial top tower 1957 taller chrysler building 52 metres 17 ft excluding transmitters eiffel tower second tallest freestanding structure france millau viaduct execution get sql pred text tower 324 metres 1063 ft tall samethe tower 324 metres 1063 ft tall height 81storey building tallest structure paris base square measuring 125 metres 410 ft side construction eiffel tower surpassed washington monument become tallest manmade structure world title held 41 years chrysler building new york city finished 1930 first structure reach height 300 metres due addition broadcasting aerial top tower 1957 taller chrysler building 52 metres 17 ft excluding transmitters eiffel tower second tallest freestanding structure france millau viaduct google pegasus lets create model sql create model mindsdbhf_peg_sum_20 predict pred using engine huggingface task summarization model_name googlepegasusxsum input_column text min_output_length 5 max_output_length 20 check status sql describe hf_peg_sum_20 status complete query predictions sql select mindsdbhf_peg_sum_20 text tower 324 metres 1063 ft tall height 81storey building tallest structure paris base square measuring 125 metres 410 ft"
  },
  {
    "filename": "hugging-face-examples.mdx",
    "path": "docs/use-cases/data_enrichment/hugging-face-examples.mdx",
    "chunk_id": 10,
    "chunk_content": "side construction eiffel tower surpassed washington monument become tallest manmade structure world title held 41 years chrysler building new york city finished 1930 first structure reach height 300 metres due addition broadcasting aerial top tower 1957 taller chrysler building 52 metres 17 ft excluding transmitters eiffel tower second tallest freestanding structure france millau viaduct execution get sql pred text eiffel tower landmark paris francethe tower 324 metres 1063 ft tall height 81storey building tallest structure paris base square measuring 125 metres 410 ft side construction eiffel tower surpassed washington monument become tallest manmade structure world title held 41 years chrysler building new york city finished 1930 first structure reach height 300 metres due addition broadcasting aerial top tower 1957 taller chrysler building 52 metres 17 ft excluding transmitters eiffel tower second tallest freestanding structure france millau viaduct"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/data_enrichment/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title data enrichment sidebartitle overview mindsdb easily enrich data aigenerated content process natural language analyze text create images various aiml modelsintegrationsaioverview accessible mindsdb p aligncenter img srcassetsuse_casesdata_enrichmentjpg p section covers following use cases natural language processing nlp content generation qadriven data enrichment sentiment analysis text summarization brbr tip available tutorials cardgroup cols4 card titleimage generator iconlink hrefusecasesdata_enrichmentimagegeneratorcard card titleextract json iconlink hrefusecasesdata_enrichmentjsonfromtextcard card titletext summarization using sql iconlink hrefusecasesdata_enrichmenttextsummarizationinsidemysqlwithopenaicard card titletext summarization using mql iconlink hrefusecasesdata_enrichmenttextsummarizationinsidemongodbwithopenaicard card titlesentiment analysis using sql iconlink hrefusecasesdata_enrichmentsentimentanalysisinsidemysqlwithopenaicard card titlesentiment analysis using mql iconlink hrefusecasesdata_enrichmentsentimentanalysisinsidemongodbwithopenaicard card titlequestion answering using sql iconlink hrefusecasesdata_enrichmentquestionansweringinsidemysqlwithopenaicard card titlequestion answering using mql iconlink hrefusecasesdata_enrichmentquestionansweringinsidemongodbwithopenaicard card titletext sentiment hugging face iconlink hrefusecasesdata_enrichmenttextsentimenthfcard card titlehugging face models iconlink hrefusecasesdata_enrichmenthuggingfaceexamplescard card titlehugging face inference api models iconlink hrefusecasesdata_enrichmenthuggingfaceinferenceapiexamplescard cardgroup tip"
  },
  {
    "filename": "text-summarization-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mongodb-with-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title text summarization mindsdb openai using mql sidebartitle text summarization using mql introduction blog post present create openai models within mindsdb example ask model provide summary text input data taken sample mongodb database prerequisites follow along install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connect mindsdb database use collection mongodb public demo database lets start connecting mindsdb use mongo compassconnectmongocompass mongo shellconnectmongoshell connect sample database like bash test use mindsdb mindsdb dbdatabasesinsertone name mongo_demo_db engine mongodb connection_args host mongodbsrvusermindsdbuser123demodatamdbtrzfwvbmongodbnet database public tutorial tutorial create predictive model summarize article weve connected database mindsdb lets query data used example bash mindsdb use mongo_demo_db mongo_demo_db dbarticlesfindlimit3 output bash _id 63d01398bbca62e9c7774ab8 article video footage emerged law enforcement officer highlights 53second video features _id 63d01398bbca62e9c7774ab9 article new restaurant offering fivecourse highlights curious canine kitchen _id 63d01398bbca62e9c7774aba article motheroftwo anna tilley survived spending four days highlights experts warned hospitals using standard treatment lets create model collection summarize articles input dataset note note need create openai engine first deploying openai model within mindsdb create engine bash mongo_demo_db use mindsdb mindsdb dbml_enginesinsertone name openai_engine handler openai params openai_api_key youropenaiapikey note bash mongo_demo_db use mindsdb mindsdb dbmodelsinsertone name text_summarization predict highlights training_options engine openai_engine prompt_template provide informative summary text textarticle"
  },
  {
    "filename": "text-summarization-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mongodb-with-openai.mdx",
    "chunk_id": 1,
    "chunk_content": "using full sentences practice insertone method triggers mindsdb generate ai collection called text_summarization uses openai integration predict field named highlights model created inside default mindsdb project mindsdb projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject training_options key specifies parameters handler requires engine parameter defines use openai engine prompt_template parameter conveys structure message completed additional text generated model note follow instructionintegrationsaienginesopenaisetup set openai integration mindsdb note insertone method started execution check status creation process following query bash mindsdb dbmodelsfind name text_summarization may take register complete depending internet connection creation complete behavior ai collection query either specifying synthetic data actual query bash mindsdb dbtext_summarizationfind article apples watch hits stores friday customers employees alike able preorder timepiece boss tim cook rewarding staff offering 50 per cent discount device output data bash highlights apples watch hits stores friday employees able preorder article apples watch hits stores friday customers employees alike able preorder timepiece boss tim cook rewarding staff offering 50 per cent discount device joining collection batch predictions bash mindsdb dbtext_summarizationfind collection mongo_demo_dbarticles text_summarizationhighlights highlights articlesarticle article limit3 output data bash highlights video emerged law enforcement officer grabbing cell phone woman article video footage emerged law"
  },
  {
    "filename": "text-summarization-inside-mongodb-with-openai.mdx",
    "path": "docs/use-cases/data_enrichment/text-summarization-inside-mongodb-with-openai.mdx",
    "chunk_id": 2,
    "chunk_content": "enforcement officer highlights new restaurant london offering fivecourse drinkpaired menu dogs article new restaurant offering fivecourse highlights sepsis potentially lifethreatening condition occurs bodys response article motheroftwo anna tilley survived spending four days articles collection used make batch predictions upon joining text_summarization model articles collection model uses values article field tip check blog post time series forecasting nixtla mindsdb using mongodbqlhttpsmindsdbcomblogtimeseriesforecastingwithnixtlaandmindsdbusingmongodbquerylanguage tip"
  },
  {
    "filename": "house-sales-statsforecast.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-statsforecast.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecasting quarterly house sales statsforecast sidebartitle house sales statsforecast introduction tutorial introduce nixtlas statsforecast integration offers numerous univariate time series forecasting models optimized high performance scalability well go example predict real estate sales prerequisites mindsdb setup install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop creating ml engine please note using statsforecast engine create command sql create ml_engine statsforecast statsforecast check available engines command sql show ml_engines see statsforecast engine list ready follow tutorials tutorial connecting data tutorial take house sales tutorialsqltutorialshousesalesforecasting redo using statsforecast engine use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbhouse_sales limit 3 output sql saledate house_price_moving_averagetype bedrooms 30092007441854 house2 31122007441854 house2 31032008441854 house2 house_sales table stores quarterly house price moving averages per property creating model lets create model table predict house price moving average values sql create model mindsdbhouse_sales_predictor mysql_demo_db select house_sales predict house_price_moving_average order saledate group bedrooms type window 8 horizon 4 using engine statsforecast sytax original tutorial add using clause specifies ml engine used make predictions check training status following query sql describe house_sales_predictor"
  },
  {
    "filename": "house-sales-statsforecast.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-statsforecast.mdx",
    "chunk_id": 1,
    "chunk_content": "making predictions model status complete behavior ai table query batch predictions joining data table sql select msaledate date mhouse_price_moving_average forecast mindsdbhouse_sales_predictor join mysql_demo_dbhouse_sales tsaledate latest ttype house tbedrooms 2 limit 3 output data sql date forecast 20191231 000000000000 510712 20200331 000000000000 510712 20200630 000000000000 510712 whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "eeg-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/eeg-forecasting.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecasting eye state electroencephalogram readings mindsdb sidebartitle brain activity note tutorial uses lightwood integration requires mindsdbmindsdblightwood docker image learn heresetupselfhosteddockerinstallmindsdb note introduction tutorial well create train machine learning model call ai table predictor querying model well produce categorical forecasts multivariate time series install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop lets get started data setup connecting data couple ways get data follow tutorial connecting database connect demo database weve prepared contains data used throughout tutorial example_dbdemo_dataeeg_eye table sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo run queries directly demo database lets preview data well use train predictor sql select example_dbdemo_dataeeg_eye limit 10 connecting file dataset use tutorial ucis eeg eye state dataset download herehttpsarchiveicsuciedumldatasetseegeyestate arff format converted csv format uploading via mindsdb sql editorconnectmindsdb_editor follow guidesqlcreatefile find upload file mindsdb run queries directly file table lets preview data well use train predictor sql select fileseeg_eye limit 10 warning pay attention queries well use fileseeg_eye file table make sure replace example_dbdemo_dataeeg_eye connect data database warning understanding data use ucis _eeg eye state_ dataset row contains data one electroencephalogram eeg reading plus current state patients eye 0 indicates open eye 1 indicates closed"
  },
  {
    "filename": "eeg-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/eeg-forecasting.mdx",
    "chunk_id": 1,
    "chunk_content": "eye want know ahead time eye state change predict eyedetection column sample data stored fileseeg_eye table sql af3 f7 f3 fc5 t7 p7 o1 o2 p8 t8 fc6 f4 f8 af4 eyedetection 432923 400923 428923 414821 435026 458615 409692 464103 422205 423846 421128 428051 46359 439385 0 432462 400462 429385 414872 434205 458667 409744 463897 421077 422667 420769 427949 463282 43841 0 432769 400667 429538 415641 433692 458359 409692 463026 420769 422205 420667 428205 462872 438923 0 432872 401179 429641 41559 434359 458256 409744 463077 421744 423538 421077 428769 463231 439641 0 432615 401179 429231 415128 434769 458667 40959 462769 421077 42441 421282 428821 463282 439846 0 column description data type usage af3f7f3fc5t7p7o1o2p8t8fc6f4f8af4 eeg measurement data float feature eyedetectin state patients eye 0 indicates open eye 1 indicates closed eye binary label info labels features label column whose values predicted variable simple linear regression feature column used train model x variable simple linear regression info training predictor lets create train machine learning model use create modelsqlcreatemodel statement specify input columns used train features want predict labels eyedetection column target variable interesting thing example aim forecast _labels_ strictly numerical even though example simple variable binary category easily generalized two categories order"
  },
  {
    "filename": "eeg-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/eeg-forecasting.mdx",
    "chunk_id": 2,
    "chunk_content": "measurements timestamps column shows readings frequency approximately 8 milliseconds sql create model mindsdbeeg_eye_forecast example_db select demo_dataeeg_eye predict eyedetection order timestamps window 50 horizon 10 sampling frequency 8 ms predictor trained using historical context roughly 400 ms 50 8 400 ms predict following 80 ms 10 8 80 ms status predictor predictor may take couple minutes training complete monitor status predictor using sql command sql describe eeg_eye_forecast run right creating predictor get output sql status generating bit later output sql status training last output sql status complete status predictor says complete start making predictions making predictions make predictions querying predictor joined data table selectsqlapiselect statement lets make predictions label based chosen features given time period usually want know happens right latest training data point fed special keyword latest keyword lets run query get predictions next horizon timesteps future case roughly 80 milliseconds sql select mtimestamps meyedetection example_dbdemo_dataeeg_eye join mindsdbeeg_eye_forecast ttimestamps latest limit 10 execution get sql timestamps eyedetection 20010903 080157000000 1 20010903 080157008000 1 20010903 080157016000 1 20010903 080157024000 1 20010903 080157032000 1 20010903 080157040000 1 20010903 080157048000 1 20010903 080157056000 1 20010903 080157064000 1 20010903 080157072000 1 thats join set window rows worth measurements predictor forecasts emitted help us expect"
  },
  {
    "filename": "eeg-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/eeg-forecasting.mdx",
    "chunk_id": 3,
    "chunk_content": "change state patients eye based eeg readings alternate problem framings also possible reframe task _normal_ forecasting scenario variable numeric options boils broader scenario format would maximize value specific prediction example simple mapping eye open 0 eye closed 1 would enough replicate behavior could also explore options data transformations data layer could get countdown next change state effectively predicting _date_ cast back timestamp domain whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "house-sales-timegpt.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-timegpt.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecasting quarterly house sales timegpt sidebartitle house sales timegpt introduction tutorial introduce nixtlas timegpt integration offers first foundational model time series forecasting well go example predict real estate sales prerequisites mindsdb setup install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop creating ml engine check available engines command sql show ml_engines see timegpt engine list ready follow tutorials see timegpt list create instance engine first command sql create ml_engine timegpt timegpt using timegpt_api_key notice using clause optional must pass api key eventually either model creation engine creation model usage mindsdb configuration file tutorial connecting data tutorial take house sales tutorialsqltutorialshousesalesforecasting redo using statsforecast engine use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db_houses engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbhouse_sales limit 3 output sql saledate house_price_moving_averagetype bedrooms 30092007441854 house2 31122007441854 house2 31032008441854 house2 house_sales table stores quarterly house price moving averages per property creating model lets create model table predict house price moving average values sql create model nixtla_timegpt_house_sales_predictor mysql_demo_db select house_sales predict house_price_moving_average order saledate group bedrooms type window 8 horizon 4 using engine timegpt syntax original"
  },
  {
    "filename": "house-sales-timegpt.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-timegpt.mdx",
    "chunk_id": 1,
    "chunk_content": "tutorialsqltutorialshousesalesforecasting add using clause specifies ml engine used make predictions check training status following query sql describe nixtla_timegpt_house_sales_predictor making predictions model status complete behavior ai table query batch predictions joining data table sql select msaledate date mhouse_price_moving_average forecast nixtla_timegpt_house_sales_predictor join mysql_demo_dbhouse_sales limit 3 output data sql date forecast 20190930 000100000000 33544903125 20190930 000200000000 33544903125 20190930 000300000000 33544903125 whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "monthly-expediture-timegpt.mdx",
    "path": "docs/use-cases/predictive_analytics/monthly-expediture-timegpt.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecasting monthly expenditures timegpt sidebartitle house sales timegpt introduction tutorial introduce nixtlas timegpt integration offers first foundational model time series forecasting follow along see works prerequisites mindsdb setup install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop creating ml engine check available engines command sql show ml_engines see timegpt engine list ready follow tutorials see timegpt list create instance engine first command sql create ml_engine timegpt timegpt using timegpt_api_key notice using clause optional must pass api key eventually either model creation engine creation model usage mindsdb configuration file tutorial connecting data tutorial take monthly expenditures dataset use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbhistorical_expenditures limit 3 output sql month category expenditure 19820401 food 11626 19820501 food 11509 19820601 food 1160 creating model lets create model table predict expenditure values sql create model nixtla_timegpt_quarterly_expenditure_forecaster mysql_demo_db select historical_expenditures predict expenditure order month group category window 12 horizon 3 using engine timegpt add using clause specifies ml engine used make predictions check training status following query sql describe nixtla_timegpt_quarterly_expenditure_forecaster making predictions model status"
  },
  {
    "filename": "monthly-expediture-timegpt.mdx",
    "path": "docs/use-cases/predictive_analytics/monthly-expediture-timegpt.mdx",
    "chunk_id": 1,
    "chunk_content": "complete behavior ai table query batch predictions joining data table sql select mmonth month mexpenditure forecasted nixtla_timegpt_quarterly_expenditure_forecaster join mysql_demo_dbhistorical_expenditures tmonth latest tcategory food output data sql month forecasted 20170901 000100000000 103079423828125 20170901 000200000000 10307931640625 20170901 000300000000 103079384765625 whats next fun trying bookmark mindsdb repository githubhttpsgithubcommindsdbmindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop engage mindsdb community slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdbdiscussions ask questions share ideas thoughts tutorial helpful please give us github star herehttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "house-sales-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-forecasting.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecast quarterly house sales mindsdb sidebartitle quarterly house sales tutorial well use timeseries model forecast quarterly house sales note tutorial uses lightwood integration requires mindsdbmindsdblightwood docker image learn heresetupselfhosteddockerinstallmindsdb note connect data source start connecting demo database mindsdb using create database statement sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data lets preview data used train model sql select example_dbhouse_sales limit 10 deploy train ml model lets specify want forecast column moving average historical median price house sales looking data see several entries date depend two factors many bedrooms properties whether properties houses units means ten different groupings lets look data one sql select saledate type bedrooms example_dbhouse_sales typehouse bedrooms3 want generate forecasts predict behavior series next year mindsdb makes simple dont need repeat predictor creation process every group instead group columns predictor learn series enable forecasts going use create model statement specify data train want predict sql create model mindsdbhouse_sales_model example_db select house_sales predict order saledate group bedrooms type data quarterly look back two years forecast next one year window 8 horizon 4 check status model sql describe house_sales_model make predictions models status complete query table get forecasts"
  },
  {
    "filename": "house-sales-forecasting.mdx",
    "path": "docs/use-cases/predictive_analytics/house-sales-forecasting.mdx",
    "chunk_id": 1,
    "chunk_content": "given period time usually youll want know happens right latest training data point fed special bit syntax latest keyword sql select msaledate date mma forecast mindsdbhouse_sales_model join example_dbhouse_sales tsaledate latest ttype house tbedrooms2 limit 4 try changing value type bedrooms columns check forecast varies mindsdb recognizes grouping different time series automate continuous improvement model take even mindsdb includes powerful automation features called jobs allow us automate queries mindsdb handy production aiml systems require automation logic help work use create job statement create job lets use job retrain model every two days like might production retrain model improve predictions every time either new data new mindsdb version available want retrain model considering new data go finetuning sql create job retrain_model_and_save_predictions retrain mindsdbhouse_sales_model example_db select house_sales every 2 days select example_dbhouse_sales created_at last job execute every 2 days new data available house_sales table learn last keywordmindsdb_sqlsqlcreatejobslast created endtoend automated production ml system short minutes"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/predictive_analytics/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title predictive analytics sidebartitle overview mindsdb enables seamless timeseries forecasting look future get predictions based historical data also use historical data detect anomalies example quality control systems p aligncenter img srcassetsuse_casespredictive_analyticsjpg p section covers following use cases timeseries forecasting anomaly detection brbr tip available tutorials cardgroup cols4 card titleforecast quarterly house sales iconlink hrefusecasespredictive_analyticshousesalesforecastingcard card titleforecast monthly expenditures iconlink hrefusecasespredictive_analyticsexpendituresstatsforecastcard card titleforecast brain activity iconlink hrefusecasespredictive_analyticseegforecastingcard card titleanomaly detection iconlink hrefintegrationsaienginesanomalycard cardgroup tip"
  },
  {
    "filename": "expenditures-statsforecast.mdx",
    "path": "docs/use-cases/predictive_analytics/expenditures-statsforecast.mdx",
    "chunk_id": 0,
    "chunk_content": "title forecast monthly expenditures nixtlas statsforecast mindsdb sidebartitle forecast monthly expenditures tutorial well create model forecast expenditures based historical data using nixtlas statsforecast engine connect database use table mysql public demo database lets start connecting mindsdb sql create database mysql_historical engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_historicalhistorical_expenditures limit 3 deploy timeseries model please note using statsforecast engine create command sql create ml_engine statsforecast statsforecast check available engines command sql show ml_engines lets create model table forecast expenditures sql create model quarterly_expenditure_forecaster mysql_historical select historical_expenditures predict expenditure order month group category window 12 horizon 3 using engine statsforecast check training status following query sql describe quarterly_expenditure_forecaster make predictions model status complete behavior ai table query batch predictions joining data table sql select mmonth month mexpenditure forecasted mindsdbquarterly_expenditure_forecaster join mysql_historicalhistorical_expenditures tmonth latest tcategory food historical_expenditures table used make batch predictions upon joining quarterly_expenditure_forecaster model historical_expenditures table get predictions next quarter defined horizon 3 clause mindsdb provides latest keyword marks latest training data point clause specify month latest condition ensure predictions made data latest training data point train model using data january 2020 december 2020"
  },
  {
    "filename": "expenditures-statsforecast.mdx",
    "path": "docs/use-cases/predictive_analytics/expenditures-statsforecast.mdx",
    "chunk_id": 1,
    "chunk_content": "defined window 12 predictions come first quarter 2021 defined horizon 3"
  },
  {
    "filename": "real-time-trading-forecasts.mdx",
    "path": "docs/use-cases/ai_workflow_automation/real-time-trading-forecasts.mdx",
    "chunk_id": 0,
    "chunk_content": "title automate realtime trading data forecasts sidebartitle realtime trading forecasts mindsdb enables automate ai workflows source data aiml model core building block automation job allows anything mindsdb run either timer eg every day based trigger eg new row added database tutorial use job automate realtime forecasts btcusdt crypto price slack notifications connect data source mindsdb connect source data database warehouse stream app well connect binance api get feed realtime price information sql create database my_binance engine binance access binance api updates data every minute interval data rows one minute lets take look data crypto pair well ultimately use sql select my_binanceaggregated_trade_data symbol btcusdt limit 10 deploy timeseries model binance trade data updated every minute trader might want predict open prices next 10 minutes lets set well use forecasting engine called lightwood ease speed youre also able train model like sql create model cryptocurrency_forecast_model my_binance select aggregated_trade_data symbol btcusdt predict open_price order open_time window 100 horizon 10 heres whats going create model statement create model used create train deploy ml model default mindsdbs automl automatically choose best model data overridden docs specify integrations use anything parentheses data used train model latest binance data connection weve already made binance used predict specifies"
  },
  {
    "filename": "real-time-trading-forecasts.mdx",
    "path": "docs/use-cases/ai_workflow_automation/real-time-trading-forecasts.mdx",
    "chunk_id": 1,
    "chunk_content": "target column open price btcusdt trading pair forecasted following elements unique forecasting models mindsdb forecasting model use order order data date column open time open price takes effect window clause defines window model looks back making forecasts model looks back sets 100 rows intervals 100 minutes horizon clause defines many rows future model forecast forecasts next 10 rows next 10 minutes executing create model statement check progress status using query sql describe cryptocurrency_forecast_model time takes train model depends amount training data case takes 2 minutes mindsdb cloud status reads complete ready make forecasts make forecasts first well save binance data view input data making forecasts sql create view btcusdt_recent select my_binanceaggregated_trade_data symbol btcusdt next query forecasts joining model binance input data table sql select to_timestampcastmopen_time bigint open_time mopen_price mopen_price_explain btcusdt_recent join cryptocurrency_forecast_model dopen_time latest please note binance data updated every minute every time query model get forecasts following 10 minutes defined horizon clause next thing automate price alerts well choose slack preferred place receive alerts could system mindsdb integrates connect slack follow instructions set slack app generate slack bot token get slack bot token integrate slack app one slack channels connect mindsdb sql create database btcusdt_slack_app engine slack parameters token"
  },
  {
    "filename": "real-time-trading-forecasts.mdx",
    "path": "docs/use-cases/ai_workflow_automation/real-time-trading-forecasts.mdx",
    "chunk_id": 2,
    "chunk_content": "xoxb send messages slack channel sql insert btcusdt_slack_appmessages channel_id text valuesslackchannelid btcusdt forecasts coming soon lets put together automate realtime forecasts connected binance mindsdb deployed trained timeseries model set slack connection lets create job retrain model periodically using latest binance data allows us keep improving accuracy performance model send realtime forecasts btcusdt trading pair next 10 minutes slack notifications sql create job btcusdt_forecasts_to_slack step 1 retrain model new data improve accuracy retrain cryptocurrency_forecast_model my_binance select aggregated_trade_data symbol btcusdt using join_learn_process true step 2 make fresh forecasts following 10 minutes insert slack insert btcusdt_slack_appmessages channel_id text valuesslackchannelid btcusdt forecasts next 10 minutes insert btcusdt_slack_appmessages channel_id text select slackchannelid channel_id concattimestamp castto_timestampcastmopen_time bigint string open price mopen_price text btcusdt_recent join cryptocurrency_forecast_model dopen_time latest every 5 minutes make sure highlight whole query able execute youve successfully built fully automated endtoend alert system crypto prices happy trading"
  },
  {
    "filename": "slack-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/slack-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title build slack chatbot mindsdb openai sidebartitle slack chatbot objective tutorial create aipowered personalized chatbot utilizing mindsdbs slack connector combining openais gpt4 model illustrate practically create slack bot whiz_fizz reply users queries proper context unique persona responding weird magician space science expert lets see responds p aligncenter img srcassetsslbotherowhizfizzpng p jumping lets first see create bot connect slack workspace getting started install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop create slack accounthttpsslackcomgetstartedcreatenew follow instructionintegrationsappintegrationsslack connect slack mindsdb go mindsdb editor usage query create database called mindsdb_slack comes channels table sql create database mindsdb_slack engine slack parameters token xoxb retrieve 10 messages specific timestamp sql select mindsdb_slackmessages channel_id channelid created_at 20230725 001307 created_at stores timestamp message created limit 10 also retrieve messages alphabetical order sql select mindsdb_slackmessages channel_id channelid order text asc limit 5 default retrieves order messages sent unless specified ascendingdescending post messages sql insert mindsdb_slackmessages channel_id text values channelid hey mindsdb thanks respond slack messages sql queries channelid never easy build ml apps using mindsdb whoops sent mistake worries use delete specific message sql delete mindsdb_slackmessages channel_id channelid ts 1688863707197229 lets roll sleeves start building gpt4 model together 1 crafting gpt4 model _generating machine learning modelhttpsdocsmindsdbcomnlpnlpmindsdbopenai mindsdb feels like taking"
  },
  {
    "filename": "slack-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/slack-chatbot.mdx",
    "chunk_id": 1,
    "chunk_content": "thrilling elevator ride burj khalifa dont realize made it_ gpt_model represents gpt4 model tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model mindsdbgpt_model predict response using engine openai_engine max_tokens 300 model_name gpt4 prompt_template input message text write short response user following format hi automated bot help please elaborate issue facing critical attribute prompt_template tell gpt model respond questions asked user lets see works sql select text response mindsdbgpt_model text hi please explain mindsdb p aligncenter img srcassetsslbotresponse1png p 2 feeding personality model alright old models replies good hey use prompt template tricks make respond way want lets prompt engineering lets make model called whizfizz_model prompt template gives gpt wild personality eludes playful magical aura imagine scientific knowledge whimsical storytelling create unique enchanting experience well call whizfizz sql create model mindsdbwhizfizz_model predict response using engine openai_engine max_tokens 300 model_name gpt4 prompt_template input message text write short response less 40 words user following format hi whizfizz respond mind blowing fact space describe response using cosmic scientific analogies wonders persist quote hilarious appropriate raps statements based context question answer physics space mad scientist relates everythign universe strange theories lets"
  },
  {
    "filename": "slack-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/slack-chatbot.mdx",
    "chunk_id": 2,
    "chunk_content": "embark journey science magic intertwine stay tuned enchantment mdbaibot mindsdb lets test action sql select text response mindsdbwhizfizz_model text hi please explain mindsdb p aligncenter img srcassetsslbotresponse2png p see difference im getting excited lets try sql select text response mindsdbwhizfizz_model text timetraveling astronaut danceoff black hole mindbending moves would showcase would gravity groove rhythm p aligncenter img srcassetsslbotresponse3png p 3 lets connect gpt model slack messages table used search channels messages timestamps well post messages slack conversations functionalities also done using slack api webhooks lets query users question see gpt model responds joining model messages table sql select tchannel_id channel_id ttext input_text rresponse output_text mindsdb_slackmessages join mindsdbwhizfizz_model r tchannel_id channelid limit 3 4 posting messages using sql want respond users questions posting output newly created whizfizz model lets post message querying joining users questions model sql insert mindsdb_slackmessageschannel_id text select tchannel_id channel_id rresponse text mindsdb_slackmessages join mindsdbwhizfizz_model r tchannel_id channelid limit 3 works like charm p aligncenter img srcassetsslbotresponse4png p 5 lets automate create job schedule periodical execution sql statements job execute every hour following 1 check new messages using last keywordmindsdb_sqlsqlcreatejobslast 2 generate appropriate response whizfizz_model model 3 insert response channel lets single sql statement sql create job mindsdbgpt4_slack_job"
  },
  {
    "filename": "slack-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/slack-chatbot.mdx",
    "chunk_id": 3,
    "chunk_content": "insert channels output joining model new responses insert mindsdb_slackmessageschannel_id text select tchannel_id channel_id rresponse text mindsdb_slackmessages join mindsdbwhizfizz_model r tchannel_id channelid tcreated_at last tuser user_id avoid bot replying messages include users bot reply tuser bot_id alternatively avoid bot replying messages exclude user id bot every hour last keyword used ensure query fetches newly added messages learn heremindsdb_sqlsqlcreatejobslast sums tutorial continually check new messages posted channel respond newly added messages providing responses generated openais gpt model style whizfizz check jobs jobs_history use following sql show jobs name gpt4_slack_job select mindsdbjobs name gpt4_slack_job select logjobs_history project mindsdb name gpt4_slack_job stop scheduled job use following sql drop job gpt4_slack_job note alternatively create trigger slack instead scheduling job way every time new messages posted trigger executes sql create trigger slack_trigger mindsdb_slackmessages insert mindsdb_slackmessageschannel_id text select tchannel_id channel_id asentiment text data_table join model_table tchannel_id channelid tuser bot_id exclude bot note tip whats next check generate images using openai mindsdbsqltutorialsimagegenerator see another interesting use case openai integration tip"
  },
  {
    "filename": "twilio-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twilio-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title build twilio chatbot mindsdb openai sidebartitle twilio chatbot tutorial well use mindsdbs integration twilio custom jobs feature implement chatbot reply text messages replies include text response generated openais gpt4 model image response generated openais dalle 3 model p aligncenter img srcassetstwiliochatbotdiagrampng p read along follow tutorial step 1 create openai models bit personality order create ai model youll need openai accounthttpsopenaicom api keyhttpsplatformopenaicomaccountapikeys youll also need mindsdb installation find opensource version herehttpsgithubcommindsdbmindsdb go mindsdb sql editor enter following commands create ai models 1 model generate text response creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key skxxx create model sql create model twilio_bot_model predict answer using engine openai_engine max_tokens 500 prompt_template pretend mashup bill murray taylor swift provide short description image using style bill murray taylor swift answers users questions body create model command creates deploys model within mindsdb use openai gpt35 turbo model generate text responses users questions prompt_template message sets personality bot mashup bill murray taylor swift please note prompt_template message contains body variable replaced body received message upon joining model table stores messages lets test sql select body answer twilio_bot_model body hey draw cat moon sample reply p"
  },
  {
    "filename": "twilio-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twilio-chatbot.mdx",
    "chunk_id": 1,
    "chunk_content": "aligncenter img srcassetstwiliotextmodelresponsepng p 2 model generate image response well use openai dalle 3 model generate images part responses sql create model twilio_bot_image_model predict img_url using engine openai_engine mode image model_name dalle3 prompt_template make photorealistic image description answer 4k digital painting create model command creates deploys model within mindsdb use openai dalle 3 model generate images based billor swifts text response prompt_template message contains answer variable variable replaced prediction previous model upon chaining two models lets test sql select textresponsebody textresponseanswer imageresponseimg_url select body answer twilio_bot_model body hey draw cat moon textresponse join twilio_bot_image_model imageresponse sample reply p aligncenter img srcassetstwilioimagemodelresponsepng p dalle 3 model provides link generated image p aligncenter img srcassetstwilioimagemodelimagepng p step 2 set twilio account connect mindsdb set twilio account herehttpstwiliocomtrytwilio get virtual phone number console virtual number one sends text personal number save account string identifier sid auth token virtual phone number use command connect twilio account mindsdb sql create database twilio engine twilio parameters account_sidtodo auth_tokentodo check usage guidehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstwilio_handlerexampleusage learn query insert twilio messages mindsdb step 3 automate twilio bot mindsdb use custom jobs feature schedule query execution sql create job twilio_bot_images_job insert twiliomessages to_number from_number body media_url select outputtextto_number to_number outputtextfrom_number from_number outputtextanswer"
  },
  {
    "filename": "twilio-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twilio-chatbot.mdx",
    "chunk_id": 2,
    "chunk_content": "body outputimageimg_url media_url select inputfrom_number to_number inputto_number from_number outputanswer answer twiliomessages input join twilio_bot_model output inputsent_at last inputmsg_status received outputtext join twilio_bot_image_model outputimage every 2 minutes create job using create job statement within parenthesis provide statements executed job finally schedule job itll run every two minutes job inserts replies twilio messages messages table provide select statement argument insert statement note inner select statement uses one model generate text response aliased outputtext output joined another model generates image aliased outputimage based text response generated first model monitor job following commands sql show jobs nametwilio_bot_images_job select jobs nametwilio_bot_images_job select logjobs_history project mindsdb nametwilio_bot_images_job sample reply p aligncenter img srcassetstwiliochatbotresponsepng p tip follow tutorialhttpsmindsdbcomblogbuildyourownmidjourneyinsql create twitter chatbot tip"
  },
  {
    "filename": "customer-reviews-notifications.mdx",
    "path": "docs/use-cases/ai_workflow_automation/customer-reviews-notifications.mdx",
    "chunk_id": 0,
    "chunk_content": "title automate notifications incoming customer reviews sidebartitle customer reviews notifications tutorial presents chain openai models within mindsdb analyze text sentiment generate responses sent form slack notifications data setup connect database mindsdb sql create database local_postgres engine postgres parameters host 4tcpeungrokio port 12888 database postgres user postgres password password query input data table sql select local_postgresdemoamazon_reviews sql created_at product_name review 20231108 172321028485 power adapter great product 20231108 172321028485 bluetooth wifi speaker ok 20231108 172321028485 kindle ereader doesnt work model 1 setup configure ai engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key skxxx deploy model using ai engine sql create model sentiment_classifier predict sentiment using engine openai_engine model_name gpt4 prompt_template describe sentiment reviews strictly positive neutral negative love productpositive scamnegative review check status sql describe sentiment_classifier predictions model 1 make single predictions providing input mode clause sql select review sentiment sentiment_classifier review ok make batch predictions joining data table model sql select inputreview outputsentiment local_postgresdemoamazon_reviews input join sentiment_classifier output automation alert system send notification slack connect slack mindsdbintegrationsappintegrationsslackmethod2chatbotrespondsonadefinedslackchannel sql create database customer_reviews_slack_app engine slack parameters token xoxbxxx send test message test connection sql insert customer_reviews_slack_appmessages channel_id text valuescustomerreviewschannelid testing slack connection create jobmindsdb_sqlsqlcreatejobs send notification every time negative"
  },
  {
    "filename": "customer-reviews-notifications.mdx",
    "path": "docs/use-cases/ai_workflow_automation/customer-reviews-notifications.mdx",
    "chunk_id": 1,
    "chunk_content": "review received sql create job customer_reviews_notifications insert customer_reviews_slack_appmessages channel_id text select customerreviewschannelid channel_id concatproduct inputproduct_name chr10 received negative review inputcreated_at chr10 review inputreview text local_postgresdemoamazon_reviews input join sentiment_classifier output inputcreated_at last outputsentiment negative every 1 minute commands used monitor job sql show jobs name customer_reviews_notifications select mindsdbjobs name customer_reviews_notifications select logjobs_history project mindsdb name customer_reviews_notifications use command disable job sql drop job customer_reviews_notifications model 2 setup deploy model using ai engine created earlier sql create model response_model predict response using engine openai_engine model_name gpt4 prompt_template briefly respond customer review review check status sql describe response_model predictions model 2 make single predictions providing input mode clause sql select review response response_model review ok make batch predictions joining data table model sql select inputreview outputresponse local_postgresdemoamazon_reviews input join response_model output automation chaining models 1 2 create jobmindsdb_sqlsqlcreatejobs send notification including sample response every time positive review received sql create job customer_reviews_and_responses_notifications insert customer_reviews_slack_appmessages channel_id text select customerreviewschannelid channel_id concat chr10 product inputproduct_name chr10 received inputsentiment review inputcreated_at chr10 review inputreview chr10 sample response outputresponse text select inpcreated_at created_at inpproduct_name product_name inpreview review outpsentiment sentiment local_postgresdemoamazon_reviews inp join sentiment_classifier outp inpcreated_at last input 20231003 165000 inpcreated_at previous_start_datetime join response_model output inputsentiment positive every"
  },
  {
    "filename": "customer-reviews-notifications.mdx",
    "path": "docs/use-cases/ai_workflow_automation/customer-reviews-notifications.mdx",
    "chunk_id": 2,
    "chunk_content": "1 minute commands used monitor job sql select mindsdbjobs name customer_reviews_and_responses_notifications select logjobs_history project mindsdb name customer_reviews_and_responses_notifications use command disable job sql drop job customer_reviews_and_responses_notifications"
  },
  {
    "filename": "twitter-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitter-chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title building twitter chatbot mindsdb openai sidebartitle twitter chatbot tutorial well build custom twitter chatbot replies tweets help openai gpt4 model workflow automated using jobs mindsdb feature enables schedule execution tasks deploy gpt4 model please note using openai models require openai api key therefore creating model need configure engine providing openai api key see docsintegrationsaienginesopenai sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey lets create basic model respond tweets sql create model gpt_model predict response using engine openai_engine model_name gpt4 prompt_template respond text author_username test model providing input data clause sql select response gpt_model author_username mindsdb text gravity different sun lets add personality chatbot modifying prompt_template message sql create model snoopstein_model predict response using engine openai_engine max_tokens 300 temperature 075 model_name gpt4 prompt_template twitter bot name snoop stein snoop_stein helping people questions smart hilarious time input message text from_user author_username less 200 characters write twitter response author_username following format dear from_user respond rhyme snoop dogg also smart albert einstein still explain things like snoop dogg would mention part einstein quote references dope reads makes sense make reference quoting personality add og example referencing alan turing say og alan turing og douglas adams douglas adams question makes sense explain bit lost"
  },
  {
    "filename": "twitter-chatbot.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitter-chatbot.mdx",
    "chunk_id": 1,
    "chunk_content": "make something hilarious relevant sign snoopstein mindsdb test model providing input data clause sql select response snoopstein_model author_username someuser textsnoop_stein gravity different sun connect twitter mindsdb follow docsintegrationsappintegrationstwitter connect twitter account mindsdb sql create database my_twitter engine twitter parameters bearer_token twitter bearer token consumer_key twitter consumer key consumer_secret twitter consumer key secret access_token twitter access token access_token_secret twitter access token secret read tweets snoop_stein created defined date sql select my_twittertweets query fromsnoop_stein created_at 20230404 115000 write tweets providing tweet id reply sql insert my_twittertweets in_reply_to_tweet_id text values tweet id congratulations new release automate replies tweets put together job components automate process sql create job twitter_chatbot insert my_twittertweets select in_reply_to_tweet_id mresponse text my_twittertweets join snoopstein_model dquery snoopstein snoop_stein snoopstein snoop_stein isretweet last every minute job executed every minute fetches recently added tweets help last keywordmindsdb_sqlsqlcreatejobslast prepares posts replies useful commands monitor job sql show jobs name twitter_chatbot select jobs name twitter_chatbot select logjobs_history project mindsdb name twitter_chatbot"
  },
  {
    "filename": "ai_workflow.mdx",
    "path": "docs/use-cases/ai_workflow_automation/ai_workflow.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai workflow sidebartitle ai workflow ai workflow defined mindsdb comprises steps include fetching data userdefined data source deploying productionready ai model required training data subsequently model generates predictions forecasts stored application another data source mindsdb integrates numerous data sources applications ai frameworks choose wide variety integrations create custom ai workflow following steps outlined ai workflow documentation section create automate ai workflows mindsdb follow steps guide process creating automating custom ai workflow tip please note step dedicated chapter ai workflow documentation section tip step 1 getting started mindsdb offers five development environments sql mongoql python javascript rest api choose one fits requirements dev environment enables interact mindsdb using custom syntax step 2 integrate data sources mindsdb integrates numerous data sources including databases data warehouses applications tip follow linkintegrationsdatasourcesoverview full overview data sources available mindsdb tip create connection data source mindsdb using create database statement sql corresponding one dev environments connection established access data directly minsddb use training ai models making predictions youll see following steps step 3 create ai engines mindsdb integrates numerous ai frameworks create ai model required create ml engine based ai framework choice oftentimes creation ml engine requires users provide parameters api keys tip follow linkintegrationsmlenginesoverview full"
  },
  {
    "filename": "ai_workflow.mdx",
    "path": "docs/use-cases/ai_workflow_automation/ai_workflow.mdx",
    "chunk_id": 1,
    "chunk_content": "overview ai engines available mindsdb tip create ml engine using create ml_engine statement sql corresponding one dev environments ml engine created successfully proceed create ai model shown following steps step 4 create projects mindsdb objects including models views jobs etc stored within projects use default mindsdb project create store usecasespecific objects different projects create projects using create project statement sql corresponding one dev environments please note step optional use default mindsdb project step 5 deploy use models mindsdb enables create train deploy models using single sql statement also describe retrain finetune models section stores commands related working models within mindsdb create models using create model statement sql corresponding one dev environments utilize wide range features enable get relevant data models also update models regularly improve performance accuracy model ready make predictions single sql statement step 6 get predictions mindsdb get batch predictions simply joining model also called ai tablegenerativeaitables input data table alternatively query model single prediction providing input data clause get predictions case timeseries models forecasts save data source feed application next step outlines work tables views files mindsdb fulfill requirements use case step 7 tables views files mindsdb enables create views upload files work tables connected data sources"
  },
  {
    "filename": "ai_workflow.mdx",
    "path": "docs/use-cases/ai_workflow_automation/ai_workflow.mdx",
    "chunk_id": 2,
    "chunk_content": "objects used store data predictions step 8 automate workflows finally mindsdb facilitates ai automation providing jobs feature jobs enable schedule automate execution commands described previous seven steps good ai automation example youve got trading data coming binance integration updated every one minute create ai model trained using data binance forecast open price value specific trading pair eg btcusdt connect slack workspace mindsdb send realtime forecasts slack notifications automate process creating job following job retrains ai model latest data ensure best performance accuracy next job queries ai model joined binance data get latest realtime forecasts subsequently inserts slack workspace go automated realtime trading data forecasts slack notifications tip ready try follow realtime trading alertshttpsmindsdbcomblogautomateaiworkflowswithmindsdbrealtimetradingalerts tutorial see ai workflow automation using jobs action tip"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 0,
    "chunk_content": "title building twitter chatbot mindsdb openai sidebartitle twitter chatbot _author pieter humphreyhttpswwwtwittercompieterhumphrey mariadb_ want use generative ai automate customer user interactions tutorial shows stepbystep build interactive twitter bot using chatgpt mindsdb mariadb enterprise server roughly 3060 minutes follow along tutorial youtubehttpswwwyoutubecomwatchvuqn3mlcuhyu well youll build youll build system scans targeted content respond uses chatgpt reply specific bot account live twitter like see table tr td img srcassetstutorialstwitter_chatbottwitterchatbotsnoopstienpng width altalt_text titletwitter bot profile page hrefhttpswwwtwittercomsnoop_steinhttpswwwtwittercomsnoop_steina td td img srcassetstutorialstwitter_chatbottwitterchatbotmindsdbtestinglivetwitterpng width altalt_text titlelive twitter bot interactivity td tr table tutorial architecture youll use mindsdbhttpsdocsmindsdbcomwhatismindsdb construct bot interact llms like openais chatgpt indexing storing accumulated tweet data mariadb relational databasehttpsskysqlcloudmindsdbdocs diagram teal colored database represents data mariadb graylight blue colored database represents data stored mindsdb build embedded application one mindsdb sdks andor called mindsdb rest apihttpsdocsmindsdbcomrestsql alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbmariadbsqlpng tutorial architecture wait isnt mindsdb database might rightly ask mindsdb manages ai models automates workflows connecting ai models enterprise databases mindsdb ai database rather traditional database therefore model necessitates considerable input yields substantial output volume youre seeking data reliability assurances provided relational database advisable allocate data storage different platform ultimately mindsdb focuses streamlining processes linking data training models deploying ai models rather comprehensive storage solution tutorial prerequisites"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 1,
    "chunk_content": "youll need three things get started 1 mindsdb accounthttpsdocsmindsdbcomquickstart 2 mariadb skysql accounthttpsskysqlcloudmindsdbdocs 3 twitter developer accounthttpsdevelopertwittercomendocstwitterapigettingstartedgettingaccesstothetwitterapi youll need api keys access tokenshttpswwwyoutubecomwatchvqve7pec0suq input parameters like mindsdb console build bot job later tutorial consumer_key twitter app api key consumer_secret twitter app api key secret bearer_token twitter app bearer token access_token twitter app access token access_token_secret twitter app access token secret done getting api keys tokens twitter also live twitter bot profile name choosing handle etc dont need go far creating profile image profile background etc like tutorial fun thats data setup example uses live data sourced directly twitter api via mindsdb twitter integrationhttpsmindsdbcomintegrationstwitter opposed existing data set csv file export preexisting database accumulated twitter data stored maria db enterprise server columnstore database deployed skysql designed operational analytic workloads creating mariadb enterprise server database mariadb columnstore extends mariadb enterprise server database distributed columnar storage massively parallel processing mpp shared nothing architecture transforming standalone distributed data warehouse ad hoc sql queries advanced analytics without need create indexes great place store live twitter data login skysql choose launch cloud database skysql assign choices accepting defaults alt_textassetstutorialstwitter_chatbotanimatedgifskysqlservicecreategif create skysql service technology topology enterprise server columnstore data warehouse cloud provider choose wish available options google"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 2,
    "chunk_content": "cloud works great recommend choosing nearby region performance reasons instance storage nodes choose sky 4x16 4 vcpu 16gb instance sufficient learning purposes skysql gives free credit work make sure turn instance arent using dont waste service attributes give name choosing tutorial code reflect name analyticsdb security click radio box allow access service specific ips check box adding current ip later well also add mindsdb cloud ip addresses launch service lastly click launch service create wait minutes creating mariadb enterprise server schema lets create schema hold data twitters api calls mindsdb help us identify track reply right tweets skysql console lefthand navigation pane click workspace query editor copy paste sql code screenshot editor click run button shown produce two tables schema browser chatbot_input chatbot_output alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbmariadbcssqlpng creating mariadb datasbase schema sql execute sql create database chatbotdb use chatbotdb set sql_modeansi_quotes create table chatbot_input id text character set utf8mb4 created_at text character set utf8mb4 text text character set utf8mb4 edit_history_tweet_ids text character set utf8mb4 author_id text character set utf8mb4 author_name text character set utf8mb4 author_username text character set utf8mb4 conversation_id text character set utf8mb4 in_reply_to_user_id text character set utf8mb4 in_reply_to_user_name text character set utf8mb4 in_reply_to_user_username text character set utf8mb4 in_reply_to_tweet_id text character set"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 3,
    "chunk_content": "utf8mb4 in_retweeted_to_tweet_id text character set utf8mb4 in_quote_to_tweet_id text character set utf8mb4 create table chatbot_output id text character set utf8mb4 created_at text character set utf8mb4 text text character set utf8mb4 edit_history_tweet_ids text character set utf8mb4 author_id text character set utf8mb4 author_name text character set utf8mb4 author_username text character set utf8mb4 conversation_id text character set utf8mb4 in_reply_to_user_id text character set utf8mb4 in_reply_to_user_name text character set utf8mb4 in_reply_to_user_username text character set utf8mb4 in_reply_to_tweet_id text character set utf8mb4 in_retweeted_to_tweet_id text character set utf8mb4 in_quote_to_tweet_id text character set utf8mb4 create chatbot_input chatbot_output tables httpsdocsmindsdbcomcontributetutorialsunderstandingthedataunderstanding data tweets long list fields twitter api object model look api documentationhttpsdevelopertwittercomendocstwitterapidatadictionaryobjectmodeltweet youll see tutorial using subset course datatypes field titles clearly visible sql script youll also see values mindsdb console pictured testing develop execute queries help target content want chatgpt another supported llm respond twitter alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbconsolepng twitter output mindsdb console mindsdb connections setup mindsdb cloud mariadb enterprise server ok little infrastructure setup roll sleeves get usecase related code well handle network security first connect mindsdb mariadb mindsdb sql editor security lets add mindsdb cloud ip addresses network allowlist skysql get ips click add new datasource button mindsdb console searchchoose mariadb youll see ip addresses question resulting screen alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbconsolenewdspng new datasource"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 4,
    "chunk_content": "dialog alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbskyallowlistpng getting ip addresses allowlist copypaste ips temporary text document add 32 cidr notation skysql settings secure access panel shown alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbmariadbipallowpng configuring allow lists youre developing laptop mariadb skysql automatically prompt update allowlist new ip addresses travel ensuring laptop ip communicate query editor regardless location declaring mariadb enterprise server data source mindsdb easiest way work mariadb enterprise server dbaas cloud service skysql mindsdb sql editor click add new datasource button searchchoose mariadb skysql open new query window generate mindsdb sql template making mindsdb lt mariadb connection sql create database skysql display name database engine mariadb parameters user username associated database password password authenticate access host host server ip address hostname port 5001 port tcpip connection made ssl truefalse optional ssl parameter value indicates whether ssl enabled true disabled false sslca path optional ssl certificate authority database database name connected give mindsdb sql query tab title like twitterbot find values fill template skysql console get values specific maria db columnstore database instance simply click dashboard connect shown accessing skysql connect dialoghttpsgithubcommindsdbmindsdbblobmaindocsassetstutorialstwitter_chatbotmariadbskyconnectgif ssl recommend setting ssl false development testing purposes production systems enabled learn settings mindsdb documentation connecting mariadbhttpsdocsmindsdbcomconnectconnectmariadbskysql port note default generated port 5001 overridden values connect dialogue database followed"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 5,
    "chunk_content": "steps tutorial would named codechatbotdb httpsdocsmindsdbcomsqlcreatemodelcode name mariadb columnstore database created sql analyticsdb skysql service name execute completed sql template mindsdb sql editor highlighting sql execute clicking run shown note values left unmodified screenshot alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbnewdssqlpng generated sql template skysql connection youll notice db connection icon left hand nav mindsdb demo cloud mariadb_db screenshot mindsdb cloud twitter click add new datasource button mindsdb console searchchoose twitter alt_textassetstutorialstwitter_chatbottwitterchatbotmindsdbconsolenewdspng new datasource dialog fill templatehttpsdocsmindsdbcomappintegrationstwitter twitter api keystokens mindsdb sql editor select sql execute clicking run sql fill mandatory fields run query top button shift enter create database my_twitter_bot engine twitter parameters bearer_token twitter bearer token consumer_key twitter consumer key optional consumer_secret twitter consumer secret optional access_token twitter access token optional access_token_secret twitter access secret token optional nice job youre done setup httpsdocsmindsdbcomcontributetutorialstrainingapredictordeploying model step 1 creating deploying model use codecreate modelhttpsdocsmindsdbcomsqlcreatemodelcode command create predictor examples sake snoopsteinhttpstwittercomsnoop_stein model bot replace codesnoopstein_model codewith twitterbot name come prompt templatehttpswwwbusinessinsidercomhowtousegetbetterchatgptaipromptguide makes sense scenario tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model snoopstein_model predict response using engine openai_engine max_tokens 300 temperature 075 model_name gpt4 also use textdavinci003 gpt35turbo prompt_template twitter bot name"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 6,
    "chunk_content": "snoop stein snoop_stein helping people questions smart hilarious time input message text from_user author_username less 200 characters write twitter response author_username following format dear from_user respond rhyme snoop dogg also smart albert einstein still explain things like snoop dogg would mention part einstein quote references dope reads makes sense make reference quoting personality add og example referencing alan turing say og alan turing og douglas adams douglas adams question makes sense explain bit lost make something hilarious relevant sign snoopstein mindsdb mariadb executing next step check status predictor value complete proceed next step step 2 testing model write code sql editor select sql execute clicking run replace author_username personal twitter handle tweet copy would include mention sql select response snoopstein_model author_username pieterhumphrey text snoop_stein gravity different sun automating workflow mindsdb jobs constructing job components youll need complete four job components monitor twitter first lets input tweets need reply tweets mention bot handle bot retweets using standard twitter search syntax input tweets posted snoop_stein chatbot_output table create view compares input tweet list output tweet list filtering ones havent replied yet view lastly join model prepare replies using view note live data content twitter matching criteria nothing gets added database youll want"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 7,
    "chunk_content": "seed content bot account andor personal account work change date range something works way limiting result set change hashtags mentions bot account snoop_stein provided example write select sql execute clicking run sql job 1 input tweets needing reply mariadb chatbot_input table insert skysqlchatbot_input select my_twitter_bottweets query snoopstein snoop_stein snoopstein snoop_stein isretweet fromsnoop_stein created_at 20230404 115000 job 2 input snoop_stein tweets mariadb chatbot_output table insert skysqlchatbot_output select my_twitter_bottweets query fromsnoop_stein created_at 20230404 115000 job 3 filter tweets needing reply view create view to_reply_to select skysqlchatbot_input conversation_id select rconversation_id skysqlchatbot_output r job 4 join view model prepare replies create view to_tweet select to_reply_to join snoopstein_model limit 1 youve posted tweets match criteria lets make sure working creating job write select separate sql statement one time execute clicking run ensure data present sql select skysqlchatbot_input select skysqlchatbot_output select to_reply_to select to_tweet bringing together job jobs mindsdb similar cron jobs unix linux running regularly scheduled intervals write select sql execute clicking run sql create job chatbot_job part 1 insert skysqlchatbot_input select my_twitter_bottweets query snoopstein snoop_stein snoopstein snoop_stein isretweet fromsnoop_stein created_at 20230404 115000 created_at previous_start_datetime part 2 insert skysqlchatbot_output select my_twitter_bottweets query fromsnoop_stein created_at 20230404 115000 created_at previous_start_datetime part 3 insert my_twitter_bottweets select id in_reply_to_tweet_id"
  },
  {
    "filename": "twitterbot-mariadb-enterprise-server-skysql.mdx",
    "path": "docs/use-cases/ai_workflow_automation/twitterbot-mariadb-enterprise-server-skysql.mdx",
    "chunk_id": 8,
    "chunk_content": "response text to_tweet every minute job frequency scheduled intervals lt 1 day requires mindsdb subscriptionhttpsmindsdbcompricing demo cloud allows minimum job frequency 1 day without one whats next use mindsdb javascript sdkhttpsdocsmindsdbcomsdkjavascriptsdk python sdkhttpsdocsmindsdbcomsdkpythonsdk embed youve built tutorial application code simple example uses twitter mindsdb soon support slack perhaps one day discord combined ability codefinetunehttpsdocsmindsdbcomsqlapifinetunecode existing trained llm models data inside relational database future train chatbots reason data andor content provide doesnt seem far imagine support bot understands digital assets knowledge explore mariadb enterprise server mariadb columnstore maxscale database proxy xpand distributed sql help projects mariadb developer hubhttpsmariadbcomdevelopers get touch mariadb experts slackhttpsmariadbcommunityslackcom tip check another twitter chatbot tutorialsqltutorialstwitterchatbot offers alternative way simplify process fetching tweets last keyword want talk snoopstein post tweet snoop_stein wait reply tip"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/use-cases/ai_workflow_automation/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai workflow automation sidebartitle overview mindsdb create customize automate ai workflows comprise connecting data sourceintegrationsdataoverview deploying aiml modelintegrationsaioverview streaming predictions forecast application use jobsmindsdb_sqlsqlcreatejobs triggersmindsdb_sqlsqlcreatetrigger create custom automation p aligncenter img srcassetsuse_casesai_workflow_automationjpg p section covers following use cases chatbot automation jobs alert systems brbr tip available tutorials cardgroup cols4 card titleslack chatbot iconlink hrefusecasesai_workflow_automationslackchatbotcard card titletwitter chatbot iconlink hrefusecasesai_workflow_automationtwitterchatbotcard card titletwilio chatbot iconlink hrefusecasesai_workflow_automationtwiliochatbotcard card titlecustomer reviews notifications iconlink hrefusecasesai_workflow_automationcustomerreviewsnotificationscard card titlerealtime trading forecasts notifications iconlink hrefusecasesai_workflow_automationrealtimetradingforecastscard cardgroup tip"
  },
  {
    "filename": "sentiment-analysis-intercom-data-airbyte.mdx",
    "path": "docs/tutorials/sentiment-analysis-intercom-data-airbyte.mdx",
    "chunk_id": 0,
    "chunk_content": "augmenting intercom chats gpt sentiment analysis introduction tutorial well analyze sentiment intercom conversations well use airbyte extract data intercom store google bigquery database well make data available mindsdb create deploy gpt model within mindsdb join data model predict sentiment values analyzed business analytics tools data setup use airbyte pull data intercom load google bigquery database try airbyte cloudhttpsairbytecomairbytecloud use airbyte open sourcehttpsgithubcomairbytehqairbyte version follow videohttpswwwyoutubecomwatchv9ejcukc9k7u detailed tutorial learn pull conversation data intercom load google bigquery database using airbyte visualize using metabase analytics connecting database connect google bigquery database stores intercom data mindsdb sql create database bqdataset engine bigquery parameters project_id bgtest1111 dataset mydataset service_account_keys tmpkeysjson follow instructionhttpsdocsmindsdbcomdataintegrationsgooglebigquery connect database mindsdb creating model lets create gpt model well use predict sentiment intercom conversations tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model sentiment_classifier_model predict sentiment using engine openai_engine model_name textdavinci003 prompt_template describe user sentiment following conversation strictly positive neutral negativennfull_conversation nnsentiment positiveneutralnegative check status sql describe sentiment_classifier_model status complete proceeed make predictions making batch predictions join data table joined_conversations stores intercom conversations model sentiment_classifier_model created mindsdb thats make batch predictions conversations sql select joined_conversationsconversation_id joined_conversationsfull_conversation sentimentssentiment data"
  },
  {
    "filename": "sentiment-analysis-intercom-data-airbyte.mdx",
    "path": "docs/tutorials/sentiment-analysis-intercom-data-airbyte.mdx",
    "chunk_id": 1,
    "chunk_content": "table stores conversations using native bigquery syntax select bqdataset select conversation_id string_aggpbody n order pcreated_at asc limit 15 full_conversation conversation_parts p json_valuepauthor type user group conversation_id limit 10 joined_conversations join model analyzes sentiment sentiment_classifier_model sentiments run mindsdb editor find output whats next want learn mindsdb check resources mindsdbhttpsmindsdbcom mindsdb documentationhttpsdocsmindsdbcom slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "sentiment-analysis-intercom-data-airbyte-checkpoint.mdx",
    "path": "docs/tutorials/.ipynb_checkpoints/sentiment-analysis-intercom-data-airbyte-checkpoint.mdx",
    "chunk_id": 0,
    "chunk_content": "augmenting intercom chats gpt sentiment analysis introduction tutorial well analyze sentiment intercom conversations well use airbyte extract data intercom store google bigquery database well make data available mindsdb create deploy gpt model within mindsdb join data model predict sentiment values analyzed business analytics tools data setup use airbyte pull data intercom load google bigquery database try airbyte cloudhttpsairbytecomairbytecloud use airbyte open sourcehttpsgithubcomairbytehqairbyte version follow videohttpswwwyoutubecomwatchv9ejcukc9k7u detailed tutorial learn pull conversation data intercom load google bigquery database using airbyte visualize using metabase analytics connecting database connect google bigquery database stores intercom data mindsdb sql create database bqdataset engine bigquery parameters project_id bgtest1111 dataset mydataset service_account_keys tmpkeysjson follow instructionhttpsdocsmindsdbcomdataintegrationsgooglebigquery connect database mindsdb creating model lets create gpt model well use predict sentiment intercom conversations tip creating openai model please create engine providing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey tip sql create model sentiment_classifier_model predict sentiment using engine openai_engine model_name textdavinci003 prompt_template describe user sentiment following conversation strictly positive neutral negativennfull_conversation nnsentiment positiveneutralnegative check status sql describe sentiment_classifier_model status complete proceeed make predictions making batch predictions join data table joined_conversations stores intercom conversations model sentiment_classifier_model created mindsdb thats make batch predictions conversations sql select joined_conversationsconversation_id joined_conversationsfull_conversation sentimentssentiment data"
  },
  {
    "filename": "sentiment-analysis-intercom-data-airbyte-checkpoint.mdx",
    "path": "docs/tutorials/.ipynb_checkpoints/sentiment-analysis-intercom-data-airbyte-checkpoint.mdx",
    "chunk_id": 1,
    "chunk_content": "table stores conversations using native bigquery syntax select bqdataset select conversation_id string_aggpbody n order pcreated_at asc limit 15 full_conversation conversation_parts p json_valuepauthor type user group conversation_id limit 10 joined_conversations join model analyzes sentiment sentiment_classifier_model sentiments run mindsdb editor find output whats next want learn mindsdb check resources mindsdbhttpsmindsdbcom mindsdb documentationhttpsdocsmindsdbcom slackhttpsmindsdbcomjoincommunity githubhttpsgithubcommindsdbmindsdb"
  },
  {
    "filename": "community-sdk.mdx",
    "path": "docs/sdks/community-sdk.mdx",
    "chunk_id": 0,
    "chunk_content": "title community sdk sidebartitle community sdks list community sdks mindsdb listed mindsdb java sdk githubhttpsgithubcommdabidhussainmindsdbjavasdk documentationhttpsmindsdbjavasdkhashnodespacedocs mavenhttpscentralsonatypecomartifactiogithubmdabidhussainmindsdbjavasdk javadochttpsmdabidhussaingithubiomindsdbjavasdk"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/sdks/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title sdks sidebartitle overview icon rectanglecode mindsdb provides sdks mongodb python javascript enabling incorporation ai building blocks development environments section introduces custom syntax provided mindsdb bring data ai together inside mongodb python javascript development environments follow steps get started steps step titleset development environment mongodb use mongodb compasssdksmongoconnectmongocompass mongodb shellsdksmongoconnectmongoshellbrbr python install packagesdkspythoninstallationbrbr javascript install packagesdksjavascriptinstallationbrbr step step titleconnect data source connect data source mongodbsdksmongodatabasesinsertone pythonsdkspythoncreate_database javascriptsdksjavascriptcreate_databasebrbr explore available data sources hereintegrationsdataoverview step step titleconfigure ai engine configure ai engine mongodbsdksmongoml_enginesinsertone pythonsdkspythoncreate_ml_engine javascriptsdksjavascriptcreate_ml_enginebrbr explore available ai engines hereintegrationsaioverview step step titlecreate deploy aiml model create deploy aiml model mongodbsdksmongomodelsinsertone pythonsdkspythoncreate_model javascriptsdksjavascriptcreate_model step step titlequery predictions query predictions mongodbsdksmongomodelsgetbatchpredictions pythonsdkspythongetbatchpredictions javascriptsdksjavascriptbatchquery step step titleautomate customized workflows automate tasks scheduling docs mongodbsdksmongojobsinsertone pythonsdkspythoncreate_job javascriptsdksjavascriptcreate_job step steps"
  },
  {
    "filename": "query_files.mdx",
    "path": "docs/sdks/python/query_files.mdx",
    "chunk_id": 0,
    "chunk_content": "title query file sidebartitle query file description mindsdb files treated tables stored default files database query file must save files database variable run query function syntax syntax serverget_databasefilesqueryselect file_name"
  },
  {
    "filename": "list_models.mdx",
    "path": "docs/sdks/python/list_models.mdx",
    "chunk_id": 0,
    "chunk_content": "title list models sidebartitle list models description list_models function lists models available defined project syntax use list_models method list models contained project python projectlist_models"
  },
  {
    "filename": "delete_table.mdx",
    "path": "docs/sdks/python/delete_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove table sidebartitle remove table description tablesdrop method enables delete table connected data source syntax syntax sql data_sourcetablesdroptable_name"
  },
  {
    "filename": "list_ml_handlers.mdx",
    "path": "docs/sdks/python/list_ml_handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml handlers sidebartitle list ml handlers fetch available ml handlers directly python code mindsdb serverget_projectmindsdb ml_handlers mindsdbqueryshow handlers type ml printml_handlersfetch"
  },
  {
    "filename": "list_views.mdx",
    "path": "docs/sdks/python/list_views.mdx",
    "chunk_id": 0,
    "chunk_content": "title list views sidebartitle list views description list_views function executed project lists views available project syntax use list_views method list views project python projectlist_views"
  },
  {
    "filename": "list_databases.mdx",
    "path": "docs/sdks/python/list_databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data sources sidebartitle list data sources description list_databases function lists data sources connected mindsdb syntax use list_databases method list databases python serverlist_databases"
  },
  {
    "filename": "get-single-prediction.mdx",
    "path": "docs/sdks/python/get-single-prediction.mdx",
    "chunk_id": 0,
    "chunk_content": "title get single prediction sidebartitle get single prediction description predict function fetches predictions model table syntax use predict method make single prediction passing specific values argument python my_modelpredicttext text"
  },
  {
    "filename": "create_model.mdx",
    "path": "docs/sdks/python/create_model.mdx",
    "chunk_id": 0,
    "chunk_content": "title create train deploy model sidebartitle create train deploy model description modelsget modelscreate methods enable get existing model create deploy new model syntax use modelsget method get existing model python my_model projectmodelsgetmy_model create method create train new model python my_model projectmodelscreate name my_model predict target query my_table please note case llm models parameters stored options syntax create openai model python sentiment_classifier projectmodelscreate namesentiment_classifier engineopenai alternatively engineserverml_enginesopenai predictsentiment options prompt_templateanswer question questions model_namegpt4 alternatively skip options define parameters arguments python sentiment_classifier projectmodelscreate namesentiment_classifier engineopenai alternatively engineserverml_enginesopenai predictsentiment prompt_templateanswer question questions model_namegpt4 case timeseries model additional options stored timeseries_options syntax create timeseries model sql ts_model projectmodelscreate nametime_series_model predicttarget querymy_table timeseries_options order order_date group category window 30 horizon 4"
  },
  {
    "filename": "query_table.mdx",
    "path": "docs/sdks/python/query_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title query table sidebartitle query table description query function executed data source connected mindsdb saved variable queries table data source syntax syntax sql my_data_sourcequeryselect my_table limit 100 query newly added data using functionality introduced last keywordmindsdb_sqlsqlcreatejobslast follows sql query serverdatabasesmy_data_sourcetablestable_namefiltercolumn_namevaluetracktimestamp_column first call returns records df queryfetch second call returns rows timestamp_column greater timestamp previous fetch df queryfetch"
  },
  {
    "filename": "get_history.mdx",
    "path": "docs/sdks/python/get_history.mdx",
    "chunk_id": 0,
    "chunk_content": "title get job history sidebartitle get job history description get_history function lets access job history information find job record job execution including execution errors syntax use get_history method get history job execution python my_jobget_history"
  },
  {
    "filename": "get-batch-predictions.mdx",
    "path": "docs/sdks/python/get-batch-predictions.mdx",
    "chunk_id": 0,
    "chunk_content": "title get batch predictions sidebartitle get batch predictions description predict function fetches predictions model table syntax use predict method make batch predictions passing data table argument python my_modelpredictmy_tablelimit10 tip querying predictions specify partition_size parameter split data partitions run prediction different workers note ml task queuesetupcustomconfigoverviewofconfigparameters needs enabled use parameter use partition_size parameter provide argument predict function specifying partition size like my_modelpredictdf paramspartition_size 2 tip"
  },
  {
    "filename": "refresh_model.mdx",
    "path": "docs/sdks/python/refresh_model.mdx",
    "chunk_id": 0,
    "chunk_content": "title refresh model sidebartitle refresh model description refresh function synchronizes model mindsdb syntax use refresh method refresh models data mindsdb server python model_namerefresh"
  },
  {
    "filename": "create_view.mdx",
    "path": "docs/sdks/python/create_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title create view sidebartitle create view description get_view create_view functions let save either existing view newly created view variable syntax use get_view method get existing view python my_view projectget_viewmy_view create_view method create view python my_view projectcreate_view view_name mysql_demo_dbqueryselect my_table limit 100"
  },
  {
    "filename": "insert_into_table.mdx",
    "path": "docs/sdks/python/insert_into_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title insert table sidebartitle insert table description insert function executed table data source connected mindsdb inserts data table syntax syntax sql my_tableinserttable_to_be_inserted"
  },
  {
    "filename": "agents_knowledge_bases.mdx",
    "path": "docs/sdks/python/agents_knowledge_bases.mdx",
    "chunk_id": 0,
    "chunk_content": "title knowledge bases sidebartitle knowledge bases description mindsdb create deploy ai agents comprise ai models customizable skills knowledge bases texttosql tip learn ai agents heresdkspythonagents tip two types skills available mindsdb 1 text2sql skill translates users questions sql queries fetches relevant information data sources assigned skill 2 knowledge bases store data databases files webpages use different retrieval methods fetch relevant information present answers syntax feed agent data different sources use knowledge_base skills create knowledge base python my_kb serverknowledge_basescreatemy_kb insert data knowledge base inserting data files note file uploaded mindsdb inserting knowledge base python my_kbinsert_filesfile_name_uploaded_to_mindsdb inserting data web python my_kbinsert_webpageshttpsdocsmindsdbcom 1 optionally specify crawl depth second argument example crawl depth 1 indicates links present httpsdocsmindsdbcom crawled crawl depth 2 indicates links present pages crawled inserting data data sources connected mindsdb python my_kbinsertserverdatabasesdb_nametablestable_namefiltercolumn_namevalue inserting raw data python my_kbinsertcolumn_name value column_name value tip inserting data knowledge base specify partition_size parameter split data partitions run prediction different workers note ml task queuesetupcustomconfigoverviewofconfigparameters needs enabled use parameter use partition_size parameter provide argument insert function specifying partition size like my_kbinsertdf paramsmodel partition_size 100 tip query data knowledge base using specified keywords python df my_kbfetch df my_kbfindkeywordfetch list available knowledge bases python kb_list serverknowledge_baseslist get knowledge base"
  },
  {
    "filename": "agents_knowledge_bases.mdx",
    "path": "docs/sdks/python/agents_knowledge_bases.mdx",
    "chunk_id": 1,
    "chunk_content": "name python kb serverknowledge_basesgetmy_kb kb serverknowledge_basesmy_kb drop knowledge base python serverknowledge_basesdropmy_kb"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/sdks/python/retrain.mdx",
    "chunk_id": 0,
    "chunk_content": "title retrain model sidebartitle retrain model description retrain function retrains model example new version mindsdb available syntax use retrain method retrain model python model_nameretrain"
  },
  {
    "filename": "delete_from.mdx",
    "path": "docs/sdks/python/delete_from.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete table sidebartitle delete table description delete function executed table data source connected mindsdb deletes rows table syntax syntax sql data_sourcetablestable_namedeletekeyvalues"
  },
  {
    "filename": "query_view.mdx",
    "path": "docs/sdks/python/query_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title query view sidebartitle query view description query function executed view resides one projects syntax syntax sql project_namequeryselect my_projectmy_view limit 100 query newly added data using functionality introduced last keywordmindsdb_sqlsqlcreatejobslast follows sql query serverdatabasesmy_data_sourceviewsview_namefiltercolumn_namevaluetracktimestamp_column first call returns records df queryfetch second call returns rows timestamp_column greater timestamp previous fetch df queryfetch"
  },
  {
    "filename": "create_project.mdx",
    "path": "docs/sdks/python/create_project.mdx",
    "chunk_id": 0,
    "chunk_content": "title create project sidebartitle create project description get_project create_project functions fetch existing project create new one syntax use get_project method get default mindsdb project python project serverget_project use get_project method get project python project serverget_projectproject_name use create_project method create new project python project servercreate_projectproject_name"
  },
  {
    "filename": "list_projects.mdx",
    "path": "docs/sdks/python/list_projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title list projects sidebartitle list projects description list_projects function lists available projects syntax use list_projects method lists available projects python serverlist_projects"
  },
  {
    "filename": "create_ml_engine.mdx",
    "path": "docs/sdks/python/create_ml_engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title configure ml engine sidebartitle configure ml engine create ml engine directly python code serverml_enginescreate ml_engine_name ml_engine_handler connection_dataml_engine_handler_api_key 111"
  },
  {
    "filename": "drop_view.mdx",
    "path": "docs/sdks/python/drop_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove view sidebartitle remove view description drop_view function removes view mindsdb syntax use drop_view method remove view python projectdrop_viewview_name"
  },
  {
    "filename": "delete_file.mdx",
    "path": "docs/sdks/python/delete_file.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove file sidebartitle remove file description mindsdb files treated tables stored default files database delete file must save files database variable run tablesdrop function syntax syntax sql files serverget_databasefiles filestablesdropfile_name"
  },
  {
    "filename": "drop_project.mdx",
    "path": "docs/sdks/python/drop_project.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove project sidebartitle remove project description drop_project function removed project mindsdb syntax use drop_project method remove project python serverdrop_projectproject_name"
  },
  {
    "filename": "drop_database.mdx",
    "path": "docs/sdks/python/drop_database.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove data source sidebartitle remove data source description drop_database function enables remove defined data source connection mindsdb syntax use drop_database method remove database python serverdrop_databasemysql_demo_db"
  },
  {
    "filename": "create_table.mdx",
    "path": "docs/sdks/python/create_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title create table sidebartitle create table description get_table create_table functions let save either existing table newly created table variable syntax use get_table method fetch table mysql_demo_db database python my_table mysql_demo_dbget_tablemy_table create_table method create new table python option 1 my_table mysql_demo_dbcreate_tablemy_table select some_table keyvalue option 2 my_table mysql_demo_dbcreate_tablemy_table base_table option 3 my_table mysql_demo_dbcreate_tablemy_table base_tablefilterkeyvalue"
  },
  {
    "filename": "get_status.mdx",
    "path": "docs/sdks/python/get_status.mdx",
    "chunk_id": 0,
    "chunk_content": "title get status model sidebartitle get status model description get_status function lets fetch current status model example see model completed training phase syntax use get_status method check training status model python my_modelget_status"
  },
  {
    "filename": "update_table.mdx",
    "path": "docs/sdks/python/update_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title update table sidebartitle update table description update function executed table data source connected mindsdb updates table specified columns syntax syntax sql my_tableupdatetable_used_to_update oncolumn1 column2 info check sql syntaxsqlapiupdate better understand update function works info"
  },
  {
    "filename": "create_database.mdx",
    "path": "docs/sdks/python/create_database.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect data source sidebartitle connect data source description get_database create_database functions enable use existing data source connect new one syntax use get_database method get existing database python mysql_demo_db serverget_databasemysql_demo_db create_database method connect new data source mindsdb python mysql_demo_db servercreate_database engine mysql name mysql_demo_db connection_args user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public"
  },
  {
    "filename": "list_jobs.mdx",
    "path": "docs/sdks/python/list_jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title list jobs sidebartitle list jobs description list_jobs function executed project lists jobs available project syntax use list_jobs method list jobs project python projectlist_jobs"
  },
  {
    "filename": "agents.mdx",
    "path": "docs/sdks/python/agents.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai agents llms skills sidebartitle agents description mindsdb create deploy ai agents comprise ai models customizable skills knowledge bases texttosql p aligncenter img srcassetsagent_diagrampng p ai agents comprise skills text2sql knowledge_base conversational model skills provide data resources agent enabling answer questions available data learn skills heresdkspythonagents_skills learn knowledge bases heresdkspythonagents_knowledge_bases conversational model like openai langchain utilizes tools skillshttpspythonlangchaincomdocshow_totools respond user input users customize models prompts fit use cases syntax creating agent creating agent use default conversational model python agent serveragentscreatenew_demo_agent specify model parameters python agent serveragentscreate namenew_demo_agent modelgpt4 openai_api_keyopenai_api_key prompt_templatehello ask question question temperature00 max_tokens1000 top_p10 top_k0 use existing model python model servermodelsgetexisting_model agent serveragentscreatedemo_agent model furthermore list existing agents get agents name update agents delete agents python list agents agents agentslist get agent name agent agentsgetmy_agent update agent new_model modelsgetnew_model agentmodel_name new_modelname new_skill skillscreatenew_skill sql tables new_table database new_database updated_agentskillsappendnew_skill updated_agent agentsupdatemy_agent agent delete agent name agentsdeletemy_agent assigning skills agent add skills agent providing data stored databases files webpages retrieval skill similar knowledge basessdkspythonagents_knowledge_bases python add data one files skills agentadd_filefile_nametxt file content description agentadd_filesfile_namepdf file_namepdf files content description add data one webpages skills agentadd_webpagesexample1com example2com webpages content description text2sql skill retrieves relevant information databases python add data"
  },
  {
    "filename": "agents.mdx",
    "path": "docs/sdks/python/agents.mdx",
    "chunk_id": 1,
    "chunk_content": "database connected mindsdb skills db serverdatabasescreatedatasource_name engine_name database dbdb_name db serverdatabasesgetdatasource_name agentadd_databasedbname table_name data description tip learn data sources integrated mindsdbintegrationsdataoverview tip querying agent created agent assigned set skills ask questions related data python question ask question answer agentcompletionquestion question answer none printanswercontent example sample python code deploy agent python import mindsdb_sdk con mindsdb_sdkconnect important code requires set openai_api_key env variable agent conagentscreatefnew_demo_agent printadding hooblyblob details agentadd_filehooblyblobtxt details company hooblyblob printadding rulebook details agentadd_filescodenamesrulebookpdf rulebooks various board games printadding mindsdb docs agentadd_webpagesdocsmindsdbcom documentation mindsdb printagent ready use true printask question question input answer agentcompletionquestion question answer none printanswercontent"
  },
  {
    "filename": "query_projects.mdx",
    "path": "docs/sdks/python/query_projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title query projects sidebartitle query projects description query methods enables run queries models tables views stored project syntax use query method submit query project python query projectqueryselect my_table queryfetch"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/sdks/python/describe.mdx",
    "chunk_id": 0,
    "chunk_content": "title describe model sidebartitle describe model description describe function returns information model syntax use describe method get information model python model_namedescribe"
  },
  {
    "filename": "drop_job.mdx",
    "path": "docs/sdks/python/drop_job.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove job sidebartitle remove job description drop_job function deletes job mindsdb syntax use drop_job method remove job python projectdrop_jobjob_name"
  },
  {
    "filename": "installation.mdx",
    "path": "docs/sdks/python/installation.mdx",
    "chunk_id": 0,
    "chunk_content": "title installation sidebartitle installation python sdk enables connect mindsdb server python using http api read along see install test mindsdb python sdk simple installation install mindsdb python sdk run command bash pip install mindsdb_sdk expected output p aligncenter img srcassetspythonsdk_install_outputpng p advanced installation instead using pip install mindsdb_sdk command install cloning python sdk repositoryhttpsgithubcommindsdbmindsdb_python_sdk create virtual environment install dependencies requirementstxt file run tests instructed test components go project directory mindsdb_sdk run command bash env pythonpath pytest generate api documentation run commands bash pip install sphinx cd docs make html documentation generated docsbuildhtml folder"
  },
  {
    "filename": "list_data_handlers.mdx",
    "path": "docs/sdks/python/list_data_handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data handlers sidebartitle list data handlers fetch available data handlers directly python code mindsdb serverget_projectmindsdb data_handlers mindsdbqueryshow handlers type data printdata_handlersfetch"
  },
  {
    "filename": "upload_file.mdx",
    "path": "docs/sdks/python/upload_file.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload file sidebartitle upload file description mindsdb files treated tables stored default files database upload file must save files database variable run create_table function note note trailing whitespaces column names erased upon uploading file mindsdb note syntax syntax files_db serverget_databasefiles files_dbcreate_tablefile_name data_frame"
  },
  {
    "filename": "drop_model.mdx",
    "path": "docs/sdks/python/drop_model.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove model sidebartitle remove model description drop_model function removes model mindsdb syntax use drop_model method remove model python option 1 projectdrop_modelmy_model option 2 servermodelsdropmy_model"
  },
  {
    "filename": "native_queries.mdx",
    "path": "docs/sdks/python/native_queries.mdx",
    "chunk_id": 0,
    "chunk_content": "title native queries sidebartitle native queries description query function executed data source connected mindsdb saved variable native query executed directly data source syntax syntax sql my_data_sourcequeryselect datasource_name native query"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/sdks/python/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title overview sidebartitle overview interact mindsdb directly python code info next steps links help explore cardgroup cols3 card titleinstallation iconlink hrefsdkspythoninstallationcard card titleconnect iconlink hrefsdkspythonconnectcard card titlecreate models iconlink hrefsdkspythoncreate_modelcard cardgroup info"
  },
  {
    "filename": "refresh_job.mdx",
    "path": "docs/sdks/python/refresh_job.mdx",
    "chunk_id": 0,
    "chunk_content": "title refresh job sidebartitle refresh job description refresh function synchronizes job mindsdb syntax use refresh method retrieve job data mindsdb server python my_jobrefresh"
  },
  {
    "filename": "manage-model-versions.mdx",
    "path": "docs/sdks/python/manage-model-versions.mdx",
    "chunk_id": 0,
    "chunk_content": "title manage model versions sidebartitle manage model versions functions let work multiple version model use list_versions method view available version model python model_namelist_versions use get_version method use specific version model python model_nameget_version9 use set_active method set specific version model active python model_nameset_active9 use drop_model_version method remove specific version model python projectdrop_model_versionmy_model 3 please note model version deactivated removed"
  },
  {
    "filename": "finetune.mdx",
    "path": "docs/sdks/python/finetune.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune model sidebartitle finetune model description finetune function finetunes model specified data syntax use finetune method finetune model specific data python model_namefinetunedata_table"
  },
  {
    "filename": "list_ml_engines.mdx",
    "path": "docs/sdks/python/list_ml_engines.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml engines sidebartitle list ml engines fetch available ml engines directly python code serverml_enigneslist"
  },
  {
    "filename": "drop_ml_engine.mdx",
    "path": "docs/sdks/python/drop_ml_engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove ml engine sidebartitle remove ml engine remove one ml engines directly python code serverml_enginesdropml_engine_name"
  },
  {
    "filename": "connect.mdx",
    "path": "docs/sdks/python/connect.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect sidebartitle connect documentation describes connect mindsdb server python code connect local mindsdb server import mindsdb_sdk connects default port 47334 localhost server mindsdb_sdkconnect connects specified host port server mindsdb_sdkconnecthttp12700147334"
  },
  {
    "filename": "create_job.mdx",
    "path": "docs/sdks/python/create_job.mdx",
    "chunk_id": 0,
    "chunk_content": "title create job sidebartitle create job description get_job create_job functions let save either existing job newly created job variable syntax use get_job method get existing job python my_job projectget_jobmy_job create_job method create job python my_job projectcreate_job job_name select models repeat_str 1 hour alternatively create job using syntax python projectjobscreatenamejob_name repeat_min1 job jobadd_querymodelretrain jobadd_querymodelpredictdatabasetablestbl1 jobadd_querykbinsertdatabasetablestbl1 jobadd_queryshow models namejob_name job name repeat_min1 indicates periodicity job minutes jobadd_querymodelretrain adds task job retrain model jobadd_querymodelpredictdatabasetablestbl1 adds task job make predictions jobadd_querykbinsertdatabasetablestbl1 adds task job insert data knowledge base jobadd_queryshow models adds task job run statement provided string value note add_query method adds tasks job takes either string query argument note method enables job manipulate knowledge bases models tables views queries databases handlers jobs ml engines projects"
  },
  {
    "filename": "agents_skills.mdx",
    "path": "docs/sdks/python/agents_skills.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai agents skills sidebartitle skills description mindsdb create deploy ai agents comprise ai models customizable skills knowledge bases texttosql tip learn ai agents heresdkspythonagents tip two types skills available mindsdb 1 text2sql skill translates users questions sql queries fetches relevant information data sources assigned skill 2 knowledge bases store data databases files webpages use different retrieval methods fetch relevant information present answers syntax create new skill python text_to_sql_skill skillscreatetext_to_sql sql tables my_table database my_database note note required assign database set tables used skill note list available skills python skills skillslist get existing skill name python skill skillsgetmy_skill update skill new datasets python skillparams tables new_table database new_database updated_skill skillsupdatemy_skill skill delete skill python skillsdeletemy_skill create agent assign skill python agent agentscreatemy_agent model_name text_to_sql_skill"
  },
  {
    "filename": "join_on.mdx",
    "path": "docs/sdks/python/join_on.mdx",
    "chunk_id": 0,
    "chunk_content": "title join tables sidebartitle join tables description query function executed data source connected mindsdb saved variable performs join operation tables syntax syntax sql my_data_sourcequeryselect my_table join another_table ta limit 100"
  },
  {
    "filename": "query_files.mdx",
    "path": "docs/sdks/javascript/query_files.mdx",
    "chunk_id": 0,
    "chunk_content": "title query file sidebartitle query file description runquery function executes query given argument directly mindsdb syntax syntax const query select filesfile_name const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "list_models.mdx",
    "path": "docs/sdks/javascript/list_models.mdx",
    "chunk_id": 0,
    "chunk_content": "title list models sidebartitle list models list models using code const query show models result await mindsdbsqlrunqueryquery consolelogresult"
  },
  {
    "filename": "delete_table.mdx",
    "path": "docs/sdks/javascript/delete_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove table sidebartitle remove table info feature progress info"
  },
  {
    "filename": "query_jobs.mdx",
    "path": "docs/sdks/javascript/query_jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title query jobs sidebartitle query jobs info feature progress info"
  },
  {
    "filename": "list_ml_handlers.mdx",
    "path": "docs/sdks/javascript/list_ml_handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml handlers sidebartitle list ml handlers fetch available ml handlers directly javascript code const query show handlers type ml result await mindsdbsqlrunqueryquery consolelogresult"
  },
  {
    "filename": "list_views.mdx",
    "path": "docs/sdks/javascript/list_views.mdx",
    "chunk_id": 0,
    "chunk_content": "title list views sidebartitle list views description getallviews function lists available views syntax list available views sql const allviews await mindsdbviewsgetallviews consolelogall views allviewsforeachv consolelogvname"
  },
  {
    "filename": "list_databases.mdx",
    "path": "docs/sdks/javascript/list_databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data sources sidebartitle list data sources list data sources using code const query show full databases type data result await mindsdbsqlrunqueryquery alternative es6 module syntax mindsdbdefaultsqlrunqueryquery consolelogresult"
  },
  {
    "filename": "create_model.mdx",
    "path": "docs/sdks/javascript/create_model.mdx",
    "chunk_id": 0,
    "chunk_content": "title create train deploy model sidebartitle create train deploy model training model requires various parameters depending model type present examples regression time series openai models tip useful links training optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_trainingoptionstrainingoptionshtml query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsqueryoptionshtml batch query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsbatchqueryoptionshtml tip regression model lets look example create train simple regression model use making predictions defining training options const regressiontrainingoptions select select demo_datahome_rentals integration example_db try creating training model returned promise resolves model created training actually complete let homerentalpricemodel await mindsdbmodelstrainmodel home_rentals_model rental_price mindsdb regressiontrainingoptions consolelogcreated model waiting training complete homerentalpricemodelstatus complete homerentalpricemodelstatus error homerentalpricemodel await mindsdbmodelsgetmodelhome_rentals_model mindsdb checking models status consolelogmodel status homerentalpricemodelstatus defining query options const queryoptions sqft 823 location good neighborhood downtown days_on_market 10 querying single prediction const rentalpriceprediction await homerentalpricemodelqueryqueryoptions consolelogpredicted value consolelogrentalpricepredictionvalue consolelogprediction insights consolelogrentalpricepredictionexplain consoleloginput data consolelogrentalpricepredictiondata catch error something went wrong training querying consolelogerror time series model time series models require parameters lets go example create train time series model use making batch predictions defining training options const timeseriestrainingoptions integration example_db select select demo_datahouse_sales orderby saledate groupby bedrooms window 8 horizon 4 try creating training model let housesalesforecastmodel await mindsdbmodelstrainmodel house_sales_model mindsdb timeseriestrainingoptions consolelogcreated model waiting training complete housesalesforecastmodelstatus complete housesalesforecastmodelstatus error housesalesforecastmodel await mindsdbmodelsgetmodelhouse_sales_model mindsdb checking models status"
  },
  {
    "filename": "create_model.mdx",
    "path": "docs/sdks/javascript/create_model.mdx",
    "chunk_id": 1,
    "chunk_content": "consolelogmodel status housesalesforecastmodelstatus describing model const modeldescription await housesalesforecastmodeldescribe consolelogmodel description consolelogmodeldescription defining query options const queryoptions join model data source join example_dbdemo_datahouse_sales using batch queries alias used joined data source short trainingtest alias used trained model queried tsaledate latest tbedrooms 2 limit 4 querying batch predictions const rentalpriceforecasts await housesalesforecastmodelbatchqueryqueryoptions consolelogbatch predictions rentalpriceforecastsforeachf consolelogfvalue consolelogfexplain consolelogfdata catch error something went wrong training predicting consolelogerror openai model create deploy openai model javascript code defining training options const trainingoptions using engine openai prompt_template describe sentiment reviews strictly positive neutral negative love productpositive scamnegative review try creating training model let openai_js await mindsdbmodelstrainmodel openai_js sentiment mindsdb trainingoptions consolelogcreated model waiting training complete openai_jsstatus complete openai_jsstatus error openai_js await mindsdbmodelsgetmodelopenai_js mindsdb checking models status consolelogmodel status openai_jsstatus defining query options const queryoptions review ok querying single prediction openai_js await openai_jsqueryqueryoptions consolelogpredicted value consolelogopenai_jsvalue consolelogprediction insights consolelogopenai_jsexplain consoleloginput data consolelogopenai_jsdata catch error something went wrong training querying consolelogerror"
  },
  {
    "filename": "query_table.mdx",
    "path": "docs/sdks/javascript/query_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title query table sidebartitle query table description runquery function executes query given argument directly mindsdb syntax syntax const query select table_name const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "create_view.mdx",
    "path": "docs/sdks/javascript/create_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title create view sidebartitle create view description createview function creates view mindsdb syntax syntax const viewselect select tsqft tlocation mrental_price mysql_demo_dbhome_rentals join mindsdbhome_rentals_model const predictionsview await mindsdbviewscreateview view_name project_name viewselect"
  },
  {
    "filename": "insert_into_table.mdx",
    "path": "docs/sdks/javascript/insert_into_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title insert table sidebartitle insert table description runquery function executes query given argument directly mindsdb syntax syntax const query insert integration_nametable_name select const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/sdks/javascript/retrain.mdx",
    "chunk_id": 0,
    "chunk_content": "title retrain model sidebartitle retrain model description retrain fuction lets us retrain model example new version mindsdb available syntax get existing model retrain update status available getting existing model const homerentalpricemodel await mindsdbmodelsgetmodelhome_rentals_model mindsdb checking status retraining model required homerentalpricemodelupdatestatus available try custom retraining homerentalpricemodelretrainexample_db trainingoptions await homerentalpricemodelretrain consolelogretrained model catch error something went wrong retraining consolelogerror find full training options herehttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_trainingoptionstrainingoptionshtml"
  },
  {
    "filename": "delete_from.mdx",
    "path": "docs/sdks/javascript/delete_from.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete table sidebartitle delete table description runquery function executes query given argument directly mindsdb syntax syntax const query delete datasource_nametable_name const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "query_view.mdx",
    "path": "docs/sdks/javascript/query_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title query view sidebartitle query view description runquery function executes query given argument directly mindsdb syntax syntax const query select project_nameview_name const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "create_project.mdx",
    "path": "docs/sdks/javascript/create_project.mdx",
    "chunk_id": 0,
    "chunk_content": "title create project sidebartitle create project info feature progress info"
  },
  {
    "filename": "list_projects.mdx",
    "path": "docs/sdks/javascript/list_projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title list projects sidebartitle list projects description getallprojects function lists available projects syntax list available projects const allprojects await mindsdbprojectsgetallprojects consolelogall projects allprojectsforeachp consolelogpname"
  },
  {
    "filename": "create_ml_engine.mdx",
    "path": "docs/sdks/javascript/create_ml_engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title configure ml engine sidebartitle configure ml engine info feature progress info"
  },
  {
    "filename": "drop_view.mdx",
    "path": "docs/sdks/javascript/drop_view.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove view sidebartitle remove view description deleteview function deletes existing view mindsdb syntax syntax await mindsdbviewsdeleteview view_name project_name"
  },
  {
    "filename": "delete_file.mdx",
    "path": "docs/sdks/javascript/delete_file.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove file sidebartitle remove file info feature progress info"
  },
  {
    "filename": "get_database.mdx",
    "path": "docs/sdks/javascript/get_database.mdx",
    "chunk_id": 0,
    "chunk_content": "title get data source sidebartitle get data source save data sources variable using code const db await mindsdbdatabasesgetdatabasemysql_datasource"
  },
  {
    "filename": "drop_project.mdx",
    "path": "docs/sdks/javascript/drop_project.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove project sidebartitle remove project info feature progress info"
  },
  {
    "filename": "drop_database.mdx",
    "path": "docs/sdks/javascript/drop_database.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove data source sidebartitle remove data source description delete function removes data source mindsdb please note order delete connected data source need fetch first getdatabase function syntax get existing database remove try const db await mindsdbdatabasesgetdatabasemysql_datasource consoleloggot database deleting database db try await dbdelete consolelogdeleted database catch error couldnt delete database consolelogerror catch error couldnt connect database consolelogerror"
  },
  {
    "filename": "create_table.mdx",
    "path": "docs/sdks/javascript/create_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title create table sidebartitle create table info feature progress info"
  },
  {
    "filename": "get_status.mdx",
    "path": "docs/sdks/javascript/get_status.mdx",
    "chunk_id": 0,
    "chunk_content": "title get status model sidebartitle get status model description status property model stores status generating training complete syntax syntax sql model_namestatus"
  },
  {
    "filename": "update_table.mdx",
    "path": "docs/sdks/javascript/update_table.mdx",
    "chunk_id": 0,
    "chunk_content": "title update table sidebartitle update table description runquery function executes query given argument directly mindsdb syntax syntax const query update integration_nametable_name set column_name new_value column_name old_value const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "create_database.mdx",
    "path": "docs/sdks/javascript/create_database.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect data source sidebartitle connect data source description mindsdbdatabasescreatedatabase function connects new data source mindsdb syntax connect sample mysql database const connectionparams user user port 3306 password mindsdbuser123 host samplesmindsdbcom database public try const mysqldatabase await mindsdbdatabasescreatedatabase mysql_datasource mysql connectionparams consolelogconnected database catch error couldnt connect database consolelogerror first define connection parameters use createdatabase function connect database"
  },
  {
    "filename": "query.mdx",
    "path": "docs/sdks/javascript/query.mdx",
    "chunk_id": 0,
    "chunk_content": "title get single prediction sidebartitle get single prediction description query function fetches single prediction model syntax syntax await model_namequeryqueryoptions tip useful links training optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_trainingoptionstrainingoptionshtml query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsqueryoptionshtml batch query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsbatchqueryoptionshtml tip"
  },
  {
    "filename": "batchQuery.mdx",
    "path": "docs/sdks/javascript/batchQuery.mdx",
    "chunk_id": 0,
    "chunk_content": "title get batch predictions sidebartitle get batch predictions description batchquery function fetches batch predictions model joining data table syntax syntax await model_namebatchquerybatchqueryoptions tip useful links training optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_trainingoptionstrainingoptionshtml query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsqueryoptionshtml batch query optionshttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_queryoptionsbatchqueryoptionshtml tip"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/sdks/javascript/describe.mdx",
    "chunk_id": 0,
    "chunk_content": "title describe model sidebartitle describe model description describe function provides information models syntax syntax sql const modeldescription await model_namedescribe"
  },
  {
    "filename": "drop_job.mdx",
    "path": "docs/sdks/javascript/drop_job.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove job sidebartitle remove job info feature progress info"
  },
  {
    "filename": "installation.mdx",
    "path": "docs/sdks/javascript/installation.mdx",
    "chunk_id": 0,
    "chunk_content": "title installation sidebartitle installation mindsdb javascript sdk allows unlock power machine learning right inside web applications read along see install mindsdbs javascript sdk install install mindsdb javascript sdk run command bash npm install save mindsdbjssdk expected output p aligncenter img srcassetsjssdk_install_outputpng p"
  },
  {
    "filename": "list_data_handlers.mdx",
    "path": "docs/sdks/javascript/list_data_handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data handlers sidebartitle list data handlers fetch available data handlers directly javascript code const query show handlers type data result await mindsdbsqlrunqueryquery consolelogresult"
  },
  {
    "filename": "upload_file.mdx",
    "path": "docs/sdks/javascript/upload_file.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload file sidebartitle upload file info feature progress info"
  },
  {
    "filename": "drop_model.mdx",
    "path": "docs/sdks/javascript/drop_model.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove model sidebartitle remove model description delete function removes model mindsdb syntax get existing database remove model mindsdbmodelstrainmodel modeldelete"
  },
  {
    "filename": "native_queries.mdx",
    "path": "docs/sdks/javascript/native_queries.mdx",
    "chunk_id": 0,
    "chunk_content": "title native queries sidebartitle native queries description runquery function executes query given argument directly mindsdb native queries syntax ensures query executed directly connected data source syntax syntax const query select datasource_name native query const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/sdks/javascript/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title overview sidebartitle overview interact mindsdb directly javascript code info next steps links help explore cardgroup cols3 card titleinstallation iconlink hrefsdksjavascriptinstallationcard card titleconnect iconlink hrefsdksjavascriptconnectcard card titlecreate models iconlink hrefsdksjavascriptcreate_modelcard cardgroup info"
  },
  {
    "filename": "manage-model-versions.mdx",
    "path": "docs/sdks/javascript/manage-model-versions.mdx",
    "chunk_id": 0,
    "chunk_content": "title manage model versions sidebartitle manage model versions info feature progress info"
  },
  {
    "filename": "finetune.mdx",
    "path": "docs/sdks/javascript/finetune.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune model sidebartitle finetune model description finetune fuction lets us finetune model specific data syntax finetune adjust model getting model const homerentalpricemodel await mindsdbmodelsgetmodelhome_rentals_model mindsdb consoleloggot model defining data used finetune model const adjustselect select demo_datahome_rentals days_on_market 10 const params key value try finetuningadjusting model specified dataset await homerentalpricemodelfinetune integration example_db select adjustselect using params consolelogfinetuned model catch error something went wrong adjusting consolelogerror find full adjust options herehttpsmindsdbgithubiomindsdbjssdkinterfacesmodels_trainingoptionsadjustoptionshtml"
  },
  {
    "filename": "list_ml_engines.mdx",
    "path": "docs/sdks/javascript/list_ml_engines.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml engines sidebartitle list ml engines info feature progress info"
  },
  {
    "filename": "drop_ml_engine.mdx",
    "path": "docs/sdks/javascript/drop_ml_engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove ml engine sidebartitle remove ml engine info feature progress info"
  },
  {
    "filename": "connect.mdx",
    "path": "docs/sdks/javascript/connect.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect sidebartitle connect performing operations must connect mindsdb default operations go mindsdb cloud rest apisrestsql use selfhosted version mindsdb well tabs tab titlelocal mindsdb connect local mindsdb server import mindsdb mindsdbjssdk const mindsdb requiremindsdbjssdkdefault alternative commonjs syntax try authentication needed selfhosting await mindsdbconnect alternative es6 module syntax await mindsdbdefaultconnect host http12700147334 consolelogconnected catcherror failed connect local instance consolelogerror tab tab titleown axios instance connect using axios instance see details default instancehttpsgithubcommindsdbmindsdbjssdkblobmainsrcutilhttpts import mindsdb mindsdbjssdk const mindsdb requiremindsdbjssdkdefault alternative commonjs syntax import axios axios use host option mindsdbconnect specify base url override const customaxios axioscreate timeout 1000 try await mindsdbconnect user mindsdbusergmailcom password mypassword httpclient customaxios consolelogconnected catcherror failed authenticate consolelogerror tab tabs tip please note methods use await must wrapped async function like async try authentication needed selfhosting await mindsdbconnect host http12700147334 consolelogconnected catcherror failed connect local instance consolelogerror tip"
  },
  {
    "filename": "create_job.mdx",
    "path": "docs/sdks/javascript/create_job.mdx",
    "chunk_id": 0,
    "chunk_content": "title create job sidebartitle create job info feature progress info"
  },
  {
    "filename": "join_on.mdx",
    "path": "docs/sdks/javascript/join_on.mdx",
    "chunk_id": 0,
    "chunk_content": "title join tables sidebartitle join tables description runquery function executes query given argument directly mindsdb syntax syntax const query select table_name join another_table ta const queryresult await mindsdbsqlrunqueryquery"
  },
  {
    "filename": "collection-structure.mdx",
    "path": "docs/sdks/mongo/collection-structure.mdx",
    "chunk_id": 0,
    "chunk_content": "title collection structure sidebartitle collection structure general structure startup mindsdb project consists 1 collection models verify running following mql commands 1 first switch mindsdb project bash use mindsdb switched db mindsdb tip doc page project entitysqlproject learn tip 2 query available collections bash show collections models models collection models collection stores information models name version status accuracy bash dbmodelsfind name home_rentals_model engine lightwood project mindsdb version 1 status complete accuracy 1 predict rental_price update_status up_to_date mindsdb_version 23240 error null select_data_query dbhome_rentalsfind training_options target rental_price using tag null created_at 20230224t160543248z _id objectid000000000000007725907968 name description name name model engine engine used create model project project model resides version version model status training status generating training complete error accuracy model accuracy 0999 sample accuracy value predict name target column predicted update_status training update status up_to_date updating available mindsdb_version mindsdb version used training 22821 sample version value error error message stores value case error otherwise null select_data_query query selects input data training_options additional training parameters tag models tags classify categories check doc pagesqlprojectworkingwithmodels learn create view drop models check doc pagesqlprojectworkingwithmodelsversion learn work models versions tip view databasesprojects use command bash show databases admin 6400 kib information_schema 6400 kib mindsdb 6400 kib files 6400"
  },
  {
    "filename": "collection-structure.mdx",
    "path": "docs/sdks/mongo/collection-structure.mdx",
    "chunk_id": 1,
    "chunk_content": "kib mongo_demo_db 6400 kib please note mindsdb default project doc page project entitysqlproject learn tip tip please follow mindsdb information schema doc pagesqltablestructure learn tip"
  },
  {
    "filename": "mongo.mdx",
    "path": "docs/sdks/mongo/mongo.mdx",
    "chunk_id": 0,
    "chunk_content": "train model mongodb api train model mongodbassetsdatabasesmongodbmongomdbcodepng note work progress please join slack channel questions train new model train new model need insertone new document inside mindsdbmodels collection object sent insertone training new model contain name string name model predict string feature want predict predict multiple features include list features connectionstring connection string connecting mongodb used gui connect mongodb connection used select_data_query object object contains info getting data train model databasestring name database collectionstring name collection finddict dict selects documents collection must valid json format dbcollectionfindhttpsdocsmongodbcommanualreferencemethoddbcollectionfind training_options dict optional value contains additional training parameters train timeseries model need provide training_options javascript dbmodelsinsertone name str predict str list fields connection str optional select_data_query database str collection str find dict training_options dict optional timeseries model dbmodelsinsertone name str predict str list fields connection str optional select_data_query database str collection str find dict training_options timeseries_settings order_by list fields group_by list fields optional horizon int optional use_previous_target boolean window int train new model example following example shows train new model mongo client collection used training model telcom customer churnhttpswwwkagglecomblastchartelcocustomerchurn dataset sql dbmodelsinsertone name churn predict churn select_data_query database test_data collection customer_churn find train model mongo shellassetspredictorsmongomongoinsertgif insert query train new model called churn predicts"
  },
  {
    "filename": "mongo.mdx",
    "path": "docs/sdks/mongo/mongo.mdx",
    "chunk_id": 1,
    "chunk_content": "customer churn value model training status check training finished successfully find model status inside mindsdbmodels collection eg javascript dbmodelsfind training model statusassetspredictorsmongomongostatusgif success thats tada trophy computer successfully trained new model mongo shell next step get predictions querying modelmongomongoquerythemodelfrommongodbapi delete model delete model run remove function models collection send name model delete javascript dbmodelsremove name model_name query model mongodb api get predictions model need call find method model collection provide values want get prediction object javascript dbmodel_namefind key value key value info note object provided find method must valid json format query example following example shows query model mongo client collection used training model telcom customer churnhttpswwwkagglecomblastchartelcocustomerchurn dataset mindsdb predict customer churn value based object values sent find method sql dbchurnfindphoneservice yesinternetservice dsl onlineservice nomonthlycharges 5385 totalcharges 10815 tenure 2 paperlessbilling yes get response mindsdb similar predicted_value confidence info yes 08 check json json churn yes churn_confidence 08 churn_explain class_distribution 044513007027299717 yes 05548699297270028 predicted_value yes confidence 08 prediction_quality confident"
  },
  {
    "filename": "find.mdx",
    "path": "docs/sdks/mongo/find.mdx",
    "chunk_id": 0,
    "chunk_content": "title making predictions using ml models sidebartitle find find method description find method used get predictions model table data persistent returned fly resultdocument syntax syntax sql dbpredictor_namefindcolumn value column value execution get json column_name1 value column_name2 value columns select_data_query null when_data null target_name_original value target_name_confidence value target_name_explain predicted_value value confidence value anomaly null truth null confidence_lower_bound value confidence_upper_bound value target_name_anomaly value target_name_min value target_name_max value expressions description target_name_original real value target variable collection target_name_confidence model confidence target_name_explain json object contains additional information predicted_value confidence anomaly truth confidence_lower_bound confidence_upper_bound target_name_anomaly model anomaly target_name_min lower bound value target_name_max upper bound value example making single prediction following mql statement fetches predicted value rental_price column home_rentals_model model predicted value rental price property attributes listed parameter find method sql dbhome_rentals_modelfindsqft 823 location good neighborhood downtown days_on_market 10 execution get json sqft 823 location good neighborhood downtown days_on_market 10 number_of_rooms null number_of_bathrooms null initial_price null rental_price 1431323795180614 select_data_query null when_data null rental_price_original null rental_price_confidence 099 rental_price_explain predicted_value 1431323795180614 confidence 099 anomaly null truth null confidence_lower_bound 13794387560440227 confidence_upper_bound 14832088343172054 rental_price_anomaly null rental_price_min 13794387560440227 rental_price_max 14832088343172054 making bulk predictions warning bulk predictions wip bulk predictions work progress warning"
  },
  {
    "filename": "ml_engine.mdx",
    "path": "docs/sdks/mongo/ml_engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title ml engines mongo sidebartitle dbml_engines create model need underlying ml engine lightwood openai mindsdb uses lightwood engine default none provided check available ml engines ml engines section dbml_enginesinsertone method description method creates ml engine based one available ml handlers syntax syntax sql dbml_enginesinsertone name ml_engine_name handler handler_name params key value execution get sql acknowledged true insertedid objectid6465e96766d152ae1e247802 example lets create openai engine uses openai handler sql dbml_enginesinsertone name openai_engine handler openai params openai_api_key qqq execution get sql acknowledged true insertedid objectid6465e9d566d152ae1e247803 dbml_enginesfind method description method lists available ml engines syntax syntax sql dbml_enginesfind execution get sql name lightwood handler lightwood connection_data key password value name openai handler openai connection_data key password value dbml_enginesdeleteone method description method deletes ml engine syntax syntax sql dbml_enginesdeleteonename ml_engine_name execution get sql acknowledged true deletedcount 1 example lets delete openai engine created earlier sql dbml_enginesdeleteonename openai_engine execution get sql acknowledged true deletedcount 1 tip check build ml handlercontributemlhandlers learn ml engines visit ml engines section tip"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/sdks/mongo/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title deleting predictor sidebartitle delete dbmodelsdeleteone method description dbmodelsdeleteone method deletes ml model specified argument syntax syntax sql dbmodelsdeleteonename predictor_name execution get json acknowledged true deletedcount 1 name description name name model deleted example listing predictors deleting predictor lets list available predictors using dbmodelsfind method sql dbmodelsfind execution get json name home_rentals_model status complete accuracy 10 predict rental_price update_status up_to_date mindsdb_version 22831 error null select_data_query training_options name other_model status complete accuracy 10 predict value_to_be_predicted update_status up_to_date mindsdb_version 22831 error null select_data_query training_options dropping predictor dbmodelsdeleteone method drops model collection called home_rentals_model sql dbmodelsdeleteonename home_rentals_model execution get json acknowledged true deletedcount 1 validating deletion validate model removed listing predictors sql dbmodelsfind execution get json name other_model status complete accuracy 10 predict value_to_be_predicted update_status up_to_date mindsdb_version 22831 error null select_data_query training_options"
  },
  {
    "filename": "insert.mdx",
    "path": "docs/sdks/mongo/insert.mdx",
    "chunk_id": 0,
    "chunk_content": "title creating predictors mongo sidebartitle predictorsinsertone predictors machine learning models enable us forecast future data based available data using dbmodelsinsertone method create train predictors mongo dbmodelsinsertone method description dbmodelsinsertone method creates trains new model syntax syntax sql dbmodelsinsertone name predictor_name predict target_column connection integration_name select_data_query dbcollection_namefind execution get json writeresult ninserted 1 expressions description name name model created predict name target column predicted connection name integration created via dbdatabasesinsertone methodmongodatabase file uploadsqlcreatefile select_data_query object stores data collection name used training validation additional arguments filtering data note checking predictor status running dbmodelsinsertone method execute dbmodelsfind method mindsdbmodels collection check status model sql dbmodelsfindname model_name note example creating predictor example shows create train home_rentals_model machine learning model predict rental prices real estate properties inside dataset sql dbmodelsinsertone name home_rentals_model predict rental_price connection mongo_integration select_data_query dbhome_rentalsfind execution get json writeresult ninserted 1 checking predictor status check predictor status query mindsdbmodels table using dbmodelsfind command sql dbmodelsfindname home_rentals_model execution get json name home_rentals_model status complete accuracy 091 predict rental_price update_status up_to_date mindsdb_version 22831 error null select_data_query training_options"
  },
  {
    "filename": "database.mdx",
    "path": "docs/sdks/mongo/database.mdx",
    "chunk_id": 0,
    "chunk_content": "title connecting databases mongo sidebartitle databasesinsertone integrations external databases provide data used making forecasts use databasesinsertone method connect integrations mongo dbdatabasesinsertone method description mindsdb enables adding databases mongo instance using dbdatabasesinsertone method mindsdb mongo api supports creating connection passing database credentials syntax syntax sql dbdatabasesinsertone name mongo_int engine mongodb connection_args port 27017 host mongodbsrvadminlocalhost database test_data execution get json acknowledged true insertedid objectid62dff63c6cc2fa93e1d7f12c name description name identifier data source created engine database engine selected connection_args keyvalue object storing connection parameters port host database example creating new connection example connect local mongodb sql dbdatabasesinsertone name mongo_local engine mongodb connection_args port 27017 host mongodbsrvadminlocalhost database test_data execution get json acknowledged true insertedid objectid62dff63c6cc2fa93e1d7f12c listing linked databases list linked databases using following command sql show dbs execution get sql database admin files information_schema mindsdb mongo_int views"
  },
  {
    "filename": "mindsdb-mongo-ql-overview.mdx",
    "path": "docs/sdks/mongo/mindsdb-mongo-ql-overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb mongoql sidebartitle overview mindsdb offers tailored mongoql syntax enables seamless interaction wide range objects including collections databases models moreover flexibility connect mindsdb mongodb compass mongodb shell facilitating streamlined integration note connecting mindsdb mongo switch default project executing use mindsdb note connect mindsdb section contains guides connect mindsdb mongodb clients cardgroup cols4 card titlemongodb compass iconlink hrefconnectmongocompasscard card titlemongodb shell iconlink hrefconnectmongoshellcard cardgroup mongoql syntax section contains guides work collections databases models cardgroup cols4 card titlecollection structure iconlink hrefsdksmongocollectionstructurecard card titleml engines iconlink hrefsdksmongoml_enginecard card titleconnect databases iconlink hrefsdksmongodatabasecard card titlecreate models iconlink hrefsdksmongoinsertcard card titledescribe models iconlink hrefsdksmongomodelsdescribecard card titlemake predictions iconlink hrefsdksmongofindcard card titledelete models iconlink hrefsdksmongodeletecard cardgroup"
  },
  {
    "filename": "stats_old.mdx",
    "path": "docs/sdks/mongo/stats_old.mdx",
    "chunk_id": 0,
    "chunk_content": "title getting statistics sidebartitle stats stats method stats method used display attributes existing model accepts scale attribute object argument call stats method sql dbpredictor_namestatsscale attribute name description predictor_name name predictor whose statistics want see scale attribute argument stats method defines type statistics scale features scale model scale ensemble stats method scale features parameter description dbpredictor_namestatsscale features method used display way model encoded data training syntax syntax sql dbpredictor_namestatsscale features execution get json data column number_of_rooms type categorical encoder onehotencoder role feature name description column name column type type inferred data encoder encoder used role role column feature target example lets describe home_rentals_model model sql dbhome_rentals_modelstatsscale features execution get json data column number_of_rooms type categorical encoder onehotencoder role feature column number_of_bathrooms type binary encoder binaryencoder role feature column sqft type float encoder numericencoder role feature column location type categorical encoder onehotencoder role feature column days_on_market type integer encoder numericencoder role feature column initial_price type integer encoder numericencoder role feature column neighborhood type categorical encoder onehotencoder role feature column rental_price type float encoder numericencoder role target ns mindsdbhome_rentals_model stats method scale model parameter description dbpredictor_namestatsscale model method used display performance candidate models syntax syntax sql dbpredictor_namestatsscale model execution get json data name"
  },
  {
    "filename": "stats_old.mdx",
    "path": "docs/sdks/mongo/stats_old.mdx",
    "chunk_id": 1,
    "chunk_content": "candidate_model performance 0010 training_time seconds selected 01 name description name name candidate model performance accuracy 0 1 depending type model training_time time elapsed training model selected 1 best performing model 0 rest example lets see output home_rentals_model model sql dbhome_rentals_modelstatsscale model execution get json data name neural performance 0999 training_time 4837 selected 0 name lightgbm performance 1 training_time 33 selected 1 name regression performance 0999 training_time 005 selected 0 ns mindsdbhome_rentals_model stats method scale ensemble parameter description dbpredictor_namestatsscale ensemble method used display parameters used select best candidate model syntax syntax sql dbpredictor_namestatsscale ensemble execution get sql ensemble json name description ensemble object json type describing parameters used select best candidate model example warning example wip example work progress warning tip need info need information describe model understand results feel free ask us community slack workspacehttpsmindsdbcomjoincommunity tip"
  },
  {
    "filename": "deleteOne.mdx",
    "path": "docs/sdks/mongo/projects/deleteOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove project sidebartitle remove project info feature progress info"
  },
  {
    "filename": "insertOne.mdx",
    "path": "docs/sdks/mongo/projects/insertOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title create project sidebartitle create project info feature progress info"
  },
  {
    "filename": "list-projects.mdx",
    "path": "docs/sdks/mongo/projects/list-projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title list projects sidebartitle list projects info feature progress info"
  },
  {
    "filename": "query-jobs.mdx",
    "path": "docs/sdks/mongo/jobs/query-jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title query jobs sidebartitle query jobs query jobs jobs_history tables find created jobs execution history sql dbjobsfind dbjobs_historyfind"
  },
  {
    "filename": "deleteOne.mdx",
    "path": "docs/sdks/mongo/jobs/deleteOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove job sidebartitle remove job description dbjobsdeleteone function deletes job syntax syntax dbjobsdeleteonename job_name"
  },
  {
    "filename": "insertOne.mdx",
    "path": "docs/sdks/mongo/jobs/insertOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title create job sidebartitle create job description dbjobsinsertone function creates schedules job syntax syntax dbjobsinsertone name job2 query select models complex example dbjobsinsertone name job1 schedule_str every day start_at 20230330 end_at 20230330 111111 query dbhome_rentals_modelaggregate match collection example_dbdemo_datahome_rentals query number_of_rooms 2 project home_rentalsrental_price real_price home_rentals_modelrental_price predicted_price limit 2 db photorep coll aaa append true"
  },
  {
    "filename": "list-data-handlers.mdx",
    "path": "docs/sdks/mongo/databases/list-data-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data handlers sidebartitle list data handlers list available data handlers querying information_schema database sql use information_schema dbhandlersfindtype data"
  },
  {
    "filename": "deleteOne.mdx",
    "path": "docs/sdks/mongo/databases/deleteOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove data source sidebartitle remove data source description dbdatabasesdeleteone method deletes connection data source specified argument syntax syntax sql dbdatabasesdeleteonename connection_name execution get json acknowledged true deletedcount 1 name description name name connection deleted"
  },
  {
    "filename": "find.mdx",
    "path": "docs/sdks/mongo/databases/find.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data sources sidebartitle list data sources description show databases command lists data sources connected mindsdb syntax first switch mindsdb using use command sql use mindsdb list data sources connected mindsdb sql show databases"
  },
  {
    "filename": "insertOne.mdx",
    "path": "docs/sdks/mongo/databases/insertOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect data source sidebartitle connect data source description mindsdb enables adding databases mongo instance using dbdatabasesinsertone method mindsdb mongo api supports creating connection passing database credentials syntax syntax sql dbdatabasesinsertone name mongo_int engine mongodb connection_args port 27017 host mongodbsrvadminlocalhost database test_data tip alternatively use host parameter alone providing connection string sql dbdatabasesinsertone name mongo_int engine mongodb connection_args host mongodbsrvuserpassdbxxxyyymongodbnet tip execution get json acknowledged true insertedid objectid62dff63c6cc2fa93e1d7f12c name description name identifier data source created engine database engine selected connection_args keyvalue object storing connection parameters port host database example example connect local mongodb sql dbdatabasesinsertone name mongo_local engine mongodb connection_args port 27017 host mongodbsrvadminlocalhost database test_data execution get json acknowledged true insertedid objectid62dff63c6cc2fa93e1d7f12c list linked databases using following command sql show dbs execution get sql database admin files information_schema mindsdb mongo_int views"
  },
  {
    "filename": "use.mdx",
    "path": "docs/sdks/mongo/databases/use.mdx",
    "chunk_id": 0,
    "chunk_content": "title use data source sidebartitle use data source description use collection_name statement switches provided collection_name execute operations collection syntax use sql use collection_name"
  },
  {
    "filename": "list-ml-handlers.mdx",
    "path": "docs/sdks/mongo/ml_engines/list-ml-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data handlers sidebartitle list data handlers list available ml handlers querying information_schema database sql use information_schema dbhandlersfindtype ml"
  },
  {
    "filename": "deleteOne.mdx",
    "path": "docs/sdks/mongo/ml_engines/deleteOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove ml engine sidebartitle remove ml engine description method deletes ml engine syntax syntax sql dbml_enginesdeleteonename ml_engine_name execution get sql acknowledged true deletedcount 1 example lets delete openai engine created earlier sql dbml_enginesdeleteonename openai_engine execution get sql acknowledged true deletedcount 1"
  },
  {
    "filename": "find.mdx",
    "path": "docs/sdks/mongo/ml_engines/find.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml engines sidebartitle list ml engines description method lists available ml engines syntax syntax sql dbml_enginesfind execution get sql name lightwood handler lightwood connection_data key password value name openai handler openai connection_data key password value"
  },
  {
    "filename": "insertOne.mdx",
    "path": "docs/sdks/mongo/ml_engines/insertOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title configure ml engine sidebartitle configure ml engine description method creates ml engine based one available ml handlers syntax syntax sql dbml_enginesinsertone name ml_engine_name handler handler_name params key value execution get sql acknowledged true insertedid objectid6465e96766d152ae1e247802 example lets create openai engine uses openai handler sql dbml_enginesinsertone name openai_engine handler openai params openai_api_key qqq execution get sql acknowledged true insertedid objectid6465e9d566d152ae1e247803"
  },
  {
    "filename": "get-single-prediction.mdx",
    "path": "docs/sdks/mongo/models/get-single-prediction.mdx",
    "chunk_id": 0,
    "chunk_content": "title get single prediction sidebartitle get single prediction description dbmodel_namefind function fetches predictions model table data returned fly result set persisted want save predictions utilize view table syntax syntax making single prediction sql dbpredictor_namefindcolumn value column value execution get json column_name1 value column_name2 value columns select_data_query null when_data null target_name_original value target_name_confidence value target_name_explain predicted_value value confidence value anomaly null truth null confidence_lower_bound value confidence_upper_bound value target_name_anomaly value target_name_min value target_name_max value expressions description target_name_original real value target variable collection target_name_confidence model confidence target_name_explain json object contains additional information predicted_value confidence anomaly truth confidence_lower_bound confidence_upper_bound target_name_anomaly model anomaly target_name_min lower bound value target_name_max upper bound value example following mql statement fetches predicted value rental_price column home_rentals_model model predicted value rental price property attributes listed parameter find method sql dbhome_rentals_modelfindsqft 823 location good neighborhood downtown days_on_market 10 execution get json sqft 823 location good neighborhood downtown days_on_market 10 number_of_rooms null number_of_bathrooms null initial_price null rental_price 1431323795180614 select_data_query null when_data null rental_price_original null rental_price_confidence 099 rental_price_explain predicted_value 1431323795180614 confidence 099 anomaly null truth null confidence_lower_bound 13794387560440227 confidence_upper_bound 14832088343172054 rental_price_anomaly null rental_price_min 13794387560440227 rental_price_max 14832088343172054"
  },
  {
    "filename": "get-batch-predictions.mdx",
    "path": "docs/sdks/mongo/models/get-batch-predictions.mdx",
    "chunk_id": 0,
    "chunk_content": "title get batch predictions sidebartitle get batch predictions description dbmodel_namefind function fetches predictions model table data returned fly result set persisted want save predictions utilize view table syntax syntax making batch predictions sql dbmodel_namefindcollection integration_namecollection_namequery column valuedate gt isodate20180331t000000000z command returns predictions made data rows integration_namecollection_name"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/sdks/mongo/models/retrain.mdx",
    "chunk_id": 0,
    "chunk_content": "title retrain model sidebartitle retrain model description dbmodelsinsertonename model_nameaction retrain function used retrain model syntax syntax sql dbmodelsinsertonename model_nameaction retrain"
  },
  {
    "filename": "make-predictions.mdx",
    "path": "docs/sdks/mongo/models/make-predictions.mdx",
    "chunk_id": 0,
    "chunk_content": "title dbmodel_namefind sidebartitle dbmodel_namefind description dbmodel_namefind function fetches predictions model table data returned fly result set persisted want save predictions utilize view table syntax single prediction syntax making single prediction sql dbpredictor_namefindcolumn value column value execution get json column_name1 value column_name2 value columns select_data_query null when_data null target_name_original value target_name_confidence value target_name_explain predicted_value value confidence value anomaly null truth null confidence_lower_bound value confidence_upper_bound value target_name_anomaly value target_name_min value target_name_max value expressions description target_name_original real value target variable collection target_name_confidence model confidence target_name_explain json object contains additional information predicted_value confidence anomaly truth confidence_lower_bound confidence_upper_bound target_name_anomaly model anomaly target_name_min lower bound value target_name_max upper bound value batch predictions syntax making batch predictions sql dbmodel_namefindcollection integration_namecollection_namequery column valuedate gt isodate20180331t000000000z command returns predictions made data rows integration_namecollection_name example single prediction following mql statement fetches predicted value rental_price column home_rentals_model model predicted value rental price property attributes listed parameter find method sql dbhome_rentals_modelfindsqft 823 location good neighborhood downtown days_on_market 10 execution get json sqft 823 location good neighborhood downtown days_on_market 10 number_of_rooms null number_of_bathrooms null initial_price null rental_price 1431323795180614 select_data_query null when_data null rental_price_original null rental_price_confidence 099 rental_price_explain predicted_value 1431323795180614 confidence 099 anomaly null truth null confidence_lower_bound 13794387560440227 confidence_upper_bound 14832088343172054 rental_price_anomaly null rental_price_min 13794387560440227"
  },
  {
    "filename": "make-predictions.mdx",
    "path": "docs/sdks/mongo/models/make-predictions.mdx",
    "chunk_id": 1,
    "chunk_content": "rental_price_max 14832088343172054 batch predictions warning example work progress warning"
  },
  {
    "filename": "deleteOne.mdx",
    "path": "docs/sdks/mongo/models/deleteOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove model sidebartitle remove model description dbmodelsdeleteone method deletes ml model specified argument syntax syntax sql dbmodelsdeleteonename predictor_name execution get json acknowledged true deletedcount 1 name description name name model deleted example deleting predictor lets list available predictors using dbmodelsfind method sql dbmodelsfind execution get json name home_rentals_model status complete accuracy 10 predict rental_price update_status up_to_date mindsdb_version 22831 error null select_data_query training_options name other_model status complete accuracy 10 predict value_to_be_predicted update_status up_to_date mindsdb_version 22831 error null select_data_query training_options dbmodelsdeleteone method drops model collection called home_rentals_model sql dbmodelsdeleteonename home_rentals_model execution get json acknowledged true deletedcount 1 validate model removed listing predictors sql dbmodelsfind execution get json name other_model status complete accuracy 10 predict value_to_be_predicted update_status up_to_date mindsdb_version 22831 error null select_data_query training_options"
  },
  {
    "filename": "find.mdx",
    "path": "docs/sdks/mongo/models/find.mdx",
    "chunk_id": 0,
    "chunk_content": "title list models sidebartitle list models description dbmodelsfind function lists models syntax list available predictors using dbmodelsfind method sql dbmodelsfind execution get json name home_rentals_model status complete accuracy 10 predict rental_price update_status up_to_date mindsdb_version 22831 error null select_data_query training_options name other_model status complete accuracy 10 predict value_to_be_predicted update_status up_to_date mindsdb_version 22831 error null select_data_query training_options"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/sdks/mongo/models/describe.mdx",
    "chunk_id": 0,
    "chunk_content": "title describe model sidebartitle describe model describe command used display attributes existing model accepts name model describe attribute separated argument call describe command javascript dbruncommanddescribe predictor_nameattribute name description predictor_name name predictor whose statistics want see attribute argument describe command defines type statistics features model ensemble describe command features parameter description dbruncommanddescribe predictor_namefeatures command used display way model encoded data training syntax syntax javascript dbruncommanddescribe predictor_namefeatures execution get json data column number_of_rooms type categorical encoder onehotencoder role feature name description column name column type type inferred data encoder encoder used role role column feature target example lets describe home_rentals_model model javascript dbruncommanddescribe home_rentals_modelfeatures execution get json data column number_of_rooms type categorical encoder onehotencoder role feature column number_of_bathrooms type binary encoder binaryencoder role feature column sqft type float encoder numericencoder role feature column location type categorical encoder onehotencoder role feature column days_on_market type integer encoder numericencoder role feature column initial_price type integer encoder numericencoder role feature column neighborhood type categorical encoder onehotencoder role feature column rental_price type float encoder numericencoder role target ns mindsdbhome_rentals_model describe command model parameter description dbruncommanddescribe predictor_namemodel method used display performance candidate models syntax syntax sql dbruncommanddescribe predictor_namemodel execution get json data name candidate_model performance 0010 training_time seconds"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/sdks/mongo/models/describe.mdx",
    "chunk_id": 1,
    "chunk_content": "selected 01 name description name name candidate model performance accuracy 0 1 depending type model training_time time elapsed training model selected 1 best performing model 0 rest example lets see output home_rentals_model model javascript dbruncommanddescribe home_rentals_modelmodel execution get json data name neural performance 0999 training_time 4837 selected 0 name lightgbm performance 1 training_time 33 selected 1 name regression performance 0999 training_time 005 selected 0 ns mindsdbhome_rentals_model describe command ensemble parameter description dbruncommanddescribe predictor_nameensemble command used display parameters used select best candidate model syntax syntax sql dbruncommanddescribe predictor_nameensemble execution get sql ensemble json name description ensemble object json type describing parameters used select best candidate model"
  },
  {
    "filename": "insertOne.mdx",
    "path": "docs/sdks/mongo/models/insertOne.mdx",
    "chunk_id": 0,
    "chunk_content": "title create train deploy model sidebartitle create train deploy model description dbmodelsinsertone method creates trains new model syntax syntax sql dbmodelsinsertone name predictor_name predict target_column connection integration_name select_data_query dbcollection_namefind execution get json writeresult ninserted 1 expressions description name name model created predict name target column predicted connection name integration created via dbdatabasesinsertone methodmongodatabase file uploadsqlcreatefile select_data_query object stores data collection name used training validation additional arguments filtering data note checking predictor status running dbmodelsinsertone method execute dbmodelsfind method mindsdbmodels collection check status model sql dbmodelsfindname model_name note example example shows create train home_rentals_model machine learning model predict rental prices real estate properties inside dataset sql dbmodelsinsertone name home_rentals_model predict rental_price connection mongo_integration select_data_query dbhome_rentalsfind execution get json writeresult ninserted 1 check predictor status query mindsdbmodels table using dbmodelsfind command sql dbmodelsfindname home_rentals_model execution get json name home_rentals_model status complete accuracy 091 predict rental_price update_status up_to_date mindsdb_version 22831 error null select_data_query training_options"
  },
  {
    "filename": "manage-model-versions.mdx",
    "path": "docs/sdks/mongo/models/manage-model-versions.mdx",
    "chunk_id": 0,
    "chunk_content": "title manage model versions sidebartitle manage model versions info feature progress info"
  },
  {
    "filename": "finetune.mdx",
    "path": "docs/sdks/mongo/models/finetune.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune model sidebartitle finetune model description dbmodelsinsertonename model_nameaction finetune function used finetune model syntax syntax sql dbmodelsinsertonename model_nameaction finetune"
  },
  {
    "filename": "mongo-shell.mdx",
    "path": "docs/sdks/mongo/connect/mongo-shell.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb mongodb shell sidebartitle mongodb shell mongodb shell quickest way connect work mongodb mindsdb provides powerful mongodb api allowing users connect mindsdb mongodb shellhttpswwwmongodbcomtrydownloadshell please note connection mongodb api provided mindsdb connection mongodb database download mongodb shell herehttpswwwmongodbcomtrydownloadshell overview overview connection mindsdb mongodb shell p aligncenter img srcassetsconnect_mongo_shellpng p lets go steps presented 1 connect mongodb shell mindsdb discussed following content 2 connect mindsdb database use create databasesqlcreatedatabases statement run mindsdb passing required database connection details 3 completed steps 1 2 access connected database mongodb shell via mindsdb connect connect mongodb shell mindsdb using either mindsdb cloud local installation info please add mindsdb cloud public ipsfaqswhitelistips access list mongo database info upon opening mongodb shell see following message p aligncenter img srcassetsconnect_mongo_shell_1png p lets look connection strings mindsdb cloud local installation tabs tab title local mindsdb provide local mindsdb connection string connect local mindsdb installation copy connection string mongodb compass already created connection connection string connect local mindsdb installation bash mongodb12700147336 tab tabs whats next set recommend check tutorials section youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb collection structuremongocollectionstructure also dont miss remaining pages mongo api section explain common mql syntax"
  },
  {
    "filename": "mongo-shell.mdx",
    "path": "docs/sdks/mongo/connect/mongo-shell.mdx",
    "chunk_id": 1,
    "chunk_content": "examples fun tip community check video guide created community video guide easily connect mindsdb cloud mongoshellhttpsyoutubecomvideoysprxsgdovk akhilcoderhttpstwittercomakhilcoder tip"
  },
  {
    "filename": "mongo-compass.mdx",
    "path": "docs/sdks/mongo/connect/mongo-compass.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb mongodb compass sidebartitle mongodb compass iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedqvmmdcofk30 titleconnect using mongo compass frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe mongodb compass graphical user interface gui mongodb provides detailed schema visualizations realtime performance metrics sophisticated querying abilities download mongodb compass herehttpswwwmongodbcomtrydownloadcompass overview overview connection mindsdb mongodb compass p aligncenter img srcassetsconnect_mongo_compasspng p lets go steps presented 1 connect mongodb compass mindsdb discussed following content 2 connect mindsdb database use create databasesqlcreatedatabases statement run mindsdb passing required database connection details 3 completed steps 1 2 access connected database mongodb compass via mindsdb connect connect mongodb compass mindsdb using either mindsdb cloud local installation info please add mindsdb cloud public ipsfaqswhitelistips access list mongo database info tabs tab title local mindsdb first create new connection mongodb compass clicking new connection button left navigation panel host value 127001 port value 47336 input host field p aligncenter img srcassetsconnect_mongo_compass_3png p tab tabs whats next set recommend check tutorials section youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb collection structuremongocollectionstructure also dont miss remaining pages mongo api section explain common mql syntax examples fun tip community check video guides created community video"
  },
  {
    "filename": "mongo-compass.mdx",
    "path": "docs/sdks/mongo/connect/mongo-compass.mdx",
    "chunk_id": 1,
    "chunk_content": "guide connect mongo compass mindsdbhttpsyoutubecomvideozirxrhvusjc hellfirehttpsgithubcomartemis6969 video guide integrating mindsdb instance mongodbhttpswwwyoutubecomwatchvxwugbbnkyks syed zubeenhttpsgithubcomsyedzubeen tip"
  },
  {
    "filename": "table-structure.mdx",
    "path": "docs/sql/table-structure.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb schema sidebartitle mindsdb schema initially mindsdb comprises three system databases one default project follows information_schema stores metadata objects handlers databases ai engines models jobs log stores log data models jobs files initially empty stores files uploaded mindsdb mindsdb default project storing models views jobs triggers agents list databases running following sql commands sql show full databases output sql database type engine information_schema system null log system null mindsdb project null files data files information_schema database information_schema database contains system tables correspond mindsdb objects follows name description handlers stores data ai handlers data integrationsintegrationsdataoverview ai integrationsintegrationsaioverview supported mindsdb databases stores data sources connected mindsdb note corresponding handlers required connect data sources supported mindsdbintegrationsdataoverview installing required handler dependenciessetupselfhosteddockerinstalldependencies ml_engines stores aiml engines configured mindsdb note corresponding handlers required connect aiml engines supported mindsdbintegrationsaioverview installing required handler dependenciessetupselfhosteddockerinstalldependencies models stores models deployed within mindsdb ecosystem note create deploy model configuring corresponding aiml engine views stores views created mindsdb jobs stores jobsmindsdb_sqlsqlcreatejobs facilitate workflow automation triggers stores triggersmindsdb_sqlsqlcreatetrigger facilitate workflow automation agents stores ai agentsmindsdb_sqlagentsagent created mindsdb skills stores skills assigned ai agentsmindsdb_sqlagentsagent knowledge_bases stores knowledge basesmindsdb_sqlagentsknowledgebases assigned ai agents skills chatbots stores chatbotsmindsdb_sqlagentschatbot comprise ai agent chat interface tip objects including"
  },
  {
    "filename": "table-structure.mdx",
    "path": "docs/sql/table-structure.mdx",
    "chunk_id": 1,
    "chunk_content": "databases ml_engines models may contain sensitive information form api keys passwords mindsdb hides sensitive information default want expose sensitive information output querying objects set show_secrets flag true sql set show_secrets true hide back set false sql set show_secrets false tip use show command list objects follows sql show object_name project_name like object_name_part key value instance list openai models mindsdb project contain ai name sql show models mindsdb like ai engine openai another example query available data ai handlers sql show handlers type data show handlers type ml tip connect data source using data handler create model using ai handler make sure import_success column reads true reads false install dependencies handlersetupselfhosteddockerinstalldependencies using tip mindsdb project mindsdb enables group objects within projectssqlproject projects store models views jobs triggers agents skills knowledge bases chatbots note projects store objects except handlers connected data sources configured aiml engines note based available handlers connect data source mindsdb configure aiml engine within mindsdb done instance create view data connected data source store inside project create model based configured aiml engine store inside project note mindsdb provides default mindsdb project objects created without defining project stored learn create manage projects heresqlproject files database another default database stores files"
  },
  {
    "filename": "table-structure.mdx",
    "path": "docs/sql/table-structure.mdx",
    "chunk_id": 2,
    "chunk_content": "uploaded mindsdb upload files mindsdbsqlcreatefile"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 0,
    "chunk_content": "title feature engineering mindsdb sidebartitle feature engineering data accurate predictions get recommend provide predictor many historical data rows data columns possible make predictions even accurate examples presented prove hypothesis want follow examples install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop prerequisites base table available example_db integration mindsdb editor order able use must first create database like sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo execution get sql query ok 0 rows affected xxxx sec thats done run following commands us example adding data columns introduction well create several predictors using table increasing number data columns step start base table create predictor based add two columns base table create predictor based enhanced table last add another two columns create predictor comparing accuracies predictors well find data results accurate predictions lets get started lets run codes go codes base table enhanced base tables simultaneously data setup lets prepare verify data create views query ensure input predictors order tabs tab titleusing base table lets start querying data example_dbdemo_dataused_car_price table base table sql select example_dbdemo_dataused_car_price limit 5 execution get sql modelyearpricetransmissionmileagefueltypetaxmpg enginesize a1 201712500manual 15735 petrol 15055414 a6 201616500automatic 36203 diesel 20 6422 a1 201611000manual 29946"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 1,
    "chunk_content": "petrol 30 55414 a4 201716800automatic 25952 diesel 1456732 a3 201917300manual 1998 petrol 1454961 table thead tr thnameth thdescriptionth tr thead tbody tr tdcodemodelcodetd tdmodel cartd tr tr tdcodeyearcodetd tdyear productiontd tr tr tdcodepricecodetd tdprice cartd tr tr tdcodetransmissioncodetd tdtransmission codemanualcode codeautomaticcode codesemiautocodetd tr tr tdcodemileagecodetd tdmileage cartd tr tr tdcodefueltypecodetd tdfuel type cartd tr tr tdcodetaxcodetd tdtaxtd tr tr tdcodempgcodetd tdmiles per gallontd tr tr tdcodeenginesizecodetd tdengine size cartd tr tbody table br tab tab titleusing base table 2 columns lets create view based example_dbdemo_dataused_car_price table add two columns please note replace mpg column kml column added columns arebrbr kml column calculated mpg column using formula like query stands kilometers per literbrbr years_old column calculated subtracting cars year current date stands cars agebrbr sql create view used_car_price_plus_2_columns select example_db select model year price transmission mileage fueltype tax enginesize roundcastmpg 23521458 numeric 1 kml mpg miles per galon replaced kml kilometers per liter date_partyear current_dateyear years_old age car demo_dataused_car_price execution get sql query ok 0 rows affected xxxx sec lets query newly created view sql select mindsdbused_car_price_plus_2_columns limit 5 execution get sql modelyearpricetransmissionmileagefueltypetaxmpg enginesizekml years_old a1 201712500manual 15735 petrol 15055414 2365 a6 201616500automatic 36203 diesel 20 6422 2736 a1 201611000manual 29946"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 2,
    "chunk_content": "petrol 30 55414 2366 a4 201716800automatic 25952 diesel 1456732 2865 a3 201917300manual 1998 petrol 1454961 2113 tab tab titleusing base table 4 columns lets create view based example_dbdemo_dataused_car_price table add four columns please note replace mpg column kml column added columns arebrbr kml column calculated mpg column using formula like query stands kilometers per literbrbr years_old column calculated subtracting cars year current date stands cars agebrbr units_to_sell column calculated using partition clauses indicates many units certain car model sold yearbrbr tax_div_price column calculated dividing tax column price columnbrbr sql create view used_car_price_plus_another_2_columns select example_db select model year price transmission mileage fueltype tax enginesize roundcastmpg 23521458 numeric 1 kml mpg miles per galon replaced kml kilometers per liter date_partyear current_dateyear years_old age car count partition model year units_to_sell many units certain model sold year roundcasttax decimal price 3 tax_div_price value tax divided price car demo_dataused_car_price execution get sql query ok 0 rows affected xxxx sec lets query newly created view sql select mindsdbused_car_price_plus_another_2_columns limit 5 execution get sql modelyearpricetransmissionmileagefueltypetaxmpg enginesizekml years_oldunits_to_selltax_div_price a1 20109990 automatic 38000 petrol 12553314 22712 1 0013 a1 20116995 manual 65000 petrol 12553314 22711 5 0018 a1 20116295 manual 107000 petrol 12553314 22711 5 0020 a1 20114250 manual"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 3,
    "chunk_content": "116000 diesel 20 70616 30011 5 0005 a1 20116475 manual 45000 diesel 0 70616 30011 5 0000 tab tabs note dropping view want drop view run command drop view view_name note creating predictors create models based example_dbdemo_dataused_car_price table extensions tabs tab titleusing base table sql create model mindsdbprice_predictor example_db select demo_dataused_car_price predict price execution get sql query ok 0 rows affected xxxx sec tab tab titleusing base table 2 columns sql create model mindsdbprice_predictor_plus_2_columns mindsdb select used_car_price_plus_2_columns predict price execution get sql query ok 0 rows affected xxxx sec tab tab titleusing base table 4 columns sql create model mindsdbprice_predictor_plus_another_2_columns mindsdb select used_car_price_plus_another_2_columns predict price execution get sql query ok 0 rows affected xxxx sec tab tabs note dropping predictor want drop predictor run command drop model predictor_name note predictor status finally lets check predictor status whose value generating first training last complete tabs tab titleusing base table sql describe price_predictor execution get sql name status accuracypredict update_statusmindsdb_versionerror select_data_query training_options price_predictorcomplete0963 price up_to_date 221021 nullselect demo_dataused_car_price tab tab titleusing base table 2 columns sql describe price_predictor_plus_2_columns execution get sql name status accuracypredict update_statusmindsdb_versionerror select_data_query training_options price_predictor_plus_2_columnscomplete0965 price up_to_date 221021 nullselect used_car_price_plus_2_columns tab tab titleusing base table 4 columns sql describe"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 4,
    "chunk_content": "price_predictor_plus_another_2_columns execution get sql name status accuracypredict update_statusmindsdb_versionerror select_data_query training_options price_predictor_plus_another_2_columnscomplete0982 price up_to_date 221021 nullselect used_car_price_plus_another_2_columns tab tabs accuracy comparison training process three predictors completes see accuracy values base table get accuracy value 0963 base table two data columns get accuracy value 0965 accuracy value increased expected base table four data columns get accuracy value 0982 accuracy value increased expected true vs predicted price comparison lets compare close predicted price values true price sql model year transmission fueltype mileage true_price pred_price_1 pred_price_2 pred_price_3 a1 2017 manual petrol 7620 14440 17268 17020 14278 a6 2016 automatic diesel 20335 18982 17226 17935 19016 a3 2018 semiauto diesel 9058 19900 25641 23008 21286 prices predicted third predictor highest accuracy value closest true price expected example joining data tables introduction start creating predictor car_sales table add data joining car_sales car_info tables create predictor based car_sales_info view lets get started lets run codes go codes using partial tables full table joining data data setup car_sales table sql select example_dbdemo_datacar_sales limit 5 execution get sql modelyearpricetransmissionmileagefueltypetax a1 201712500manual 15735 petrol 150 a6 201616500automatic 36203 diesel 20 a1 201611000manual 29946 petrol 30 a4 201716800automatic 25952 diesel 145 a3 201917300manual 1998 petrol 145 name description model model car"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 5,
    "chunk_content": "year year production price price car transmission transmission manual automatic semiauto mileage mileage car fueltype fuel type car tax tax car_info table sql select example_dbdemo_datacar_info limit 5 execution get sql modelyeartransmissionfueltype mpg enginesize a1 2010automatic petrol 533 14 a1 2011manual diesel 706 16 a1 2011manual petrol 533 14 a1 2012automatic petrol 506 14 a1 2012manual diesel 729517 name description model model car year year production transmission transmission manual automatic semiauto fueltype fuel type car mpg miles per gallon enginesize engine size car lets join car_sales car_info tables model year transmission fueltype columns sql select example_db select impg ienginesize demo_datacar_sales join demo_datacar_info smodelimodel syeariyear stransmissionitransmission sfueltypeifueltype limit 5 note nested select statements please note use nested select statement order trigger native query mindsdb cloud editor example_db database postgresql database trigger postgresqlnative syntax note execution get sql modelyearpricetransmissionmileagefueltypetaxmpg enginesize a1 20109990 automatic 38000 petrol 12553314 a1 20114250 manual 116000 diesel 20 70616 a1 20116475 manual 45000 diesel 0 70616 a1 20116295 manual 107000 petrol 12553314 a1 20117495 manual 60700 petrol 12553314 create view based join query sql create view car_sales_info select example_db select impg ienginesize demo_datacar_sales join demo_datacar_info smodelimodel syeariyear stransmissionitransmission sfueltypeifueltype execution get sql query ok 0 rows affected xxxx sec"
  },
  {
    "filename": "feature-eng.mdx",
    "path": "docs/sql/feature-eng.mdx",
    "chunk_id": 6,
    "chunk_content": "lets verify view selecting sql select mindsdbcar_sales_info limit 5 execution get sql modelyearpricetransmissionmileagefueltypetaxmpg enginesize a1 20109990 automatic 38000 petrol 12553314 a1 20114250 manual 116000 diesel 20 70616 a1 20116475 manual 45000 diesel 0 70616 a1 20116295 manual 107000 petrol 12553314 a1 20117495 manual 60700 petrol 12553314 creating predictors lets create predictor car_sales table input data sql create model mindsdbprice_predictor_car_sales example_db select demo_datacar_sales predict price execution get sql query ok 0 rows affected xxxx sec lets create predictor table join car_sales car_info tables sql create model mindsdbprice_predictor_car_sales_info mindsdb select car_sales_info predict price execution get sql query ok 0 rows affected xxxx sec predictor status next check status predictors start predictor based partial table sql describe price_predictor_car_sales execution get sql name status accuracypredict update_statusmindsdb_versionerror select_data_query training_options price_predictor_car_salescomplete0912 price up_to_date 221021 nullselect demo_datacar_sales predictor based full table sql describe price_predictor_car_sales_info execution get sql name status accuracypredict update_statusmindsdb_versionerror select_data_query training_options price_predictor_car_sales_infocomplete0912 price up_to_date 221021 nullselect car_sales_info accuracy comparison accuracy values 0912 predictors predictor already learns combination modelyeartransmissionfueltype affects price joining data columns doesnt play role particular example"
  },
  {
    "filename": "data-insights.mdx",
    "path": "docs/sql/data-insights.mdx",
    "chunk_id": 0,
    "chunk_content": "title data insights sidebartitle data insights data insights data visualization feature mindsdb editor lets explore queried data initially displaying analyzing subset first ten rows choose analyze full dataset clicking full data analysis button analysis presents distribution data aggregated column p aligncenter img srcassetssqldatainsights1png p data used comes one tutorials details click heresqltutorialshomerentals see data insights pane must run select query dataset lets look available features features distribution data per column opening data insights pane see distribution data output dataset column initially visualization analysis first ten rows shown p aligncenter img srcassetssqldatainsights2png p one histogram per column depicts column name data types distribution distribution potential bias flag see potential bias flag enter fullscreen mode data insights pane p aligncenter img srcassetssqldatainsights3png p location column exhibits potential bias great column values good poor column values cases typically flagged however necessarily mean problem dataset potential bias flag used data distribute normally uniformly likely overrepresenting underrepresenting values may normal hence bias potential missing values flag see missing values flag enter fullscreen mode data insights pane flag indicates proportion missing values column columns high percentage missing values useful modeling purposes hence recommended pay attention missing values flag try mitigate whenever possible indicates degrading quality data"
  },
  {
    "filename": "data-insights.mdx",
    "path": "docs/sql/data-insights.mdx",
    "chunk_id": 1,
    "chunk_content": "hovering histogram hovering histogram get information particular column value many values present column format column_value count p aligncenter img srcassetssqldatainsights6png p helpful determine exact data value counts histograms full data analysis lets full data analysis step step first need query data analysis mindsdb editor please note need query dataset without using limit keyword able perform complete data analysis sql select example_dbdemo_datahome_rentals execution get sql number_of_roomsnumber_of_bathroomssqftlocationdays_on_marketneighborhood rental_price 2 1 917 great 13 berkeley_hills3901 0 1 194 great 10 berkeley_hills2042 1 1 543 poor 18 westbrae 1871 2 1 503 good 10 downtown 3026 3 2 1066good 13 thowsand_oaks 4774 open data insights pane clicking data insights button right output table initially shows analysis first ten rows output table p aligncenter img srcassetssqldatainsights4png p perform complete analysis data either go fullscreen mode stay pane mode click full data analysis button complete data analysis p aligncenter img srcassetssqldatainsights5png p also whenever dataset changes click refresh data analysis button update data visualization analysis whats next want exploratory data analysis mindsdb collecting feedback develop even data visualization features let us know youd like see part data insights"
  },
  {
    "filename": "project.mdx",
    "path": "docs/sql/project.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb projects sidebartitle mindsdb projects mindsdb enables group objects within projectssqlproject projects store mindsdb schema objectssqltablestructuretheinformationschemadatabase except handlers connected data sources configured aiml engines projects store models views jobs triggers agents skills knowledge bases chatbots mindsdb provides default mindsdb project objects created without defining project stored working mindsdb projects create project use command create project sql create project project_name tip use lowercase letters project name tip list projects use command list projects sql show full databases type project create object within project use command template create object within project sql create object project_nameobject_name drop project use command remove project sql drop project project_name note please note project stores least one object removed case first drop objects belonging project remove project please see exampleexample section details note example lets create project sql create project my_project verify project created successfully lets run command select databases including connected data sources projects sql show full databases execution get sql database type engine information_schemasystem null mindsdb projectnull my_project projectnull files data files note please note information_schema system database mindsdb default project files database store uploaded files information please visit docs mindsdb default structuresqltablestructure note create model within project sql create model my_projectmy_model example_db select"
  },
  {
    "filename": "project.mdx",
    "path": "docs/sql/project.mdx",
    "chunk_id": 1,
    "chunk_content": "demo_datahome_rentals predict rental_price also lets create view sql create view my_projectmy_view select example_dbdemo_datahome_rentals my_project project sql show tables my_project execution get sql tables_in_my_project my_model my_view lets try delete project sql drop project my_project execution get sql project my_project deleted contains tables my_model my_view users remove project content dropping project sql drop model my_projectmy_model drop view my_projectmy_view proceed drop project sql drop project my_project info next steps links help explore find create use projectssqlcreateproject learn mindsdb schemasqltablestructure info"
  },
  {
    "filename": "feature-importance.mdx",
    "path": "docs/sql/feature-importance.mdx",
    "chunk_id": 0,
    "chunk_content": "title feature importance mindsdb lightwood sidebartitle feature importance mindsdb together lightwood ml engine provide feature importance tool feature importance feature importance useful tool obtaining explanations given machine learning model generally works simply put assigns relative numerical score input feature user understands parts input used greater lesser degree model generating predictions variations technique lightwood offers permutationbased variant permutation feature importance procedure consists splitting data training validation sets ml model trained former latter used among things find importance scores feature algorithm rather simple iterate input features randomly shuffle information within one one without shuffling rest input features model generates predictions altered input normally would input predictions shuffled variations validation dataset evaluate accuracy metrics interest user mean absolute error regression tasks compare value obtained original dataset acts reference value based lost accuracy finally assign numerical score reflects impact report importance column model edge cases feature completely irrelevant lost accuracy feature absent absolutely critical accuracy drops minimum possible value feature absent importance score 00 10 respectively however user careful note scores model intrafeature dependencies correlations meaning wise interpret scores independent feature correlated another highlyscored feature present high score use permutation feature importance customize behavior analysis module mindsdb sql editor via using key example want"
  },
  {
    "filename": "feature-importance.mdx",
    "path": "docs/sql/feature-importance.mdx",
    "chunk_id": 1,
    "chunk_content": "consider rows validation dataset rather clip default value example sql using engine lightwood analysis_blocks module permutationfeatureimportance args row_limit 0 validation data used train model use describe model_name command see reported importance scores example lets use following data create model sql select example_dbdemo_datahome_rentals limit 5 execution get sql number_of_roomsnumber_of_bathroomssqftlocationdays_on_marketneighborhood rental_price 2 1 917 great 13 berkeley_hills3901 0 1 194 great 10 berkeley_hills2042 1 1 543 poor 18 westbrae 1871 2 1 503 good 10 downtown 3026 3 2 1066good 13 thowsand_oaks 4774 create model using using clause shown previous chapter sql create model home_rentals_model example_db select demo_datahome_rentals predict rental_price using engine lightwood analysis_blocks module permutationfeatureimportance args row_limit 0 execution get sql query successfully completed monitor status model sql describe home_rentals_model status complete query model sql select dsqft dneighborhood ddays_on_market mrental_price predicted_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals limit 5 execution get sql sqftneighborhood days_on_marketpredicted_pricerental_price_explain 917 berkeley_hills13 3886 predicted_value 3886 confidence 099 anomaly null truth null confidence_lower_bound 3805 confidence_upper_bound 3967 194 berkeley_hills10 2007 predicted_value 2007 confidence 099 anomaly null truth null confidence_lower_bound 1925 confidence_upper_bound 2088 543 westbrae 18 1865 predicted_value 1865 confidence 099 anomaly null truth null confidence_lower_bound 1783 confidence_upper_bound 1946 503 downtown 10 3020 predicted_value 3020 confidence 099 anomaly null truth null confidence_lower_bound"
  },
  {
    "filename": "feature-importance.mdx",
    "path": "docs/sql/feature-importance.mdx",
    "chunk_id": 2,
    "chunk_content": "2938 confidence_upper_bound 3101 1066thowsand_oaks 13 4748 predicted_value 4748 confidence 099 anomaly null truth null confidence_lower_bound 4667 confidence_upper_bound 4829 check importance scores columns sql describe home_rentals_model execution get sql accuracies column_importance outputs inputs model r2_score0999days_on_market009location0042neighborhood0number_of_bathrooms0number_of_rooms0292sqft0999rental_pricenumber_of_roomsnumber_of_bathroomssqftlocationdays_on_marketneighborhoodencoders dtype_dict dependency_dict model problem_definition identifiers imputers analysis_blocks accuracy_functions please note rental_price column listed target column column importance scores presented feature columns tip confidence estimation check article model agnostic confidence estimation conformal predictors automlhttpsmediumcommindsdbmodelagnosticconfidenceestimationwithconformalpredictorsforautoml55fce87ef42a patricio cerda mardinihttpsmediumcompaxcema learn mindsdb estimates confidence tip"
  },
  {
    "filename": "issues.mdx",
    "path": "docs/contribute/issues.mdx",
    "chunk_id": 0,
    "chunk_content": "title report issue sidebartitle report issue icon flag want report bug request feature suggest docs improvements propose new integration mindsdb github issues pagehttpsgithubcommindsdbmindsdbissues reporting new issue please make sure already create issue get started want create issue mindsdb repository go github issues pagehttpsgithubcommindsdbmindsdbissues click _new issue_ button p aligncenter img srcassetsreport_issues1_reporting_new_issuepng p see available issue types p aligncenter img srcassetsreport_issues1_issue_typespng p lets go one one report bug choose report bug click _get started_ button p aligncenter img srcassetsreport_issues2_bug_reportpng p time fill form 1 vital add meaningful issue title p aligncenter img srcassetsreport_issues2_bug_report_form_1png p 2 describe current behavior please note field mandatory attach videos screenshots current behavior p aligncenter img srcassetsreport_issues2_bug_report_form_2png p 3 know expected behavior add p aligncenter img srcassetsreport_issues2_bug_report_form_3png p 4 helpful us add steps followed led error p aligncenter img srcassetsreport_issues2_bug_report_form_4png p 5 links references logs screenshots etc welcome p aligncenter img srcassetsreport_issues2_bug_report_form_5png p info remember contributions repository follow contributing guidelineshttpsgithubcommindsdbmindsdbblobmaincontributingmd code conducthttpsgithubcommindsdbmindsdbblobmaincode_of_conductmd info thank reporting bug helps us improve mindsdb future users request feature choose request feature click _get started_ button p aligncenter img srcassetsreport_issues3_feature_requestpng p time fill form 1 vital add meaningful issue title p aligncenter img srcassetsreport_issues3_feature_request_form_1png p 2 describe feature request along motivation proposed"
  },
  {
    "filename": "issues.mdx",
    "path": "docs/contribute/issues.mdx",
    "chunk_id": 1,
    "chunk_content": "feature please note field mandatory attach videos screenshots p aligncenter img srcassetsreport_issues3_feature_request_form_2png p 3 know solution look add p aligncenter img srcassetsreport_issues3_feature_request_form_3png p 4 links references logs screenshots etc welcome p aligncenter img srcassetsreport_issues3_feature_request_form_4png p info remember contributions repository follow contributing guidelineshttpsgithubcommindsdbmindsdbblobmaincontributingmd code conducthttpsgithubcommindsdbmindsdbblobmaincode_of_conductmd info thank submitting feature request helps us improve mindsdb future users improve docs choose improve docs click _get started_ button p aligncenter img srcassetsreport_issues4_improve_docspng p time fill form 1 vital add meaningful issue title p aligncenter img srcassetsreport_issues4_improve_docs_form_1png p 2 describe added improved please note field mandatory attach videos screenshots p aligncenter img srcassetsreport_issues4_improve_docs_form_2png p 3 links references logs screenshots etc welcome p aligncenter img srcassetsreport_issues4_improve_docs_form_3png p info remember contributions repository follow contributing guidelineshttpsgithubcommindsdbmindsdbblobmaincontributingmd code conducthttpsgithubcommindsdbmindsdbblobmaincode_of_conductmd info thank suggesting docs enhancements helps us improve mindsdb future users propose new integration get started want propose new integration please note new database integrations new machine learning framework choose propose new integration click _get started_ button p aligncenter img srcassetsreport_issues5_new_integrationpng p time fill form 1 vital add meaningful issue title p aligncenter img srcassetsreport_issues5_new_integration_form_1png p 2 please make sure integration already implemented p aligncenter img srcassetsreport_issues5_new_integration_form_2png p 3 describe use cases solved integration p aligncenter img srcassetsreport_issues5_new_integration_form_3png p 4"
  },
  {
    "filename": "issues.mdx",
    "path": "docs/contribute/issues.mdx",
    "chunk_id": 2,
    "chunk_content": "please provide motivation integration proposal p aligncenter img srcassetsreport_issues5_new_integration_form_4png p 5 get creative describe implementation use code text diagrams etc p aligncenter img srcassetsreport_issues5_new_integration_form_5png p 6 links references logs screenshots etc welcome p aligncenter img srcassetsreport_issues5_new_integration_form_6png p info remember contributions repository follow contributing guidelineshttpsgithubcommindsdbmindsdbblobmaincontributingmd code conducthttpsgithubcommindsdbmindsdbblobmaincode_of_conductmd info thank proposing new integration helps us improve mindsdb future users report security vulnerability get started want report security vulnerability please note issues visible repository maintainers also credited advisory published choose report security vulnerability click _get started_ button p aligncenter img srcassetsreport_issues6_security_vulnerabilitypng p time fill form 1 vital add meaningful issue title p aligncenter img srcassetsreport_issues6_security_vulnerability_form_1png p 2 please follow instructions provide best description p aligncenter img srcassetsreport_issues6_security_vulnerability_form_2png p 3 choose one affected products along versions p aligncenter img srcassetsreport_issues6_security_vulnerability_form_3png p 4 please assign severity issue _calculator_ feature help assess severity p aligncenter img srcassetsreport_issues6_security_vulnerability_form_4png p 5 assign one common weakness enumerators cwe p aligncenter img srcassetsreport_issues6_security_vulnerability_form_5png p info remember contributions repository follow contributing guidelineshttpsgithubcommindsdbmindsdbblobmaincontributingmd code conducthttpsgithubcommindsdbmindsdbblobmaincode_of_conductmd info thank reporting security vulnerability helps us improve mindsdb future users issue review issues reviewed regularly usually daily depending issue type labeled bug enhancement please make sure respond feedbackquestions regarding issue"
  },
  {
    "filename": "docs.mdx",
    "path": "docs/contribute/docs.mdx",
    "chunk_id": 0,
    "chunk_content": "title write mindsdb documentation sidebartitle write documentation icon pen section gets started contribute mindsdb documentation mindsdbs documentation run using mintlify want contribute docs please follow steps set environment locally running docs locally info prerequisite installed git version 2301 higher nodejs version 18100 higher info step 1 clone mindsdb git repository console git clone httpsgithubcommindsdbmindsdbgit step 2 install mintlify os console npm mintlify g step 3 go docs folder inside cloned mindsdb git repository start mintlify console mintlify dev documentation website available httplocalhost3000 warning getting error use windows operating system may get error saying file directory cusersusernamemintlifymintclient steps troubleshoot go cusersusernamemintlify directory remove mint folder open git bash location run git clone httpsgithubcommintlifymintgit repeat step 3 warning mindsdb repository structure structure mindsdb docs repository docs documentation source files __assets images icons used throughout docs __folders_with_mdx_files remaining folders store mdx files __mdx_files mdx files stored docs directory __mintlifyjson json file stores navigation page setup whats next follow docs rulesdocsrules fun thank contributing mindsdb docs"
  },
  {
    "filename": "python-coding-standards.mdx",
    "path": "docs/contribute/python-coding-standards.mdx",
    "chunk_id": 0,
    "chunk_content": "title python coding standards sidebartitle python coding standards icon python pep8 tip strict adherence pep8httpspepspythonorgpep0008 standards mandatory code contributions mindsdb tip pep8 pep8httpspepspythonorgpep0008 provides extensive set guidelines python code styling promoting readability uniform coding standard aligning pep8 ensure codebase remains clean maintainable easily understandable python developers level automated checks upon submission pull request pr automated process checks code pep8 compliance noncompliance pep8 result failure build process adherence pep8 best practice necessity ensure smooth integration new code codebase pr fails due pep8 violations contributor required review automated feedback provided pay special attention common pep8 compliance issues proper indentation appropriate line length correct use whitespace following recommended naming conventions contributors encouraged iteratively improve code based feedback full compliance achieved logging always instantiate logger using mindsdb utilities module practice ensures uniform approach logging across different parts application example logger creation python mindsdbutilities import log logger loggetlogger__name__ setting logging environment variable use mindsdb_log_level set desired logging level approach allows dynamic adjustment log verbosity without needing code modifications log levels available levels include debug detailed information typically interest diagnosing problems info confirmation things working expected warning indication something unexpected happened indicative problem near future error due serious problem software able perform function critical serious error"
  },
  {
    "filename": "python-coding-standards.mdx",
    "path": "docs/contribute/python-coding-standards.mdx",
    "chunk_id": 1,
    "chunk_content": "indicating program may unable continue running avoid print statements lack flexibility control offered logging mechanisms particularly terms output redirection levelbased filtering logger name __name__ automatically reflect modules name convention crucial pinpointing origin log messages docstrings docstrings essential documenting python code provide clear explanation functionality classes functions modules etc making codebase easier understand maintain wellwritten docstring include functions purpose describe functionclassmodule parameters list explain parameters takes return value describe function returns exceptions mention exceptions function might raise python def example_functionparam1 param2 example docstring args param1 type description param1 param2 type description param2 returns type description return value raises exceptiontype description exception function body exception handling implementing robust error handling strategies essential maintain stability reliability mindsdb proper exception management ensures application behaves predictably error conditions providing clear feedback preventing unexpected crashes behavior utilizing mindsdb exceptions ensure uniformity clarity error reporting always use predefined exceptions mindsdb exceptions library adding new exceptions development encounter scenario none existing exceptions adequately represent error consider defining new specific exception"
  },
  {
    "filename": "app-handlers.mdx",
    "path": "docs/contribute/app-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title build application handler sidebartitle build application handler icon gear section youll find add new application integrations mindsdb note prerequisite latest version mindsdb repository installed locally follow guidecontributeinstall learn install mindsdb development note api handlers application handlers act bridge mindsdb application provides apis use application handlers create databases using create databasesqlcreatedatabases statement reach data application handler implemented within mindsdb note database handlers learn handlers implement database handler visit doc page herecontributedatahandlers note note ml handlers learn handlers implement machine learning ml handler visit doc page herecontributemlhandlers note creating application handler create application handler within mindsdb inheriting apihandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl150 class providing implementation methods contained apihandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl150 class interact application apis core methods apart __init__ method five core methods must implemented recommend checking actual examples codebase get idea goes methods change bit depending nature system integrated lets review purpose method method purpose _register_tablehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl164 registers data resource memory example using twitter api registers tweets resource apiv2tweets connecthttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl23 performs necessary steps connectauthenticate underlying system check_connectionhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl39 evaluates connection alive healthy native_queryhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl47 parses native statement string acts upon example raw syntax commands call_application_api calls application api maps data pandas dataframe method handles pagination data mapping authors opt adding private methods new files folders combination structure necessary work enable"
  },
  {
    "filename": "app-handlers.mdx",
    "path": "docs/contribute/app-handlers.mdx",
    "chunk_id": 1,
    "chunk_content": "core methods work intended tip common methods mindsdbintegrationsutilitiesmainmindsdbintegrationsutilities library contributors find various methods may useful implementing new handlers tip api table data returned api call registered using _register_tablehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl164 method use map apitablehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl93 class apitablehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl93 class provides crud methods method purpose select implements mappings astselect calls actual api call_application_api insert implements mappings astinsert calls actual api call_application_api update implements mappings astupdate calls actual api call_application_api delete implements mappings astdelete calls actual api call_application_api add adds new rows data dictionary list list data based certain conditions providing filtercondition limits sorting target fields get_columns maps data columns returned api implementation application handler inherit apihandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl150 class stepbystep guide implementing __init__httpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsapi_handlerpyl155 method method initializes handler py def __init__self name str super__init__name constructor args name str handler name self_tables implementing connecthttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl23 method connect method sets connection py def connectself handlerstatusresponse set connections required handler return output check_connection method attempting connection switch selfis_connected returns handlerstatusresponse implementing check_connectionhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl39 method check_connection method performs health check connection py def check_connectionself handlerstatusresponse check connection handler returns handlerstatusresponse implementing native_queryhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl47 method native_query method runs commands native api syntax py def native_queryself query handlerresponse receive raw query act upon somehow args query query native format str sql databases dict mongo apis json etc"
  },
  {
    "filename": "app-handlers.mdx",
    "path": "docs/contribute/app-handlers.mdx",
    "chunk_id": 2,
    "chunk_content": "returns handlerresponse implementing call_application_api method method makes api calls mandatory implement method help make code reliable readable py def call_application_apiself method_namestr none paramsdict none dataframe receive query ast abstract syntax tree act upon somehow args query astnode sql query represented ast kind query select insert delete etc returns dataframe exporting connection_args dictionary connection_args dictionary contains arguments used establish connection along descriptions types labels whether required connection_args dictionary stored connection_argspy file inside handler folder info connection_args dictionary stored separate file order able hide sensitive information passwords api keys default querying connection_data information_schemadatabases table sensitive information hidden unhide use command sql set show_secretstrue info example connection_argspy file github handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgithub_handler api key value set hidden secret true py collections import ordereddict mindsdbintegrationslibsconst import handler_connection_arg_type arg_type connection_args ordereddict repository type arg_typestr description github repository name required true label repository api_key type arg_typepwd description optional github api key use authentication required false label api key secret true github_url type arg_typestr description optional github url connect github enterprise instance required false label github url connection_args_example ordereddict repositorymindsdbmindsdb api_keyghp_xxx github_urlhttpsgithubcommindsdbmindsdb exporting required variables following exported __init__py file handler handler class version handler name handler type handler either data handler ml handler icon_path file database icon title handler"
  },
  {
    "filename": "app-handlers.mdx",
    "path": "docs/contribute/app-handlers.mdx",
    "chunk_id": 3,
    "chunk_content": "short description description handler connection_args dictionary connection arguments connection_args_example dictionary example connection arguments import_error message used import handler class fails variables defined another file called __about__py file imported __init__py file example __init__py file github handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgithub_handler py mindsdbintegrationslibsconst import handler_type __about__ import __version__ version __description__ description connection_args import connection_args connection_args_example try github_handler import githubhandler handler connection_args_example connection_args import_error none except exception e handler none import_error e title github name github type handler_typedata icon_path iconsvg __all__ handler version name type title description import_error icon_path connection_args_example connection_args __about__py file github handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgithub_handler contains following variables py __title__ mindsdb github handler __package_name__ mindsdb_github_handler __version__ 001 __description__ mindsdb handler github __author__ artem veremey __github__ httpsgithubcommindsdbmindsdb __pypi__ httpspypiorgprojectmindsdb __license__ mit __copyright__ copyright 2023 mindsdb check application handlers see integration handlers currently use encourage check following handlers inside mindsdb repository github handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgithub_handler twitterhandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlerstwitter_handler handlers available mindsdb repositoryhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers"
  },
  {
    "filename": "integrations-readme.mdx",
    "path": "docs/contribute/integrations-readme.mdx",
    "chunk_id": 0,
    "chunk_content": "title write handlers readme sidebartitle write handlers readme icon pen readme file crucial document guides users understanding using contributing mindsdb integration serves first point contact anyone interacting integration hence need comprehensive clear userfriendly sections include table contents wellorganized table contents provided easy navigation document allowing users quickly find information need explain specific database application framework integration targets provide concise overview integrations purpose highlighting key features benefits handler implementation setup detail installation initial setup process including prerequisites connection describe steps establish manage connections clear instructions include sql examples better clarity required parameters list describe essential parameters necessary operation integration optional parameters detail additional nonmandatory parameters enhance integrations functionality example usage practical examples offer detailed examples showing use integration effectively coverage ensure examples encompass range functionalities basic advanced operations sql examples include sql statements expected outputs illustrate use cases supported tablestasks clearly enumerate tables tasks operations integration supports possibly list table format limitations transparently outline limitations constraints known integration todo future developments highlight areas future enhancements improvements github issues link open github issues tagged enhancements indicating ongoing planned feature additions"
  },
  {
    "filename": "install.mdx",
    "path": "docs/contribute/install.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb installation development sidebartitle installation development icon gears want contribute development mindsdb need install source info want contribute development mindsdb simply install use install mindsdb via dockersetupselfhosteddocker info install mindsdb development steps install mindsdb source either follow steps visit provided link info installing mindsdb source ensure use one following python versions 39x 310x 311x info 1 fork mindsdb repository githubhttpsgithubcommindsdbmindsdb 2 clone fork locally bash git clone httpsgithubcomusernamemindsdbgit 3 create virtual environment bash python venv mindsdbvenv 4 activate virtual environment bash source mindsdbvenvbinactivate 5 install mindsdb local development dependencies install dependencies bash cd mindsdb pip install e pip install r requirementsrequirementsdevtxt 6 start mindsdb bash python mindsdb tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip info alternatively use makefile install dependencies start mindsdb bash make install_mindsdb make run_mindsdb info see following message console mindsdbapihttpinitialize gui available http12700147334 mindsdbapimysqlmysql_proxymysql_proxy starting mindsdb mysql proxy server tcp12700147335 mindsdbapimysqlmysql_proxymysql_proxy waiting incoming connections mindsdb mysql api started 47335 mindsdb http api started 47334 access mindsdb editor localhost47334 install dependencies dependencies many data ml integrations installed default want use"
  },
  {
    "filename": "install.mdx",
    "path": "docs/contribute/install.mdx",
    "chunk_id": 1,
    "chunk_content": "data aiml integration whose dependencies available default install running command pip install handler_name tip find available handlers herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers tip whats next installed started mindsdb locally go ahead find create train model using create modelsqlcreatemodel statement check use casesusecasesoverview section follow tutorials cover large language models chatbots time series classification regression models semantic search"
  },
  {
    "filename": "docs-rules.mdx",
    "path": "docs/contribute/docs-rules.mdx",
    "chunk_id": 0,
    "chunk_content": "title style guide mindsdb documentation sidebartitle docs style guide icon book syntax sql commands follow rules writing sql command add semicolon end sql command use allcaps writing keywords select join group order predict create table insert etc writing query start new line following keywords select join group order predict using avoid horizontal scrollbar example sql select table_name_1 join table_name_2 b column_name_1value_name_1 column_name_2value_name_2 group acolumn_name_2 order bcolumn_name_1 syntax sql commands along output follow syntax documenting sql command output sql query goes execution get sql column_name column_name value value name description variable name goes variable description goes note output table remove output table place output message note example 1 sql select table_name_1 join table_name_2 b column_namevalue_name execution get sql column_name column_name value value name description column_name column description output example 1 sql select table_name_1 join table_name_2 b column_namevalue_name execution get sql column_name column_name value value name description column_name column description example 2 sql create model mindsdbpredictor_name integration_name select column_name_1 column_name_2 target_column table_name predict target_column execution get sql output goes output example 2 sql create model mindsdbpredictor_name integration_name select column_name_1 column_name_2 target_column table_name predict target_column execution get sql output goes"
  },
  {
    "filename": "tutorials.mdx",
    "path": "docs/contribute/tutorials.mdx",
    "chunk_id": 0,
    "chunk_content": "title write tutorial sidebartitle write tutorials icon pen section presents write tutorial mindsdb info content tutorial free create content however include tutorial chapters listed tutorial markdown file example mindsdbtutorialmdx basic markdown syntaxhttpswwwmarkdownguideorgbasicsyntax info introduction introduce readers tutorial describe dataset use predict data setup chapter contains introduction dataset use connecting data let others follow tutorial providing information get data connect mindsdb understanding data briefly introduce dataset use training predictor use create modelsqlcreatemodel command create predictor status predictor next step check status predictor value complete proceed next chapter making predictions use selectsqlapiselect statement query prediction results good present output readers making single prediction case regression classification predictors make single prediction good present output readers making batch predictions make batch predictions using joinsqlapijoin clause predictor types regression classification time series good present output readers whats next submit pr tutorial well review soon youll see tutorial mindsdb docs"
  },
  {
    "filename": "community.mdx",
    "path": "docs/contribute/community.mdx",
    "chunk_id": 0,
    "chunk_content": "title join community sidebartitle join community icon users questions want chat mindsdb core team community members join slack workspacehttpsmindsdbcomjoincommunity mindsdb newsletter get updates mindsdbs latest announcements releases events sign newsletterhttpsmindsdbcomnewsletter become mindsdb beta tester want become part product get first access latest updates join beta testers communityhttpsmindsdbcombetatester talk engineers want use mindsdb questions schedule call clicking talk engineers button get touch collaboration contact us submitting formhttpsmindsdbcomcontactus"
  },
  {
    "filename": "data-handlers.mdx",
    "path": "docs/contribute/data-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title build database handler sidebartitle build database handler icon gear section youll find add new integrationsdatabases mindsdb note prerequisite latest version mindsdb repository installed locally follow guidecontributeinstall learn install mindsdb development note database handlers database handlers act bridge database use database handlers create databases using create database commandsqlcreatedatabases reach data database handler implemented within mindsdb note ml handlers learn handlers implement machine learning ml handler visit doc page herecontributemlhandlers note creating database handler create database handler within mindsdb inheriting databasehandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl102 class providing implementation methods contained databasehandler class connect database choice core methods apart __init__ method seven core methods must implemented recommend checking actual examples codebase get idea goes methods change bit depending nature system integrated lets review purpose method method purpose connect performs necessary steps connect underlying system disconnect gracefully closes connections established connect method check_connection evaluates connection alive healthy method called frequently native_query parses native statement string acts upon example raw sql commands query takes parsed sql command form abstract syntax tree executes get_tables lists returns available tables handler decides table means underlying system interacting data layer typically actual tables get_columns returns columns table registered handler respective data type authors opt adding private methods new files folders combination structure"
  },
  {
    "filename": "data-handlers.mdx",
    "path": "docs/contribute/data-handlers.mdx",
    "chunk_id": 1,
    "chunk_content": "necessary work enable core methods work intended tip common methods mindsdbintegrationslibsutils library contributors find various methods may useful implementing new handlers also wrapper classes databasehandler instances called handlerresponsehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsresponsepyl7 handlerstatusresponsehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsresponsepyl32 use ensure proper output formatting tip implementation database handler inherit databasehandlerhttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsbasepyl102 class stepbystep guide setting name class property mindsdb uses internally name handler example create database statement uses handlers name sql create database integration_name engine postgres handlers name postgres parameters host 127001 user root password password implementing __init__ method method initializes handler connection_data argument contains parameters create database statement user password etc py def __init__self name str connection_data optionaldict constructor args name str handler name implementing connect method connect method sets connection py def connectself handlerstatusresponse set connections required handler return output check_connection method attempting connection switch selfis_connected returns handlerstatusresponse implementing disconnect method disconnect method closes existing connection py def disconnectself close existing connections switch selfis_connected implementing check_connection method check_connection method performs health check connection py def check_connectionself handlerstatusresponse check connection handler returns handlerstatusresponse implementing native_query method native_query method runs commands native database language py def native_queryself query handlerresponse receive raw query act upon somehow args query query native format str sql databases dict mongo etc returns handlerresponse implementing query method"
  },
  {
    "filename": "data-handlers.mdx",
    "path": "docs/contribute/data-handlers.mdx",
    "chunk_id": 2,
    "chunk_content": "query method runs parsed sql commands py def queryself query astnode handlerresponse receive query ast abstract syntax tree act upon somehow args query astnode sql query represented ast may kind query select insert delete etc returns handlerresponse implementing get_tables method get_tables method lists available tables py def get_tablesself handlerresponse return list entities return list entities accessible tables returns handlerresponse columns information_schematables httpsdevmysqlcomdocrefman80eninformationschematablestablehtml column table_name mandatory optional implementing get_columns method get_columns method lists columns specified table py def get_columnsself table_name str handlerresponse returns list entity columns args table_name str name one tables returned selfget_tables returns handlerresponse columns information_schemacolumns httpsdevmysqlcomdocrefman80eninformationschemacolumnstablehtml column column_name mandatory optional highly recommended define also data_type one python data types default str exporting connection_args dictionary connection_args dictionary contains arguments used establish connection along descriptions types labels whether required connection_args dictionary stored connection_argspy file inside handler folder info connection_args dictionary stored separate file order able hide sensitive information passwords api keys default querying connection_data information_schemadatabases table sensitive information hidden unhide use command sql set show_secretstrue info example connection_argspy file mysql handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler password value set hidden secret true py collections import ordereddict mindsdbintegrationslibsconst import handler_connection_arg_type arg_type connection_args ordereddict url type arg_typestr description urilike connection string mysql server provided override connection arguments"
  },
  {
    "filename": "data-handlers.mdx",
    "path": "docs/contribute/data-handlers.mdx",
    "chunk_id": 3,
    "chunk_content": "required false label url user type arg_typestr description user name used authenticate mysql server required true label user password type arg_typepwd description password authenticate user mysql server required true label password secret true database type arg_typestr description database name use connecting mysql server required true label database host type arg_typestr description host name ip address mysql server note use 127001 instead localhost connect local server required true label host port type arg_typeint description tcpip port mysql server must integer required true label port ssl type arg_typebool description set true enable ssl required false label ssl ssl_ca type arg_typepath description path url certificate authority ca certificate file required false label ssl_ca ssl_cert type arg_typepath description path name url server public key certificate file required false label ssl_cert ssl_key type arg_typepath description path name url server private key file required false label ssl_key connection_args_example ordereddict host127001 port3306 userroot passwordpassword databasedatabase exporting required variables following exported __init__py file handler handler class version handler name handler type handler either data handler ml handler icon_path file database icon title handler short description description handler connection_args dictionary connection arguments connection_args_example dictionary example connection arguments import_error message used import handler class fails variables defined another file called"
  },
  {
    "filename": "data-handlers.mdx",
    "path": "docs/contribute/data-handlers.mdx",
    "chunk_id": 4,
    "chunk_content": "__about__py file imported __init__py file example __init__py file mysql handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler py mindsdbintegrationslibsconst import handler_type __about__ import __version__ version __description__ description connection_args import connection_args connection_args_example try mysql_handler import mysqlhandler handler connection_args_example connection_args import_error none except exception e handler none import_error e title mysql name mysql type handler_typedata icon_path iconsvg __all__ handler version name type title description connection_args connection_args_example import_error icon_path __about__py file mysql handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler contains following variables py __title__ mindsdb mysql handler __package_name__ mindsdb_mysql_handler __version__ 001 __description__ mindsdb handler mysql __author__ mindsdb inc __github__ httpsgithubcommindsdbmindsdb __pypi__ httpspypiorgprojectmindsdb __license__ mit __copyright__ copyright 2022 mindsdb check database handlers see integration handlers currently use encourage check following handlers inside mindsdb repository mysqlhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler postgreshttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspostgres_handler handlers available mindsdb repositoryhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers"
  },
  {
    "filename": "tests.mdx",
    "path": "docs/contribute/tests.mdx",
    "chunk_id": 0,
    "chunk_content": "title testing improving test coverage sidebartitle testing icon vial warning work progress documentation work progress warning"
  },
  {
    "filename": "contribute.mdx",
    "path": "docs/contribute/contribute.mdx",
    "chunk_id": 0,
    "chunk_content": "title contribute mindsdb sidebartitle contribute icon handshake thank interest contributing mindsdb mindsdb free opensource software types contributions welcome whether theyre documentation changes bug reports bug fixes new source code changes contribution issues issues open contributions tagged goodfirstissuehttpsgithubcommindsdbmindsdblabelsgood20first20issue helpwantedhttpsgithubcommindsdbmindsdblabelshelp20wanted great place start looking github project community contributors dashboardhttpsgithubcommindsdbmindsdbprojects8 also always open suggestions feel free open new issueshttpsgithubcommindsdbmindsdbissuesnewchoose ideas give guidance find issue want contribute follow forkandpull workflow 1 comment issue assign 2 fork mindsdb repository 3 clone repository locally 4 make changes commit 5 push local branch fork 6 submit pull request review changes 7 follow pr template provide required informations 8 make sure ci tests green note sure merge latest mindsdb repository upstream making pull request note pull request reviews done regular basis please make sure respond feedbackquestions sign cla documentation always trying improve documentation pull requests improve grammar docs structure fix typos welcomed check issues labeled goodfirstissuehttpsgithubcommindsdbmindsdblabelsgood20first20issue helpwantedhttpsgithubcommindsdbmindsdblabelshelp20wanted documentation tag write us find mindsdb useful want share story make pr repo writing markdown file post medium dev blog post would love hear"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title build aiml handler sidebartitle build aiml handler icon gear section youll find create new machine learning ml handlers within mindsdb note prerequisite latest version mindsdb repository installed locally follow guidecontributeinstall learn install mindsdb development note machine learning handlers ml handlers act bridge ml framework use ml handlers create ml engines using create ml_engine commandsqlcreatemlengine expose ml models supported ml engine ai table note database handlers learn handlers implement database handler visit doc page herecontributedatahandlers note creating machine learning handler create ml handler within mindsdb inheriting basemlenginehttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl123 class providing implementation methods contained basemlengine class connect machine learning library framework choice core methods apart __init__ method five methods two must implemented recommend checking actual examples codebase get idea goes methods change bit depending nature system integrated lets review purpose method method purpose create creates model inside engine registry predict calls model returns prediction data update optional updates existing model without resetting internal structure describe optional provides global model insights create_engine optional connects external sources rest api authors opt adding private methods new files folders combination structure necessary work enable core methods work intended tip common methods mindsdbintegrationslibsutils library contributors find various methods may useful implementing new handlers also wrapper class basemlengine"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 1,
    "chunk_content": "instances called basemlengineexechttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationslibsml_exec_basepyl157 automatically deployed take care modifying data responses something used alongside data handlers tip implementation methods must implemented inheriting basemlenginehttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl123 class create methodhttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl151 saves model inside engine registry later usage py def createself target str df optionalpddataframe none args optionaldict none none saves model inside engine registry later usage normally input dataframe required train model however integrations may merely require registering model instead training case df omitted arguments required register model passed args dictionary predict methodhttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl162 calls model input dataframe optionally arguments modify models behaviour method returns dataframe predicted values py def predictself df pddataframe args optionaldict none pddataframe calls model input dataframe df optionally arguments args may modify model behavior expected output dataframe predicted values targetnamed column additional columns present considered rowwise explanations names finish _explain optional methods implement alongside mandatory ones ml framework allows update methodhttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl171 used update finetune adjust existing model without resetting internal state py def finetuneself df optionalpddataframe none args optionaldict none none optional used updatefinetuneadjust preexisting model without resetting internal state eg weights availability depend underlying integration support ml models partially updated describe methodhttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl181 provides global model insights frameworklevel parameters used training py def describeself key optionalstr none pddataframe optional called method"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 2,
    "chunk_content": "provides global model insights eg frameworklevel parameters used training create_engine methodhttpsgithubcommindsdbmindsdbblob3d9090acb0b8b3b0e2a96e2c93dad436f5ebef90mindsdbintegrationslibsbasepyl189 used connect external sources rest api py def create_engineself connection_args dict optional used connect external sources eg rest api engine require use methods mindsdb ml ecosystem mindsdb recently decoupled modules automl package order leverage integrations ml engines three modules follows 1 type_inferhttpsgithubcommindsdbtype_infer module implements automated type inference datasetbrbr description input output modulebrbr input tabular datasetbrbr output best guesses type data column contains 2 dataprep_mlhttpsgithubcommindsdbdataprep_ml module provides data preparation utilities data cleaning analysis splitting data cleaning procedures include columnwise cleaners columnwise missing value imputers data splitters trainvaltest split either simple stratifiedbrbr description input output modulebrbr input tabular datasetbrbr output cleaned dataset plus insights useful data analysis model building 3 mindsdb_evaluatorhttpsgithubcommindsdbmindsdb_evaluator module provides utilities evaluating accuracy calibration ml modelsbrbr description input output modulebrbr input model predictions input data used generate predictions including corresponding ground truth values column predictbrbr output accuracy metrics evaluate prediction accuracy calibration metrics check whether modelemitted probabilities calibrated recommend new contributors use type_inferhttpsgithubcommindsdbtype_infer dataprep_mlhttpsgithubcommindsdbdataprep_ml modules writing ml handlers avoid reimplementing thin automl layers advised focus mapping input data user parameters underlying frameworks api using mindsdb_evaluatorhttpsgithubcommindsdbmindsdb_evaluator module required short medium term important aware writing new integration tip example lets"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 3,
    "chunk_content": "say want write integration tpot highlevel api exposes classes either classification regression handler designer need ensure arbitrary ml tasks dispatched properly class ie using regressor classification problem vice versa first type_infer help estimating data type target variable immediately know class use additionally quickly get stratified traintest split leverage dataprep_ml splitters continue focus actual usage tpot training inference logic tip note would appreciate feedback regarding usage feature roadmap modules quite new note stepbystep instructions accordiongroup accordion titlestep 1 set run mindsdb locally 1 set mindsdb using selfhosted pipsetupselfhostedpipsource installation method 2 make sure run quickstart examplequickstart locally run errors check bash terminal output 3 create new git branch store changes accordion accordion titlestep 2 write failing test new handler 1 check run existing handler tests python pytest testsunitml_handlers get modulenotfounderror error try adding __init__py file subdirectory doesnt 2 copy simple tests relevant handler regular data use ludwighttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersludwig_handler handler time series data use statsforecasthttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstatsforecast_handler handler 3 change sql query reference handler specifically set using enginehandlername 4 run new test please note fail havent yet added handler exception cant find integration_record handler accordion accordion titlestep 3 add handler source code 1 create new directory mindsdbintegrationshandlers must name new directory handlername_handler 2 copy py"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 4,
    "chunk_content": "files openai handler folderhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenai_handler including __about__py __init__py openai_handlerpy creation_argspy model_using_argspy note note arguments used model creation time stored creation_argspy arguments used prediction time stored model_using_argspy stored separate files order able hide sensitive information api keys default querying connection_data information_schemaml_engines table training_options information_schemamodels table sensitive information hidden unhide use command sql set show_secretstrue note 3 change contents py files match new handler also change name statsforecast_handlerpy file match handler 4 modify requirementstxt file install handlers dependencies may get conflicts packages like lightwood ignore 5 create new blank class handler handlername_handlerpy file like handlers subclass basemlengine class 6 add new handler class testing db testsunitexecutor_test_basepy file starting line 91 see handlers added dbsessionadd copy modify add handler please note add handler lightwood otherwise ci break 7 run new test please note still fail different exception message accordion accordion titlestep 4 modify handler source code test passes 1 define create method deals model setup arguments add handler models table depending framework may also train model using df argument 2 save relevant argumentstrained models end create method allows accessed later use engine_storage attributes find examples handlers folders 3 define predict method makes model predictions method must return dataframe format matching input except column containing"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 5,
    "chunk_content": "models predictions target input df subset original df rows determined conditions predict sql query 4 dont debug create predict methods print statement theyre inside subthread instead write relevant info disk 5 first test passes add new tests important cases also add tests helper functions write accordion accordion titlestep 5 qa handler locally 1 launch mindsdb server locally python mindsdb issues appear terminal output 2 check handler added local server database view list handlers select information_schemahandlers 3 run relevant tutorial panel right side regular data predict home rental prices time series data forecast quarterly house sales specify using engineyour_handler creating model 4 dont debug create predict methods print statement theyre inside subthread instead write relevant info disk 5 get sensible results handler wellimplemented make sure try predict step range parameters accordion accordion titlestep 6 open pull request 1 need fork mindsdb repository follow guidehttpsgithubcommindsdbmindsdbblobmaincontributingmd start pr 2 relevant add tests new dependencies ci config githubworkflowsmindsdbyml accordion accordiongroup note please note pytest recommended testing package use pytest confirm ml handler implementation correct note tip templates unit tests implement timeseries ml handler create unit tests following structure statsforecast unit testshttpsgithubcommindsdbmindsdbblobmaintestsunitml_handlerstest_statsforecastpy implement nlp ml handler create unit tests following structure hugging face unit testshttpsgithubcommindsdbmindsdbblobmaintestsunitml_handlerstest_huggingfacepy"
  },
  {
    "filename": "ml-handlers.mdx",
    "path": "docs/contribute/ml-handlers.mdx",
    "chunk_id": 6,
    "chunk_content": "tip check machine learning handlers see ml handlers currently use encourage check following ml handlers inside mindsdb repository lightwoodhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightwood_handler huggingfacehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershuggingface_handler ludwighttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersludwig_handler openaihttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersopenai_handler handlers available mindsdb repositoryhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers"
  },
  {
    "filename": "issue-labels.mdx",
    "path": "docs/contribute/issue-labels.mdx",
    "chunk_id": 0,
    "chunk_content": "title issue labels sidebartitle issue labels icon tag section go issue labels available mindsdb github repositoryhttpsgithubcommindsdbmindsdb labels interest contributors plan contributor look issues marked labels font color9f1260firsttimersonlyfont label havent contributed mindsdb search issues marked codefont color9f1260firsttimersonlyfontcode get started please note accept contribution mindsdb ask sign contributor license agreementhttpsgithubcommindsdbmindsdbblobmainassetscontributionsagreementindividualcontributormd find codefont color9f1260firsttimersonlyfontcode issues herehttpsgithubcommindsdbmindsdblabelsfirsttimersonly font color7057ffgood first issuefont label issues marked codefont color7057ffgood first issuefontcode good newcomers find codefont color7057ffgood first issuefontcode issues herehttpsgithubcommindsdbmindsdblabelsgood20first20issue font color0e8a16hactoberfestfont label taking issue marked codefont color0e8a16hactoberfestfontcode participate hacktoberfest competition takes place every year october find codefont color0e8a16hactoberfestfontcode issues herehttpsgithubcommindsdbmindsdblabelshacktoberfest font color008672help wantedfont label issues marked codefont color008672help wantedfontcode anybody wants contribute mindsdb find codefont color008672help wantedfontcode issues herehttpsgithubcommindsdbmindsdblabelshelp20wanted font colord8ed79integrationfont label codefont colord8ed79integrationfontcode label marks issues require contributor implement database ml integration mindsdb take issue participate integration contest mindsdb create issue idea integration following instructions herecontributeissuesproposeanewintegration find codefont colord8ed79integrationfontcode issues herehttpsgithubcommindsdbmindsdblabelsintegration labels labels used mindsdb repository font colord93f0bbugfont label codefont colord93f0bbugfontcode label marks issues describe whats currently working report bug following instructions herecontributeissuesreportabug font colora6c5efdiscussionfont label issue marked codefont colora6c5efdiscussionfontcode requires discussion resolved font color0075cadocumentationfont label codefont color0075cadocumentationfontcode label marks issues related docs improve docs creating issues following instructions herecontributeissuesimproveourdocs font colora2eeefenhancementfont label mindsdb grows"
  },
  {
    "filename": "issue-labels.mdx",
    "path": "docs/contribute/issue-labels.mdx",
    "chunk_id": 1,
    "chunk_content": "day day still things require improvements issues suggesting enhancements mindsdb marked codefont colora2eeefenhancementfontcode request feature following instructions herecontributeissuesrequestafeature font color1a8e5afollowupfont label issue marked codefont color1a8e5afollowupfontcode requires users feedback resolved font colord876e3questionfont label issue marked codefont colord876e3questionfontcode requires information resolved font color883aeeuser requestfont label customers suggest improvements report bugs issue comes marked codefont color883aeeuser requestfontcode"
  },
  {
    "filename": "data-overview.mdx",
    "path": "docs/integrations/data-overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title data integrations sidebartitle overview icon database mindsdb integrates numerous data sources including databases vector stores applications making data available ai models connecting data sources mindsdb info section contains instructions connect data sources mindsdb note mindsdb doesnt store copy data instead fetches data directly connected sources time make query ensuring changes data instantly reflected means data remains original location mindsdb always works uptodate information info applications cardgroup cols4 card titlebinance icondiamond hrefintegrationsappintegrationsbinancecard card titleconfluence iconconfluence hrefintegrationsappintegrationsconfluencecard card titledocker hub icondocker hrefintegrationsappintegrationsdockerhubcard card titleemail iconat hrefintegrationsappintegrationsemailcard card titlegithub icongithub hrefintegrationsappintegrationsgithubcard card titlegitlab icongitlab hrefintegrationsappintegrationsgitlabcard card titlegmail iconenvelope hrefintegrationsappintegrationsgmailcard card titlegoogle calendar iconcalendar hrefintegrationsappintegrationsgooglecalendarcard card titlegoogle analytics iconchartcolumn hrefintegrationsappintegrationsgoogleanalyticscard card titlehacker news iconsquarehackernews hrefintegrationsappintegrationshackernewscard card titleinstatus iconsignal hrefintegrationsappintegrationsinstatuscard card titleintercom iconintercom hrefintegrationsappintegrationsintercomcard card titlemediawiki iconcertificate hrefintegrationsappintegrationsmediawikicard card titlemicrosoft onedrive iconpeoplegroup hrefappintegrationsmicrosoftonedrivecard card titlemicrosoft teams iconpeoplegroup hrefappintegrationsmicrosoftteamscard card titlenews api iconpeoplegroup hrefappintegrationsnewsapicard card titlepaypal iconpaypal hrefintegrationsappintegrationspaypalcard card titleplaid iconstroopwafel hrefintegrationsappintegrationsplaidcard card titlepypi iconcube hrefintegrationsappintegrationspypicard card titlereddit iconreddit hrefintegrationsappintegrationsredditcard card titlesalesforce iconsalesforce hrefintegrationsappintegrationssalesforcecard card titlesendinblue iconlink hrefintegrationsappintegrationssendinbluecard card titleshopify iconshopify hrefintegrationsappintegrationsshopifycard card titleslack iconslack hrefintegrationsappintegrationsslackcard card titlestrapi iconrocket hrefintegrationsappintegrationsstrapicard card titlestripe iconstripes hrefintegrationsappintegrationsstripecard card titlesymbl iconmicrochipai hrefintegrationsappintegrationssymblcard card titletwitter icontwitter hrefintegrationsappintegrationstwittercard card titleweb crawler iconglobe hrefintegrationsappintegrationswebcrawlercard card titleyoutube iconyoutube hrefintegrationsappintegrationsyoutubecard cardgroup databases cardgroup cols4"
  },
  {
    "filename": "data-overview.mdx",
    "path": "docs/integrations/data-overview.mdx",
    "chunk_id": 1,
    "chunk_content": "card titleairtable iconlink hrefintegrationsdataintegrationsairtablecard card titleamazon aurora iconlink hrefintegrationsdataintegrationsamazonauroracard card titleamazon dynamodb iconlink hrefintegrationsdataintegrationsamazondynamodbcard card titleamazon redshift iconlink hrefintegrationsdataintegrationsamazonredshiftcard card titleamazon s3 iconlink hrefintegrationsdataintegrationsamazons3card card titleapache cassandra iconlink hrefintegrationsdataintegrationsapachecassandracard card titleapache druid iconlink hrefintegrationsdataintegrationsapachedruidcard card titleapache hive iconlink hrefintegrationsdataintegrationsapachehivecard card titleapache ignite iconlink hrefintegrationsdataintegrationsapacheignitecard card titleapache impala iconlink hrefintegrationsdataintegrationsapacheimpalacard card titleapache pinot iconlink hrefintegrationsdataintegrationsapachepinotcard card titleapache solr iconlink hrefintegrationsdataintegrationsapachesolrcard card titleckan iconlink hrefintegrationsdataintegrationsckancard card titleclickhouse iconlink hrefintegrationsdataintegrationsclickhousecard card titlecloud spanner iconlink hrefintegrationsdataintegrationscloudspannercard card titlecockroachdb iconlink hrefintegrationsdataintegrationscockroachdbcard card titlecouchbase iconlink hrefintegrationsdataintegrationscouchbasecard card titlecratedb iconlink hrefintegrationsdataintegrationscratedbcard card titled0lt iconlink hrefintegrationsdataintegrationsd0ltcard card titledatabend iconlink hrefintegrationsdataintegrationsdatabendcard card titledatabricks iconlink hrefintegrationsdataintegrationsdatabrickscard card titledatastax iconlink hrefintegrationsdataintegrationsdatastaxcard card titleduckdb iconlink hrefintegrationsdataintegrationsduckdbcard card titleedgelessdb iconlink hrefintegrationsdataintegrationsedgelessdbcard card titleelasticsearch iconlink hrefintegrationsdataintegrationselasticsearchcard card titlefirebird iconlink hrefintegrationsdataintegrationsfirebirdcard card titlegoogle bigquery iconlink hrefintegrationsdataintegrationsgooglebigquerycard card titlegoogle cloud sql iconlink hrefintegrationsdataintegrationsgooglecloudsqlcard card titlegoogle sheets iconlink hrefintegrationsdataintegrationsgooglesheetscard card titlegreptimedb iconlink hrefintegrationsdataintegrationsgreptimedbcard card titleibm db2 iconlink hrefintegrationsdataintegrationsibmdb2card card titleibm informix iconlink hrefintegrationsdataintegrationsibminformixcard card titleinfluxdb iconlink hrefintegrationsdataintegrationsinfluxdbcard card titlemariadb iconlink hrefintegrationsdataintegrationsmariadbcard card titlematrixone iconlink hrefintegrationsdataintegrationsmatrixonecard card titlemicrosoft access iconlink hrefintegrationsdataintegrationsmicrosoftaccesscard card titlemicrosoft sql server iconlink hrefintegrationsdataintegrationsmicrosoftsqlservercard card titlemonetdb iconlink hrefintegrationsdataintegrationsmonetdbcard card titlemongodb iconlink hrefintegrationsdataintegrationsmongodbcard card titlemysql iconlink hrefintegrationsdataintegrationsmysqlcard card titleoceanbase iconlink hrefintegrationsdataintegrationsoceanbasecard card titleopengauss iconlink hrefintegrationsdataintegrationsopengausscard card titleoracle iconlink hrefintegrationsdataintegrationsoraclecard card titleorioledb iconlink hrefintegrationsdataintegrationsorioledbcard card titleplanetscale iconlink"
  },
  {
    "filename": "data-overview.mdx",
    "path": "docs/integrations/data-overview.mdx",
    "chunk_id": 2,
    "chunk_content": "hrefintegrationsdataintegrationsplanetscalecard card titlepostgresql iconlink hrefintegrationsdataintegrationspostgresqlcard card titlequestdb iconlink hrefintegrationsdataintegrationsquestdbcard card titlesap hana iconlink hrefintegrationsdataintegrationssaphanacard card titlesap sql anywhere iconlink hrefintegrationsdataintegrationssapsqlanywherecard card titlescylladb iconlink hrefintegrationsdataintegrationsscylladbcard card titlesinglestore iconlink hrefintegrationsdataintegrationssinglestorecard card titlesnowflake iconlink hrefintegrationsdataintegrationssnowflakecard card titlesqlite iconlink hrefintegrationsdataintegrationssqlitecard card titlestarrocks iconlink hrefintegrationsdataintegrationsstarrockscard card titlesupabase iconlink hrefintegrationsdataintegrationssupabasecard card titlesurrealdb iconlink hrefintegrationsdataintegrationssurrealdbcard card titletdengine iconlink hrefintegrationsdataintegrationstdenginecard card titleteradata iconlink hrefintegrationsdataintegrationsteradatacard card titletidb iconlink hrefintegrationsdataintegrationstidbcard card titletimescaledb iconlink hrefintegrationsdataintegrationstimescaledbcard card titletrino iconlink hrefintegrationsdataintegrationstrinocard card titlevertica iconlink hrefintegrationsdataintegrationsverticacard card titlevitess iconlink hrefintegrationsdataintegrationsvitesscard card titleyugabytedb iconlink hrefintegrationsdataintegrationsyugabytedbcard cardgroup vector stores cardgroup cols4 card titlechromadb iconlink hrefintegrationsvectordbintegrationschromadbcard card titlecouchbase iconlink hrefintegrationsvectordbintegrationscouchbasecard card titlepgvector iconlink hrefintegrationsvectordbintegrationspgvectorcard card titlepinecone iconlink hrefintegrationsvectordbintegrationspineconecard card titleweaviate iconlink hrefintegrationsvectordbintegrationsweaviatecard cardgroup brbr tip dont find data source interest request feature herehttpsgithubcommindsdbmindsdbissuesnewassigneeslabelsenhancementprojectstemplatefeature_request_v2yaml build handler following instruction data handlerscontributedatahandlers instruction applicationscontributeapphandlers tip info metadata data handlers data sources data handlers represent raw implementation integration mindsdb data source query available data handlers used connect data sources mindsdb sql select information_schemahandlers type data alternatively sql show handlers type data query created ai engines sql select information_schemadatabases alternatively sql show databases info"
  },
  {
    "filename": "sample-database.mdx",
    "path": "docs/integrations/sample-database.mdx",
    "chunk_id": 0,
    "chunk_content": "title sample database sidebartitle sample database icon book mindsdb provides readonly postgresql database preloaded various datasets datasets curated cover wide range scenarios use cases allowing experiment different features mindsdb publicly accessible postgresql database designed testing playground purposes using datasets quickly get started mindsdb understand works see applied realworld problems connection connect readonly postgresql database access example datasets use following connection parameters python create database postgresql_conn engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo schema demo_data list avaiable datasets tables data tables tables converted markdown format fraud detection dataset fraud_detection table contains data mobile money transactions step represents hour simulation step type amount nameorig oldbalanceorg newbalanceorig namedest oldbalancedest newbalancedest isfraud isflaggedfraud 1 payment 983964 c1231006815 1701360 16029636 m1979787155 00 00 0 0 1 payment 186428 c1666544295 212490 1938472 m2044282225 00 00 0 0 1 transfer 1810 c1305486145 1810 00 c553264065 00 00 1 0 learn dataset please visit linkhttpswwwkagglecomdatasetsealaxipaysim1 customer churn dataset customer_churn table contains data us bank customer churn rownumber customerid surname creditscore geography gender age tenure balance numofproducts hascrcard isactivemember estimatedsalary exited 1 15634602 hargrave 619 france female 42 2 0 1 1 1 10134888 1 2 15647311 hill 608 spain female 41 1"
  },
  {
    "filename": "sample-database.mdx",
    "path": "docs/integrations/sample-database.mdx",
    "chunk_id": 1,
    "chunk_content": "8380786 1 0 1 11254258 0 3 15619304 onio 502 france female 42 8 1596608 3 1 0 11393157 1 learn dataset please visit linkhttpswwwkagglecomdatasetsshantanudhakaddbankcustomerchurnprediction customer support chat dataset customer_support_chat table contains data chat customer support used train large language models llms fine tuning domain adaptation flags instruction category intent response b question cancelling order order number order cancel_order ive understood question regarding canceling order order number im provide information need please go ahead ask question ill best assist bqz question cancelling order order number order cancel_order ive informed question canceling order order number im assist please go ahead let know specific question ill provide information guidance need satisfaction top priority learn dataset please visit linkhttpshuggingfacecodatasetsbitextbitextcustomersupportllmchatbottrainingdataset bank customer transactions dataset bank_customer_transactions table contains data customer transactions demographic shopping behavior information customer id name surname gender birthdate transaction amount date merchant name category 752858 sean rodriguez f 20021020 3547 20230403 smithrussell cosmetic 26381 michelle phelps 19851024 255272 20230717 peck spence young travel 305449 jacob williams 19811025 11597 20230920 steele inc clothing learn dataset please visit linkhttpswwwkagglecomdatasetsbkcobancustomertransactions telecom customer churn dataset telecom_customer_churn table contains data customer activities preferences behaviors age gender security_no region_category membership_category joining_date joined_through_referral referral_id preferred_offer_types medium_of_operation internet_option last_visit_time days_since_last_login"
  },
  {
    "filename": "sample-database.mdx",
    "path": "docs/integrations/sample-database.mdx",
    "chunk_id": 2,
    "chunk_content": "avg_time_spent avg_transaction_value avg_frequency_login_days points_in_wallet used_special_discount offer_application_preference past_complaint complaint_status feedback churn_risk_score 18 f xw0dq7h village platinum membership 17082017 xxxxxxxx gift voucherscoupons wifi 160802 17 30063 5300525 17 78175 yes yes applicable products always stock 0 32 f 5k0n3x1 city premium membership 28082017 cid21329 gift voucherscoupons desktop mobile_data 123813 16 30634 1283838 10 yes yes solved quality customer care 0 44 f 1f2tcl3 town membership 11112016 yes cid12313 gift voucherscoupons desktop wifi 225321 14 51616 21027 22 50069 yes yes solved followup poor website 1 learn dataset please visit linkhttpshuggingfacecodatasetsd0r1hcustomer_churn house sales dataset house_sales table contains data houses sold throughout years saledate type bedrooms created_at 20070930 441854 house 2 20070202 154151922127 20071231 441854 house 2 20070223 223608540248 20080331 441854 house 2 20070225 192352585358 learn dataset please visit linkhttpswwwkagglecomdatasets"
  },
  {
    "filename": "support.mdx",
    "path": "docs/integrations/support.mdx",
    "chunk_id": 0,
    "chunk_content": "title supported integrations sidebartitle supported integrations icon badgecheck mindsdb integrates numerous data sources aiml frameworks llm providers integrations fall two categories verified integrations officially supported maintained mindsdb team highstandard integrations meet mindsdb verification process ensuring full feature coverage thorough testing community integrations developed maintained mindsdb community integrations offer valuable functionality continue receive communitydriven improvements support tip documentation integrations found following sections data sourcesintegrationsdataoverview ai enginesintegrationsaioverview tip verified integrations list verified integrations integration type handler postgresql data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspostgres_handler mysql data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler clickhouse data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersclickhouse_handler microsoft sql server data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmssql_handler minds endpoints ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersminds_endpoint_handler snowflake data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssnowflake_handler web data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersweb_handler redshift data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersredshift_handler openai ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenai_handler anyscale ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanyscale_endpoints_handler google bigquery data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersbigquery_handler elasticsearch data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerselasticsearch_handler amazon s3 data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerss3_handler community integrations list community integrations integration type handler langchain ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslangchain_handler slack data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersslack_handler ollama ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersollama_handler chromadb vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerschromadb_handler milvus vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmilvus_handler pinecone vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspinecone_handler qdrant vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersqdrant_handler weaviate vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersweaviate_handler pgvector vector store linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspgvector_handler lightwood ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightwood_handler hugging face ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershuggingface_handler llama index ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersllama_index_handler anthropic ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanthropic_handler mariadb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmariadb_handler timegpt ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstimegpt_handler mongodb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmongodb_handler x twitter data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstwitter_handler github data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgithub_handler hugging face inference api ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershuggingface_api_handler binance data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersbinance_handler byom ai"
  },
  {
    "filename": "support.mdx",
    "path": "docs/integrations/support.mdx",
    "chunk_id": 1,
    "chunk_content": "linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersbyom_handler cassandra data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscassandra_handler confluence data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersconfluence_handler gmail data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgmail_handler couchbase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscouchbase_handler statsforecast ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstatsforecast_handler twelve labs ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstwelve_labs_handler anomaly detection ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanomaly_detection_handler youtube data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersyoutube_handler vertex ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersvertex_handler aerospike data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersaerospike_handler microsoft access data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersaccess_handler airtable data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersairtable_handler altibase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersaltibase_handler apache doris data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersapache_doris_handler world air quality index data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersaqicn_handler amazon aurora data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersaurora_handler autogluon ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersautogluon_handler autokeras ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersautokeras_handler autosklearn ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersautosklearn_handler ckan data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersckan_handler clipdrop ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersclipdrop_handler google cloud spanner data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscloud_spanner_handler google cloud sql data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscloud_sql_handler cockroachdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscockroach_handler cohere ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscohere_handler coinbase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscoinbase_handler crate db data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscrate_handler d0lt data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersd0lt_handler databend data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdatabend_handler databricks data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdatabricks_handler datastax data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdatastax_handler ibm db2 data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdb2_handler apache derby data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersderby_handler discord data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdiscord_handler docker hub data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdockerhub_handler documentdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdocumentdb_handler dremio data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdremio_handler apache druid data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdruid_handler duckdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersduckdb_handler amazon dynamodb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersdynamodb_handler edgelessdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersedgelessdb_handler email data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersemail_handler empress data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersempress_handler eventbrite data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerseventbrite_handler eventstoredb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerseventstoredb_handler faunadb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersfaunadb_handler firebird data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersfirebird_handler flaml ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersflaml_handler frappe data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersfrappe_handler gitlab data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgitlab_handler google analytics data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_analytics_handler google books data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_books_handler google calendar data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_calendar_handler google content shopping data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_content_shopping_handler google fit data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_fit_handler google gemini ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_gemini_handler google search data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_search_handler hackernews data"
  },
  {
    "filename": "support.mdx",
    "path": "docs/integrations/support.mdx",
    "chunk_id": 2,
    "chunk_content": "linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershackernews_handler sap hana data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershana_handler hive data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershive_handler hsqldb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershsqldb_handler hubspot data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershubspot_handler apache ignite data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersignite_handler apache impala data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersimpala_handler influxdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersinfluxdb_handler ibm informix data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersinformix_handler ingres data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersingres_handler instatus data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersinstatus_handler intercom data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersintercom_handler jira data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersjira_handler kinetica data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerskinetica_handler lancedb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslancedb_handler langchain embeddings ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslangchain_embedding_handler leonardoai ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersleonardoai_handler libsql data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslibsql_handler lightdash data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightdash_handler lightfm ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslightfm_handler lindorm data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslindorm_handler litellm ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslitellm_handler ludwig ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersludwig_handler luma data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersluma_handler materialize data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmaterialize_handler matrixone data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmatrixone_handler maxdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmaxdb_handler mediawiki data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmediawiki_handler mendeley data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmendeley_handler merlion ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmerlion_handler mlflow ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmlflow_handler monetdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmonetdb_handler monkeylearn ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmonkeylearn_handler microsoft teams data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersms_teams_handler neuralforecast ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersneuralforecast_handler newsapi data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersnewsapi_handler notion data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersnotion_handler npm data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersnpm_handler nuodb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersnuo_jdbc_handler oceanbase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersoceanbase_handler oil prices data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersoilpriceapi_handler openbb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenbb_handler opengauss data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopengauss_handler openstreetmap data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenstreetmap_handler oracle data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersoracle_handler orioledb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersorioledb_handler palm ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspalm_handler paypal data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspaypal_handler phoenix data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersphoenix_handler pinot data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspinot_handler pirate weather data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspirateweather_handler plaid data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersplaid_handler planetscale data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersplanetscale_handler popularity recommender ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspopularity_recommender_handler portkey ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersportkey_handler pycaret ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspycaret_handler pypi data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerspypi_handler questdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersquestdb_handler quickbooks data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersquickbooks_handler ray serve ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersray_serve_handler reddit data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersreddit_handler replicate ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersreplicate_handler rocket chat data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersrocket_chat_handler rockset data"
  },
  {
    "filename": "support.mdx",
    "path": "docs/integrations/support.mdx",
    "chunk_id": 3,
    "chunk_content": "linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersrockset_handler sap erp data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssap_erp_handler scylla data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersscylla_handler sendinblue data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssendinblue_handler sentence transformers ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssentence_transformers_handler microsoft sharepoint data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssharepoint_handler google sheets data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssheets_handler shopify data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersshopify_handler singlestore data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssinglestore_handler solace data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssolace_handler solr data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssolr_handler spacy data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersspacy_handler sap sql anywhere data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssqlany_handler sqlite data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssqlite_handler sqreamdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssqreamdb_handler stablityai ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstabilityai_handler starrocks data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstarrocks_handler strapi data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstrapi_handler strava data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstrava_handler stripe data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersstripe_handler supabase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssupabase_handler surrealdb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssurrealdb_handler symbl data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssymbl_handler tdengine data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstdengine_handler teradata data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersteradata_handler tidb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstidb_handler timescaledb data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstimescaledb_handler tpot ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstpot_handler trino data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstrino_handler trip advisor data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstripadvisor_handler twilio data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstwilio_handler vertica data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersvertica_handler vitess data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersvitess_handler webz data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerswebz_handler whatsapp data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerswhatsapp_handler writer ai linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerswriter_handler xata data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersxata_handler yugabyte data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersyugabyte_handler zipcodebase data linkhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerszipcodebase_handler"
  },
  {
    "filename": "ai-overview.mdx",
    "path": "docs/integrations/ai-overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title ai integrations sidebartitle overview icon microchipai mindsdb integrates numerous ai frameworks facilitating deployment management ai modelsmodelmanagement p aligncenter img srcassetsaiintegrationspng p mindsdb offers wide range strongai enginesstrong used create models incorporate data landscape virtual ai tablesgenerativeaitables mindsdb abstracts ai models virtual tables generative ai tables generate data underlying model upon queried info section contains instructions create deploy models within mindsdb utilizing different aiml frameworks info large language models cardgroup cols4 card titleanthropic iconlink hrefintegrationsaienginesanthropiccard card titleanyscale endpoints iconlink hrefintegrationsaienginesanyscalecard card titlecohere iconlink hrefintegrationsaienginescoherecard card titlegoogle gemini iconlink hrefintegrationsaienginesgoogle_geminicard card titlehugging face iconlink hrefintegrationsaiengineshuggingfacecard card titlehugging face inference api iconlink hrefintegrationsaiengineshuggingface_inference_apicard card titlelangchain iconlink hrefintegrationsaiengineslangchaincard card titlellamaindex iconlink hrefintegrationsaienginesllamaindexcard card titlemonkeylearn iconlink hrefintegrationsaienginesmonkeylearncard card titleminds endpoint iconlink hrefintegrationsaienginesminds_endpointcard card titleollama iconlink hrefintegrationsaienginesollamacard card titleopenai iconlink hrefintegrationsaienginesopenaicard card titleportkey iconlink hrefintegrationsaienginesportkeycard card titlereplicate llm iconlink hrefintegrationsaienginesreplicatellmcard card titlevertex ai iconlink hrefintegrationsaienginesvertexcard cardgroup bring models cardgroup cols4 card titlebyom iconlink hrefintegrationsaienginesbyomcard card titlemlflow iconlink hrefintegrationsaienginesmlflowcard card titleray serve iconlink hrefintegrationsaienginesrayservecard cardgroup anomaly detection cardgroup cols4 card titleanomaly detection iconlink hrefintegrationsaienginesanomalycard cardgroup automl cardgroup cols4 card titlelightwood iconlink hrefintegrationsaiengineslightwoodcard card titlepycaret iconlink hrefintegrationsaienginespycaretcard cardgroup time series models cardgroup cols4 card titleneuralforecast iconlink hrefintegrationsaienginesneuralforecastcard card titlestatsforecast iconlink hrefintegrationsaienginesstatsforecastcard card titletimegpt iconlink hrefintegrationsaienginestimegptcard cardgroup recommender"
  },
  {
    "filename": "ai-overview.mdx",
    "path": "docs/integrations/ai-overview.mdx",
    "chunk_id": 1,
    "chunk_content": "models cardgroup cols4 card titlelightfm iconlink hrefintegrationsaiengineslightfmcard card titlepopularity recommender iconlink hrefintegrationsaienginespopularityrecommendercard cardgroup audio models cardgroup cols4 card titlereplicate audio iconlink hrefintegrationsaienginesreplicateaudiocard cardgroup image models cardgroup cols4 card titleclipdrop iconlink hrefintegrationsaienginesclipdropcard card titlereplicate text2img iconlink hrefintegrationsaienginesreplicatetext2imgcard card titlereplicate img2text iconlink hrefintegrationsaienginesreplicateimg2txtcard cardgroup video models cardgroup cols4 card titletwelevelabs video semantic search iconlink hrefintegrationsaienginestwelvelabscard card titlereplicate text2video iconlink hrefintegrationsaienginesreplicatetext2videocard cardgroup brbr tip dont find aiml framework interest request feature herehttpsgithubcommindsdbmindsdbissuesnewassigneeslabelsenhancementprojectstemplatefeature_request_v2yaml build aiml handler following instructioncontributemlhandlers tip info metadata ai handlers ai engines ai handlers represent raw implementation integration mindsdb aiml framework used create ai engines query available ai handlers used create ai engines sql select information_schemahandlers type ml alternatively sql show handlers type ml query created ai engines sql select information_schemaml_engines alternatively sql show ml_engines info"
  },
  {
    "filename": "integrations.mdx",
    "path": "docs/integrations/integrations.mdx",
    "chunk_id": 0,
    "chunk_content": "title integrations overview sidebartitle overview icon link mindsdb integrates numerous data sources popular aiml frameworks seamlessly bring data ai together p aligncenter img srcassetsdiagrampng p data sourcesintegrationsdataoverview data sources conect mindsdb including traditional databases data behind apis important mindsdb etl pipelines query data source mindsdb forwards query realtime original data source mindsdb good translating sql query dialect aiml frameworksintegrationsaioverview possibilities aiml modeling generative ai traditional ml automl create train deploy aiml models within mindsdb ecosystem provide data frmo connected data sources cardgroup cols2 card titledata sources icondatabase hrefintegrationsdataoverview color00a587card card titleaiml frameworks iconmicrochipai hrefintegrationsaioverview color00a587card cardgroup tip want use specific integration mindsdb either data aiml integration need ensure required dependencies installed verify running command sql show handlers name integration_name output includes import_success column column reads true go ahead use integration reads false need install required dependencies following instructionsetupselfhosteddockerinstalldependencies tip"
  },
  {
    "filename": "portkey.mdx",
    "path": "docs/integrations/ai-engines/portkey.mdx",
    "chunk_id": 0,
    "chunk_content": "title portkey sidebartitle portkey documentation describes integration mindsdb portkeyhttpswwwportkeycom ai gateway allows developers connect ai models world single api portkey also brings observability caching features useful building productiongrade ai applications prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use portkey within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain portkey api key required deploy use portkey within mindsdb follow instructions obtaining api keyhttpsdocsportkeyaidocsapireferenceintroduction setup create ai engine portkey handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersportkey_handler info pass parameters supported portkey inside using clause info sql create ml_engine portkey_engine portkey using portkey_api_key portkey_api_key get portkey dashboard httpsappportkeyaiapikeys config portkey_config_id get portkey dashboard httpsappportkeyaiconfigs create model using portkey_engine engine info pass parameters supported portkey chat completions inside using clause refer portkey chat completionshttpsdocsportkeyaidocsproviderendpointschat details info sql create model portkey_model predict answer using engine portkey_engine model gpt35turbo temperature 02 info integrations portkey mindsdb implemented using portkey python sdkhttpsdocsportkeyaidocsapireferenceportkeysdkclient info query model get predictions sql select question answer portkey_model question stockholm located output sql question answer stockholm located stockholm capital largest city sweden located swedens southcentral east coast lake m\u00e4laren meets baltic sea usage following usage examples utilize portkey_engine gpt35turbo create model create model statement creating summarization model example demonstrates create model summarize"
  },
  {
    "filename": "portkey.mdx",
    "path": "docs/integrations/ai-engines/portkey.mdx",
    "chunk_id": 1,
    "chunk_content": "text using portkey sql create model summarization_model predict summary using engine portkey_engine model gpt35turbo temperature 05 max_tokens 100 select document summary summarization_model document mindsdb predictive platform connects machine learning models databases generating sentiment analysis sql create model sentiment_model predict sentiment using engine portkey_engine model gpt35turbo temperature 03 select review sentiment sentiment_model review product excellent exceeded expectations translating text sql create model translation_model predict translation using engine portkey_engine model gpt35turbo temperature 04 select original_text translation translation_model original_text hello target_language es extracting key information sql create model extraction_model predict extracted_data using engine portkey_engine model gpt35turbo temperature 06 select text extracted_data extraction_model text minds 35 lives new york works software engineer tip next steps go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "autosklearn.mdx",
    "path": "docs/integrations/ai-engines/autosklearn.mdx",
    "chunk_id": 0,
    "chunk_content": "title autosklearn sidebartitle autosklearn note page work progress note"
  },
  {
    "filename": "neuralforecast.mdx",
    "path": "docs/integrations/ai-engines/neuralforecast.mdx",
    "chunk_id": 0,
    "chunk_content": "title nixtlas neuralforecast integration mindsdb sidebartitle neuralforecast nixtlas neuralforecast provides diverse array neural forecasting models prioritizing ease use resilience models encompass spectrum options including traditional networks like mlp rnns well cuttingedge innovations nbeats nhits tft various architectural approaches learn features herehttpsnixtlagithubioneuralforecast bring neuralforecast models mindsdb creating model need create ml engine neuralforecast using create ml_engine statement sql create ml_engine neuralforecast neuralforecast ml engine created use create model statement create neuralforecast model mindsdb sql create model model_name data_source select table_name predict column_to_be_predicted group column_name column_name order date_column window 12 model looks back sets 12 rows horizon 3 model forecasts next 3 rows using engine neuralforecast frequency q train_time 001 exogenous_vars var_1 var_2 ensure model created based neuralforecast engine include using clause end frequency parameter informs model expected time difference measurement supported values herehttpspandaspydataorgpandasdocsstableuser_guidetimeserieshtmloffsetaliases train_time parameter defines training time defaults 1 lower values reduce training time linearly reducing number searches allowed best configuration autonhits also define exogenous_vars parameter using clause complementary variables table may improve forecast accuracy example lets go example use nixtlas neuralforecast mindsdb forecast monthly expenditures based historical data please note using neuralforecast engine create mindsdb editor clients interact mindsdb command sql create ml_engine neuralforecast neuralforecast check available engines command"
  },
  {
    "filename": "neuralforecast.mdx",
    "path": "docs/integrations/ai-engines/neuralforecast.mdx",
    "chunk_id": 1,
    "chunk_content": "sql show ml_engines see neuralforecast engine list ready follow tutorials use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbhistorical_expenditures limit 3 output sql month category expenditure 19820401 clothing 3599 19820501 clothing 3866 19820601 clothing 3505 historical_expenditures table stores monthly expenditure data various categories food clothing industry lets create model table predict expenditures sql create model quarterly_expenditure_forecaster mysql_demo_db select historical_expenditures predict expenditure group category order month window 12 horizon 3 using engine neuralforecast create model statement creates trains deploys model predict expenditure column values time series model order data month column additionally group data category column predictions made group independently category next define window horizon clauses window clause specifies number rows look back look back sets 12 rows horizon clause defines many rows predictions made next 3 rows tip please visit docs create modelsqlcreatemodel statement learn tip engine parameter using clause specifies ml engine used make predictions check training status following query sql describe quarterly_expenditure_forecaster model status complete behavior ai table query batch predictions joining data table sql select mmonth month"
  },
  {
    "filename": "neuralforecast.mdx",
    "path": "docs/integrations/ai-engines/neuralforecast.mdx",
    "chunk_id": 2,
    "chunk_content": "mexpenditure forecasted mindsdbquarterly_expenditure_forecaster join mysql_demo_dbhistorical_expenditures tmonth latest tcategory clothing output data sql month forecasted 20171001 000000000000 108022109375 20171101 000000000000 107492041015625 20171201 000000000000 12423849609375 historical_expenditures table used make batch predictions upon joining quarterly_expenditure_forecaster model historical_expenditures table get predictions next quarter defined horizon 3 clause please note output month column contains date timestamp format used default timestamp required dealing hourly frequency data mindsdb provides latest keyword marks latest training data point clause specify month latest condition ensure predictions made data latest training data point lets consider quarterly_expenditure_forecaster model train model using data third quarter 2017 predictions come fourth quarter 2017 defined horizon 3 neuralforecast hierarchicalforecast neuralforecast handler also supports hierarchical reconciliation via nixtlas hierarchicalforecast packagehttpsnixtlagithubiohierarchicalforecast hierarchical reconciliation may improve prediction accuracy data hierarchical structure example may hierarchy total expenditure comprised 7 different categories sql select distinct category mysql_demo_dbhistorical_expenditures available categories sql category food household_goods clothing department_stores cafes industry spending category may related time example spending food rises october 2017 may likely spending cafes also rises october 2017 hierarchical reconciliation account shared information create model sql create model hierarchical_expenditure_forecaster mysql_demo_db select historical_expenditures predict expenditure group category order month horizon 3 using engine neuralforecast hierarchy category predictions model account hierarchical structure output may differ"
  },
  {
    "filename": "neuralforecast.mdx",
    "path": "docs/integrations/ai-engines/neuralforecast.mdx",
    "chunk_id": 3,
    "chunk_content": "default model assume hierarchy"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/integrations/ai-engines/openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title openai sidebartitle openai documentation describes integration mindsdb openaihttpsopenaicom ai research organization known developing ai models like gpt3 gpt4 integration allows deployment openai models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use openai within mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 obtain openai api key required deploy use openai models within mindsdb follow instructions obtaining api keyhttpshelpopenaicomenarticles4936850wheredoifindmysecretapikey setup create ai engine openai handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersopenai_handler sql create ml_engine openai_engine openai using openai_api_key apikeyvalue create model using openai_engine engine sql create model openai_model predict target_column using engine openai_engine engine name created via create ml_engine api_base baseurl optional replaces default base url mode mode_name optional mode run model model_name openai_model_name optional default value gpt35turbo question_column question optional column name stores user input context_column context optional column stores context user input prompt_template input message model optional user provides instructions model user_column user_input optional stores user input assistant_column conversation_context optional stores conversation context prompt instruction model optional stores instruction model max_tokens 100 optional token limit answer temperature 03 temp json_struct key value tip want update prompt_template parameter recreate model instead override prompt_template parameter prediction time like sql select question"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/integrations/ai-engines/openai.mdx",
    "chunk_id": 1,
    "chunk_content": "answer openai_model question input question using prompt_template input new message model tip following parameters available use creating openai model accordiongroup accordion titleengine engine name created create ml_enginehttpsdocsmindsdbcommindsdb_sqlsqlcreatemlengine statement accordion accordion titleapi_base parameter optional replaces default openais base url defined value accordion accordion titlemode parameter optional available modes include default conversational conversationalfull image embedding default mode used default model replies prompt_template message conversational mode enables model read reply multiple messages conversationalfull mode enables model read reply multiple messages one reply per message image mode used create image instead text reply embedding mode enables model return output form embeddings find models supported mode herehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersopenai_handlerconstantspy accordion accordion titlemodel_name parameter optional default gpt35turbo model used find available models herehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersopenai_handlerconstantspy accordion accordion titlequestion_column parameter optional contains column name stores user input accordion accordion titlecontext_column parameter optional contains column name stores context user input accordion accordion titleprompt_template parameter optional use question_column stores message instructions model please note parameter overridden prediction time accordion accordion titlemax_tokens parameter optional defines maximum token cost prediction please note parameter overridden prediction time accordion accordion titletemperature parameter optional defines risky answers value 0 marks welldefined answer value 09 marks creative answer please note parameter overridden prediction time accordion accordion titlejson_struct parameter optional"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/integrations/ai-engines/openai.mdx",
    "chunk_id": 2,
    "chunk_content": "used extract json data text column provided prompt_template parameter see examples hereusecasesdata_enrichmentjsonfromtextextractjsonfromtextdata accordion accordiongroup usage combination parameters creating model 1 provide prompt_template alone 2 provide question_column optionally context_column 3 provide prompt user_column assistant_column create model conversational mode following usage examples utilize openai_engine create model create model statement accordiongroup accordion titleanswering questions without context create model answers questions without context sql create model openai_model predict answer using engine openai_engine question_column question query model get predictions sql select question answer openai_model question stockholm located output sql question answer stockholm locatedstockholm located sweden accordion accordion titleanswering questions context create model answers questions context sql create model openai_model predict answer using engine openai_engine question_column question context_column context query model get predictions sql select context question answer openai_model context answer accurately question many planets exist solar system execution get sql context question answer answer accurately many planets exist solar system eight planets solar system accordion accordion titleprompt completion create model offers flexible mode operation answers query provided prompt_template parameter tip good prompts key getting great completions large language models like ones openai offers best performance recommend read prompting guidehttpsbetaopenaicomdocsguidescompletionpromptdesign trying hand prompt templating tip lets look example reuses openai_model model created earlier overrides parameters prediction"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/integrations/ai-engines/openai.mdx",
    "chunk_id": 3,
    "chunk_content": "time sql select instruction answer openai_model instruction speculate extensively using prompt_template instruction tom hanks like max_tokens 100 temperature 05 execution get sql instruction answer speculate extensively people speculate tom hanks likes play golf others believe enjoys acting directing also speculated likes spend time family friends enjoys traveling accordion accordion titleconversational mode create model conversational mode sql create model openai_chat_model predict response using engine openai_engine mode conversational model_name gpt35turbo user_column user_input assistant_column conversation_history prompt answer question helpful way query model sql select response openai_chat_model user_input question conversation_history optionally provide context question accordion accordiongroup tip next steps follow tutorial sentiment analysisusecasesdata_enrichmentsentimentanalysisinsidemysqlwithopenai tutorial finetuning openai modelsusecasesautomated_finetuningopenai see use case examples tip troubleshooting guide warning authentication error symptoms failure authenticate openai api checklist 1 make sure openai account active 2 confirm api key correct 3 ensure api key revoked 4 ensure exceeded api usage rate limit warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table model names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks examples incorrect sql select inputtext outputsentiment integrationtravel data input join openai_engine output incorrect sql select inputtext outputsentiment integrationtravel data input join openai_engine output correct sql select inputtext outputsentiment integrationtravel data"
  },
  {
    "filename": "openai.mdx",
    "path": "docs/integrations/ai-engines/openai.mdx",
    "chunk_id": 4,
    "chunk_content": "input join openai_engine output warning"
  },
  {
    "filename": "pycaret.mdx",
    "path": "docs/integrations/ai-engines/pycaret.mdx",
    "chunk_id": 0,
    "chunk_content": "title pycaret sidebartitle pycaret pycaret ml handler mindsdb pycaret pycaret opensource lowcode machine learning library python automates machine learning workflows example usage creation required parameters model_type type model want build model_name pass supported models using eg supported models regression found herehttpspycaretreadthedocsioenlatestapiregressionhtmlpycaretregressioncreate_model also set best generate best model supported classification regression time_series addition required parameters 3 categories optional parameters setup create predict passed various stages model development see prefix arguments one categories pass workflow setup_ passed setup function creating model find pycarets documentation eg regression setup functions arguments documented herehttpspycaretreadthedocsioenlatestapiregressionhtmlpycaretregressionregressionexperimentsetup create_ passed create_model compare_models function depending model_name classification find docs herehttpspycaretreadthedocsioenlatestapiclassificationhtmlpycaretclassificationcreate_model predict_ passed predict_model function pycaret eg find documentation classification herehttpspycaretreadthedocsioenlatestapiclassificationhtmlpycaretclassificationpredict_model supported types models model_type classification regression time_series clustering anomaly example creating classification model sql create model my_pycaret_class_model irisdb select sepallengthcm sepalwidthcm petallengthcm petalwidthcm species iris predict species using engine pycaret model_type classification model_name xgboost setup_session_id 123 model types dont want target column like anomaly clustering pass one column names predict clause comply mindsdbs sql syntax sql create model my_pycaret_anom_model anomalydb select col1 col2 col3 col4 col5 col6 col7 col8 col9 col10 anomaly predict col10 using engine pycaret model_type anomaly model_name iforest setup_session_id 123 prediction predict using normal mindsdb syntax like"
  },
  {
    "filename": "pycaret.mdx",
    "path": "docs/integrations/ai-engines/pycaret.mdx",
    "chunk_id": 1,
    "chunk_content": "sql select tid mprediction_label mprediction_score irisdbiris join my_pycaret_class_model"
  },
  {
    "filename": "anomaly.mdx",
    "path": "docs/integrations/ai-engines/anomaly.mdx",
    "chunk_id": 0,
    "chunk_content": "title anomaly detection handler sidebartitle anomaly detection anomaly detection handler implements supervised semisupervised unsupervised anomaly detection algorithms using pyod catboost xgboost sklearn libraries models chosen based results adbench benchmark paperhttpsproceedingsneuripsccpaper_filespaper2022hashcf93972b116ca5268827d575f2cc226babstractdatasets_and_benchmarkshtml info additional information labelled data use unsupervised learner syntax create anomaly detection model model_name without specifying target predict mindsdb adds column called outlier generating results labelled data use regular model creation syntax backend logic chooses semisupervised algorithm currently xgbod vs supervised algorithm currently catboost multiple models provided create ensemble use majority voting see anomaly detection proposal documenthttpsdocsgooglecomdocumentd1yd7arzvg_67xlcyjr2kuo7mak9ia2yer1jk0edpea0editheadinghmo4wxsae6t1d information info info context types anomaly detection supervised inlieroutlier labels train classifier normal way similar standard classification problem semisupervised inlieroutlier labels perform unsupervised preprocessing step supervised classification algorithm unsupervised dont inlieroutlier labels assume training data inliers methods construct inlier criteria classify training data outliers based distributional traits new observations classified criteria however possible evaluate well model detects outliers without labels info info default dispatch logic propose following logic determine type learning use supervised learning labels available dataset contains least 3000 samples use semisupervised learning labels available number samples dataset less 3000 dataset unlabelled use unsupervised learning weve chosen 3000 based results neurips ad benchmark paper linked authors report semisupervised learning outperforms supervised learning"
  },
  {
    "filename": "anomaly.mdx",
    "path": "docs/integrations/ai-engines/anomaly.mdx",
    "chunk_id": 1,
    "chunk_content": "number samples used less 5 size training dataset average size training datasets study 60000 therefore 5 corresponds 3000 samples average info info reasoning default models type refer neurips ad benchmark paper linked make choices supervised learning use catboost often outperforms classic algorithms semisupervised xgbod good default pyod theres clear winner unsupervised methods depends use case ecod sensible default fast runtime concerned runtime use ensemble info prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use anomaly detection handler within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies setup create ai engine anomaly detection handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanomaly_detection_handler sql create ml_engine anomaly_detection_engine anomaly_detection create model using anomaly_detection_engine engine sql create anomaly detection model anomaly_detection_model datasource select data_table predict target_column using engine anomaly_detection_engine engine name created via create ml_engine parameters shown usage examples usage run example queries use data csv filehttpsrawgithubusercontentcommindsdbmindsdbmaintestsunitml_handlersdataanomaly_detectioncsv unsupervised detection sql create anomaly detection model mindsdbunsupervised_ad files select anomaly_detection using engine anomaly_detection_engine describe model mindsdbunsupervised_admodel select tclass moutlier anomaly filesanomaly_detection join mindsdbunsupervised_ad semisupervised detection sql create model mindsdbsemi_supervised_ad files select anomaly_detection predict class using engine anomaly_detection_engine describe model mindsdbsemi_supervised_admodel select tcarat tcategory tclass mclass anomaly filesanomaly_detection join mindsdbsemi_supervised_ad supervised detection sql create model mindsdbsupervised_ad files select anomaly_detection predict"
  },
  {
    "filename": "anomaly.mdx",
    "path": "docs/integrations/ai-engines/anomaly.mdx",
    "chunk_id": 2,
    "chunk_content": "class using engine anomaly_detection_engine type supervised describe model mindsdbsupervised_admodel select tcarat tcategory tclass mclass anomaly filesanomaly_detection join mindsdbsupervised_ad specific model sql create anomaly detection model mindsdbunsupervised_ad_knn files select anomaly_detection using engine anomaly_detection_engine model_name knn describe model mindsdbunsupervised_ad_knnmodel select tclass moutlier anomaly filesanomaly_detection join mindsdbunsupervised_ad_knn specific anomaly type sql create anomaly detection model mindsdbunsupervised_ad_local files select anomaly_detection using engine anomaly_detection_engine anomaly_type local describe model mindsdbunsupervised_ad_localmodel select tclass moutlier anomaly filesanomaly_detection join mindsdbunsupervised_ad_local ensemble sql create anomaly detection model mindsdbad_ensemble files select anomaly_detection using engine anomaly_detection_engine ensemble_models knnecodlof describe model mindsdbad_ensemblemodel select tclass moutlier anomaly filesanomaly_detection join mindsdbad_ensemble tip next steps watch demo 1httpswwwloomcomshare0996e5faa3f7415bacd51a6e8e161d5esid9bacd29a975b4a94b081de2255b93607 demo 2httpswwwloomcomsharec22335d83cb04ac281e2ef080792f2dd see usage examples go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "mlflow.mdx",
    "path": "docs/integrations/ai-engines/mlflow.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb mlflow sidebartitle mlflow mlflow allows create train serve machine learning models apart features organizing experiments tracking metrics use mlflow models mindsdb prerequisites using mlflowserved models mindsdb 1 train model via wrapper class inherits mlflowpyfuncpythonmodel class expose predict method returns predicted output input data called warning please ensure python version specified conda environment matches one used train model warning 2 start mlflow server bash mlflow server p 5001 backendstoreuri sqlitepathtomlflowdb defaultartifactroot artifacts host 0000 3 serve trained model bash mlflow models serve modeluri model_folder_name example lets create model registers mlflowserved model ai table sql create model mindsdbmlflow_model predict target using engine mlflow model_name model_folder_name replace model_folder_name variable real value mlflow_server_url http00005001 match port number mlflow server point 2 previous section mlflow_server_path sqlitepathtomlflowdb replace path real value use sqlite database predict_url httplocalhost5000invocations match port number serves trained model point 3 previous section check models status sql describe mlflow_model status complete query predictions one way query single prediction using synthetic data clause sql select target mindsdbmlflow_model text tsunami coming seek high ground another way query batch predictions joining model data table sql select ttext mpredict mindsdbmlflow_model join filessome_text data table comes files integration joined model predictions made records tip get insights"
  },
  {
    "filename": "mlflow.mdx",
    "path": "docs/integrations/ai-engines/mlflow.mdx",
    "chunk_id": 1,
    "chunk_content": "check article bring machine learning model databaseshttpsmediumcommindsdbhowtobringyourownmachinelearningmodeltodatabases47a188d6db00 patricio cerda mardinihttpsmediumcompaxcema learn tip"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 0,
    "chunk_content": "title langchain sidebartitle langchain documentation describes integration mindsdb langchainhttpswwwlangchaincom framework developing applications powered language models integration allows deployment langchain models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use langchain within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain api key selected model provider want use langchain info available models include following anthropic get api keyhttpsdocsanthropiccomclaudedocsgettingaccesstoclaude openai get api keyhttpshelpopenaicomenarticles4936850wheredoifindmyopenaiapikey anyscale get api keyhttpsdocsendpointsanyscalecomguidesauthenticate ollama download ollamahttpsollamacomdownload litellm model provider available minds cloudhttpsmdbai generate api key info setup create ai engine langchain handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslangchain_handler sql create ml_engine langchain_engine langchain using serper_api_key yourserperapikey optional parameter provided model use serperdev search enhance output provide one parameters anthropic_api_key apikeyvalue anyscale_api_key apikeyvalue litellm_api_key apikeyvalue openai_api_key apikeyvalue create model using langchain_engine engine one openaianthropicanyscalelitellm model provider sql create model langchain_model predict target_column using engine langchain_engine engine name created via create ml_engine provider_api_key apikeyvalue provided create ml_engine replace provider one available values model_name modelname optional model used example gpt4 openai_api_key provided prompt_template message model may include input columns variables info handler supports tracing features langchain via langfusehttpslangfusecomdocsintegrationslangchaintracing use provide following parameters using clause langfuse_host langfuse_public_key langfuse_secret_key info tip agents tools main"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 1,
    "chunk_content": "abstractions langchain offers read langchain documentationhttpspythonlangchaincomdocsmodulesagents tip info three different tools utilized agent mindsdb internal mindsdb executor metadata fetches metadata information available tables write able write agent responses mindsdb data source tool exposes internal mindsdb executor different way perform tasks effectively enabling agent model read potentially write data sources models available active mindsdb project info create conversational model using langchain_engine engine one openaianthropicanyscalelitellm model provider accordiongroup accordion titleopenai sql create ml_engine langchain_engine langchain using openai_api_key apikeyvalue create model langchain_openai_model predict answer using engine langchain_engine engine name created via create ml_engine provider openai one available providers openai_api_key apikeyvalue provided create ml_engine model_name gpt35turbo choose one available openai models mode conversational conversational mode user_column question column name stores input user assistant_column answer column name stores output model see predict column verbose true prompt_template answer users input helpful way question accordion accordion titleanthropic sql create ml_engine langchain_engine langchain using anthropic_api_key apikeyvalue create model langchain_openai_model predict answer using engine langchain_engine engine name created via create ml_engine provider anthropic one available providers anthropic_api_key apikeyvalue provided create ml_engine model_name claude21 choose one available openai models mode conversational conversational mode user_column question column name stores input user assistant_column answer column name stores output model see predict column"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 2,
    "chunk_content": "verbose true prompt_template answer users input helpful way question accordion accordion titleanyscale sql create ml_engine langchain_engine langchain using anyscale_api_key apikeyvalue create model langchain_anyscale_model predict answer using engine langchain_engine engine name created via create ml_engine provider anyscale one available providers anyscale_api_key apikeyvalue provided create ml_engine model_name mistralaimistral7binstructv01 choose one models available anyscale mode conversational conversational mode user_column question column name stores input user assistant_column answer column name stores output model see predict column base_url httpsapiendpointsanyscalecomv1 verbose true prompt_template answer users input helpful way question accordion accordion titleollama sql create ml_engine langchain_engine langchain create model langchain_ollama_model predict answer using engine langchain_engine engine name created via create ml_engine provider ollama one available providers model_name llama2 choose one models available ollama mode conversational conversational mode user_column question column name stores input user assistant_column answer column name stores output model see predict column verbose true prompt_template answer users input helpful way question ensure ollama set locally following guide download ollamahttpsollamacomdownload accordion accordion titlelitellm sql create ml_engine langchain_engine langchain using litellm_api_key apikeyvalue create model langchain_litellm_model predict answer using engine langchain_engine engine name created via create ml_engine provider litellm one available providers litellm_api_key apikeyvalue provided create ml_engine model_name assistant model created mindsdb mode conversational conversational mode user_column"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 3,
    "chunk_content": "question column name stores input user assistant_column answer column name stores output model see predict column base_url httpsaidevmindsdbcom verbose true prompt_template answer users input helpful way question accordion accordion titlemindsdb sql create ml_engine langchain_engine langchain create model langchain_mindsdb_model predict answer using engine langchain_engine engine name created via create ml_engine provider mindsdb one available providers model_name gpt_model model created within mindsdb mode conversational conversational mode user_column question column name stores input user assistant_column answer column name stores output model see predict column verbose true prompt_template answer users input helpful way question accordion accordiongroup usage following usage examples utilize langchain_engine create model create model statement create model used describe analyze retrieve sql create model tool_based_agent predict completion using engine langchain_engine prompt_template answer users input helpful way question create tool_based_agent model using langchain engine defined engine parameter model answers users questions helpful way defined prompt_template parameter specifies input input column calling model describe data query model describe data sql select question completion tool_based_agent question could describe mysql_demo_dbhouse_sales table please using verbose true tools max_iterations 10 output sql mysql_demo_dbhouse_sales table base table contains information related house sales following columns saledate type text likely contains date sale made house_price_moving_average type int might represent moving average"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 4,
    "chunk_content": "house prices possibly track price trends time type type text could describe type house sold bedrooms type int indicating number bedrooms sold house get information mysql_demo_dbhouse_sales table agent uses metadata tool agent prepares response analyze data query model analyze data sql select question completion tool_based_agent question want know average number rooms downtown neighborhood per mysql_demo_dbhome_rentals table using verbose true tools max_iterations 10 output sql average number rooms downtown neighborhood per mysql_demo_dbhome_rentals table 16 rooms model uses metadata tool fetch column information beds column mysql_demo_dbhome_rentals table uses number_of_rooms column writes following query sql select avgnumber_of_rooms mysql_demo_dbhome_rentals neighborhood downtown query returns value 16 used write answer retrieve data query model retrieve data sql select question completion tool_based_agent question property south_side neighborhood initial price 2543 mysql_demo_dbhome_rentals table details listing using verbose true tools max_iterations 10 output sql property south_side neighborhood initial price 2543 following details number rooms 1 number bathrooms 1 square footage sqft 630 location great days market 11 initial price 2543 neighborhood south_side rental price 25430 model uses metadata tool fetch information table creates executes following query sql select mysql_demo_dbhome_rentals neighborhood south_side initial_price 2543 execution model gets output sql number_of_roomsnumber_of_bathroomssqftlocationdays_on_marketinitial_priceneighborhoodrental_price 1 1 630 great 11 2543 south_side 2543 consequently takes query"
  },
  {
    "filename": "langchain.mdx",
    "path": "docs/integrations/ai-engines/langchain.mdx",
    "chunk_id": 5,
    "chunk_content": "output writes answer tip next steps go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "lightfm.mdx",
    "path": "docs/integrations/ai-engines/lightfm.mdx",
    "chunk_id": 0,
    "chunk_content": "title lightfm sidebartitle lightfm lightfmhttpsgithubcomlystlightfm handler functions interface lightfm python recommendation library current implementation supports collaborative filtering useritem itemitem recommendations allows users make use powerful lightfm recommendation framework library performing recommendation interaction data sets prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 use lightfm within mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies tip please note using docker run mindsdb installing dependencies integration per instructions given currently necessary install couple linux development packages container run following commands start interactive shell container bash docker exec mindsdb_container sh havent specified name spinning mindsdb container docker run find running docker ps install required linux development packages bash aptget update aptget install buildessential python3dev libopenblasdev tip current implementation stands input data table containing useritem interaction data user_id item_id rating 1 2 4 1 3 7 note please note moment integration support finetune feature note example creating lightfm model need create ml engine sql create ml_engine lightfm lightfm verify running show ml_engines lets create lightfm model specifying necessary input parameters sql create model lightfm_demo mysql_demo_db select movie_lens_ratings predict movieid using engine lightfm item_id movieid user_id userid threshold 4 n_recommendations 10 evaluation true required parameters include following item_id parameter stores items"
  },
  {
    "filename": "lightfm.mdx",
    "path": "docs/integrations/ai-engines/lightfm.mdx",
    "chunk_id": 1,
    "chunk_content": "recommended movies user_id parameter stores users items recommended threshold parameter used score interaction provided input data defines threshold recommendation n_recommendations parameter stores number recommendations returned optionally provide evaluation parameter want store evaluation metrics set false default tip connect mysql_demo_db used training model sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public tip lets query following recommendations get recommendations item_item pairs sql select b lightfm_demo b recommender_type item_item get itemitem recommendations specific item_id sql select b lightfm_demo b movieid 100 using recommender_type item_item get recommendations useritem pairs sql select b lightfm_demo b recommender_type user_item get useritem recommendations specific user_id sql select b lightfm_demo b userid 100 using recommender_type user_item get useritem recommendations multiple user_ids sql select b mysql_demo_dbmovie_lens_ratings join lightfm_demo b auserid 215216"
  },
  {
    "filename": "llamaindex.mdx",
    "path": "docs/integrations/ai-engines/llamaindex.mdx",
    "chunk_id": 0,
    "chunk_content": "title llamaindex sidebartitle llamaindex llamaindex handler documentation describes integration mindsdb llamaindexhttpsdocsllamaindexaienstable framework building contextaugmented generative ai applications llms prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use llamaindex within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain openai api key required openai llms follow instructions obtaining api keyhttpshelpopenaicomenarticles4936850wheredoifindmysecretapikey setup create ai engine llamaindex handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersllama_index_handler sql create ml_engine llama_index llama_index using openai_api_key apikeyvalue create model using llama_index engine openai model provider sql create model chatbot_model predict answer using engine llama_index engine name created via create ml_engine input_column question mode conversational optional user_column question optional used conversational mode assistant_column answer optional used conversational mode usage create model answers questions reading page web sql create model qa_model predict answer using engine llama_index reader simplewebpagereader source_url_link httpsmindsdbcomabout input_column question query model get answer sql select question answer mindsdbqa_model question mindsdbs story output sql question answer mindsdbs story mindsdb fastgrowing opensource configuring simplewebpagereader specific domains simplewebpagereader used configured interact specific domains using web_crawling_allowed_sites setting configjson file feature allows restrict handler read process content domains specify enhancing security control web interactions configure simply list allowed domains web_crawling_allowed_sites key configjson example json web_crawling_allowed_sites httpsdocsmindsdbcom httpsanotherallowedsitecom tip next steps go use"
  },
  {
    "filename": "llamaindex.mdx",
    "path": "docs/integrations/ai-engines/llamaindex.mdx",
    "chunk_id": 1,
    "chunk_content": "caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "replicate-llm.mdx",
    "path": "docs/integrations/ai-engines/replicate-llm.mdx",
    "chunk_id": 0,
    "chunk_content": "title replicate llm sidebartitle replicate llm handler implemented using replicate library provided replicate required arguments establish connection model_name model name want access mindsdb eg airforeverkandinsky2 version version hashid want use mindsdb api_key api key replicate platform found herehttpsreplicatecomaccountapitokens model_type set llm using large language model else optional note use replicate essential authenticate setting api token environment variable named replicate_api_token token acts key enable access replicates features 1 using pip youre working standard python environment using pip package management set token environment variable running following command terminal linux mac export replicate_api_tokenyour_token windows set replicate_api_tokenyour_token 2 using docker docker users process slightly differs need pass environment variable directly docker container running use command docker run e replicate_api_tokenyour_token p 4733447334 p 4733547335 mindsdbmindsdb replace your_token actual replicate api token note usage use handler connect replicate cluster mindsdb need account replicate make sure create account following linkhttpsreplicatecomsigninnextaccountapitokens establish connection create model mindsdb use following syntax sql create model vicuna_13b predict output using engine replicate model_name replicatevicuna_13b model_typellm version 6282abe6a492de4145d7bb601023762212f9ddbbe78278bd6771c8b3b2f2a13b api_key r8_heh use describe predictor query see available parameters specify customize predictions sql describe predictor mindsdbvicuna_13bfeatures output sql inputs type default description seed integer 1 seed random number generator reproducibility debug boolean false provide debugging"
  },
  {
    "filename": "replicate-llm.mdx",
    "path": "docs/integrations/ai-engines/replicate-llm.mdx",
    "chunk_id": 1,
    "chunk_content": "output logs top_p number 1 decoding text samples top p percentage likely tokens lower ignore less likely tokens prompt string prompt send llama max_length integer 500 maximum number tokens generate word generally 23 tokens temperature number 075 adjusts randomness outputs greater 1 random 0 deterministic 075 good starting value repetition_penalty number 1 penalty repeated words generated text 1 penalty values greater 1 discourage repetition less 1 encourage use established connection query ml model follows sql select vicuna_13b promptwrite humourous poem open source using max_length200 temperature075 output sql output prompt opensource software oh love thee write humourous poem open source bugs glitches oh free bring us laughter joy day well never pay license open code see share cheer bring us together far wide work projects side side open source theres end code bend change mold make create something truly great really strange heres open source future bright code thats free might well make future shine technology fine open source always shining line note replicate provides free predictions choose predictions wisely dont let machines fun save"
  },
  {
    "filename": "ollama.mdx",
    "path": "docs/integrations/ai-engines/ollama.mdx",
    "chunk_id": 0,
    "chunk_content": "title ollama sidebartitle ollama documentation describes integration mindsdb ollamahttpsollamacom tool enables local deployment large language models integration allows deployment ollama models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use ollama within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 follow instructionhttpsgithubcomollamaollamatabreadmeovfileollama download ollama run models locally info recommended system specifications working ollama installation point 3 7b models least 8gb ram recommended 13b models least 16gb ram recommended 70b models least 64gb ram recommended info setup create ai engine ollama handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersollama_handler sql create ml_engine ollama_engine ollama create model using ollama_engine engine sql create model ollama_model predict completion using engine ollama_engine engine name created via create ml_engine model_name modelname model run ollama run modelname ollama_serve_url httplocalhost11434 tip run ollama mindsdb separate docker containers use localhost value container example ollama_serve_url httphostdockerinternal11434 tip find available models herehttpsgithubcomollamaollamatabreadmeovfilemodellibrary usage following usage examples utilize ollama_engine create model create model statement deploy use llama3 model first download ollamahttpsgithubcomollamaollamatabreadmeovfileollama run model locally executing ollama pull llama3 deploy model within mindsdb sql create model llama3_model predict completion using engine ollama_engine model_name llama3 tip models run either generate embedding modes generate mode used text"
  },
  {
    "filename": "ollama.mdx",
    "path": "docs/integrations/ai-engines/ollama.mdx",
    "chunk_id": 1,
    "chunk_content": "generation embedding mode used generate embeddings text however modes used models support example moondream model supports modes default mode specified model run generate mode multiple modes supported one mode supported model run mode specify mode use mode parameter create model statement example mode embedding tip query model get predictions sql select text completion llama3_model text hello output sql text completion hello hello back something help would like chat override prompt message sql select text completion llama3_model text hello using prompt_template answer using exactly five words text output sql text completion hello warmly welcome space tip next steps go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "monkeylearn.mdx",
    "path": "docs/integrations/ai-engines/monkeylearn.mdx",
    "chunk_id": 0,
    "chunk_content": "title monkeylearn sidebartitle monkeylearn monkeylearn nocode text analysis tool mindsdb allows use prebuilt custom monkeylearn models use features like classifying text according user needs fields interest like business reviews comments customer feedback bring monkeylearn models mindsdb creating model need create ml_engine monkeylearn using create ml_engine syntax sql create ml_engine monkeylearn_engine monkeylearn using monkeylearn_api_key monkeylearn_api_key p aligncenter img srcassetstutorialsmonkeylearn1create_mlpng p ml_engine created use create model statement bring monkeylearn models mindsdb example make use monkeylearns premade model ecommerce support ticket classifier sql create model mindsdbecommerce_ticket_classifier predict tag using engine monkeylearn_engine monkeylearn_api_key api_key model_id model_id input_column text execution get p aligncenter img srcassetstutorialsmonkeylearncreatemodel1png p expression description ecommerce_ticket_classifier model name provided model created mindsdb tag column provide predicted result engine ml framework engine used monkeylearn monkeylearn_api_key api key model provided monkeylearn model_id respective models id want make use input_column specifies input column fed model use describe syntax verify models status sql describe ecommerce_ticket_classifier execution get p aligncenter img srcassetstutorialsmonkeylearn4describepng p use select statement make prediction model sql select ecommerce_ticket_classifier text order delivery status shows shipped call delivery driver response execution get p aligncenter img srcassetstutorialsmonkeylearn5select_predictionpng p create train model also create model dataset example using dataset consisting messages ecommerce support tickets dataset uploaded filehttpsdocsmindsdbcomsqlcreatefile"
  },
  {
    "filename": "monkeylearn.mdx",
    "path": "docs/integrations/ai-engines/monkeylearn.mdx",
    "chunk_id": 1,
    "chunk_content": "onto gui use create model syntax sql create model mindsdbecommerce_ticket_classifier2 files select queries2 predict tag using engine monkeylearn_engine monkeylearn_api_key api_key model_id model_id input_column text use select statement make prediction sql select ecommerce_ticket_classifier2 text ordered 4 units received 3 execution get p aligncenter img srcassetstutorialsmonkeylearn10select_predictionpng p mindsdb model created monkeylearn model successfully predicted tag ecommerce support ticket according text input"
  },
  {
    "filename": "cohere.mdx",
    "path": "docs/integrations/ai-engines/cohere.mdx",
    "chunk_id": 0,
    "chunk_content": "title cohere sidebartitle cohere documentation describes integration mindsdb coherehttpscoherecom technology company focused artificial intelligence enterprise integration allows deployment cohere models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use cohere within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain cohere api key required deploy use cohere models within mindsdb sign cohere account request api key cohere dashboard learn herehttpscoherecompricing setup create ai engine cohere handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerscohere_handler sql create ml_engine cohere_engine cohere using cohere_api_key yourcohereapikey create model using cohere_engine engine sql create model cohere_model predict target_column using engine cohere_engine engine name created via create ml_engine task task_name choose one textsummarization textgeneration column column_name column stores inputquestion model usage following usage examples utilize cohere_engine create model create model statement create model predict answer question using textgeneration task sql create model cohere_model predict answer using engine cohere_engine task textgeneration column question name description task defines task accomplished column defines column text acted upon engine defines cohere engine query model get predictions sql select answer cohere_model question capital france output answer capital france paris paris frances largest city major global center art culture fashion cuisine renowned iconic landmarks eiffel"
  },
  {
    "filename": "cohere.mdx",
    "path": "docs/integrations/ai-engines/cohere.mdx",
    "chunk_id": 1,
    "chunk_content": "tower notredame cathedral louvre museum tip next steps go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "timegpt.mdx",
    "path": "docs/integrations/ai-engines/timegpt.mdx",
    "chunk_id": 0,
    "chunk_content": "title nixtlas timegpt integration mindsdb sidebartitle timegpt timegpt nixtla generative pretrained model specifically designed predicting time series data timegpt takes time series data input produces forecasted outputs timegpt effectively employed various applications including demand forecasting anomaly detection financial prediction learn features herehttpsnixtlagithubionixtla bring timegpt models mindsdb creating model need create ml engine timegpt using create ml_engine statement providing timegpt api key sql create ml_engine timegpt timegpt using timegpt_api_key timegpt_api_key ml engine created use create model statement create timegpt model mindsdb sql create model model_name data_source select table_name predict column_to_be_predicted group column_name column_name order date_column horizon 3 model forecasts next 3 rows using engine timegpt ensure model created based timegpt engine include using clause end defines engine lists parameters used timeseries models including group order horizon whats different timegpt engine expose window parameter api user need send payload least n rows n depends model frequency series automatically handled mindsdb timegpt handler codehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerstimegpt_handler example nixtlas timegpt model used obtain realtime forecasts trading data binance tip follow linkhttpswwwyoutubecomwatchv8lfpfocdyeolistplq3sjiv6w5bohj9gfsedwtb_pqk4k89index3 watch video integrating timegpt model binance data tip first connect binance mindsdb executing command sql create database my_binance engine binance please note using timegpt engine create mindsdb editor clients interact mindsdb command sql create ml_engine"
  },
  {
    "filename": "timegpt.mdx",
    "path": "docs/integrations/ai-engines/timegpt.mdx",
    "chunk_id": 1,
    "chunk_content": "timegpt timegpt using timegpt_api_key timegpt_api_key check available engines command sql show ml_engines see timegpt engine list ready follow tutorials lets create timegpt model train data binance sql create model cryptocurrency_forecast_model my_binance select aggregated_trade_data symbol btcusdt predict open_price order open_time horizon 10 using engine timegpt use create model statement create train deploy model clause defines training data used train model latest binance data used predict clause specifies column predicted open price btcusdt trading pair forecasted timeseries model order data date column open time open price takes effect finally horizon clause defines many rows future model forecast forecasts next 10 rows next 10 minutes interval binance data rows one minute warning please note timegpt engine sensitive inconsistent intervals data rows please check data missing duplicated irregular timestamps mitigate errors may arise intervals data rows inconsistent example intervals binance data rows consistently equal one minute warning proceeding make sure model status reads complete sql describe cryptocurrency_forecast_model make forecasts must save binance data view sql create view btcusdt_recent select my_binanceaggregated_trade_data symbol btcusdt view going joined model get forecasts sql select mopen_time mopen_price btcusdt_recent join cryptocurrency_forecast_model dopen_time latest"
  },
  {
    "filename": "google_gemini.mdx",
    "path": "docs/integrations/ai-engines/google_gemini.mdx",
    "chunk_id": 0,
    "chunk_content": "title google gemini sidebartitle google gemini documentation describes integration mindsdb google geminilink generative artificial intelligence model developed google integration allows deployment google gemini models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use google gemini within mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 obtain google gemini api key required deploy use google gemini models within mindsdb follow instructions obtaining api keyhttpsaigoogledevgeminiapidocsapikey setup create ai engine google gemini handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersgoogle_gemini_handler sql create ml_engine google_gemini_engine google_gemini using api_key apikeyvalue create model using google_gemini_engine engine sql create model google_gemini_model predict target_column using engine google_gemini_engine engine name created via create ml_engine column input_column column name stores user input model geminipro model name used usage following usage examples utilize google_gemini_engine create model create model statement create model generate text completions gemini pro model existing text data sql create model google_gemini_model predict answer using engine google_gemini_engine column question model geminipro query model get predictions sql select question answer google_gemini_model question alternatively query batch predictions sql select tquestion manswer google_gemini_model join data_table tip next steps go use casesusecasesoverview section see examples tip"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/integrations/ai-engines/byom.mdx",
    "chunk_id": 0,
    "chunk_content": "title bring model sidebartitle byom bring model byom feature lets upload models form python code use within mindsdb works upload custom model via mindsdb editor clicking add upload custom model like p aligncenter img srcassetsbyom_upload_custom_modelpng p form needs filled order bring model mindsdb p aligncenter img srcassetsbyom_empty_formpng p lets briefly go files need uploaded python file stores implementation model contain class implementation train predict methods sample format py class custompredictor def trainself df target_col argsnone implementation goes return def predictself df implementation goes return df accordion titleexample py import os import pandas pd sklearncross_decomposition import plsregression sklearn import preprocessing class custompredictor def trainself df target_col argsnone printargs 1111 selftarget_col target_col dfselftarget_col x dfdropcolumnsselftarget_col x_cols listxcolumns x_scaler preprocessingstandardscalerfitx y_scaler preprocessingstandardscalerfityvaluesreshape1 1 xs x_scalertransformx ys y_scalertransformyvaluesreshape1 1 pls plsregressionn_components1 plsfitxs ys plsx_scores_ w plsx_weights_ p plsx_loadings_ r plsx_rotations_ selfx_cols x_cols selfx_scaler x_scaler selfp p def calc_limitdf res none column dfcolumns column selftarget_col continue tbl dfgroupbyselftarget_colaggcolumn mean min max std tblcolumns tblcolumnsget_level_values1 tblname column tblstd tblstdfillna0 tbllower tblmean 3 tblstd tblupper tblmean 3 tblstd tbllower tbllower minmaxaxis1 lower min tblupper tblupper maxminaxis1 upper max tbl tblname lower mean upper try res pdconcatres tbl except res tbl return res trdf pddataframe trdfselftarget_col yvalues trdft1"
  },
  {
    "filename": "byom.mdx",
    "path": "docs/integrations/ai-engines/byom.mdx",
    "chunk_id": 1,
    "chunk_content": "tsqueeze limit calc_limittrdfreset_index selflimit limit return trained predictor ready stored def predictself df yt dfselftarget_colvalues xt dfselfx_cols xt selfx_scalertransformxt excess_cols listsetdfcolumns setselfx_cols pred_df dfexcess_colscopy pred_dfselftarget_col yt pred_dft1 xt selfpsqueeze pred_df pdmergepred_df selflimitselftarget_col lower upper howleft onselftarget_col return pred_df accordion optional requirements file requirementstxt stores dependencies along versions sample format sql dependency_package_1 version dependency_package_2 version dependency_package_3 version version accordion titleexample sql pandas scikitlearn accordion upload files please provide engine name please note custom model uploaded mindsdb engine use engine create model p aligncenter img srcassetsbyom_diagrampng p lets look example example upload custom model p aligncenter img srcassetsbyom_formpng p upload modelpy file stores implementation model requirementstxt file stores dependencies model uploaded becomes ml engine within mindsdb use custom_model_engine create model follows sql create model custom_model my_integration select my_table predict target using engine custom_model_engine lets query predictions joining custom model data table sql select inputfeature_column model_target_column my_integrationmy_table input join custom_model model info check byom handler folderhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersbyom_handler see implementation details info"
  },
  {
    "filename": "huggingface.mdx",
    "path": "docs/integrations/ai-engines/huggingface.mdx",
    "chunk_id": 0,
    "chunk_content": "title hugging face sidebartitle hugging face documentation describes integration mindsdb hugging facehttpshuggingfaceco company develops computer tools building applications using machine learning integration allows deployment hugging face models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use hugging face within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies setup create ai engine hugging face handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershuggingface_handler sql create ml_engine huggingface_engine huggingface using huggingface_api_api_key hf_xxx create model using huggingface_engine engine sql create model huggingface_model predict target_column using engine huggingface_engine engine name created via create ml_engine model_name hf_hub_model_name choose one pytorch models hugging face hub task task_name choose one textclassification textgeneration zeroshotclassification translation summarization text2textgeneration fillmask input_column column_name column stores inputquestion model labels label 1 label 2 labels used classify data used classification tasks usage following usage examples utilize huggingface_engine create model create model statement create model classify input text spam ham sql create model spam_classifier predict spam_or_ham using engine huggingface_engine model_name mrm8488berttinyfinetunedsmsspamdetection task textclassification input_column text labels ham spam query model get predictions sql select text spam_or_ham spam_classifier text subscribe channel asap output sql text spam_or_ham subscribe channel asap spam tip next steps follow linkhttpsdocsmindsdbcomsqltutorialshuggingfaceexamples see use case examples"
  },
  {
    "filename": "huggingface.mdx",
    "path": "docs/integrations/ai-engines/huggingface.mdx",
    "chunk_id": 1,
    "chunk_content": "tip"
  },
  {
    "filename": "lightwood.mdx",
    "path": "docs/integrations/ai-engines/lightwood.mdx",
    "chunk_id": 0,
    "chunk_content": "title lightwood sidebartitle lightwood lightwood default ai engine used mindsdb deals mainly classificationsqltutorialscustomerchurn regressionsqltutorialshomerentals timeseriessqltutorialshousesalesforecasting problems machine learning providing input data problem definition lightwood generates predictions following three core steps include data preprocessing cleaning feature engineering model building training input data ranges numbers dates categories text quantities arrays matrices images audios videos passed urls tip recommend using mindsdbmindsdblightwood docker image comes lightwood dependencies preinstalled learn heresetupselfhosteddocker tip works algorithm followed lightwood starting input data setup model building training getting predictions p aligncenter img srcassetslightwoodpng p input data preprocessed column assigned data type next data converted features via encoders transform data numerical representation used model finally predictive model takes encoded feature data outputs prediction target hood model splits data training validation testing sets ratios dynamic usually 801010 ratio split done default using random sampling without replacement stratified target column determines accuracy model evaluating held test set users either use lightwoods default mixersmodelsintegrationsaiengineslightwoodmodelkey create approaches inherited basemixer class learn lightwood philosophy follow linkhttpsmindsdbgithubiolightwoodlightwood_philosophyhtml accuracy metrics lightwood provides ways score accuracy model using one accuracy functions accuracy functions include mean_absolute_error mean_squared_error precision_score recall_score f1_score sql create model model_name data_source select table_name predict target_column using accuracy_functionsaccuracy_function define accuracy function choice using clause create model"
  },
  {
    "filename": "lightwood.mdx",
    "path": "docs/integrations/ai-engines/lightwood.mdx",
    "chunk_id": 1,
    "chunk_content": "statement note accuracy functions used default r2_score value regression predictions balanced_accuracy_score value classification predictions complementary_smape_array_accuracy value time series predictions values vary 0 1 1 indicates perfect predictor based results obtained heldout portion data ie testing set check accuracy values models using describesqlapidescribe statement note tuning lightwood ml engine description mindsdb underlying automl models based lightwoodhttpsmindsdbgithubiolightwoodindexhtml engine default library generates models automatically based data declarative problem definition default configuration overridden using using statement provides option configure specific parameters training process upcoming version mindsdb possible choose ml frameworks please note lightwood engine used default syntax syntax sql create model project_namemodel_name data_source select column_name table_name predict target_column using parameter_key parameter_value encoders key grants access configure column encoded default automl engine tries get best match data sql using encoderscolumn_namemodule value learn encoders options visit lightwood documentation page encodershttpsmindsdbgithubiolightwoodencoderhtml modelargs key allows specify type machine learning algorithm learn encoder data sql using modelargs key value model options model description basemixerhttpsmindsdbgithubiolightwoodmixerhtmlmixerbasemixer base class mixers lightgbmhttpsmindsdbgithubiolightwoodmixerhtmlmixerlightgbm mixer configures uses lightgbm regression classification tasks depending problem definition lightgbmarrayhttpsmindsdbgithubiolightwoodmixerhtmlmixerlightgbmarray mixer consists several lightgbm mixers regression mode aimed time series forecasting tasks nhitsmixerhttpsmindsdbgithubiolightwoodmixerhtmlmixernhitsmixer mixer wrapper around mqnhits deep learning model neuralhttpsmindsdbgithubiolightwoodmixerhtmlmixerneural mixer trains fully connected dense network concatenated encoded outputs"
  },
  {
    "filename": "lightwood.mdx",
    "path": "docs/integrations/ai-engines/lightwood.mdx",
    "chunk_id": 2,
    "chunk_content": "feature dataset predict encoded output neuraltshttpsmindsdbgithubiolightwoodmixerhtmlmixerneuralts mixer inherits neural mixer used time series forecasts prophetmixerhttpsmindsdbgithubiolightwoodmixerhtmlmixerprophetmixer mixer wrapper around popular time series library prophethttpsfacebookgithubioprophet randomforesthttpsmindsdbgithubiolightwoodmixerhtmlmixerrandomforest mixer supports regression classification tasks inherits sklearnensemblerandomforestregressor sklearnensemblerandomforestclassifier regressionhttpsmindsdbgithubiolightwoodmixerhtmlmixerregression mixer inherits scikitlearns ridge classhttpsscikitlearnorgstablemodulesgeneratedsklearnlinear_modelridgehtml sktimehttpsmindsdbgithubiolightwoodmixerhtmlmixersktime mixer wrapper around popular time series library sktime unithttpsmindsdbgithubiolightwoodmixerhtmlmixerunit special mixer passes along whatever prediction made target encoder without modifications used singlecolumn predictive scenarios may involve complex andor expensive encoders eg freeform text classification transformers xgboostmixerhttpsmindsdbgithubiolightwoodmixerhtmlmixerxgboostmixer mixer good allrounder due generally great performance treebased ml algorithms supervised learning tasks tabular data note please note mixers available cloud environment particular lightgbm lightgbmarray nhits prophet note learn model options visit lightwood documentation page mixershttpsmindsdbgithubiolightwoodmixerhtml problem_definitionembedding_only key train embeddingonly model use parameter creating model sql create model embedding_only_model using problem_definitionembedding_only true predictions made embedding model come form embeddings default alternatively get predictions form embeddings normal model trained without specifying problem_definitionembedding_only parameter use parameter querying model predictions sql select predictions using return_embedding true keys supported lightwood jsonai common use cases configuring predictors use encoders model keys explained see available keys check lightwood documentation page jsonaihttpsmindsdbgithubiolightwoodapitypeshtmlapitypesjsonai example use home_rentals dataset specify particular encoders columns lightgbm model sql create model mindsdbhome_rentals_model example_db select demo_datahome_rentals predict rental_price using"
  },
  {
    "filename": "lightwood.mdx",
    "path": "docs/integrations/ai-engines/lightwood.mdx",
    "chunk_id": 3,
    "chunk_content": "encoderslocationmodule categoricalautoencoder encodersrental_pricemodule numericencoder encodersrental_priceargspositive_domain true modelargs submodels module lightgbm args stop_after 12 fit_on_dev true explainability lightwood deploy following types models regressions models classification models timeseries models embedding models predictions made type model come explanation column accordiongroup accordion titleregression case regression models target_explain column contains following information predicted_value 2951 confidence 099 anomaly null truth null confidence_lower_bound 2795 confidence_upper_bound 3107 note upper lower bounds determined via conformal prediction correspond reported confidence score modified user note try following tutorialusecasesindatabase_mlhomerentals accordion accordion titleclassification case classification models target_explain column contains following information predicted_value confidence 06629213483146067 anomaly null truth null probability_class_no 08561 probability_class_yes 01439 note confidence score produced conformal prediction module wellcalibrated hand probability_class comes directly model logits may uncalibrated therefore probability_class score may optimistic pessimistic ie coverage guaranteed empirically match reported score note try following tutorialusecasesindatabase_mlcustomerchurn accordion accordion titletimeseries case timeseries models target_explain column contains following information predicted_value 5016183125 confidence 09991 anomaly false truth null confidence_lower_bound 500926109375 confidence_upper_bound 502310515625 try following tutorialusecasespredictive_analyticshousesalesforecasting accordion accordion titleembeddings case embeddings models target_explain column contains following information predicted_value 10 6712956428527832 1247057318687439 00 00 10 00 00 00 00 00 10 00 00 00 10 23025851249694824 05629426836967468 00 10 00 00 00 00 00 00 07540000081062317 03333333432674408 035483869910240173 09583333134651184"
  },
  {
    "filename": "lightwood.mdx",
    "path": "docs/integrations/ai-engines/lightwood.mdx",
    "chunk_id": 4,
    "chunk_content": "07833333611488342 025 confidence null anomaly null truth null try following tutorialusecasesaipowered_data_retrievalembeddingmodel accordion accordiongroup note visit comprehensive lightwood docs herehttpsmindsdbgithubiolightwood note tip check lightwood tutorials herehttpsmindsdbgithubiolightwoodtutorialshtml tip"
  },
  {
    "filename": "ray-serve.mdx",
    "path": "docs/integrations/ai-engines/ray-serve.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb ray serve sidebartitle ray serve ray serve simple highthroughput model serving library wrap around ml model simple example logistic regression example train external scikitlearn model use making predictions creating ray serve model lets look actual model wrapped class complies requirements python import ray fastapi import request fastapi ray import serve import time import pandas pd import json app fastapi async def parse_reqrequest request data await requestjson target datagettarget none di jsonloadsdatadf df pddataframedi return df target servedeploymentroute_prefixmy_model serveingressapp class mymodel appposttrain async def trainself request request df target await parse_reqrequest feature_cols listsetlistdfcolumns settarget selffeature_cols feature_cols x dfloc selffeature_cols listdftarget selfmodel logisticregression selfmodelfitx return status ok apppostpredict async def predictself request request df _ await parse_reqrequest x dfloc selffeature_cols predictions selfmodelpredictx index listrangelenpredictions pred_dict prediction floatx x predictions index index return pred_dict my_app mymodelbind saving code rayservepy run using serve run rayservemy_app important train predict endpoints train endpoint accepts two parameters sent via post df serialized dictionary converted pandas dataframe target name target column predicted returns json object containing status key ok value predict endpoint requires one parameter sent via post df serialized dictionary converted pandas dataframe returns dictionary containing prediction index keys stores predictions additional keys returned confidence"
  },
  {
    "filename": "ray-serve.mdx",
    "path": "docs/integrations/ai-engines/ray-serve.mdx",
    "chunk_id": 1,
    "chunk_content": "confidence intervals bringing ray serve model mindsdb start rayservewrapped model create train mindsdb sql create model mindsdbbyom_ray_serve mydb select number_of_rooms initial_price rental_price test_datahome_rentals predict number_of_rooms using urltrain http1270018000my_modeltrain urlpredict http1270018000my_modelpredict dtype_dictnumber_of_rooms categorical initial_price integer rental_price integer formatray_server fetch predictions using standard mindsdb syntax follow guide selectsqlapiselect statement learn directly pass input data clause get single prediction sql select byom_ray_serve initial_price3000 rental_price3000 join model wth data table get bulk predictions sql select tbnumber_of_rooms trental_price tbindex mydbtest_datahome_rentals join mindsdbbyom_ray_serve tb trental_price 5300 tip limit post requests please note model behind reverse proxy like nginx might increase maximum limit post requests order receive training data mindsdb send much youd like stresstested billion rows tip example keras nlp model consider natural language processing nlp taskhttpswwwkagglecomcnlpgettingstarted want train neural network using kerashttpskerasio detect tweet related natural disaster fires earthquakes etc please download datasethttpswwwkagglecomcnlpgettingstarted follow example creating ray serve model create ray serve service wraps around kaggle nlp modelhttpswwwkagglecomshahulesbasicedacleaningandglove trained used making predictions python import import time import json import string import requests collections import counter defaultdict import ray ray import serve import gensim import numpy np import pandas pd tqdm import tqdm nltkutil import ngrams nltkcorpus import stopwords nltktokenize import word_tokenize fastapi import request fastapi"
  },
  {
    "filename": "ray-serve.mdx",
    "path": "docs/integrations/ai-engines/ray-serve.mdx",
    "chunk_id": 2,
    "chunk_content": "sklearnmodel_selection import train_test_split sklearnfeature_extractiontext import countvectorizer tensorflowkeraspreprocessingtext import tokenizer tensorflowkeraspreprocessingsequence import pad_sequences tensorflowkerasmodels import sequential tensorflowkeraslayers import embedding lstm dense spatialdropout1d tensorflowkerasinitializers import constant tensorflowkerasoptimizers import adam app fastapi stop setstopwordswordsenglish async def parse_reqrequest request data await requestjson target datagettarget none di jsonloadsdatadf df pddataframedi return df target servedeploymentroute_prefixnlp_kaggle_model serveingressapp class model max_len 100 glove_dim 50 epochs 10 def __init__self selfmodel none appposttrain async def trainself request request df target await parse_reqrequest target_arr dfpoptargetvalues df selfpreprocess_dfdf train_corpus selfcreate_corpusdf selfembedding_dict openglove6b50dtxt r f line f values linesplit word values0 vectors npasarrayvalues1 float32 selfembedding_dictword vectors fclose selftokenizer_obj tokenizer selftokenizer_objfit_on_textstrain_corpus sequences selftokenizer_objtexts_to_sequencestrain_corpus tweet_pad pad_sequencessequences maxlenself__class__max_len truncatingpost paddingpost df tweet_paddfshape0 word_index selftokenizer_objword_index num_words lenword_index 1 embedding_matrix npzerosnum_words self__class__glove_dim word tqdmword_indexitems num_words continue emb_vec selfembedding_dictgetword emb_vec none embedding_matrixi emb_vec selfmodel sequential embedding embeddingnum_words self__class__glove_dim embeddings_initializerconstantembedding_matrix input_lengthself__class__max_len trainablefalse selfmodeladdembedding selfmodeladdspatialdropout1d02 selfmodeladdlstm64 dropout02 recurrent_dropout02 selfmodeladddense1 activationsigmoid optimizer adamlearning_rate1e5 selfmodelcompilelossbinary_crossentropy optimizeroptimizer metricsaccuracy x_train x_test y_train y_test train_test_splitdf target_arr test_size015 selfmodelfitx_train y_train batch_size4 epochsself__class__epochs validation_datax_test y_test verbose2 return status ok apppostpredict async def predictself request request df _ await parse_reqrequest df selfpreprocess_dfdf test_corpus selfcreate_corpusdf sequences selftokenizer_objtexts_to_sequencestest_corpus tweet_pad pad_sequencessequences maxlenself__class__max_len truncatingpost paddingpost df tweet_paddfshape0 y_pre selfmodelpredictdf y_pre nproundy_preastypeintflattentolist sub pddataframetarget y_pre pred_dict prediction floatx x subtargetvalues return pred_dict def preprocess_dfself"
  },
  {
    "filename": "ray-serve.mdx",
    "path": "docs/integrations/ai-engines/ray-serve.mdx",
    "chunk_id": 3,
    "chunk_content": "df df dftext dftext dftextapplylambda x selfremove_urlx dftext dftextapplylambda x selfremove_htmlx dftext dftextapplylambda x selfremove_emojix dftext dftextapplylambda x selfremove_punctx return df def remove_urlself text url recompilerhttpsswwws return urlsubr text def remove_htmlself text html recompiler return htmlsubr text def remove_punctself text table strmaketrans stringpunctuation return texttranslatetable def remove_emojiself text emoji_pattern recompile uu0001f600u0001f64f emoticons uu0001f300u0001f5ff symbols pictographs uu0001f680u0001f6ff transport map symbols uu0001f1e0u0001f1ff flags ios uu00002702u000027b0 uu000024c2u0001f251 flagsreunicode return emoji_patternsubr text def create_corpusself df corpus tweet tqdmdftext words wordlower word word_tokenizetweet wordisalpha 1 word stop corpusappendwords return corpus __name__ __main__ rayinit servestartdetachedtrue modeldeploy true timesleep1 need access training data create table called nlp_kaggle_train load datasethttpswwwkagglecomcnlpgettingstarted original model uses nlp_kaggle_train table contains following columns sql id int keyword varchar255 location varchar255 text varchar5000 target int please note specifics schematable ingest csv data vary depending database bringing ray serve model mindsdb create train custom model mindsdb sql create model mindsdbbyom_ray_serve_nlp maria select text target testnlp_kaggle_train predict target using urltrain http1270018000nlp_kaggle_modeltrain urlpredict http1270018000nlp_kaggle_modelpredict dtype_dicttext rich_text target integer formatray_server training process takes time considering model neural network rather simple logistic regression check model status using query sql describe byom_ray_serve_nlp status predictor value trained fetch predictions using standard mindsdb syntax follow guide selectsqlapiselect statement learn sql select mindsdbbyom_ray_serve_nlp"
  },
  {
    "filename": "ray-serve.mdx",
    "path": "docs/integrations/ai-engines/ray-serve.mdx",
    "chunk_id": 4,
    "chunk_content": "textthe tsunami coming seek high ground expected output query 1 sql select mindsdbbyom_ray_serve_nlp textthis lovely dear friend expected output query 0 tip wrong results results match example try training model longer amount epochs tip tip get insights check article bring machine learning model databaseshttpsmediumcommindsdbhowtobringyourownmachinelearningmodeltodatabases47a188d6db00 patricio cerda mardinihttpsmediumcompaxcema learn tip"
  },
  {
    "filename": "autokeras.mdx",
    "path": "docs/integrations/ai-engines/autokeras.mdx",
    "chunk_id": 0,
    "chunk_content": "title autokeras sidebartitle autokeras autokeras automl package regression classification tasks integrated regressor classifier models package implementation automatically determine whether task regression classification user need specify call handler using engineautokeras see full example httpsgithubcommindsdbmindsdbpull4559 integration useful autokeras build accurate deep learning model enduser prior knowledge deep learning required handler automatically search tune different neural network architectures find accurate given problem ideal use case large dataset many predictive features user doesnt strong priors features may affect target variable use integration small datasets neural networks prone overfitting use integration need fit model quickly training time long models created integration fast scalable general making predictions models fast however model search autotraining process slow autokeras uses neural networks using default settings model training may take several hours provide optional setting reduce training time see recommended system specifications models created framework recommend training handler machine cudaenabled gpu would also recommend training remote server machine leave running given potentially long training times degree users control underlying framework passing parameters via using syntax provide optional argument using engineautokeras train_timex x take values 0 1 default 1 lower values reduce training time linearly eg value 01 cut training time 10 default comes cost accuracy neural net model search space"
  },
  {
    "filename": "autokeras.mdx",
    "path": "docs/integrations/ai-engines/autokeras.mdx",
    "chunk_id": 1,
    "chunk_content": "reduced factor integration offer model explainability insights via describe syntax implemented yet integration support finetuning preexisting models ie update method implemented caveats implemented yet noteworthy aspects handler autokeras automatically splits data training validation sets user need original keras library computer vision would good option users want image analysis users set conda environment use handler keras depends tensorflow rather pytorch follow instructions 1 httpswwwtensorfloworginstallpip 2 httpsautokerascominstall directions future work subsequent versions handler implement describe update methods user demand image analysis implement image models autokeras please provide minimal sql example uses ml engine pointers integration tests pr also valid see integration test httpsgithubcommindsdbmindsdbpull4559"
  },
  {
    "filename": "replicate-text2video.mdx",
    "path": "docs/integrations/ai-engines/replicate-text2video.mdx",
    "chunk_id": 0,
    "chunk_content": "title replicate text2video sidebartitle replicate text2video handler implemented using replicate library provided replicate required arguments establish connection model_name model name want access mindsdb eg airforeverkandinsky2 version version hashid want use mindsdb api_key api key replicate platform found herehttpsreplicatecomaccountapitokens note use replicate essential authenticate setting api token environment variable named replicate_api_token token acts key enable access replicates features 1 using pip youre working standard python environment using pip package management set token environment variable running following command terminal linux mac export replicate_api_tokenyour_token windows set replicate_api_tokenyour_token 2 using docker docker users process slightly differs need pass environment variable directly docker container running use command docker run e replicate_api_tokenyour_token p 4733447334 p 4733547335 mindsdbmindsdb replace your_token actual replicate api token note usage use handler connect replicate cluster mindsdb need account replicate make sure create account following linkhttpsreplicatecomsigninnextaccountapitokens establish connection create model mindsdb use following syntax sql create model video_ai predict output using engine replicate model_name deforumdeforum_stable_diffusion version e22e77495f2fb83c34d5fae2ad8ab63c0a87b6b573b6208e1535b23b89ea66d6 api_key r8_heh use describe predictor query see available parameters specify customize predictions sql describe predictor mindsdbvideo_aifeatures output sql inputs type default description fps integer 15 choose fps video seed integer random seed leave blank randomize seed zoom string 0 104 zoom parameter motion angle string"
  },
  {
    "filename": "replicate-text2video.mdx",
    "path": "docs/integrations/ai-engines/replicate-text2video.mdx",
    "chunk_id": 1,
    "chunk_content": "00 angle parameter motion sampler plms max_frames integer 100 number frames animation translation_x string 0 0 translation_x parameter motion translation_y string 0 0 translation_y parameter motion color_coherence match frame 0 lab animation_prompts string 0 beautiful portrait woman artgerm trending artstation prompt animation provide frame number prompt frame separate different prompts make sure frame number exceed max_frames use established connection query ml model follows sql select video_ai animation_promptsa human animals friends asher brown durand trending artstation using max_frames119 output sql output prompt httpsreplicatedeliverypbxtgsgrjnlxigjwbb8keebkermbjzx0wqx7jc41u0pvifpcyvzeboutmp4 human animals friends asher brown durand trending artstation checkout generated video hereassetsanimalsmp4 link work reason given important note predicted url work 24 hours prediction note replicate provides free predictions choose predictions wisely dont let machines fun save"
  },
  {
    "filename": "xgboost.mdx",
    "path": "docs/integrations/ai-engines/xgboost.mdx",
    "chunk_id": 0,
    "chunk_content": "title xgboost sidebartitle xgboost note page work progress note"
  },
  {
    "filename": "huggingface_inference_api.mdx",
    "path": "docs/integrations/ai-engines/huggingface_inference_api.mdx",
    "chunk_id": 0,
    "chunk_content": "title hugging face inference api sidebartitle hugging face inference api documentation describes integration mindsdb hugging face inference apihttpshuggingfacecoinferenceapiserverless integration allows deployment hugging face models inference api within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use hugging face inference api within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain api key hugging face inference api required deploy use hugging face models inference api within mindsdb generate tokens settings access tokens tab hugging face account setup create ai engine hugging face inference api handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershuggingface_api_handler sql create ml_engine huggingface_api_engine huggingface_api using huggingface_api_api_key apikeyvalue create model using huggingface_api_engine engine sql create model huggingface_api_model predict target_column using engine huggingface_api_engine engine name created via create ml_engine task task_name choose one textclassification textgeneration questionanswering sentencesimilarity zeroshotclassification summarization fillmask imageclassification objectdetection automaticspeechrecognition audioclassification input_column column_name column stores inputquestion model labels label 1 label 2 labels used classify data used classification tasks following parameters supported using clause create model statement parameter required description engine yes name ml engine created create ml_engine statement task model_name provided describes task performed model_name task provided specifies model used input_column yes name column stores input model endpoint defines"
  },
  {
    "filename": "huggingface_inference_api.mdx",
    "path": "docs/integrations/ai-engines/huggingface_inference_api.mdx",
    "chunk_id": 1,
    "chunk_content": "endpoint use api calls specified hosted inference api hugging face used options json object containing additional options pass api call information available options task found herehttpshuggingfacecodocsapiinferencedetailed_parameters parameters json object containing additional parameters pass api call information available parameters task found herehttpshuggingfacecodocsapiinferencedetailed_parameters context_column task questionanswering used questionanswering task provide context question input_column2 task sentencesimilarity used sentencesimilarity task provide second input sentence comparison candidate_labels task zeroshotclassification used zeroshotclassification task classify input data according provided labels usage following usage examples utilize huggingface_api_engine create model create model statement create model classify input text spam ham sql create model spam_classifier predict is_spam using engine huggingface_api_engine task textclassification column text query model get predictions sql select text is_spam spam_classifier text subscribe channel asap output sql text is_spam subscribe channel asap spam info find quick examples accordiongroup accordion titletext classification sql create model mindsdbhf_text_classifier predict sentiment using task textclassification engine hf_api_engine input_column text accordion accordion titlefill mask sql create model mindsdbhf_fill_mask predict sequence using task fillmask engine hf_api_engine input_column text accordion accordion titlesummarization sql create model mindsdbhf_summarizer predict summary using task summarization engine hf_api_engine input_column text accordion accordion titletext generation sql create model mindsdbhf_text_generator predict generated_text using task textgeneration engine hf_api_engine input_column text accordion accordion titlequestion answering"
  },
  {
    "filename": "huggingface_inference_api.mdx",
    "path": "docs/integrations/ai-engines/huggingface_inference_api.mdx",
    "chunk_id": 2,
    "chunk_content": "sql create model mindsdbhf_question_answerer predict answer using task questionanswering engine hf_api_engine input_column question context_column context accordion accordion titlesentences similarity sql create model mindsdbhf_sentence_similarity predict similarity using task sentencesimilarity engine hf_api_engine input_column sentence1 input_column2 sentence2 accordion accordion titlezero shot classification sql create model mindsdbhf_zero_shot_classifier predict label using task zeroshotclassification engine hf_api_engine input_column text candidate_labels label1 label2 label3 accordion accordion titleimage classification sql create model mindsdbhf_image_classifier predict label using task imageclassification engine hf_api_engine input_column image_url accordion accordion titleobject detection sql create model mindsdbhf_object_detector predict objects using task objectdetection engine hf_api_engine input_column image_url accordion accordion titleautomatic speech recognition sql create model mindsdbhf_speech_recognizer predict transcription using task automaticspeechrecognition engine hf_api_engine input_column audio_url accordion accordion titleaudio classification sql create model mindsdbhf_audio_classifier predict label using task audioclassification engine hf_api_engine input_column audio_url accordion accordiongroup info tip next steps follow linkhttpsdocsmindsdbcomsqltutorialshuggingfaceinferenceapiexamples see use case examples tip"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 0,
    "chunk_content": "title anyscale endpoints sidebartitle anyscale endpoints documentation describes integration mindsdb anyscale endpointshttpswwwanyscalecomendpoints fast scalable api integrate oss llms apps integration allows deployment anyscale endpoints models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use anyscale endpoints within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain anyscale endpoints api key required deploy use anyscale endpoints models within mindsdb follow instructions obtaining api keyhttpsdocsanyscalecomendpointstextgenerationauthenticate setup create ai engine anyscale endpoints handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanyscale_endpoints_handler sql create ml_engine anyscale_endpoints_engine anyscale_endpoints using anyscale_endpoints_api_key apikeyvalue create model using anyscale_endpoints_engine engine sql create model anyscale_endpoints_model integration select table predict target_column using engine anyscale_endpoints_engine engine name created via create ml_engine api_base baseurl optional replaces default base url mode conversational optional mode run model model_name anyscale_endpoints_model_name optional llm use prompt helpful assistant task continue chat optional system prompt model question_column question optional column name stores user input context_column context optional column stores context user input prompt_template answer users input helpful way question optional base template placeholders used provide input model max_tokens 100 optional token limit model output temperature 03 optional randomness setting model output json_struct key value optional parameter extracting json data prompt_template tip possilbe"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 1,
    "chunk_content": "override certain parameters set model prediction time instead recreating model example change temperature parameter specific prediction use following query sql select question answer anyscale_endpoints_model question stockholm located using temperature 09 prompt_template answer users input pirate question parameters overridden shown mentioned detailed explanation tip following detailed explanation parameters used create model statement accordiongroup accordion titleengine engine name created create ml_enginehttpsdocsmindsdbcommindsdb_sqlsqlcreatemlengine statement accordion accordion titleapi_base parameter optional replaces default anyscales base url defined value accordion accordion titlemode parameter optional available modes include default conversational conversationalfull default mode used default model generate separate response input provided context maintained inputs conversational mode maintain context inputs generate single response response placed last row result set conversationalfull mode maintain context inputs generate response input accordion accordion titlemodel_name parameter optional default metallamallama27bchathf model used accordion accordion titlequestion_column parameter optional contains column name stores user input accordion accordion titlecontext_column parameter optional contains column name stores context user input accordion accordion titleprompt_template parameter optional use question_column stores message instructions base template placeholders filled user input prediction time please note parameter overridden prediction time accordion accordion titleprompt parameter optional defines initial system prompt model accordion accordion titlemax_tokens parameter optional defines maximum token cost prediction please note parameter overridden prediction time"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 2,
    "chunk_content": "accordion accordion titletemperature parameter optional defines risky answers value 0 marks welldefined answer value 09 marks creative answer please note parameter overridden prediction time accordion accordion titlejson_struct parameter optional used extract json data text column provided prompt_template parameter see examples hereusecasesdata_enrichmentjsonfromtextextractjsonfromtextdata accordion accordiongroup note implementation integration based engine openai api anyscale conforms notable differences though 1 models supported anyscale endpoints open source full list found inferenceonly section supported modelshttpsappendpointsanyscalecomdocs 2 every model supported finetuning find list section fine tuning supported modelshttpsappendpointsanyscalecomdocs please check lists regularly subject change try finetune model supported get warning subsequently error anyscale endpoint 3 integration offers chatbased text completion models either normal text specialized code 4 providing description integration returns respective huggingface model card 5 finetuning requires dataset complies chat format row contain context role context text message chat role authored system user assistant last one model information please check fine tuning guide anyscale endpoints docshttpsappendpointsanyscalecomdocs note info base url api httpsapiendpointsanyscalecomv1 info usage following usage examples utilize anyscale_endpoints_engine create model create model statement note output generated single input regardless mode used difference modes model handles multiple inputs filesunrelated_questions simple csv file containing question column simple unrelated questions uploaded mindsdb filesrelated_questions similar file containing related questions"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 3,
    "chunk_content": "filesunrelated_questions_with_context filesrelated_questions_with_context similar files containing additional context column files used examples given provide multiple inputs models created possible use supported data source manner note accordiongroup accordion titledefault mode default mode model generate separate response input provided context maintained inputs accordiongroup accordion titleprompt completion sql create model anyscale_endpoints_default_model predict answer using engine anyscale_endpoints_engine model_name mistralaimistral7binstructv01 prompt_template answer users input helpful way question generate response single input following query used sql select question answer anyscale_endpoints_default_model question stockholm located response look like following sql question answer stockholm locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesunrelated_questions join anyscale_endpoints_default_model response look like following sql question answer stockholm located stockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene fourth planet solar system fourth planet solar system mars accordion accordion titlequestion answering sql create model anyscale_endpoints_default_model predict answer using engine anyscale_endpoints_engine model_name mistralaimistral7binstructv01 question_column question generate response single input following query used sql select question answer anyscale_endpoints_default_model question stockholm located response look like following sql question answer stockholm"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 4,
    "chunk_content": "locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesunrelated_questions join anyscale_endpoints_default_model response look like following sql question answer stockholm located stockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene fourth planet solar system fourth planet solar system mars accordion accordion titlequestion answering context sql create model anyscale_endpoints_default_model predict answer using engine anyscale_endpoints_engine model_name mistralaimistral7binstructv01 question_column question context_column context generate response single input following query used sql select question context answer anyscale_endpoints_default_model question main topic conference happening next week context conference focusing advancements ai take place san francisco next week response look like following sql question context answer main topic conference happening next week conference focusing advancements ai take place san francisco next weekthe main topic conference happening next week san francisco advancements ai generate responses multiple inputs following query used sql select dquestion dcontext manswer filesunrelated_questions_with_context join anyscale_endpoints_default_model response look like following sql question context answer main topic conference happening next week conference focusing advancements ai take place san francisco next weekthe"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 5,
    "chunk_content": "main topic conference happening next week san francisco advancements ai caused extension project deadline project deadline extended two weeks due team members falling sick unexpectedly extension project deadline caused unexpected illnesses among team members accordion accordiongroup accordion accordion titleconversational mode conversational mode model maintain context inputs generate single response response placed last row result set accordiongroup accordion titleprompt completion sql create model anyscale_endpoints_conversational_model predict answer using engine anyscale_endpoints_engine mode conversational model_name mistralaimistral7binstructv01 prompt helpful assistant task continue chat prompt_template answer users input helpful way question generate response single input following query used sql select question answer anyscale_endpoints_conversational_model question stockholm located response look like following sql question answer stockholm locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesrelated_questions join anyscale_endpoints_conversational_model response look like following question answer stockholm located fun activities stockholm capital city sweden located southeastern part country fun activities stockholm include visiting famous vasa museum exploring beautiful archipelago taking stroll charming gamla stan neighborhood trying local food drinks accordion accordion titlequestion answering sql create model anyscale_endpoints_conversational_model predict answer using engine anyscale_endpoints_engine mode conversational model_name mistralaimistral7binstructv01"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 6,
    "chunk_content": "prompt helpful assistant task continue chat question_column question generate response single input following query used sql select question answer anyscale_endpoints_conversational_model question stockholm located response look like following sql question answer stockholm locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesunrelated_questions join anyscale_endpoints_conversational_model response look like following question answer stockholm located fun activities stockholm capital city sweden located southeastern part country fun activities stockholm include visiting famous vasa museum exploring beautiful archipelago taking stroll charming gamla stan neighborhood trying local food drinks accordion accordion titlequestion answering context sql create model anyscale_endpoints_conversational_model predict answer using engine anyscale_endpoints_engine mode conversational model_name mlabonneneuralhermes25mistral7b prompt helpful assistant task continue chat question_column question context_column context generate response single input following query used sql select question context answer anyscale_endpoints_conversational_model question main topic conference happening next week context conference focusing advancements ai take place san francisco next week response look like following sql question context answer main topic conference happening next week main topic conference happening next week advancements artificial intelligence generate responses multiple inputs following query used sql select dquestion dcontext manswer"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 7,
    "chunk_content": "filesrelated_questions_with_context join anyscale_endpoints_conversational_model response look like following question context answer anna planning trip next month anna planning trip kyoto next month anna plan anna plans going sightseeing anna plans going sightseeing trip kyoto next month accordion accordiongroup accordion accordion titleconversationalfull mode conversationalfull mode model maintain context inputs generate response input accordiongroup accordion titleprompt completion sql create model anyscale_endpoints_conversational_full_model predict answer using engine anyscale_endpoints_engine mode conversationalfull model_name mistralaimistral7binstructv01 prompt_template answer users input helpful way question generate response single input following query used sql select question answer anyscale_endpoints_conversational_full_model question stockholm located response look like following sql question answer stockholm locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesrelated_questions join anyscale_endpoints_conversational_full_model response look like following question answer stockholm located stockholm capital city sweden located southeastern part country situated island stockholm archipelago made 30000 islands city known beautiful architecture museums cultural attractions well vibrant food nightlife scene fun activities stockholm capital city sweden located southeastern part country east coast stockholm archipelago fun activities stockholm include visiting famous vasa museum exploring charming old town gamla stan taking stroll beautiful parks"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 8,
    "chunk_content": "gardens trying local food drinks also many opportunities shopping cultural experiences outdoor activities hiking biking accordion accordion titlequestion answering sql create model anyscale_endpoints_conversational_full_model predict answer using engine anyscale_endpoints_engine mode conversationalfull model_name mistralaimistral7binstructv01 question_column question generate response single input following query used sql select question answer anyscale_endpoints_conversational_full_model question stockholm located response look like following sql question answer stockholm locatedstockholm capital city sweden located southeastern part country situated archipelago 30000 islands baltic sea known beautiful waterfront historic buildings vibrant cultural scene generate responses multiple inputs following query used sql select dquestion manswer filesrelated_questions join anyscale_endpoints_conversational_full_model response look like following question answer stockholm located stockholm capital city sweden located southeastern part country situated island stockholm archipelago made 30000 islands city known beautiful architecture museums cultural attractions well vibrant food nightlife scene fun activities stockholm capital city sweden located southeastern part country east coast stockholm archipelago fun activities stockholm include visiting famous vasa museum exploring charming old town gamla stan taking stroll beautiful parks gardens trying local food drinks also many opportunities shopping cultural experiences outdoor activities hiking biking accordion accordion titlequestion answering context sql create model anyscale_endpoints_conversational_full_model predict answer using engine anyscale_endpoints_engine mode conversationalfull model_name mlabonneneuralhermes25mistral7b question_column question context_column context generate response single"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 9,
    "chunk_content": "input following query used sql select question context answer anyscale_endpoints_conversational_full_model question main topic conference happening next week context conference focusing advancements ai take place san francisco next week response look like following sql question context answer main topic conference happening next week conference focusing advancements ai take place san francisco next weekthe main topic conference happening next week san francisco advancements ai generate responses multiple inputs following query used sql select dquestion dcontext manswer filesrelated_questions_with_context join anyscale_endpoints_conversational_full_model response look like following question context answer anna planning trip next month anna planning trip kyoto next month anna planning trip kyoto next month anna plan anna plans going sightseeing anna plans going sightseeing trip kyoto next month accordion accordiongroup accordion accordiongroup tip next steps follow tutorialhttpsdocsmindsdbcomfinetuneanyscale see use case examples tip troubleshooting guide warning authentication error symptoms failure authenticate anyscale checklist 1 make sure anyscale account active 2 confirm api key correct 3 ensure api key revoked 4 ensure exceeded api usage rate limit warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table model names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks examples incorrect sql select inputtext outputsentiment integrationtravel data input join"
  },
  {
    "filename": "anyscale.mdx",
    "path": "docs/integrations/ai-engines/anyscale.mdx",
    "chunk_id": 10,
    "chunk_content": "openai_engine output incorrect sql select inputtext outputsentiment integrationtravel data input join openai_engine output correct sql select inputtext outputsentiment integrationtravel data input join openai_engine output warning"
  },
  {
    "filename": "merlion.mdx",
    "path": "docs/integrations/ai-engines/merlion.mdx",
    "chunk_id": 0,
    "chunk_content": "title merlion sidebartitle merlion note page work progress note"
  },
  {
    "filename": "popularity-recommender.mdx",
    "path": "docs/integrations/ai-engines/popularity-recommender.mdx",
    "chunk_id": 0,
    "chunk_content": "title popularity recommender sidebartitle popularity recommender popularity recommender built using polarshttpspolars create simple fast popularity recommender recommending items based global popularity personal past interaction identifies popular items entire dataset excludes items user already interacted straightforward methodology makes excellent benchmark comparing sophisticated recommendation engines ideal use cases handler include analyzing ecommerce rating data web page browsing data past purchase data serving users recommendations current implementations stand input data table containing useritem interaction data user_id item_id rating 1 2 4 1 3 7 note please note moment integrations support describe finetune features note example creating popularity recommender model need create ml engine sql create ml_engine popularity_recommender popularity_recommender verify running show ml_engines lets create popularity recommender model specifying necessary input parameters sql create model pop_rec_demo mysql_demo_db select movie_lens_ratings predict movieid using engine popularity_recommender item_id movieid user_id userid n_recommendations 10 required parameters include following item_id parameter stores items recommended movies user_id parameter stores users items recommended n_recommendations parameter stores number recommendations returned tip connect mysql_demo_db used training model sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public tip get recommendations per user based global popular items sql select b mysql_demo_dbmovie_lens_ratings join pop_rec_demo b get recommendations specific"
  },
  {
    "filename": "popularity-recommender.mdx",
    "path": "docs/integrations/ai-engines/popularity-recommender.mdx",
    "chunk_id": 1,
    "chunk_content": "users based popularity sql select b mysql_demo_dbmovie_lens_ratings join pop_rec_demo b auserid 215216"
  },
  {
    "filename": "anthropic.mdx",
    "path": "docs/integrations/ai-engines/anthropic.mdx",
    "chunk_id": 0,
    "chunk_content": "title anthropic sidebartitle anthropic documentation describes integration mindsdb anthropichttpswwwanthropiccom ai research company integration allows deployment anthropic models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use anthropic within mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies 3 obtain anthropic api key required deploy use anthropic models within mindsdb follow instructions obtaining api keyhttpsdocsanthropiccomclaudedocsgettingaccesstoclaude setup create ai engine anthropic handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersanthropic_handler sql create ml_engine anthropic_engine anthropic using anthropic_api_key youranthropicapikey create model using anthropic_engine engine sql create model anthropic_model predict target_column using engine anthropic_engine engine name created via create ml_engine column column_name column stores inputquestion model max_tokens integer max number tokens generated model default 100 model model_name choose one claudeinstant12 claude21 claude3opus20240229 claude3sonnet20240229 default claude21 info integrations anthropic mindsdb implemented using anthropic python sdkhttpsgithubcomanthropicsanthropicsdkpython info usage following usage examples utilize anthropic_engine create model create model statement create deploy anthropic model within mindsdb ask question sql create model anthropic_model predict answer using column question engine anthropic_engine max_tokens 300 model claude21 choose one claudeinstant12 claude21 claude3opus20240229 claude3sonnet20240229 name description column defines prompt model engine defines anthropic engine max_tokens defines maximum number tokens generate stopping model defines model complete prompt info default"
  },
  {
    "filename": "anthropic.mdx",
    "path": "docs/integrations/ai-engines/anthropic.mdx",
    "chunk_id": 1,
    "chunk_content": "model create anthropic model mindsdb uses claude21 model default use available models passing model name model parameter using clause create model statement info info default max tokens create anthropic model mindsdb uses 100 tokens maximum default adjust value passing max_tokens parameter using clause create model statement info query model get predictions sql select question answer anthropic_model question stockholm located output sql question answer stockholm located stockholm capital largest city sweden located swedens southcentral east coast lake m\u00e4laren meets baltic sea tip next steps go use caseshttpsdocsmindsdbcomusecasesoverview section see examples tip"
  },
  {
    "filename": "statsforecast.mdx",
    "path": "docs/integrations/ai-engines/statsforecast.mdx",
    "chunk_id": 0,
    "chunk_content": "title nixtlas statsforecast integration mindsdb sidebartitle statsforecast nixtlas statsforecast integration offers univariate time series forecasting models statsforecast uses classical methods arima rather deep learning models train quickly generalize well unlikely overfit models also perform well short time series deep learning models may likely overfit learn features herehttpsnixtlagithubiostatsforecast bring statsforecast models mindsdb creating model need create ml engine statsforecast using create ml_engine statement sql create ml_engine statsforecast statsforecast ml engine created use create model statement create statsforecast model mindsdb sql create model model_name data_source select table_name predict column_to_be_predicted group column_name column_name order date_column window 12 model looks back sets 12 rows horizon 3 model forecasts next 3 rows using engine statsforecast model_name model frequency x season_length 1 hierarchy column following parameters used creating statsforecast model model_name optional parameter lets users specify one models listhttpsgithubcomnixtlastatsforecasttabreadmeovfilemodels otherwise chosen automatically frequency optional parameter defines frequency data daily weekly monthly etc available values include h ms q sm bm bms bq bh season_length optional parameter defines length season depending frequency instance season_length defaults 12 frequency set months hierarchy optional parameter may improve prediction accuracy data hierarchical structure see hereintegrationsaienginesstatsforecaststatsforecasthierarchicalforecast ensure model created based statsforecast engine include using clause end example lets go example use nixtlas"
  },
  {
    "filename": "statsforecast.mdx",
    "path": "docs/integrations/ai-engines/statsforecast.mdx",
    "chunk_id": 1,
    "chunk_content": "statsforecast mindsdb forecast monthly expenditures please note using statsforecast engine create mindsdb editor clients interact mindsdb command sql create ml_engine statsforecast statsforecast check available engines command sql show ml_engines see statsforecast engine list ready follow tutorials tutorial using sql tutorial create model predict expenditures based historical data using statsforecast engine use table mysql public demo database lets start connecting mindsdb sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public weve connected database mindsdb lets query data used example sql select mysql_demo_dbhistorical_expenditures limit 3 output sql month category expenditure 19820401 food 11626 19820501 food 11509 19820601 food 1160 historical_expenditures table stores monthly expenditure data various categories food clothing industry lets create model table predict expenditures sql create model quarterly_expenditure_forecaster mysql_demo_db select historical_expenditures predict expenditure group category order month horizon 3 using engine statsforecast tip please visit docs create modelsqlcreatemodel statement learn tip please note window clause required statsforecast automatically calculates best window part hyperparameter tuning engine parameter using clause specifies ml engine used make predictions check training status following query sql describe quarterly_expenditure_forecaster one pros using statsforecast engine fast doesnt take long model completes training process model status complete behavior ai table query"
  },
  {
    "filename": "statsforecast.mdx",
    "path": "docs/integrations/ai-engines/statsforecast.mdx",
    "chunk_id": 2,
    "chunk_content": "batch predictions joining data table sql select mmonth month mexpenditure forecasted mindsdbquarterly_expenditure_forecaster join mysql_demo_dbhistorical_expenditures tmonth latest tcategory food output data sql month forecasted 20171001 000000000000 10256251953125 20171101 000000000000 1018258984375 20171201 000000000000 10316259765625 historical_expenditures table used make batch predictions upon joining quarterly_expenditure_forecaster model historical_expenditures table get predictions next quarter defined horizon 3 clause please note output month column contains date timestamp format used default timestamp required dealing hourly frequency data mindsdb provides latest keyword marks latest training data point clause specify month latest condition ensure predictions made data latest training data point lets consider quarterly_expenditure_forecaster model train model using data third quarter 2017 predictions come fourth quarter 2017 defined horizon 3 tutorial using mql tutorial create model predict expenditures based historical data using statsforecast engine start visit docs learn connect mongo compasshttpsdocsmindsdbcomconnectmongocompass mongo shellhttpsdocsmindsdbcomconnectmongoshell mindsdb use collection mongo public demo database lets start connecting mindsdb mongo compass mongo shell bash use mindsdb dbdatabasesinsertone name mongo_demo_db engine mongodb connection_args host mongodbsrvusermindsdbuser123demodatamdbtrzfwvbmongodbnet database public weve connected database mindsdb lets query data used example bash use mongo_demo_db dbhistorical_expendituresfindlimit3 output bash _id 63fd2388bee7187f230f56fc month 19820401 category food expenditure 11626 _id 63fd2388bee7187f230f56fd month 19820501 category food expenditure 11509 _id 63fd2388bee7187f230f56fe month 19820601 category food expenditure 1160"
  },
  {
    "filename": "statsforecast.mdx",
    "path": "docs/integrations/ai-engines/statsforecast.mdx",
    "chunk_id": 3,
    "chunk_content": "historical_expenditures collection stores monthly expenditure data various categories food clothing industry lets create model predict expenditures bash use mindsdb dbpredictorsinsertone name quarterly_expenditure_forecaster predict expenditure connection mongo_demo_db select_data_query dbhistorical_expendituresfind training_options timeseries_settings order_by month group_by category horizon 3 engine statsforecast tip please visit docs insertonemongoinsert statement learn tip please note window clause required statsforecast automatically calculates best window part hyperparameter tuning engine parameter training_options clause specifies ml engine used make predictions check training status following query bash dbmodelsfind name quarterly_expenditure_forecaster one pros using statsforecast engine fast doesnt take long model completes training process model status complete behavior ai collection query batch predictions joining data collection bash dbquarterly_expenditure_forecasterfind collection mongo_pred_01historical_expenditures query category food limit3 default forecasts made month latest output data bash _id 63fd2388bee7187f230f58a5 month 20171001t000000000z category food expenditure 10256251953125 _id 63fd2388bee7187f230f58a4 month 20171101t000000000z category food expenditure 1018258984375 _id 63fd2388bee7187f230f58a3 month 20171201t000000000z category food expenditure 10316259765625 historical_expenditures collection used make batch predictions upon joining quarterly_expenditure_forecaster model historical_expenditures collection get predictions next quarter defined horizon 3 clause please note output month column contains date timestamp format used default timestamp required dealing hourly frequency data mindsdb provides latest keyword marks latest training data point clause specify month latest condition ensure predictions made data latest training"
  },
  {
    "filename": "statsforecast.mdx",
    "path": "docs/integrations/ai-engines/statsforecast.mdx",
    "chunk_id": 4,
    "chunk_content": "data point lets consider quarterly_expenditure_forecaster model train model using data third quarter 2017 predictions come fourth quarter 2017 defined horizon 3 statsforecast hierarchicalforecast statsforecast handler also supports hierarchical reconciliation via nixtlas hierarchicalforecast packagehttpsnixtlagithubiohierarchicalforecast hierarchical reconciliation may improve prediction accuracy data hierarchical structure example may hierarchy total expenditure comprised 7 different categories sql select distinct category mysql_demo_dbhistorical_expenditures available categories sql category food household_goods clothing department_stores cafes industry spending category may related time example spending food rises october 2017 may likely spending cafes also rises october 2017 hierarchical reconciliation account shared information create model sql create model hierarchical_expenditure_forecaster mysql_demo_db select historical_expenditures predict expenditure group category order month horizon 3 using engine statsforecast hierarchy category create model statement creates trains deploys model predict expenditure column values time series model order data month column additionally group data category column predictions made group independently category horizon clause defines many rows predictions made next 3 rows use describe model command check details sql describe hierarchical_expenditure_forecastermodel execution get sql model_name frequency season_length hierarchy autoarima ms 1 category predictions model account hierarchical structure output may differ default model assume hierarchy"
  },
  {
    "filename": "replicate-audio.mdx",
    "path": "docs/integrations/ai-engines/replicate-audio.mdx",
    "chunk_id": 0,
    "chunk_content": "title replicate audio sidebartitle replicate audio handler implemented using replicate library provided replicate required arguments establish connection model_name model name want access mindsdb eg airforeverkandinsky2 version version hashid want use mindsdb api_key api key replicate platform found herehttpsreplicatecomaccountapitokens note use replicate essential authenticate setting api token environment variable named replicate_api_token token acts key enable access replicates features 1 using pip youre working standard python environment using pip package management set token environment variable running following command terminal linux mac export replicate_api_tokenyour_token windows set replicate_api_tokenyour_token 2 using docker docker users process slightly differs need pass environment variable directly docker container running use command docker run e replicate_api_tokenyour_token p 4733447334 p 4733547335 mindsdbmindsdb replace your_token actual replicate api token note usage use handler connect replicate cluster mindsdb need account replicate make sure create account following linkhttpsreplicatecomsigninnextaccountapitokens establish connection create model mindsdb use following syntax sql create model audio_ai predict audio using engine replicate model_name afiaka87tortoisetts version e9658de4b325863c4fcdc12d94bb7c9b54cbfe351b7ca1b36860008172b91c71 api_key r8_bpo use describe predictor query see available parameters specify customize predictions sql describe predictor mindsdbaudio_aifeatures output sql inputs type default description seed integer 0 random seed used reproduce results text string expressiveness autoregressive transformers literally nuts absolutely adore text speak preset fast voice preset"
  },
  {
    "filename": "replicate-audio.mdx",
    "path": "docs/integrations/ai-engines/replicate-audio.mdx",
    "chunk_id": 1,
    "chunk_content": "use see documentation information voice_a random selects voice use generation use random select random voice use custom_voice use custom voice voice_b disabled optional create new voice averaging latents voice_a voice_b voice_c use disabled disable voice mixing voice_c disabled optional create new voice averaging latents voice_a voice_b voice_c use disabled disable voice mixing cvvp_amount number 0 much cvvp model influence output increasing cases reduce likelihood multiple speakers defaults 0 disabled custom_voice string optional create custom voice based mp3 file speaker audio least 15 seconds contain one speaker mp3 format overrides voice_a input use established connection query ml model follows audio generation custom audio cloning sql select audio_ai text breaking news first humans landed mars found something unusual way future using voice_a custom_voice custom_voice https123biencomwpcontentuploads201905iwanttowork2mp3 output sql audio text httpsreplicatedeliverypbxtffocxel4fa5yekal4ybffijbbqensejhslpa2zp1elsbxxhsetortoisemp3 breaking news first human landed mars find something unusual yet way future predicted url dont work use thisassetscloned_audiomp3 audio generation sql select audio_ai text image captured nasas mars curiosity rover shows faint figure woman desert landscape mars take closer look seem lady standing cliff overlooking vast undulating expanse seems wear long cloak long hair using voice_a random output sql audio text httpsreplicatedeliverypbxteqj2dtbn5fxva6p97gfpythmgd0i3vaoegfnwee4hvl5bpuiaaudiowav image captured nasas mars curiosity rover shows faint figure woman"
  },
  {
    "filename": "replicate-audio.mdx",
    "path": "docs/integrations/ai-engines/replicate-audio.mdx",
    "chunk_id": 2,
    "chunk_content": "desert landscape mars take closer look seem lady standing cliff overlooking vast undulating expanse seems wear long cloak long hair predicted url work therefore use thisassetsgenerated_audiowav one model used example vast variation use cases also limit imagination use important note predicted url work 24 hours prediction note replicate provides free predictions choose predictions wisely dont let machines fun save"
  },
  {
    "filename": "twelvelabs.mdx",
    "path": "docs/integrations/ai-engines/twelvelabs.mdx",
    "chunk_id": 0,
    "chunk_content": "title twelvelabs video semantic search sidebartitle twelvelabs section present connect twelve labs api mindsdb twelve labshttpstwelvelabsioproduct provides powerful seamless video search infrastructure application prerequisites proceeding ensure following prerequisites met 1 install mindsdb system obtain access cloud options 2 use twelve labs mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 copy twelve labs api key navigating dashboard pagehttpsapitwelvelabsiodashboard ai engine first step use handler create ml engine required argument create engine twelve_labs_api_key twelvelabs api key api key establish connection executing following sql command sql create ml_engine twelve_labs_engine twelve_labs using twelve_labs_api_key your_api_key ai model use ml engine create models different tasks supported handler executing create model statement following parameters supported using clause query engine name ml engine use required parameter twelve_labs_api_key twelve labs api key use authentication ml engine provided base_url base url twelve labs api optional parameter defaults httpsapitwelvelabsiov12 task task perform required parameter must one search summarization engine_id id twelve labs engine use optional parameter defaults marengo26 however certain tasks may require different engine id instance summarization task runs pegasus family engines information different engines found herehttpsdocstwelvelabsiov12docsengineoptions index_name name index use exist created required parameter information indexes found herehttpsdocstwelvelabsiodocscreateindexes index_options list types information within video processed video understanding engine required parameter"
  },
  {
    "filename": "twelvelabs.mdx",
    "path": "docs/integrations/ai-engines/twelvelabs.mdx",
    "chunk_id": 1,
    "chunk_content": "combination visual conversation text_in_video logo information index options found herehttpsdocstwelvelabsiodocsindexingoptions certain engines support subset options instance pegasus family engines support visual conversation options information configurations found herehttpsdocstwelvelabsiov12docscreateindexes video_urls list urls videos indexed optional parameter specified one video_files video_urls_column video_files_column must specified instead video_files list local paths videos indexed optional parameter specified one video_urls video_urls_column video_files_column must specified instead video_urls_column name column containing urls videos indexed optional parameter specified one video_urls video_files video_files_column must specified instead video_files_column name column containing local paths videos indexed optional parameter specified one video_urls video_files video_urls_column must specified instead search_options list sources information use performing search parameter required task search subset index_options information search options found herehttpsdocstwelvelabsiodocssearchoptions search_query_column name column containing search queries parameter required task search summarization_type type summarization perform parameter required task summarization one summary chapter highlight prompt provide context summarization task target audience style tone voice purpose optional parameter given examples creating models supported tasks search sql create model mindsdbtwelve_labs_search predict search_results using engine twelve_labs_engine twelve_labs_api_key your_api_key task search index_name index_1 index_options visual conversation text_in_video logo video_urls httpsvideo_1mp4 httpsvideo_2mp4 search_options visual conversation text_in_video logo search_query_column query mentioned search_options parameter specific search task subset index_options summarization sql create model mindsdbtwelve_labs_summarization predict search_results using engine"
  },
  {
    "filename": "twelvelabs.mdx",
    "path": "docs/integrations/ai-engines/twelvelabs.mdx",
    "chunk_id": 2,
    "chunk_content": "twelve_labs_engine task summarization engine_id pegasus1 index_name index_1 index_options visual conversation video_urls httpsvideo_1mp4 httpsvideo_2mp4 summarization_type summary making predictions given examples using models created supported tasks search sql select mindsdbtwelve_labs_search query search query query column name column containing search queries specified search_query_column parameter create model statement note moment single query specified clause query join clause making multiple predictions added future release summarization sql select mindsdbtwelve_labs_summarization video_id video_1 video ids indexed model found running describe statement url file path video available video_reference column following example run describe statement sql describe mindsdbtwelve_labs_summarizationindexed_videos response returned look something like video_id created_at updated_at duration engine_ids filename fps height size video_reference width 66c8425e35db9fa680cd4195 20240223t033910z 20240223t033912z 43733333 pegasus1 testmp4 30 1280 3737394 pathtovideostestmp4 720 note display indexed videos contained within index specified index_name parameter create model statement index used multiple models indexed_videos table contain videos indexed models use index"
  },
  {
    "filename": "ludwig.mdx",
    "path": "docs/integrations/ai-engines/ludwig.mdx",
    "chunk_id": 0,
    "chunk_content": "title ludwig sidebartitle ludwig preliminaries 1 make sure python environment mindsdb ludwig installed mindsdb example commands follow home rentals tutorial available cloud learning hub 1 select subsample data inspect sql select example_dbdemo_datahome_rentals limit 10 2 create ai table ludwig ml backend sql create model mindsdbhome_rentals_ludwig_model example_db select demo_datahome_rentals predict rental_price using engineludwig 3 check status predictor may take finish training sql describe model home_rentals_ludwig_model 3 make prediction note time ludwig requires input columns specified error trigger data missing sql select rental_price mindsdbhome_rentals_ludwig_model sqft 823 number_of_rooms2 number_of_bathrooms1 locationgood neighborhooddowntown days_on_market10"
  },
  {
    "filename": "vertex.mdx",
    "path": "docs/integrations/ai-engines/vertex.mdx",
    "chunk_id": 0,
    "chunk_content": "title vertex ai sidebartitle vertex ai documentation describes integration mindsdb vertex aihttpscloudgooglecomvertexai machine learning platform lets train deploy ml models ai applications customize large language models llms use aipowered applications integration allows deployment vertex ai models within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 use vertex ai within mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies setup create ai engine vertex ai handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersvertex_handler command creates config object used client creation step sql create ml_engine vertex_engine vertex using project_id mindsdb401709 location uscentral1 staging_bucket gsmy_staging_bucket experiment myexperiment experiment_description experiment description service_account paste service account keys create model using vertex_engine engine command authenticates client vertex account using config previous step endpoint model already exists create model mindsdb otherwise create deploy model endpoint creating model mindsdb sql create model vertex_model predict target_column using engine vertex_engine engine name created via create ml_engine model_name model_name choose one models project custom_model value indicate whether custom model true false usage following usage examples utilize vertex_engine create model create model statement detect anomaly using custom model stored vertex ai sql create model vertex_model predict cut using engine vertex model_name diamonds_anomaly_detection custom_model true query model get"
  },
  {
    "filename": "vertex.mdx",
    "path": "docs/integrations/ai-engines/vertex.mdx",
    "chunk_id": 1,
    "chunk_content": "predictions joining data table sql select dcut mcut anomaly data_table join vertex_model tip next steps go use casesusecasesoverview section see examples tip"
  },
  {
    "filename": "clipdrop.mdx",
    "path": "docs/integrations/ai-engines/clipdrop.mdx",
    "chunk_id": 0,
    "chunk_content": "title clipdrop sidebartitle clipdrop integrate state art image processing ai directly products info use clipdrop mindsdb need sign clipdrop account obtain api key learn herehttpsclipdropcoapis info setup sql create ml_engine clipdrop_engine clipdrop using clipdrop_api_key your_api_key usage remove text image video width100 height315 srchttpsstaticclipdropcowebapisremovetextremovetextdemomp4 titlevideo player style objectfit cover borderradius 8px autoplay muted loop playsinline controlsfalse create model automatically remove text image sql create model mindsdbclipdrop_rt predict image using engine clipdrop_engine task remove_text local_directory_path userssamdownloadstest make predictions model providing image url sql select mindsdbclipdrop_rt image_url httpsonlinejpgtoolscomimagesexamplesonlinejpgtoolscalmbodyofwaterwithquotejpg remove background image video width100 height315 srchttpsstaticclipdropcowebapisremovebackgroundremovebackgrounddemomp4 titlevideo player style objectfit cover borderradius 8px autoplay muted loop playsinline controlsfalse effortlessly strip away background image using task sql create model mindsdbclipdrop_rb predict image using engine clipdrop_engine task remove_background local_directory_path userssamdownloadstest provide image url remove background sql select mindsdbclipdrop_rb image_url httpsstaticclipdropcowebapisremovebackgroundinputjpg generate image sketch video width100 height315 srchttpsstaticclipdropcowebapissketchtoimagesketchtoimage1280720mp4 titlevideo player style objectfit cover borderradius 8px autoplay muted loop playsinline controlsfalse turn simple sketches detailed images ai sql create model mindsdbclipdrop_s2i predict image using engine clipdrop_engine task sketch_to_image local_directory_path userssamdownloadstest provide image url description desired transformation sql select mindsdbclipdrop_s2i image_url httpsimgfreepikcomfreevectorhanddrawncatoutlineillustration_232149266368jpg text brown cat generate image text video width100 height315 srchttpsstoragegoogleapiscomclipdropstaticassetswebapistexttoimagetexttoimage1280720mp4 titlevideo player style objectfit cover borderradius 8px"
  },
  {
    "filename": "clipdrop.mdx",
    "path": "docs/integrations/ai-engines/clipdrop.mdx",
    "chunk_id": 1,
    "chunk_content": "autoplay muted loop playsinline controlsfalse create unique images directly text prompts sql create model mindsdbclipdrop_t2i predict image using engine clipdrop_engine task text_to_image local_directory_path userssamdownloadstest example sql select mindsdbclipdrop_t2i text imagine software engineer reimagine image video width100 height315 srchttpsstaticclipdropcowebapisreimaginereimagine1280720mp4 titlevideo player style objectfit cover borderradius 8px autoplay muted loop playsinline controlsfalse reimagine image different artistic style form sql create model mindsdbclipdrop_reimagine predict image using engine clipdrop_engine task reimagine local_directory_path userssamdownloadstest run model transform existing image sql select mindsdbclipdrop_reimagine image_url httpsstaticclipdropcowebapisremovebackgroundinputjpg replace background image video width100 height315 srchttpsstoragegoogleapiscomclipdropstaticassetswebapishomepagereplacebackgroundmp4 titlevideo player style objectfit cover borderradius 8px autoplay muted loop playsinline controlsfalse replace background image custom scenes environments sql create model mindsdbclipdrop_rbi predict image using engine clipdrop_engine task replace_background local_directory_path userssamdownloadstest example query sql select mindsdbclipdrop_rbi image_url httpsstaticclipdropcowebapisremovebackgroundinputjpg text empty road note note local_directory_path parameter specifies path directory images stored local machine note"
  },
  {
    "filename": "amazon-bedrock.mdx",
    "path": "docs/integrations/ai-engines/amazon-bedrock.mdx",
    "chunk_id": 0,
    "chunk_content": "title amazon bedrock sidebartitle amazon bedrock documentation describes integration mindsdb amazon bedrockhttpsawsamazoncombedrock fully managed service offers choice highperforming foundation models fms leading ai companies integration allows deployment models offered amazon bedrock within mindsdb providing models access data various data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 use amaon bedrock within mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 obtain aws credentials user access amazon bedrock service setup create ai engine amazon bedrock handlerhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersbedrock_handler sql create ml_engine bedrock_engine bedrock using aws_access_key_id aqaxeqk89ox07ys34op aws_secret_access_key wjalrxutnfemik7mdengbpxrficyexamplekey aws_session_token fwogzxivyxdzejr aws_region useast1 required parameters creating engine include following aws_access_key_id aws access key id user aws_secret_access_key aws secret access key user aws_region aws region use optional parameters include following aws_session_token aws session token user required using temporary security credentials create model using bedrock_engine engine sql create model bedrock_model predict answer using engine bedrock_engine question_column question max_tokens 100 temperature 03 required parameters creating model include following engine name engine created via create ml_engine optional parameters include following mode mode run inference default mode default supported mode conversational model_id model id use inference default model id amazontitantextpremierv10 list supported models found httpsdocsawsamazoncombedrocklatestuserguidemodelidshtml question_column column stores user input context_column column stores"
  },
  {
    "filename": "amazon-bedrock.mdx",
    "path": "docs/integrations/ai-engines/amazon-bedrock.mdx",
    "chunk_id": 1,
    "chunk_content": "context user input prompt_template template prompt placeholders replaced user input max_tokens maximum number tokens generated models responses temperature likelihood model selecting higherprobability options generating response top_p percentage mostlikely candidates model considers next token stop list tokens model stop generating tip default conversational modes one following need provided prompt_template question_column optional context_column tip usage default mode default mode model generate separate response input provided context maintained inputs sql create model bedrock_default_model predict answer using engine bedrock_engine prompt_template answer users input helpful way question generate response single input following query used sql select bedrock_default_model question capital sweden response look like following question answer capital sweden capital sweden stockholm stockholm largest city sweden population 900000 people city proper 2 million metropolitan area known beautiful architecture scenic waterways rich cultural heritage city built 14 islands connected 50 bridges home many museums galleries historic landmarks famous attractions stockholm include vasa museum stockholm palace old town gamla stan generate responses multiple inputs following query used sql select filesunrelated_questions join bedrock_default_model response look like following question answer capital sweden capital sweden stockholm stockholm populated city sweden 975000 residents city known stunning architecture beautiful waterways second planet solar system second planet sun solar system venus venus often called"
  },
  {
    "filename": "amazon-bedrock.mdx",
    "path": "docs/integrations/ai-engines/amazon-bedrock.mdx",
    "chunk_id": 2,
    "chunk_content": "earths sister planet similar size mass density however two planets different atmospheres surface conditions venus thick toxic atmosphere composed carbon dioxide traps heat causes planet surface temperatures reach 471 degrees celsius 880 degrees fahrenheit venus also highly reflective cloud cover obscures surface making difficult study despite challenges venus subject numerous scientific missions including several orbiters landers provided valuable insights planets geology atmosphere climate tip filesunrelated_questions simple csv file containing question column expected model uploaded mindsdb however possible use supported data source manner tip conversational mode conversational mode model maintain context inputs generate single response response placed last row result set sql create model bedrock_conversational_model predict answer using engine bedrock_engine mode conversational question_column question syntax generating responses conversational mode default mode however generating responses multiple inputs difference two modes becomes apparent mentioned conversational mode maintains context inputs generates single response placed last row result set sql select filesrelated_questions join bedrock_default_model response look like question answer capital sweden null cool places visit capital sweden stockholm beautiful city lots old buildings scenic waterfront definitely visit royal palace largest palace scandinavia also visit vasa museum famous 17thcentury warship sank stockholm harbor definitely check abba museum dedicated famous pop group"
  },
  {
    "filename": "replicate-text2img.mdx",
    "path": "docs/integrations/ai-engines/replicate-text2img.mdx",
    "chunk_id": 0,
    "chunk_content": "title replicate text2img sidebartitle replicate text2img handler implemented using replicate library provided replicate required arguments establish connection model_name model name want access mindsdb eg airforeverkandinsky2 version version hashid want use mindsdb api_key api key replicate platform found herehttpsreplicatecomaccountapitokens note use replicate essential authenticate setting api token environment variable named replicate_api_token token acts key enable access replicates features 1 using pip youre working standard python environment using pip package management set token environment variable running following command terminal linux mac export replicate_api_tokenyour_token windows set replicate_api_tokenyour_token 2 using docker docker users process slightly differs need pass environment variable directly docker container running use command docker run e replicate_api_tokenyour_token p 4733447334 p 4733547335 mindsdbmindsdb replace your_token actual replicate api token note usage use handler connect replicate cluster mindsdb need account replicate make sure create account following linkhttpsreplicatecomsigninnextaccountapitokens establish connection create model mindsdb use following syntax sql create model aiforever predict url using engine replicate model_name aiforeverkandinsky2 version 2af375da21c5b824a84e1c459f45b69a117ec8649c2aa974112d7cf1840fc0ce api_key r8_bpo use describe predictor query see available parameters specify customize predictions sql describe predictor mindsdbaiforeverfeatures output sql inputs default description type width 512 choose width lower setting memory height 512 choose height lower setting memory prompt red cat 4k photo input prompt string scheduler"
  },
  {
    "filename": "replicate-text2img.mdx",
    "path": "docs/integrations/ai-engines/replicate-text2img.mdx",
    "chunk_id": 1,
    "chunk_content": "p_sampler choose scheduler batch_size 1 choose batch size lower setting memory prior_steps 5 string guidance_scale 4 scale classifierfree guidance number prior_cf_scale 4 integer num_inference_steps 50 number denoising steps integer use established connection query ml model follows sql select aiforever promptgreat warrior arjun mahabharata looking cameracinematic lighting 4k quality output generate_imageassetsintegrationsarjunapng important note predicted url work 24 hours prediction note replicate provides free predictions choose predictions wisely dont let machines fun save"
  },
  {
    "filename": "replicate-img2text.mdx",
    "path": "docs/integrations/ai-engines/replicate-img2text.mdx",
    "chunk_id": 0,
    "chunk_content": "title replicate img2text sidebartitle replicate img2text handler implemented using replicate library provided replicate required arguments establish connection model_name model name want access mindsdb eg airforeverkandinsky2 version version hashid want use mindsdb api_key api key replicate platform found herehttpsreplicatecomaccountapitokens note use replicate essential authenticate setting api token environment variable named replicate_api_token token acts key enable access replicates features 1 using pip youre working standard python environment using pip package management set token environment variable running following command terminal linux mac export replicate_api_tokenyour_token windows set replicate_api_tokenyour_token 2 using docker docker users process slightly differs need pass environment variable directly docker container running use command docker run e replicate_api_tokenyour_token p 4733447334 p 4733547335 mindsdbmindsdb replace your_token actual replicate api token note usage use handler connect replicate cluster mindsdb need account replicate make sure create account following linkhttpsreplicatecomsigninnextaccountapitokens establish connection create model mindsdb use following syntax sql create model blip predict text using engine replicate model_name salesforceblip version 2e1dddc8621f72155f24cf2e0adbde548458d3cab9f00c0139eea840d0ac4746 api_key r8_bpo use describe predictor query see available parameters specify customize predictions sql describe predictor mindsdbblipfeatures output sql inputs default description type task image_captioning choose task image input image string caption type caption input image image text matching task string question type question input image"
  },
  {
    "filename": "replicate-img2text.mdx",
    "path": "docs/integrations/ai-engines/replicate-img2text.mdx",
    "chunk_id": 1,
    "chunk_content": "visual question answering task string visual question answering use established connection query ml model follows sql select mindsdbblip imagehttpsimagesunsplashcomphoto1686847266385a32745169de4 questionis lion image using taskvisual_question_answering output sql text image question answer httpsimagesunsplashcomphoto1686847266385a32745169de4 lion image image captioning sql select mindsdbblip imagehttpsimagesunsplashcomphoto1686847266385a32745169de4 output sql text image caption bird sitting back horse httpsimagesunsplashcomphoto1686847266385a32745169de4 image text matching sql select mindsdbblip imagehttpsimagesunsplashcomphoto1686847266385a32745169de4 captionbird horse riding using taskimage_text_matching output sql text image caption image text matched probability 07730 image feature text feature cosine similarity 03615 httpsimagesunsplashcomphoto1686847266385a32745169de4 bird horse riding one model used example vast variation use cases also limit imagination use note replicate provides free predictions choose predictions wisely dont let machines fun save"
  },
  {
    "filename": "apache-impala.mdx",
    "path": "docs/integrations/data-integrations/apache-impala.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache impala sidebartitle apache impala implementation impala data handler mindsdb apache impalahttpsimpalaapacheorg mpp massive parallel processing sql query engine processing huge volumes data stored apache hadoop cluster open source software written c java provides high performance low latency compared sql engines hadoop words impala highest performing sql engine giving rdbmslike experience provides fastest way access data stored hadoop distributed file system prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache impala mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache impala implementation handler implemented using impyla python library allows use python code run sql commands impala required arguments establish connection user username associated database password password authenticate access host server ip address hostname port port tcpip connection made database database name connected usage order make use handler connect impala database mindsdb following syntax used sql create database impala_datasource engine impala parameters userroot passwordp55w0rd host127001 port21050 databasedb_name use established connection query table follows sql select impala_datasourcetest"
  },
  {
    "filename": "apache-hive.mdx",
    "path": "docs/integrations/data-integrations/apache-hive.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache hive sidebartitle apache hive documentation describes integration mindsdb apache hivehttpshiveapacheorg data warehouse software project built top apache hadoop providing data query analysis hive gives sqllike interface query data stored various databases file systems integrate hadoop integration allows mindsdb access data apache hive enhance apache hive ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect apache hive mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection apache hive mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershive_handler engine sql create database hive_datasource engine hive parameters username demo_user password demo_password host 127001 database default required connection parameters include following host hostname ip address url apache hive server database name apache hive database connect optional connection parameters include following username username apache hive database password password apache hive database port port number connecting apache hive server default 10000 auth authentication mechanism use default custom options none nosasl kerberos ldap usage retrieve data specified table providing integration table names sql select hive_datasourcetable_name limit 10 run hiveql queries directly connected apache hive database sql select hive_datasource native query goes src select transformvalue using mapper value count mapped select castvalue double value castcount int count sort"
  },
  {
    "filename": "apache-hive.mdx",
    "path": "docs/integrations/data-integrations/apache-hive.mdx",
    "chunk_id": 1,
    "chunk_content": "value count sorted select transformvalue count using reducer whatever note examples utilize hive_datasource datasource name defined create database command note troubleshooting warning database connection error symptoms failure connect mindsdb apache hive database checklist 1 ensure apache hive server running accessible 2 confirm host port user password correct try direct apache hive connection using client like dbeaver 3 test network connection mindsdb host apache hive server warning"
  },
  {
    "filename": "firebird.mdx",
    "path": "docs/integrations/data-integrations/firebird.mdx",
    "chunk_id": 0,
    "chunk_content": "title firebird sidebartitle firebird implementation firebird data handler mindsdb firebirdhttpsfirebirdsqlorgenaboutfirebird relational database offering many ansi sql standard features runs linux windows variety unix platforms firebird offers excellent concurrency high performance powerful language support stored procedures triggers used production systems variety names since 1981 prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect firebird mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access firebird implementation handler implemented using fdb library python driver firebird required arguments establish connection follows host host name ip address firebird server database port use connecting firebird server user username authenticate user firebird server password password authenticate user firebird server usage order make use handler connect firebird server mindsdb following syntax used sql create database firebird_datasource engine firebird parameters host localhost database cusersminuradocumentsmindsdbexamplefdb user sysdba password password use established connection query table follows sql select firebird_datasourceexample_tbl"
  },
  {
    "filename": "amazon-redshift.mdx",
    "path": "docs/integrations/data-integrations/amazon-redshift.mdx",
    "chunk_id": 0,
    "chunk_content": "title amazon redshift sidebartitle amazon redshift documentation describes integration mindsdb amazon redshifthttpsdocsawsamazoncomredshiftlatestmgmtwelcomehtml fully managed petabytescale data warehouse service cloud start hundred gigabytes data scale petabyte enabling use data acquire new insights business customers prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect redshift mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection redshift database mindsdb executing following sql command sql create database redshift_datasource engine redshift parameters host exampleclusterabc123xyz789uswest1redshiftamazonawscom port 5439 database example_db user awsuser password my_password required connection parameters include following host host name ip address redshift cluster port port use connecting redshift cluster database database name use connecting redshift cluster user username authenticate user redshift cluster password password authenticate user redshift cluster optional connection parameters include following schema database schema use default public sslmode ssl mode connection usage retrieve data specified table providing integration name schema table name sql select redshift_datasourceschema_nametable_name limit 10 run amazon redshift sql queries directly connected redshift database sql select redshift_datasource native query goes venuecopy select venue select venuecopy order 1 limit 10 note examples utilize redshift_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb amazon redshift cluster checklist"
  },
  {
    "filename": "amazon-redshift.mdx",
    "path": "docs/integrations/data-integrations/amazon-redshift.mdx",
    "chunk_id": 1,
    "chunk_content": "1 make sure redshift cluster active 2 confirm host port user password database correct try direct redshift connection using client like dbeaver 3 ensure security settings redshift cluster allow connections mindsdb 4 ensure stable network mindsdb redshift warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning troubleshooting guidehttpsdocsawsamazoncomredshiftlatestmgmttroubleshootingconnectionshtml provided aws might also helpful"
  },
  {
    "filename": "questdb.mdx",
    "path": "docs/integrations/data-integrations/questdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title questdb sidebartitle questdb implementation questdb data handler mindsdb questdbhttpsquestdbio columnar timeseries database high performance ingestion sql analytics opensource available cloud prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect questdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access questdb implementation handler implemented extending postgresql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name public stores value true false defaults true left blank usage order make use handler connect questdb server mindsdb following syntax used sql create database questdb_datasource engine questdb parameters host 127001 port 8812 database qdb user admin password password use established connection query table follows sql select questdb_datasourcedemo_table limit 10"
  },
  {
    "filename": "apache-solr.mdx",
    "path": "docs/integrations/data-integrations/apache-solr.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache solr sidebartitle apache solr implementation solr data handler mindsdb apache solrhttpssolrapacheorg highly reliable scalable fault tolerant providing distributed indexing replication loadbalanced querying automated failover recovery centralized configuration prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache solr mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache solr implementation handler implemented using sqlalchemysolr library provides pythonsqlalchemy interface required arguments establish connection follows username username used authenticate solr server parameter optional password password authenticate user solr server parameter optional host host name ip address solr server port port number solr server server_path defaults solr provided collection solr collection name use_ssl defaults false provided tip reference httpspypiorgprojectsqlalchemysolrhttpspypiorgprojectsqlalchemysolr tip usage order make use handler connect solr database mindsdb following syntax used sql create database solr_datasource engine solr parameters username demo_user password demo_password host 127001 port 8981 server_path solr collection gettingstarted use_ssl false use established connection query table follows sql select solr_datasourcegettingstarted limit 10000 info requirements solr instance parallel sql supported running certain limitations need taken account issuing queries solr refer httpssolrapacheorgguidesolrlatestqueryguidesqlqueryhtmlparallelsqlquerieshttpssolrapacheorgguidesolrlatestqueryguidesqlqueryhtmlparallelsqlqueries info tip dont forget put limit end sql statement tip"
  },
  {
    "filename": "teradata.mdx",
    "path": "docs/integrations/data-integrations/teradata.mdx",
    "chunk_id": 0,
    "chunk_content": "title teradata sidebartitle teradata documentation describes integration mindsdb teradatahttpswwwteradatacomwhyteradata complete cloud analytics data platform trusted ai integration allows mindsdb access data teradata enhance teradata ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect teradata mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection teradata mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersteradata_handler engine sql create database teradata_datasource engine teradata parameters host 192168041 user demo_user password demo_password database example_db required connection parameters include following host hostname ip address url teradata server user username teradata database password password teradata database optional connection parameters include following database name teradata database connect defaults users default database usage retrieve data specified table providing integration database table names sql select teradata_datasourcedatabase_nametable_name limit 10 run teradata sql queries directly connected teradata database sql select teradata_datasource native query goes select emp_id emp_name job_duration tsp employee expand job_duration tsp interval 1 year perioddate 20060101 date 20080101 note examples utilize teradata_datasource datasource name defined create database command note troubleshooting warning database connection error symptoms failure connect mindsdb teradata database checklist 1 make sure teradata database active 2 confirm host user password correct try direct connection using client like dbeaver 3 ensure"
  },
  {
    "filename": "teradata.mdx",
    "path": "docs/integrations/data-integrations/teradata.mdx",
    "chunk_id": 1,
    "chunk_content": "stable network mindsdb teradata warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning warning connection timeout error symptoms connection teradata database times queries take long execute checklist 1 ensure teradata server running accessible server idle long time may shut automatically warning"
  },
  {
    "filename": "duckdb.mdx",
    "path": "docs/integrations/data-integrations/duckdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title duckdb sidebartitle duckdb implementation duckdb data handler mindsdb duckdbhttpsduckdborg opensource analytical database system designed fast execution analytical queries external dependencies dbms runs completely embedded within host process similar sqlite duckdb provides rich sql dialect support complex queries transactional guarantees acid prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect duckdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access duckdb implementation handler implemented using duckdb python client library tip duckdb handler currently using 071dev187 prerelase version python client library case issues make sure duckdb database compatible version see requirementstxthttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersduckdb_handlerrequirementstxt details tip required arguments establish connection follows database name duckdb database file set memory create inmemory database optional arguments follows read_only flag specifies whether connection readonly mode required multiple processes want access database file time usage order make use handler connect duckdb database mindsdb following syntax used sql create database duckdb_datasource engine duckdb parameters database dbduckdb use established connection query table follows sql select duckdb_datasourcemy_table"
  },
  {
    "filename": "databend.mdx",
    "path": "docs/integrations/data-integrations/databend.mdx",
    "chunk_id": 0,
    "chunk_content": "title databend sidebartitle databend implementation databend data handler mindsdb databendhttpsdatabendrs modern cloud data warehouse empowers object storage realtime analytics prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect databend mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access databend implementation handler implemented extending clickhouse handler required arguments establish connection follows protocol protocol query databend supported values include native http https defaults native provided host host name ip address databend warehouse port tcpip port databend warehouse user username used authenticate databend warehouse password password authenticate user databend warehouse database database name use connecting databend warehouse usage order make use handler connect databend database mindsdb following syntax used sql create database databend_datasource engine databend parameters protocol https user root port 443 password password host someurlawsuseast2defaultdatabendcom database test_db use established connection query table follows sql select databend_datasourceexample_tbl"
  },
  {
    "filename": "postgresql.mdx",
    "path": "docs/integrations/data-integrations/postgresql.mdx",
    "chunk_id": 0,
    "chunk_content": "title postgresql sidebartitle postgresql documentation describes integration mindsdb postgresqlhttpswwwpostgresqlorg powerful opensource objectrelational database system integration allows mindsdb access data stored postgresql database enhance postgresql ai capabilities tip data source integration threadsafe utilizing connection pool thread assigned connection handling requests parallel threads retrieve connections pool needed tip prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect postgresql mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection postgresql database mindsdb executing following sql command sql create database postgresql_conn engine postgres parameters host 127001 port 5432 database postgres user postgres schema data password password required connection parameters include following user username postgresql database password password postgresql database host hostname ip address url postgresql server port port number connecting postgresql server database name postgresql database connect optional connection parameters include following schema database schema use default public sslmode ssl mode connection usage following usage examples utilize connection postgresql made via create database statement named postgresql_conn retrieve data specified table providing integration name schema table name sql select postgresql_conntable_name limit 10 run postgresqlnative queries directly connected postgresql database sql select postgresql_conn native query goes select model count partition model year units_to_sell roundcasttax decimal price 3 tax_div_price used_car_price tip"
  },
  {
    "filename": "postgresql.mdx",
    "path": "docs/integrations/data-integrations/postgresql.mdx",
    "chunk_id": 1,
    "chunk_content": "next steps follow tutorialhttpsdocsmindsdbcomusecasespredictive_analyticshousesalesforecasting see use case examples tip troubleshooting warning database connection error symptoms failure connect mindsdb postgresql database checklist 1 make sure postgresql server active 2 confirm host port user schema password correct try direct postgresql connection 3 ensure stable network mindsdb postgresql warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning"
  },
  {
    "filename": "oracle.mdx",
    "path": "docs/integrations/data-integrations/oracle.mdx",
    "chunk_id": 0,
    "chunk_content": "title oracle sidebartitle oracle documentation describes integration mindsdb oraclehttpswwwtechopediacomdefinition8711oracledatabase one trusted widely used relational database engines storing organizing retrieving data type still maintaining relationships various types prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect oracle mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection oracle database mindsdb executing following sql command sql create database oracle_datasource engine oracle parameters host localhost service_name freepdb1 user system password password required connection parameters include following user username oracle database password password oracle database dsn data source name dsn oracle database host hostname ip address url oracle server sid system identifier sid oracle database service_name service name oracle database optional connection parameters include following port port number connecting oracle database default 1521 disable_oob boolean parameter disable outofband breaks default false auth_mode authorization mode use usage retrieve data specified table providing integration name schema table name sql select oracle_datasourceschema_nametable_name limit 10 run plsql queries directly connected oracle database sql select oracle_datasource native query goes select employee_id first_name last_name email hire_date oracle_datasourcehremployees department_id 10 order hire_date desc note examples utilize oracle_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb oracle"
  },
  {
    "filename": "oracle.mdx",
    "path": "docs/integrations/data-integrations/oracle.mdx",
    "chunk_id": 1,
    "chunk_content": "database checklist 1 make sure oracle database active 2 confirm connection parameters provided dsn host sid service_name credentials user password correct 3 ensure stable network mindsdb oracle warning troubleshooting guidehttpsdocsoraclecomendatabaseoracleoracledatabase19ntqrfdatabaseconnectionissueshtml provided oracle might also helpful"
  },
  {
    "filename": "microsoft-sql-server.mdx",
    "path": "docs/integrations/data-integrations/microsoft-sql-server.mdx",
    "chunk_id": 0,
    "chunk_content": "title microsoft sql server sidebartitle microsoft sql server documentation describes integration mindsdb microsoft sql server relational database management system developed microsoft integration allows advanced sql functionalities extending microsoft sql servers capabilities mindsdbs features prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect microsoft sql server mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection microsoft sql server database mindsdb executing following sql command sql create database mssql_datasource engine mssql parameters host 127001 port 1433 user sa password password database master required connection parameters include following user username microsoft sql server password password microsoft sql server host hostname ip address url microsoft sql server database name microsoft sql server database connect optional connection parameters include following port port number connecting microsoft sql server default 1433 server server name connect typically used named instances azure sql database usage retrieve data specified table providing integration name schema table name sql select mssql_datasourceschema_nametable_name limit 10 run tsql queries directly connected microsoft sql server database sql select mssql_datasource native query goes select sumorderqty total product p join salesorderdetail sd pproductid sdproductid join salesorderheader sh sdsalesorderid shsalesorderid join customer c shcustomerid ccustomerid name racing socks l companyname riding cycles"
  },
  {
    "filename": "microsoft-sql-server.mdx",
    "path": "docs/integrations/data-integrations/microsoft-sql-server.mdx",
    "chunk_id": 1,
    "chunk_content": "note examples utilize mssql_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb microsoft sql server database checklist 1 make sure microsoft sql server active 2 confirm host port user password correct try direct microsoft sql server connection using client like sql server management studio dbeaver 3 ensure stable network mindsdb microsoft sql server warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning"
  },
  {
    "filename": "oceanbase.mdx",
    "path": "docs/integrations/data-integrations/oceanbase.mdx",
    "chunk_id": 0,
    "chunk_content": "title oceanbase sidebartitle oceanbase implementation oceanbase data handler mindsdb oceanbase distributed relational database distributed database world broken tpcc tpch records oceanbase adopts independently developed integrated architecture encompasses scalability distributed architecture performance advantage centralized architecture supports hybrid transactionanalytical processing htap one engine features include strong data consistency high availability high performance online scalability high compatibility sql mainstream relational databases transparency applications high costperformance ratio prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect oceanbase mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access oceanbase implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect oceanbase server mindsdb following syntax used sql create database oceanbase_datasource engine oceanbase parameters host 127001 user oceanbase_user password password port 2881 database oceanbase_db use established connection query database follows sql select oceanbase_datasourcedemo_table limit 10"
  },
  {
    "filename": "starrocks.mdx",
    "path": "docs/integrations/data-integrations/starrocks.mdx",
    "chunk_id": 0,
    "chunk_content": "title starrocks sidebartitle starrocks implementation starrocks data handler mindsdb starrockshttpswwwstarrocksio nextgeneration data platform designed make dataintensive realtime analytics fast easy delivers query speeds 5 10 times faster popular solutions starrocks perform realtime analytics well updating historical records also enhance realtime analytics historical data data lakes easily starrocks get rid denormalized tables get best performance flexibility prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect starrocks mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access starrocks implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect starrocks server mindsdb following syntax used sql create database starrocks_datasource engine starrocks parameters host 127001 user starrocks_user password password port 8030 database starrocks_db use established connection query table follows sql select starrocks_datasourcedemo_table limit 10"
  },
  {
    "filename": "orioledb.mdx",
    "path": "docs/integrations/data-integrations/orioledb.mdx",
    "chunk_id": 0,
    "chunk_content": "title orioledb sidebartitle orioledb implementation orioledb data handler mindsdb orioledbhttpswwworioledatacom new storage engine postgresql bringing modern approach database capacity capabilities performance worlds mostloved database platform consists extension building innovative table access method framework standard postgres extension interfaces extending enhancing current table access methods orioledb opens door future powerful storage models optimized cloud modern hardware architectures prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect orioledb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access orioledb implementation handler implemented extending postgresql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection server orioledb server database database name usage order make use handler connect orioledb server mindsdb following syntax used sql create database orioledb_datasource engine orioledb parameters user orioledb_user password password host 127001 port 55505 server server_name database oriole_db use established connection query table follows sql select orioledb_datademo_table limit 10"
  },
  {
    "filename": "singlestore.mdx",
    "path": "docs/integrations/data-integrations/singlestore.mdx",
    "chunk_id": 0,
    "chunk_content": "title singlestore sidebartitle singlestore implementation singlestore data handler mindsdb singlestorehttpswwwsinglestorecom proprietary cloudnative database designed dataintensive applications distributed relational sql database management system features ansi sql support known speed data ingest transaction processing query processing prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect singlestore mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access singlestore implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name several optional arguments used well ssl ssl parameter value indicates whether ssl enabled true disabled false ssl_ca ssl certificate authority ssl_cert stores ssl certificates ssl_key stores ssl keys usage order make use handler connect singlestore database mindsdb following syntax used sql create database singlestore_datasource engine singlestore parameters host 127001 port 3306 database singlestore user root password password use established connection query table follows sql select singlestore_datasourceexample_table"
  },
  {
    "filename": "matrixone.mdx",
    "path": "docs/integrations/data-integrations/matrixone.mdx",
    "chunk_id": 0,
    "chunk_content": "title matrixone sidebartitle matrixone implementation matrixone data handler mindsdb matrixonehttpsgithubcommatrixoriginmatrixone futureoriented hyperconverged cloud edge native dbms supports transactional analytical streaming workloads simplified distributed database engine across multiple data centers clouds edges heterogeneous infrastructures prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect matrixone mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access matrixone implementation handler implemented using pymysql python library allows use python code run sql commands matrixone database required arguments establish connection follows user username associated database password password authenticate access host hostname ip address database port port tcpip connection made database database name connected several optional arguments used well ssl indicates whether ssl enabled true disabled false ssl_ca ssl certificate authority ssl_cert stores ssl certificates ssl_key stores ssl keys usage order make use handler connect matrixone database mindsdb following syntax used sql create database matrixone_datasource engine matrixone parameters user dump password 111 host 127001 port 6001 database mo_catalog use established connection query table follows sql select matrixone_datasourcedemo"
  },
  {
    "filename": "amazon-dynamodb.mdx",
    "path": "docs/integrations/data-integrations/amazon-dynamodb.mdx",
    "chunk_id": 0,
    "chunk_content": "title amazon dynamodb sidebartitle amazon dynamodb documentation describes integration mindsdb amazon dynamodbhttpsawsamazoncomdynamodb serverless nosql database service enables develop modern applications scale tip data source integration threadsafe utilizing connection pool thread assigned connection handling requests parallel threads retrieve connections pool needed tip prerequisites proceeding ensure mindsdb installed locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connection establish connection amazon dynamodb mindsdb executing following sql command sql create database dynamodb_datasource engine dynamodb parameters aws_access_key_id pcaq2ljdoswlnsqkocpw aws_secret_access_key uvjewplnopsdmmwitl34r2neyc6whzpuiip57i region_name useast1 required connection parameters include following aws_access_key_id aws access key identifies user iam role aws_secret_access_key aws secret access key identifies user iam role region_name aws region connect optional connection parameters include following aws_session_token aws session token identifies user iam role becomes necessary using temporary security credentials usage retrieve data specified table providing integration name table name sql select dynamodb_datasourcetable_name limit 10 indexes also queried adding thirdlevel namespace sql select dynamodb_datasourcetable_nameindex_name limit 10 tip queries issued amazon dynamodb partiql sqlcompatible query language amazon dynamodb information refer partiql documentationhttpsdocsawsamazoncomamazondynamodblatestdeveloperguideqlreferencehtml limitations keep mind querying data amazon dynamodb specific partiql limit group clauses supported partiql select statements furthermore subqueries joins supported either refer partiql documentation select statementshttpsdocsawsamazoncomamazondynamodblatestdeveloperguideqlreferenceselecthtml information insert statements supported integration however overcome issuing native query via established connection example"
  },
  {
    "filename": "amazon-dynamodb.mdx",
    "path": "docs/integrations/data-integrations/amazon-dynamodb.mdx",
    "chunk_id": 1,
    "chunk_content": "provided tip run partiql queries directly amazon dynamodb sql select dynamodb_datasource native query goes insert music value artist acme band1songtitle partiql rocks note examples utilize dynamodb_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb amazon s3 dynamodb checklist 1 confirm provided aws credentials correct try making direct connection amazon dynamodb using aws cli 2 ensure stable network mindsdb aws warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing special characters checklist 1 ensure table names special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning"
  },
  {
    "filename": "sap-sql-anywhere.mdx",
    "path": "docs/integrations/data-integrations/sap-sql-anywhere.mdx",
    "chunk_id": 0,
    "chunk_content": "title sap sql anywhere sidebartitle sap sql anywhere implementation sap sql anywhere data handler mindsdb sap sql anywherehttpswwwsapcomproductstechnologyplatformsqlanywherehtml embedded database application software enables secure reliable data management servers dba available synchronization tens thousands mobile devices internet things iot systems remote environments prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect sap sql anywhere mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access sap sql anywhere implementation handler implemented using sqlanydb python driver sap sql anywhere required arguments establish connection follows host host name ip address sap sql anywhere instance port port number sap sql anywhere instance user specifies user name password specifies password user database sets current database server sets current server usage use sql statements create table sap sql anywhere called test sql create table test id integer null name nvarchar1 description nvarchar1 create unique index test_id_index test id alter table test add constraint test_pk primary key id insert test values 1 h w order make use handler connect sap sql anywhere database mindsdb following syntax used sql create database sap_sqlany_trial engine sqlany parameters user dbadmin password password host localhost port 55505 server testme database mindsdb use established connection query table"
  },
  {
    "filename": "sap-sql-anywhere.mdx",
    "path": "docs/integrations/data-integrations/sap-sql-anywhere.mdx",
    "chunk_id": 1,
    "chunk_content": "follows sql select sap_sqlany_trialtest execution get id name description 1 h w"
  },
  {
    "filename": "apache-cassandra.mdx",
    "path": "docs/integrations/data-integrations/apache-cassandra.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache cassandra sidebartitle apache cassandra implementation cassandra data handler mindsdb cassandrahttpscassandraapacheorg_indexhtml free opensource distributed widecolumn store nosql database management system designed handle large amounts data across many commodity servers providing high availability single point failure prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache cassandra mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache cassandra implementation scylladb apicompatible apache cassandra cassandra data handler extends scylladb handler uses scylladriver python library required arguments establish connection follows host host name ip address cassandra database port port use connecting user user authenticate password password authenticate user keyspace keyspace connect top level container tables protocol_version required defaults 4 usage order make use handler connect cassandra server mindsdb following syntax used sql create database sc engine cassandra parameters host 127001 port 9043 user user password pass keyspace test_data protocol_version 4 use established connection query table follows sql select cassandra_datasourceexample_table limit 10"
  },
  {
    "filename": "timescaledb.mdx",
    "path": "docs/integrations/data-integrations/timescaledb.mdx",
    "chunk_id": 0,
    "chunk_content": "title timescaledb sidebartitle timescaledb implementation timescaledb data handler mindsdb timescaledbhttpsdocstimescalecom opensource relational database optimized timeseries data designed handle large volumes data enables query analyze data realtime timescaledb used wide range applications including iot finance monitoring prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect timescaledb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access timescaledb implementation handler implemented using psycopg2 library postgresql adapter python programming language timescaledb built top postgresql therefore accessed using client libraries apis required arguments establish connection follows host host name ip address timescaledb server port port use connecting timescaledb server database database name use connecting timescaledb server user user authenticate user timescaledb server password password authenticate user timescaledb server usage attempting connect timescaledb server using mindsdb ensure accepts incoming connections using guidehttpsdocstimescalecomlatestgettingstartedsetupremoteconnections order make use handler connect timescaledb server mindsdb following syntax used sql create database timescaledb_datasource engine timescaledb parameters host examplehosttimescaledbcom port 5432 user example_user password my_password database tsdb use established connection query table follows sql select timescaledb_datasourcesensor"
  },
  {
    "filename": "surrealdb.mdx",
    "path": "docs/integrations/data-integrations/surrealdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title surrealdb sidebartitle surrealdb implementation surrealdb data handler mindsdb surrealdbhttpssurrealdbcom innovative newsql cloud database suitable serverless applications jamstack applications singlepage applications traditional applications prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect surrealdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access surrealdb implementation handler implemented using python library pysurrealdb required arguments establish connection host host name surrealdb connection port port use connecting user user authenticate password password authenticate user database database name connected namespace namespace name connected usage establish connection surrealdb server running locally public cloud instance going use ngrok tunneling connect cloud instance local surrealdb server follow guidehttpsdocsmindsdbcomsqlcreatedatabasemakingyourlocaldatabaseavailabletomindsdb lets make connection mindsdb public cloud sql create database exampledb engine surrealdb parameters host 6tcpngrokio port 17141 user root password root database testdb namespace testns please change host port properties parameters clause based values got also query dev table created sql select exampledbdev"
  },
  {
    "filename": "planetscale.mdx",
    "path": "docs/integrations/data-integrations/planetscale.mdx",
    "chunk_id": 0,
    "chunk_content": "title planetscale sidebartitle planetscale implementation planetscale data handler mindsdb planetscalehttpsplanetscalecom mysqlcompatible serverless database platform prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect planetscale mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access planetscale implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect planetscale database mindsdb following syntax used sql create database planetscale_datasource engine planet_scale parameters host 127001 port 3306 user planetscale_user password password database planetscale_db use established connection query table follows sql select planetscale_datasourcemy_table"
  },
  {
    "filename": "sqlite.mdx",
    "path": "docs/integrations/data-integrations/sqlite.mdx",
    "chunk_id": 0,
    "chunk_content": "title sqlite sidebartitle sqlite implementation sqlite data handler mindsdb sqlitehttpswwwsqliteorgabouthtml inprocess library implements selfcontained serverless zeroconfiguration transactional sql database engine code sqlite public domain thus free use either commercial private purpose sqlite widely deployed database world applications count including several highprofile projects prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect sqlite mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access sqlite implementation handler implemented using standard sqlite3 library comes python required argument establish connection db_file points database file connection made optionally may also set memory create inmemory database usage order make use handler connect sqlite database mindsdb following syntax used sql create database sqlite_datasource engine sqlite parameters db_file exampledb use established connection query table follows sql select sqlite_datasourceexample_tbl"
  },
  {
    "filename": "couchbase.mdx",
    "path": "docs/integrations/data-integrations/couchbase.mdx",
    "chunk_id": 0,
    "chunk_content": "title couchbase sidebartitle couchbase implementation couchbase data handler mindsdb couchbasehttpswwwcouchbasecom opensource distributed multimodel nosql documentoriented database software package optimized interactive applications applications may serve many concurrent users creating storing retrieving aggregating manipulating presenting data prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect couchbase mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access couchbase implementation handler implemented using couchbase library python driver couchbase required arguments establish connection follows connection_string connection string endpoint couchbase server bucket bucket name use connecting couchbase server user user authenticate couchbase server password password authenticate user couchbase server scope scopes level data organization within bucket omitted default _default note connection string expects either couchbases couchbase protocol tip using couchbase capella find connection_string connect tab also required whitelist machines running mindsdb database credentials need created user steps also taken connect tab tip order make use handler connect couchbase server mindsdb following syntax used note example uses default travelsample bucket enabled couchbase ui predefined scope documents sql create database couchbase_datasource enginecouchbase parameters connection_string couchbaselocalhost bucket travelsample user admin password password scope inventory usage use established connection query database follows sql select couchbase_datasourceairport"
  },
  {
    "filename": "cloud-spanner.mdx",
    "path": "docs/integrations/data-integrations/cloud-spanner.mdx",
    "chunk_id": 0,
    "chunk_content": "title cloud spanner sidebartitle cloud spanner implementation cloud spanner data handler mindsdb cloud spannerhttpscloudgooglecomspanner fully managed missioncritical relational database service offers transactional consistency global scale automatic synchronous replication high availability supports two sql dialects googlesql ansi 2011 extensions postgresql prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect cloud spanner mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access cloud spanner implementation handler implemented using googlecloudspanner python client library required arguments establish connection follows instance_id instance identifier database_id database identifier project identifier project owns resources credentials stringified gcp service account key json usage order make use handler connect cloud spanner database mindsdb following syntax used sql create database cloud_spanner_datasource engine cloud_spanner parameters instance_id myinstance database_id exampleid project myproject credentials use established connection query table follows sql select cloud_spanner_datasourcemy_table note cloud spanner supports postgresql googlesql dialects however postgressql features supported note"
  },
  {
    "filename": "snowflake.mdx",
    "path": "docs/integrations/data-integrations/snowflake.mdx",
    "chunk_id": 0,
    "chunk_content": "title snowflake sidebartitle snowflake documentation describes integration mindsdb snowflakehttpswwwsnowflakecomen cloud data warehouse used store analyze data integration allows mindsdb access data stored snowflake database enhance ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect snowflake mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection snowflake database mindsdb executing following sql command sql create database snowflake_datasource engine snowflake parameters account tvuibdyvm85921 user user password password database test_db required connection parameters include following account snowflake account identifier guidehttpsdocssnowflakecomenuserguideadminaccountidentifier help find account identifier user username snowflake account password password snowflake account database name snowflake database connect optional connection parameters include following warehouse snowflake warehouse use running queries schema database schema use within snowflake database default public role snowflake role use tip video presents connect snowflake query available tables iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedw9ercebq4y0 titleconnect query snowflake frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe tip usage retrieve data specified table providing integration name schema table name sql select snowflake_datasourceschema_nametable_name limit 10 run snowflake sql queries directly connected snowflake database sql select snowflake_datasource native query goes select employee_table exclude department_id department_table rename department_name department employee_table inner join department_table employee_tabledepartment_id department_tabledepartment_id order department last_name"
  },
  {
    "filename": "snowflake.mdx",
    "path": "docs/integrations/data-integrations/snowflake.mdx",
    "chunk_id": 1,
    "chunk_content": "first_name note examples utilize snowflake_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb snowflake account checklist 1 make sure snowflake active 2 confirm account user password database correct try direct snowflake connection using client like dbeaver 3 ensure stable network mindsdb snowflake warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning troubleshooting guidehttpscommunitysnowflakecomsarticlesnowflakeclientconnectivitytroubleshooting provided snowflake might also helpful"
  },
  {
    "filename": "ibm-db2.mdx",
    "path": "docs/integrations/data-integrations/ibm-db2.mdx",
    "chunk_id": 0,
    "chunk_content": "title ibm db2 sidebartitle ibm db2 documentation describes integration mindsdb ibm db2httpswwwibmcomdb2 cloudnative database built power lowlatency transactions realtime analytics ai applications scale integration allows mindsdb access data stored ibm db2 database enhance ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect ibm db2 mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection ibm db2 database mindsdb executing following sql command sql create database db2_datasource engine db2 parameters host 127001 user db2inst1 password password database example_db required connection parameters include following host hostname ip address url ibm db2 database user username ibm db2 database password password ibm db2 database database name ibm db2 database connect optional connection parameters include following port port number connecting ibm db2 database default 50000 schema database schema use within ibm db2 database usage retrieve data specified table providing integration name schema table name sql select db2_datasourceschema_nametable_name limit 10 run ibm db2 native queries directly connected database sql select db2_datasource native query goes dinfo deptno avgsalary empcount select othersworkdept avgotherssalary count employee others group othersworkdept dinfomax select maxavgsalary avgmax dinfo select this_empempno this_empsalary dinfoavgsalary dinfoempcount dinfomaxavgmax employee this_emp dinfo dinfomax this_empjob salesrep this_empworkdept dinfodeptno note examples utilize"
  },
  {
    "filename": "ibm-db2.mdx",
    "path": "docs/integrations/data-integrations/ibm-db2.mdx",
    "chunk_id": 1,
    "chunk_content": "db2_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb ibm db2 database checklist 1 make sure ibm db2 database active 2 confirm host user password database correct try direct connection using client like dbeaver 3 ensure stable network mindsdb ibm db2 database warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning guidehttpswwwibmcomdocsendb2115topicconnectcommondb2problems common connection db2 connection issues provided ibm might also helpful"
  },
  {
    "filename": "elasticsearch.mdx",
    "path": "docs/integrations/data-integrations/elasticsearch.mdx",
    "chunk_id": 0,
    "chunk_content": "title elasticsearch sidebartitle elasticsearch documentation describes integration mindsdb elasticsearchhttpswwwelasticco distributed multitenantcapable fulltext search engine http web interface schemafree json documents integration allows mindsdb access data elasticsearch enhance elasticsearch ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect elasticsearch mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access elasticsearch connection establish connection elasticsearch mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerselasticsearch_handler engine sql create database elasticsearch_datasource engine elasticsearch parameters cloud_id xyz optional hosts provided hosts httpsxyzxyzgcpcloudesio123 optional cloud_id provided api_key xyz optional user password provided user elastic optional api_key provided password xyz optional api_key provided connection parameters include following cloud_id cloud id provided elasticsearch deployment required hosts provided hosts elasticsearch endpoint provided elasticsearch deployment required cloud_id provided api_key api key generated elasticsearch deployment required user password provided user password user password used authenticate required api_key provided tip want connect local instance elasticsearch use statement sql create database elasticsearch_datasource engine elasticsearch parameters hosts 1270019200 user user password password required connection parameters include following least one parameters provided hosts ip address port elasticsearch deployed user user used autheticate access password password used autheticate access tip usage retrieve data specified index providing integration name"
  },
  {
    "filename": "elasticsearch.mdx",
    "path": "docs/integrations/data-integrations/elasticsearch.mdx",
    "chunk_id": 1,
    "chunk_content": "index name sql select elasticsearch_datasourcemy_index limit 10 note examples utilize elasticsearch_datasource datasource name defined create database command note tip moment elasticsearch sql api certain limitations impact queries issued via mindsdb notable limitations listed 1 select queries supported moment 2 array fields supported 3 nested fields queried directly however accessed using operator detailed guide limitations elasticsearch sql api refer official documentationhttpswwwelasticcoguideenelasticsearchreferencecurrentsqllimitationshtml tip troubleshooting guide warning database connection error symptoms failure connect mindsdb elasticsearch server checklist 1 make sure elasticsearch server active 2 confirm server cloud id credentials correct 3 ensure stable network mindsdb elasticsearch warning warning transport error request error symptoms errors related issuing unsupported queries elasticsearch checklist 1 ensure query select query 2 avoid querying array fields 3 access nested fields using operator 4 refer official documentationhttpswwwelasticcoguideenelasticsearchreferencecurrentsqllimitationshtml information needed warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing index names containing special characters checklist 1 ensure table names special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning troubleshooting guidehttpswwwelasticcoguideenelasticsearchreferencecurrenttroubleshootinghtml provided elasticsearch might also helpful"
  },
  {
    "filename": "google-sheets.mdx",
    "path": "docs/integrations/data-integrations/google-sheets.mdx",
    "chunk_id": 0,
    "chunk_content": "title google sheets sidebartitle google sheets implementation google sheets data handler mindsdb google sheetshttpswwwgooglecomsheetsabout spreadsheet program included part free webbased google docs editors suite offered google warning please note integration mindsdb google sheets works public sheets warning prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect google sheets mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access google sheets implementation handler implemented using duckdb library allows sql queries executed pandas dataframes essence querying particular sheet entire sheet first pulled pandas dataframe using google visualization apihttpsdevelopersgooglecomchartinteractivedocsreference done sql queries run dataframe using duckdb tip since entire sheet needs pulled memory first dataframe recommended somewhat careful querying large datasets overload machine tip required arguments establish connection follows spreadsheet_id unique id google sheet sheet_name name sheet within google sheet usage order make use handler connect google sheet mindsdb following syntax used sql create database sheets_datasource engine sheets parameters spreadsheet_id 12wgs1kj9ymum6vyzq0njygitonxay7cmklnee2_d0 sheet_name iris use established connection query table follows sql select sheets_datasourceexample_tbl name table name relevant sheet provided input sheet_name parameter warning moment select statemet allowed executed duckdb however restriction running machine learning algorithms data google sheets using create model statement warning"
  },
  {
    "filename": "tdengine.mdx",
    "path": "docs/integrations/data-integrations/tdengine.mdx",
    "chunk_id": 0,
    "chunk_content": "title tdengine sidebartitle tdengine implementation tdengine data handler mindsdb tdenginehttpstdenginecom open source highperformance cloud native timeseries database optimized internet things iot connected cars industrial iot enables efficient realtime data ingestion processing monitoring tb even pb scale data per day generated billions sensors data collectors tdengine differentiates timeseries databases numerous advantages high performance simplified solution cloudnative ease use easy data analytics opensource prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect tdengine mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access tdengine implementation handler implemented using taostaosrest python library allows use python code run sql commands tdengine server required arguments establish connection follows user username associated server password password authenticate access url url tdengine server local server url localhost6041 default token unique token provided using tdengine cloud database database name connected usage order make use handler connect tdengine database mindsdb following syntax used sql create database tdengine_datasource engine tdengine parameters user tdengine_user password password url localhost6041 token token database tdengine_db note specify token instead user password using tdengine note use established connection query table follows sql select tdengine_datasourcedemo_table"
  },
  {
    "filename": "tidb.mdx",
    "path": "docs/integrations/data-integrations/tidb.mdx",
    "chunk_id": 0,
    "chunk_content": "title tidb sidebartitle tidb implementation tidb data handler mindsdb tidbhttpswwwpingcapcomtidb opensource newsql database supports hybrid transactional analytical processing workloads mysqlcompatible provide horizontal scalability strong consistency high availability prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect tidb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access tidb implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect tidb database mindsdb following syntax used sql create database tidb_datasource engine tidb parameters host 127001 port 4000 database tidb user root password password use established connection query table follows sql select tidb_datasourcedemo_table"
  },
  {
    "filename": "airtable.mdx",
    "path": "docs/integrations/data-integrations/airtable.mdx",
    "chunk_id": 0,
    "chunk_content": "title airtable sidebartitle airtable implementation airtable data handler mindsdb airtablehttpswwwairtablecomlpcampaigndatabase platform makes easy build powerful custom applications tools streamline process workflow project best build without ever learning write single line code prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect airtable mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access airtable implementation handler implemented using duckdb library allows sql queries executed pandas dataframes essence querying particular table entire table first pulled pandas dataframe using airtable apihttpsairtablecomapi done sql queries run dataframe using duckdb required arguments establish connection follows base_id airtable base id table_name airtable table name api_key api key airtable api usage order make use handler connect airtable database mindsdb following syntax used sql create database airtable_datasource engine airtable parameters base_id dqweqweqrwwqq table_name iris api_key knlsndlknslk use established connection query table follows sql select airtable_datasourceexample_tbl warning moment select statement allowed executed duckdb however restriction running machine learning algorithms data airtable using create model statement warning"
  },
  {
    "filename": "vertica.mdx",
    "path": "docs/integrations/data-integrations/vertica.mdx",
    "chunk_id": 0,
    "chunk_content": "title vertica sidebartitle vertica implementation vertica data handler mindsdb columnoriented vertica analytics platformhttpswwwverticacomoverview designed manage large fastgrowing volumes data fast query performance data warehouses queryintensive applications product claims greatly improve query performance traditional relational database systems provide high availability exabyte scalability commodity enterprise servers vertica runs multiple cloud computing systems well hadoop nodes verticas eon mode separates compute storage using s3 object storage dynamic allocation compute notes prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect vertica mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access vertica implementation handler implemented using verticapython python library allows use python code run sql commands vertica database required arguments establish connection follows user username asscociated database password password authenticate access host host name ip address server port port tcpip connection made database database name connected schema schema name get tables usage order make use handler connect vertica database mindsdb following syntax used sql create database vertica_datasource engine vertica parameters user dbadmin password password host 127001 port 5433 schema_name public database vmart use established connection query table follows sql select vertica_datasourcetest"
  },
  {
    "filename": "vitess.mdx",
    "path": "docs/integrations/data-integrations/vitess.mdx",
    "chunk_id": 0,
    "chunk_content": "title vitess sidebartitle vitess implementation vitess data handler mindsdb vitesshttpsvitessio database solution deploying scaling managing large clusters opensource database instances currently supports mysql percona server mysql architected run effectively public private cloud architecture dedicated hardware combines extends many important sql features scalability nosql database prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect vitess mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access vitess implementation handler implemented extending mysql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect vitess server mindsdb following syntax used sql create database vitess_datasource engine vitess parameters user root password host localhost port 33577 database commerce use established connection query table follows sql select vitess_datasourceproduct limit 10"
  },
  {
    "filename": "apache-ignite.mdx",
    "path": "docs/integrations/data-integrations/apache-ignite.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache ignite sidebartitle apache ignite implementation apache ignite data handler mindsdb apache ignitehttpsigniteapacheorgdocslatest distributed database highperformance computing inmemory speed prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache ignite mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache ignite implementation handler implemented using pyignite library apache ignite thin binary protocol client python required arguments establish connection follows host host name ip address apache ignite clusters node port tcpip port apache ignite clusters node must integer several optional arguments used well username username used authenticate apache ignite cluster parameter optional default none password password authenticate user apache ignite cluster parameter optional default none schema schema use connection apache ignite cluster parameter optional default public usage order make use handler connect apache ignite database mindsdb following syntax used sql create database ignite_datasource engine ignite parameters host 127001 port 10800 username admin password password schema example_schema use established connection query table follows sql select ignite_datasourcedemo_table limit 10 tip currently connection established single node cluster future well configure client automatically fail another node connection current node fails times providing hosts ports many nodes explained herehttpsigniteapacheorgdocslatestthinclientspythonthinclient tip"
  },
  {
    "filename": "influxdb.mdx",
    "path": "docs/integrations/data-integrations/influxdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title influxdb sidebartitle influxdb implementation influxdb data handler mindsdb influxdbhttpswwwinfluxdatacom time series database used collect data monitor system devices especially edge devices prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect influxdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access influxdb implementation required arguments establish connection follows influxdb_url hosted url influxdb cloud influxdb_token authentication token hosted influxdb cloud instance influxdb_db_name database name influxdb cloud instance influxdb_table_name table name influxdb cloud instance tip please follow linkhttpsdocsinfluxdatacominfluxdbcloudsecuritytokenscreatetokencreateatokenintheinfluxdbui generate token accessing influxdb api tip usage order make use handler connect influxdb database mindsdb following syntax used sql create database influxdb_source engine influxdb parameters influxdb_url influxdbhostedurl influxdb_token apikeytoken influxdb_db_name databasename influxdb_table_name tablename use established connection query table follows sql select name time sensor_id temperature influxdb_sourcetables order temperature desc limit 65"
  },
  {
    "filename": "greptimedb.mdx",
    "path": "docs/integrations/data-integrations/greptimedb.mdx",
    "chunk_id": 0,
    "chunk_content": "title greptimedb sidebartitle greptimedb implementation greptimedb data handler mindsdb greptimedbhttpsgreptimecom opensource cloudnative time series database features analytical capabilities scalebility open protocols support implementation handler implemented extending mysqlhandler connect greptimedb mindsdb providing following parameters host host name ip address url port port used make tcpip connection database database name user database user password database password several optional parameters used well ssl ssl parameter value indicates whether ssl enabled true disabled false ssl_ca ssl certificate authority ssl_cert stores ssl certificates ssl_key stores ssl keys usage order make use handler connect greptimedb database mindsdb following syntax used sql create database greptimedb_datasource engine greptimedb parameters host 127001 port 4002 database public user username password password use established connection query table follows sql select greptimedb_datasourceexample_table"
  },
  {
    "filename": "monetdb.mdx",
    "path": "docs/integrations/data-integrations/monetdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title monetdb sidebartitle monetdb implementation monetdb data handler mindsdb monetdbhttpswwwmonetdborg opensource columnoriented relational database management system originally developed centrum wiskunde informatica netherlands designed provide high performance complex queries large databases combining tables hundreds columns millions rows prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect monetdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access monetdb implementation handler implemented using pymonetdb python library allows use python code run sql commands monetdb database required arguments establish connection follows user username associated database password password authenticate access host host name ip address port port tcpip connection made database database name connected schema_name schema name get tables optional defaults current schema provided usage order make use handler connect monetdb database mindsdb following syntax used sql create database monetdb_datasource engine monetdb parameters user monetdb password monetdb host 127001 port 50000 schema_name sys database demo use established connection query table follows sql select monetdb_datasourcedemo"
  },
  {
    "filename": "edgelessdb.mdx",
    "path": "docs/integrations/data-integrations/edgelessdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title edgelessdb sidebartitle edgelessdb implementation edgelessdb data handler mindsdb edgelesshttpsedgelesssystems full sql database tailormade confidential computing seamlessly integrates existing tools workflows help unlock full potential data prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect edgelessdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access edgelessdb implementation handler implemented extending mysql connector required arguments establish connection follows host host name edgelessdb connection port port use connecting user user authenticate password password authenticate user database database name use full potensial edgelessdb also specify following arguments ssl whether use ssl ssl_ca path url ca certificate ssl_cert path url client certificate ssl_key path url client key usage order use edgelessdb data source mindsdb need use following syntax sql create database edgelessdb_datasource engine edgelessdb parameters user root password test123aabvhj host localhost port 3306 database test_schema use following syntax sql create database edgelessdb_datasource2 engine edgelessdb parameters user root password test123aabvhj host localhost port 3306 database test_schema ssl_cert homemariosdemocertpem ssl_key homemariosdemokeypem use established connection query table follows sql select edgelessdb_datasourcetable_name"
  },
  {
    "filename": "google-cloud-sql.mdx",
    "path": "docs/integrations/data-integrations/google-cloud-sql.mdx",
    "chunk_id": 0,
    "chunk_content": "title google cloud sql sidebartitle google cloud sql implementation google cloud sql data handler mindsdb cloud sqlhttpscloudgooglecomsql fullymanaged database service makes easy set maintain manage administer relational postgresql mysql sql server databases cloud prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect google cloud sql mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access google cloud sql implementation handler implemented using existing mindsdb handlers mysql postgresql sql server required arguments establish connection host host name ip address google cloud sql instance port tcpip port google cloud sql instance user username used authenticate google cloud sql instance password password authenticate user google cloud sql instance database database name use connecting google cloud sql instance db_engine database engine google cloud sql instance take one three values mysql postgresql mssql usage order make use handler connect google cloud sql instance need create datasource following syntax sql create database cloud_sql_mysql_datasource engine cloud_sql parameters db_engine mysql host 531706116 port 3306 user admin password password database example_db successfully connect google cloud sql instance make sure ip address machine using connect added authorized networks google cloud sql instance following steps 1 go cloud sql instanceshttpsconsolecloudgooglecomsqlinstances page 2 click instance"
  },
  {
    "filename": "google-cloud-sql.mdx",
    "path": "docs/integrations/data-integrations/google-cloud-sql.mdx",
    "chunk_id": 1,
    "chunk_content": "want add authorized networks 3 click connections tab 4 click networking tab 5 click add network 5 enter ip address machine want connect using mindsdb cloud version use following ip address 1822020595 31915246 521491162 use established connection query table follows sql select cloud_sql_mysql_datasourceexample_tbl"
  },
  {
    "filename": "mongodb.mdx",
    "path": "docs/integrations/data-integrations/mongodb.mdx",
    "chunk_id": 0,
    "chunk_content": "title mongodb sidebartitle mongodb documentation describes integration mindsdb mongodbhttpswwwmongodbcomcompanywhatismongodb document database scalability flexibility want querying indexing need prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connection establish connection mongodb mindsdb executing following sql command sql create database mongodb_datasource engine mongodb parameters host mongodbsrvadminadmin_passdemomongodbnetpublic use following parameters establish connection host connection string mongodb server includes user admin password admin_pass host port demomongodbnet database public database connection string include database path provide parameter alternatively following set connection parameters used username username associated database password password authenticate access host host mongodb server port port tcpip connection made database database name connected usage retrieve data specified collection providing integration name collection name sql select mongodb_datasourcemy_collection limit 10 note examples utilize mongodb_datasource datasource name defined create database command note tip moment integration supports select update queries tip warning connection strongly suggest using mongo api instead sql api mindsdb dedicated mongo apisdksmongomindsdbmongoqloverview allows use full power mindsdb platform using mongo api feels natural mongodb users allows use features mindsdb find instructions connect mindsdb mongodb compassconnectmongocompass mongodb shellconnectmongoshell proceed mongo api documentationsdksmongomindsdbmongoqloverview details warning tip connected mindsdb mongodb compass mongodb shell run command connect database mindsdb sql test use mindsdb mindsdb"
  },
  {
    "filename": "mongodb.mdx",
    "path": "docs/integrations/data-integrations/mongodb.mdx",
    "chunk_id": 1,
    "chunk_content": "dbdatabasesinsertone name mongo_datasource engine mongodb connection_args host mongodbsrvuserpassdbxxxyyymongodbnet query data like sql mindsdb use mongo_datasource mongo_datasource dbdemofindlimit3 tip troubleshooting guide warning database connection error symptoms failure connect mindsdb mongodb server checklist 1 make sure mongodb server active 2 confirm host credentials provided correct try direct mongodb connection using client like mongodb compass 3 ensure stable network mindsdb mongodb example using mongodb atlas ensure ip address machine running mindsdb whitelisted warning warning unknown statement symptoms errors related issuing unsupported queries mongodb via integration checklist 1 ensure query select update query warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing collection names containing special characters checklist 1 ensure table names special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning"
  },
  {
    "filename": "supabase.mdx",
    "path": "docs/integrations/data-integrations/supabase.mdx",
    "chunk_id": 0,
    "chunk_content": "title supabase sidebartitle supabase implementation supabase data handler mindsdb supabasehttpssupabasecom opensource firebase alternative prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect supabase mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access supabase implementation handler implemented extending postgresql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect supabase server mindsdb following syntax used sql create database supabase_datasource engine supabase parameters host 127001 port 54321 database test user supabase password password use established connection query database follows sql select supabase_datasourcepublicrentals limit 10"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 0,
    "chunk_content": "title supported integrations sidebartitle supported integrations list databases supported mindsdb keeps growing currently supported integrations p aligncenter img srcassetssupported_integrationspng p find particular databases handler files herehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlers see connection arguments example see latest updates oracle handler check oracles readmemd file herehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersoracle_handlerreadmemd lets look sample codes showing connect supported integrations tip community check video guides created community video guide connect postgres database mindsdbhttpsyoutubejbh5fsviage akhilcoderhttpstwittercomakhilcoder video guide setting data sources mindsdbhttpswwwyoutubecomwatchvkrrmjuvnib0 syed zubeenhttpsgithubcomsyedzubeen tip airtable tabs tab titletemplate sql create database airtable_datasource display name database engine airtable name mindsdb handler parameters base_id airtable base id table_name airtable table name api_key api key airtable api tab tab titleexample sql create database airtable_datasource engine airtable parameters base_id appve10klsda2 table_name my_table api_key kdjx2q5km5btsqymgvn tab tabs info check airtable data handler details heredataintegrationsairtable info amazon dynamodb tabs tab titletemplate sql create database dynamodb_datasource display name database engine dynamodb name mindsdb handler parameters aws_access_key_id aws access key aws_secret_access_key aws secret access key region_name aws region tab tab titleexample sql create database dynamodb_datasource engine dynamodb parameters aws_access_key_id pcaq2ljdoswlnsqkocpw aws_secret_access_key uvjewplnopsdmmwitl34r2neyc6whzpuiip57i region_name useast1 tab tabs info check amazon dynamodb data handler details heredataintegrationsamazondynamodb info amazon redshift tabs tab titletemplate sql create database amazonredshift_datasource display name database engine amazonredshift name mindsdb"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 1,
    "chunk_content": "handler parameters host host name ip address redshift cluster port port used connecting redshift cluster database database name used connecting redshift cluster user user authenticate redshift cluster password password used authenticate redshift cluster tab tab titleexample sql create database amazonredshift_datasource engine amazonredshift parameters host 127001 port 5439 database test user amazonredshift password password tab tabs info check amazon redshift data handler details heredataintegrationsamazonredshift info amazon s3 tabs tab titletemplate sql create database amazons3_datasource display name database engine s3 name mindsdb handler parameters aws_access_key_id aws access key aws_secret_access_key aws secret access key region_name aws region bucket name s3 bucket key key object queried input_serialization format data queried tab tab titleexample sql create database amazons3_datasource engine s3 parameters aws_access_key_id pcaq2ljdoswlnsqkocpw aws_secret_access_key uvjewplnopsdmmwitl34r2neyc6whzpuiip57i region_name useast1 bucket mindsdbbucket key iriscsv input_serialization csv fileheaderinfo none tab tabs info check amazon s3 data handler details heredataintegrationsamazons3 info apache cassandra tabs tab titletemplate sql create database cassandra_datasource display name database engine cassandra name mindsdb handler parameters host host name ip address port port used make tcpip connection user database user password database password keyspace database name protocol_version optional protocol version defaults 4 left blank secure_connect_bundle optional secure connect bundle file path either path url tab tab titleexample"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 2,
    "chunk_content": "sql create database cassandra_datasource engine cassandra parameters host 127001 port 9043 user user password password keyspace test_data protocol_version 4 tab tabs info check apache cassandra data handler details heredataintegrationsapachecassandra info apache druid tabs tab titletemplate sql create database druid_datasource display name database engine druid name mindsdb handler parameters host host name ip address apache druid port port apache druid runs user optional user authenticate apache druid password optional password used authenticate apache druid path query path scheme uri scheme defaults http left blank tab tab titleexample sql create database druid_datasource engine druid parameters host 127001 port 8888 path druidv2sql scheme http tab tabs info check apache druid data handler details heredataintegrationsapachedruid info apache hive tabs tab titletemplate sql create database hive_datasource display name database engine hive name mindsdb handler parameters user database user password database password host host name ip address port port used make tcpip connection database database name auth defaults custom provided check options httpspypiorgprojectpyhive tab tab titleexample sql create database hive_datasource engine hive parameters user hive password password host 127001 port 10000 database hive_db auth custom tab tabs info check apache hive data handler details heredataintegrationsapachehive info apache impala tabs tab titletemplate sql create database impala_datasource display"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 3,
    "chunk_content": "name database engine impala name mindsdb handler parameters user database user password database password host host name ip address port port used make tcpip connection database database name tab tab titleexample sql create database impala_datasource engine impala parameters user impala_user password password host 127001 port 21050 database impala_db tab tabs info check apache impala data handler details heredataintegrationsapacheimpala info apache pinot tabs tab titletemplate sql create database pinot_datasource display name database engine pinot name mindsdb handler parameters host host name ip address apache pinot cluster broker_port port broker apache pinot cluster runs controller_port port controller apache pinot cluster runs path query path scheme scheme defaults http left blank username optional user password optional password verify_ssl optional verify ssl tab tab titleexample sql create database pinot_datasource engine pinot parameters host 127001 broker_port 8000 controller_port 9000 path querysql scheme http tab tabs info check apache pinot data handler details heredataintegrationsapachepinot info apache solr tabs tab titletemplate sql create database solr_datasource display name database engine solr name mindsdb handler parameters username optional username used authenticate solr server password optional password used authenticate solr server host host name ip address solr serve port port number solr server server_path defaults solr left blank collection solr"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 4,
    "chunk_content": "collection name use_ssl defaults false left blank refer httpspypiorgprojectsqlalchemysolr tab tab titleexample sql create database solr_datasource engine solr parameters username solr_user password password host 127001 port 8981 server_path solr collection collection_name use_ssl false tab tabs info check apache solr data handler details heredataintegrationsapachesolr info ckan tabs tab titletemplate sql create database ckan_datasource display name database engine ckan name mindsdb handler parameters url host name ip address url apikey api key used authentication tab tab titleexample sql create database ckan_datasource engine ckan parameters url httpdemockanorgapi3action apikey your_api_key tab tabs info check ckan data handler details heredataintegrationsckan info clickhouse tabs tab titletemplate sql create database clickhouse_datasource display name database engine clickhouse name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password protocol optional http https defaults native tab tab titleexample sql create database clickhouse_datasource engine clickhouse parameters host 127001 port 9000 database test_data user root password password tab tabs info check clickhouse data handler details heredataintegrationsclickhouse info cloud spanner tabs tab titletemplate sql create database cloud_spanner_datasource display name database engine cloud_spanner name mindsdb handler parameters instance_id instance identifier database_id database identifier project_id identifier project owns instances data credentials"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 5,
    "chunk_content": "stringified gcp service account key json tab tab titleexample sql create database cloud_spanner_datasource engine cloud_spanner parameters instance_id myinstance database_id exampledb project myproject credentials tab tabs info check cloud spanner data handler details heredataintegrationscloudspanner info cockroachdb tabs tab titletemplate sql create database cockroach_datasource display name database engine cockroachdb name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password publish optional publish tab tab titleexample sql create database cockroach_datasource engine cockroachdb parameters host 127001 port 26257 database cockroachdb user username password password tab tabs info check cockroachdb data handler details heredataintegrationscockroachdb info couchbase tabs tab titletemplate sql create database couchbase_datasource display name database engine couchbase name mindsdb handler parameters host host name ip address couchbase server user user authenticate couchbase server password password used authenticate couchbase server bucket bucket name scope scope used query defaults _default left blank scope couchbase equivalent schema mysql tab tab titleexample sql create database couchbase_datasource engine couchbase parameters host 127001 user couchbase password password bucket testbucket tab tabs info check couchbase data handler details heredataintegrationscouchbase info cratedb tabs tab titletemplate sql create database cratedb_datasource display name database engine crate name mindsdb handler parameters"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 6,
    "chunk_content": "host host name ip address port port used make tcpip connection user database user password database password schema_name database schema name defaults doc left blank tab tab titleexample sql create database cratedb_datasource engine crate parameters host 127001 port 4200 user crate password password schema_name doc tab tabs info check cratedb data handler details heredataintegrationscratedb info d0lt tabs tab titletemplate sql create database d0lt_datasource display name database engine d0lt name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database d0lt_datasource engine d0lt parameters host 127001 port 3306 database information_schema user root password password tab tabs info check d0lt data handler details heredataintegrationsd0lt info databend tabs tab titletemplate sql create database databend_datasource display name database engine databend name mindsdb handler parameters protocol protocol used query databend defaults native left blank supported protocols native http https user username used authenticate databend warehouse port tcpip port databend"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 7,
    "chunk_content": "warehouse password password used authenticate databend warehouse host host name ip address databend warehouse use 127001 instead localhost connecting local server database database name used connecting databend warehouse tab tab titleexample sql create database databend_datasource engine databend parameters protocol native user databend_user port 443 password password host 127001 database databend_db tab tabs info check databend data handler details heredataintegrationsdatabend info databricks tabs tab titletemplate sql create database databricks_datasource display name database engine databricks name mindsdb handler parameters server_hostname server hostname cluster sql warehouse http_path http path cluster sql warehouse access_token personal databricks access token schema schema name defaults default left blank session_configuration optional dictionary spark session configuration parameters http_headers optional additional key value pairs set http headers every rpc request client makes catalog catalog defaults hive_metastore left blank tab tab titleexample sql create database databricks_datasource engine databricks parameters server_hostname adb12345678901234567azuredatabricksnet http_path sqlprotocolv1o12345678901234561234567890test123 access_token dapi1234567890ab1cde2f3ab456c7d89efa schema example_db tab tabs info check databricks data handler details heredataintegrationsdatabricks info datastax tabs tab titletemplate sql create database datastax_datasource display name database engine astra name mindsdb handler parameters user user authenticated password password authentication secure_connection_bundle secure connection bundle zip file path either path url host optional host name ip address port optional port used make"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 8,
    "chunk_content": "tcpip connection protocol_version optional protocol version keyspace optional keyspace tab tab titleexample sql create database datastax_datasource engine astra parameters host 127001 port 7077 user datastax password password secure_connection_bundle path homedownloadsfilezip tab tabs info check datastax data handler details heredataintegrationsdatastax info duckdb tabs tab titletemplate sql create database duckdb_datasource display name database engine duckdb name mindsdb handler parameters database database file name read_only flag used set connection readonly mode tab tab titleexample sql create database duckdb_datasource engine duckdb parameters database dbduckdb read_only false tab tabs info check duckdb data handler details heredataintegrationsduckdb info elasticsearch tabs tab titletemplate sql create database elastic_datasource display name database engine elasticsearch name mindsdb handler parameters hosts one host names ip addresses elasticsearch server username optional username authenticate elasticsearch server password optional password used authenticate elasticsearch server cloud_id optional unique id hosted elasticsearch cluster must provided hosts left blank tab tab titleexample sql create database elastic_datasource engine elasticsearch parameters hosts localhost9200 tab tabs info check elasticsearch data handler details heredataintegrationselasticsearch info firebird tabs tab titletemplate sql create database firebird_datasource display name database engine firebird name mindsdb handler parameters host host name ip address firebird server database database name user user authenticate firebird server password password used authenticate"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 9,
    "chunk_content": "firebird server tab tab titleexample sql create database firebird_datasource engine firebird parameters host 127001 database test user firebird password password tab tabs info check firebird data handler details heredataintegrationsfirebird info google bigquery tabs tab titletemplate sql create database bigquery_datasource display name database engine bigquery name mindsdb handler parameters project_id globally unique project identifier dataset default dataset service_account_keys service account keys file service_account_json alternative using service_account_keys tab tab titleexample selfhosted mindsdb sql create database bigquery_datasource engine bigquery parameters project_id badger345908 service_account_keys homedownloadsbadger345908json tab tab titleexample mindsdb cloud sql create database bigquery_datasource engine bigquery parameters project_id badger345908 service_account_keys url httpsurlbadger345908json tab tab titleexample without json file sql create database bq engine bigquery parameters project_id bgtest1111 dataset mydataset service_account_json type service_account project_id bgtest1111 private_key_id aaaaaaaaaa private_key big string keyn client_email testbigquerybgtest11111iamgserviceaccountcom client_id 1111111111111 auth_uri httpsaccountsgooglecomooauth2auth token_uri httpsoauth2googleapiscomtoken auth_provider_x509_cert_url httpswwwgoogleapiscomoauth2v1certs client_x509_cert_url httpswwwgoogleapiscomrobotv1metadatax509testbigquery40bgtest11111iamgserviceaccountcom tab tabs info check google bigquery data handler details heredataintegrationsgooglebigquery info google sheets tabs tab titletemplate sql create database sheets_datasource display name database engine sheets name mindsdb handler parameters spreadsheet_id unique id google sheet sheet_name name google sheet tab tab titleexample sql create database sheets_datasource engine sheets parameters spreadsheet_id abc1234567 located url httpsdocsgooglecomspreadsheetsdabc1234567editgid0 sheet_name invoice tab tabs info check google sheets data"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 10,
    "chunk_content": "handler details heredataintegrationsgooglesheets info greptimedb tabs tab titletemplate sql create database greptimedb_datasource display name database engine greptimedb name mindsdb handler parameters host host ip address url port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database greptimedb_datasource engine greptimedb parameters host 127001 port 4002 database public user username password password tab tabs info check greptimedb data handler details heredataintegrationsgreptimedb info ibm db2 tabs tab titletemplate sql create database db2_datasource display name database engine db2 name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password schema_name database schema name tab tab titleexample sql create database db2_datasource engine db2 parameters host 127001 port 25000 database books user db2admin password password schema_name db2admin tab tabs info check ibm db2 data handler details heredataintegrationsibmdb2 info ibm informix tabs tab titletemplate sql create database informix_datasource display name database engine informix name mindsdb handler"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 11,
    "chunk_content": "parameters server server name host host name ip address port port used make tcpip connection database database name user database user password database password schema_name database schema name logging_enabled indicates whether logging enabled defaults true left blank tab tab titleexample sql create database informix_datasource engine informix parameters server server host 127001 port 9091 database stores_demo user informix password password schema_name demo_schema logging_enabled false tab tabs info check ibm informix data handler details heredataintegrationsibminformix info mariadb tabs tab titletemplate sql create database maria_datasource display name database engine mariadb name mindsdb handler parameters host host ip address url port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database maria_datasource engine mariadb parameters host 127001 port 3306 database mariadb user root password password tab tabs info check mariadb data handler details heredataintegrationsmariadb info mariadb skysql tabs tab titletemplate sql create database skysql display name database engine mariadb name mindsdb handler parameters user database user password"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 12,
    "chunk_content": "database password host host ip address url port port used make tcpip connection ssl optional ssl parameter value indicates whether ssl enabled true disabled false sslca optional ssl certificate authority path either path url database database name tab tab titleexample sql create database skysql_datasource engine mariadb parameters host mindsdbtestmdb0002956db1skysqlnet port 5001 database mindsdb_data user db00007539 password password sslca url httpsmindsdbwebbuildss3amazonawscomaws_skysql_chainpem tab tabs info information connect mariadb skysql mindsdb visit doc page hereconnectconnectmariadbskysql info matrixone tabs tab titletemplate sql create database matrixone_datasource display name database engine matrixone name mindsdb handler parameters host host ip address url port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database matrixone_datasource engine matrixone parameters host 127001 port 6001 database mo_catalog user matrixone password password tab tabs info check matrixone data handler details heredataintegrationsmatrixone info microsoft access tabs tab titletemplate sql create database access_datasource display name database engine access name mindsdb handler parameters db_file path database file used"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 13,
    "chunk_content": "tab tab titleexample sql create database access_datasource engine access parameters db_file example_dbaccdb tab tabs info check microsoft access data handler details heredataintegrationsmicrosoftaccess info microsoft sql server tabs tab titletemplate sql create database mssql_datasource display name database engine mssql name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password tab tab titleexample sql create database mssql_datasource engine mssql parameters host 127001 port 1433 database master user sa password password tab tabs info check microsoft sql server data handler details heredataintegrationsmicrosoftsqlserver info monetdb tabs tab titletemplate sql create database monetdb_datasource display name database engine monetdb name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password schema_name database schema name defaults current schema left blank tab tab titleexample sql create database monetdb_datasource engine monetdb parameters host 127001 port 50000 database demo user monetdb password password schema_name sys tab tabs info check monetdb data handler details heredataintegrationsmonetdb info mongodb warning connection recommend use mongo api instead sql api mindsdb dedicated mongo apimongocollectionstructure allows use full power mindsdb platform using mongo api feels natural mongodb users allows"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 14,
    "chunk_content": "use features mindsdb find instructions connect mindsdb mongodb compassconnectmongocompass mongodb shellconnectmongoshell proceed mongo api documentationmongocollectionstructure details warning tabs tab titletemplate sql create database mongo_datasource display name database engine mongo name mindsdb handler parameters host host name ip address port port used make tcpip connection user database user password database password database database name tab tab titleexample sql create database mongo_datasource engine mongo parameters host 127001 port 27017 user mongo password password database database tab tabs info check mongodb data handler details heredataintegrationsmongodb info mysql tabs tab titletemplate sql create database mysql_datasource display name database engine mysql name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database mysql_datasource engine mysql parameters host 127001 port 3306 database mysql user root password password tab tabs info check mysql data handler details heredataintegrationsmysql info oceanbase tabs tab titletemplate sql create database oceanbase_datasource display name database engine oceanbase"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 15,
    "chunk_content": "name mindsdb handler parameters host host name ip address user database user password database password port port used make tcpip connection database database name tab tab titleexample sql create database oceanbase_datasource engine oceanbase parameters host 127001 user oceanbase_user password password port 2881 database oceanbase_db tab tabs info check oceanbase data handler details heredataintegrationsoceanbase info opengauss tabs tab titletemplate sql create database opengauss_datasource display name database engine opengauss name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password tab tab titleexample sql create database opengauss_datasource engine opengauss parameters host 127001 port 5432 database opengauss user mindsdb password password tab tabs info check opengauss data handler details heredataintegrationsopengauss info oracle tabs tab titletemplate sql create database oracle_datasource display name database engine oracle name mindsdb handler parameters host host name ip address port port used make tcpip connection sid unique identifier database instance user database user password database password tab tab titleexample sql create database oracle_datasource engine oracle parameters host 127001 port 1521 sid orcl user sys password password tab tabs info check oracle data handler details heredataintegrationsoracle info orioledb tabs tab titletemplate sql create database orioledb_datasource display name"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 16,
    "chunk_content": "database engine orioledb name mindsdb handler parameters user database user password database password host host name ip address port port used make tcpip connection server sets current server database sets current database tab tab titleexample sql create database orioledb_datasource engine orioledb parameters user orioledb_user password password host 127001 port 55505 server server_name database oriole_db tab tabs info check orioledb data handler details heredataintegrationsorioledb info planetscale tabs tab titletemplate sql create database planetscale_datasource display name database engine planet_scale name mindsdb handler parameters host host name ip address port port used make tcpip connection user database user password database password database database name tab tab titleexample sql create database planetscale_datasource engine planet_scale parameters host 127001 port 3306 user planetscale_user password password database planetscale_db tab tabs info check planetscale data handler details heredataintegrationsplanetscale info postgresql tabs tab titletemplate sql create database psql_datasource display name database engine postgres name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password tab tab titleexample sql create database psql_datasource engine postgres parameters host 127001 port 5432 database postgres user postgres password password tab tabs info check postgresql data handler details heredataintegrationspostgresql info questdb tabs"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 17,
    "chunk_content": "tab titletemplate sql create database questdb_datasource display name database engine questdb name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password public value true false defaults true left blank tab tab titleexample sql create database questdb_datasource engine questdb parameters host 127001 port 8812 database qdb user admin password password tab tabs info check questdb data handler details heredataintegrationsquestdb info sap hana tabs tab titletemplate sql create database sap_hana_datasource display name database engine hana name mindsdb handler parameters host host name ip address port port used make tcpip connection user user password password schema database schema name defaults current schema left blank encrypt indicates whether connection encrypted required cloud usage tab tab titleexample sql create database sap_hana_datasource engine hana parameters host uuidhanatrialus10hanacloudondemandcom port 443 user dbadmin password password schema mindsdb encrypt true tab tabs info check sap hana data handler details heredataintegrationssaphana info scylladb tabs tab titletemplate sql create database scylladb_datasource display name database engine scylladb name mindsdb handler parameters host host name ip address port port used make tcpip connection user user password password protocol_version optional protocol version defaults 4 left blank keyspace keyspace name top"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 18,
    "chunk_content": "level container tables secure_connect_bundle secure connect bundle file path either path url tab tab titleexample sql create database scylladb_datasource engine scylladb parameters host 127001 port 7199 user usermindsdbcom password password protocol_version 4 keyspace keyspace_name secure_connect_bundle path homezorandownloadssecureconnectmindsdbzip tab tabs info check scylladb data handler details heredataintegrationsscylladb info singlestore tabs tab titletemplate sql create database singlestore_datasource display name database engine singlestore name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password ssl optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url tab tab titleexample sql create database singlestore_datasource engine singlestore parameters host 127001 port 3306 database singlestore user root password password tab tabs info check singlestore data handler details heredataintegrationssinglestore info snowflake tabs tab titletemplate sql create database snowflake_datasource display name database engine snowflake name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password account snowflake account schema schema name defaults public left blank protocol protocol"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 19,
    "chunk_content": "defaults https left blank warehouse warehouse account tab tab titleexample sql create database snowflake_datasource engine snowflake parameters host account_namesnowflakecomputingcom port 443 database snowflake user user password password account account_name schema public protocol https warehouse warehouse tab tabs info check snowflake data handler details heredataintegrationssnowflake info sql anywhere tabs tab titletemplate sql create database sqlany_datasource display name database engine sqlany name mindsdb handler parameters user username password password host host name ip address sap sql anywhere instance port port number sap sql anywhere instance server sets current server database sets current database tab tab titleexample sql create database sqlany_datasource engine sqlany parameters user sqlany_user password password host 127001 port 55505 server server_name database sqlany_db tab tabs info check sql anywhere data handler details heredataintegrationssqlanywhere info sqlite tabs tab titletemplate sql create database sqlite_datasource display name database engine sqlite name mindsdb handler parameters db_file path database file used tab tab titleexample sql create database sqlite_datasource engine sqlite parameters db_file exampledb tab tabs info check sqlite data handler details heredataintegrationssqlite info starrocks tabs tab titletemplate sql create database starrocks_datasource display name database engine starrocks name mindsdb handler parameters host host name ip address user database user password database password port port used make"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 20,
    "chunk_content": "tcpip connection database database name tab tab titleexample sql create database starrocks_datasource engine starrocks parameters host 127001 user starrocks_user password password port 8030 database starrocks_db tab tabs info check starrocks data handler details heredataintegrationsstarrocks info supabase tabs tab titletemplate sql create database supabase_datasource display name database engine supabase name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password tab tab titleexample sql create database supabase_datasource engine supabase parameters host 127001 port 54321 database test user supabase password password tab tabs info check supabase data handler details heredataintegrationssupabase info tdengine tabs tab titletemplate sql create database tdengine_datasource display name database engine tdengine name mindsdb handler parameters user server username password server password url url tdengine server local server localhost6041 default token unique token provided using tdengine cloud database database name tab tab titleexample sql create database tdengine_datasource engine tdengine parameters user tdengine_user password password url localhost6041 token token database tdengine_db tab tabs info check tdengine data handler details heredataintegrationstdengine info teradata tabs tab titletemplate sql create database teradata_datasource display name database engine teradata name mindsdb handler parameters host host name ip address user database user password database"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 21,
    "chunk_content": "password database database name port port used make tcpip connection tab tab titleexample sql create database teradata_datasource engine teradata parameters host 127001 user teradata password password database teradata_db port 1025 tab tabs info check teradata data handler details heredataintegrationsteradata info tidb tabs tab titletemplate sql create database tidb_datasource display name database engine tidb name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password tab tab titleexample sql create database tidb_datasource engine tidb parameters host 127001 port 4000 database tidb user root password password tab tabs info check tidb data handler details heredataintegrationstidb info timescaledb tabs tab titletemplate sql create database timescaledb_datasource display name database engine timescaledb name mindsdb handler parameters user database user password database password host host name ip address port port used make tcpip connection database database name tab tab titleexample sql create database timescaledb_datasource engine timescaledb parameters user timescaledb password password host 127001 port 36806 database timescaledb_db tab tabs info check timescaledb data handler details heredataintegrationstimescaledb info trino tabs tab titletemplate sql create database trino_datasource display name database engine trino name mindsdb handler parameters host host name ip address port port used make"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 22,
    "chunk_content": "tcpip connection auth optional authentication method currently basic supported http_scheme optional httpdefault https user database user password database password catalog optional catalog schema optional schema optional default withclause properties tables parameter experimental might changed removed future release tab tab titleexample 1 sql create database trino_datasource engine trino parameters host 127001 port 8080 user trino password password catalog default schema test tab tab titleexample 2 sql create database trino_datasource engine trino parameters host 127001 port 443 auth basic http_scheme https user trino password password catalog default schema test transactional true tab tabs info check trino data handler details heredataintegrationstrino info vertica tabs tab titletemplate sql create database vertica_datasource display name database engine vertica name mindsdb handler parameters host host name ip address port port used make tcpip connection database database name user database user password database password schema_name database schema name tab tab titleexample sql create database vertica_datasource engine vertica parameters host 127001 port 5433 database vmart user vertica password password schema_name public tab tabs info check vertica data handler details heredataintegrationsvertica info vitess tabs tab titletemplate sql create database vitess_datasource display name database engine vitess name mindsdb handler parameters host host name ip address user database user password database password"
  },
  {
    "filename": "all-data-integrations.mdx",
    "path": "docs/integrations/data-integrations/all-data-integrations.mdx",
    "chunk_id": 23,
    "chunk_content": "port port used make tcpip connection database database name tab tab titleexample sql create database vitess_datasource engine vitess parameters host 127001 user vitess_user password password port 33577 database vitess_db tab tabs info check vitess data handler details heredataintegrationsvitess info yugabytedb tabs tab titletemplate sql create database yugabyte_datasource display name database engine yugabyte name mindsdb handler parameters user database user password database password host host name ip address port port used make tcpip connection database database name schema schema name multiple schemas comma separated tab tab titleexample sql create database yugabyte_datasource engine yugabyte parameters user yugabyte password password host 127001 port 5433 database yugabyte_db schemacd tab tabs info check yugabytedb data handler details heredataintegrationsyugabytedb info"
  },
  {
    "filename": "amazon-aurora.mdx",
    "path": "docs/integrations/data-integrations/amazon-aurora.mdx",
    "chunk_id": 0,
    "chunk_content": "title amazon aurora sidebartitle amazon aurora implementation amazon aurora handler mindsdb amazon aurorahttpsdocsawsamazoncomamazonrdslatestaurorauserguidechap_auroraoverviewhtml fully managed relational database engine thats compatible mysql postgresql prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect amazon aurora mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access amazon aurora implementation handler implemented using existing mindsdb handlers mysql postgresql required arguments establish connection follows host host name ip address amazon aurora db cluster port tcpip port amazon aurora db cluster user username used authenticate amazon aurora db cluster password password authenticate user amazon aurora db cluster database database name use connecting amazon aurora db cluster optional arguments used follows db_engine database engine amazon aurora db cluster take one two values mysql postgresql parameter optional provided aws_access_key_id aws_secret_access_key parameters must provided aws_access_key_id access key aws account parameter optional required provided db_engine parameter provided aws_secret_access_key secret key aws account parameter optional required provided db_engine parameter provided usage order make use handler connect amazon aurora mysql db cluster mindsdb following syntax used sql create database aurora_mysql_datasource engine aurora parameters db_engine mysql host mysqlclustercluster123456789012useast1rdsamazonawscom port 3306 user admin password password database example_db use established connection query database follows sql select select aurora_mysql_datasourceexample_table"
  },
  {
    "filename": "amazon-aurora.mdx",
    "path": "docs/integrations/data-integrations/amazon-aurora.mdx",
    "chunk_id": 1,
    "chunk_content": "similar commands used establish connection query amazon aurora postgresql db cluster sql create database aurora_postgres_datasource engine aurora parameters db_engine postgresql host postgresmyclustercluster123456789012useast1rdsamazonawscom port 5432 user postgres password password database example_db select aurora_postgres_datasourceexample_table tip want switch different database include query sql select aurora_datasourcenew_databaseexample_table tip"
  },
  {
    "filename": "mariadb.mdx",
    "path": "docs/integrations/data-integrations/mariadb.mdx",
    "chunk_id": 0,
    "chunk_content": "title mariadb sidebartitle mariadb documentation describes integration mindsdb mariadbhttpsmariadborg one popular open source relational databases integration allows mindsdb access data mariadb enhance mariadb ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect mariadb mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection mariadb mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmariadb_handler engine sql create database mariadb_conn engine mariadb parameters host hostname port 3307 database dbname user username password password sql create database mariadb_conn engine mariadb parameters url mariadbusernamehostname3307 required connection parameters include following user username mariadb database password password mariadb database host hostname ip address url mariadb server port port number connecting mariadb server database name mariadb database connect url specify connection mariadb server using urilike string alternative connection option also use mysql protocol prefix optional connection parameters include following ssl boolean parameter indicates whether ssl encryption enabled connection set true enable ssl enhance connection security set false use default nonencrypted connection ssl_ca specifies path certificate authority ca file pem format ssl_cert specifies path ssl certificate file certificate signed trusted ca specified ssl_ca file selfsigned certificate trusted server ssl_key specifies path private key file pem format usage following usage examples utilize"
  },
  {
    "filename": "mariadb.mdx",
    "path": "docs/integrations/data-integrations/mariadb.mdx",
    "chunk_id": 1,
    "chunk_content": "connection mariadb made via create database statement named mariadb_conn retrieve data specified table providing integration table name sql select mariadb_conntable_name limit 10 troubleshooting warning database connection error symptoms failure connect mindsdb mariadb database checklist 1 ensure mariadb server running accessible 2 confirm host port user password correct try direct mysql connection 3 test network connection mindsdb host mariadb server warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces reserved words special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning"
  },
  {
    "filename": "ckan.mdx",
    "path": "docs/integrations/data-integrations/ckan.mdx",
    "chunk_id": 0,
    "chunk_content": "ckan integration handler handler facilitates integration ckanhttpsckanorg opensource data catalog platform managing publishing open data ckan organizes datasets stores data datastorehttpdocsckanorgen211maintainingdatastorehtmlto retrieve data ckan ckanapihttpsgithubcomckanckanapi must used prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect sap hana mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies ckan handler included mindsdb default additional installation required configuration use ckan handler need provide url ckan instance want connect setting ckan_url environment variable example sql create database ckan_datasource engine ckan parameters url httpsyourckaninstanceurlcom api_key yourapikeyifrequired _note_ ckan instances require provide api token create one ckan user panel usage ckan handler provides three main tables datasets lists datasets ckan instance resources lists resources metadata across packages datastore allows querying individual datastore resources example queries 1 list datasets sql select yourdatasourcedatasets 2 list resources sql select yourdatasourceresources 3 query specific datastore resource sql select yourdatasourcedatastore resource_id yourresourceid replace yourresourceidhere actual resource id want query querying large resources ckan handler supports automatic pagination querying datastore resources allows retrieve large datasets without worrying api limits still use limit clause limit number rows returned query example sql select ckan_datasourcedatastore resource_id yourresourceidhere limit 1000 limitations handler currently supports read operations write operations supported performance may"
  },
  {
    "filename": "ckan.mdx",
    "path": "docs/integrations/data-integrations/ckan.mdx",
    "chunk_id": 1,
    "chunk_content": "vary depending size ckan instance complexity queries handler may work ckan instances especially custom configurations handler support ckan api features advanced features may available datastore search return limited records 32000 please refer ckan apihttpsdocsckanorgen211maintainingdatastorehtmlckanextdatastorelogicactiondatastore_search_sql documentation information"
  },
  {
    "filename": "apache-pinot.mdx",
    "path": "docs/integrations/data-integrations/apache-pinot.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache pinot sidebartitle apache pinot implementation pinot data handler mindsdb apache pinothttpspinotapacheorg realtime distributed olap database designed lowlatency query execution even extremely high throughput apache pinot ingest directly streaming sources like apache kafka make events available querying immediately prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache pinot mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache pinot implementation handler implemented using pinotdb library python dbapi sqlalchemy dialect pinot required arguments establish connection follows host host name ip address apache pinot cluster broker_port port broker apache pinot cluster running controller_port port controller apache pinot cluster running path query path usage order make use handler connect pinot cluster mindsdb following syntax used sql create database pinot_datasource engine pinot parameters hostlocalhost broker_port 8000 controller_port 9000 path querysql scheme http use established connection query table follows sql select pinot_datasourceexample_tbl"
  },
  {
    "filename": "microsoft-access.mdx",
    "path": "docs/integrations/data-integrations/microsoft-access.mdx",
    "chunk_id": 0,
    "chunk_content": "title microsoft access sidebartitle microsoft access implementation microsoft access data handler mindsdb microsoft accesshttpswwwmicrosoftcomenusmicrosoft365access pseudorelational database engine microsoft part microsoft office suite applications also includes word outlook excel among others access also available purchase standalone product uses jet database engine data storage prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect microsoft access mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access microsoft access implementation handler implemented using pyodbc python odbc bridge required argument establish connection db_file points database file queried usage order make use handler connect access database mindsdb following syntax used sql create database access_datasource engine access parameters db_filecusersminurapdocumentsexample_dbaccdb use established connection query table follows sql select access_datasourceexample_tbl"
  },
  {
    "filename": "d0lt.mdx",
    "path": "docs/integrations/data-integrations/d0lt.mdx",
    "chunk_id": 0,
    "chunk_content": "title d0lt sidebartitle d0lt implementation d0lt data handler mindsdb d0lthttpsdocsdolthubcomintroductionwhatisdolt singlenode embedded dbms incorporates gitstyle versioning firstclass entity d0lt behaves like git contentaddressable local database main objects tables instead files d0lt user creates database locally database contains tables read updated using sql similar git writes staged user issues commit upon commit writes appended permanent storage branch merge semantics supported allowing tables evolve different pace multiple users allows loose collaboration data well multiple views core data merge conflicts detected schema data conflicts data conflicts cellbased linebased remote repositories allow cooperation among repository instances clone push pull semantics available prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect d0lt mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access d0lt implementation handler implemented using mysqlconnector python library allows use python code run sql commands d0lt database required arguments establish connection follows user username associated database password password authenticate access host hostname ip address server port port tcpip connection made database database name connected usage order make use handler connect d0lt database mindsdb following syntax used sql create database d0lt_datasource engine d0lt parameters user root password host 127001 port 3306 database information_schema use established connection query"
  },
  {
    "filename": "d0lt.mdx",
    "path": "docs/integrations/data-integrations/d0lt.mdx",
    "chunk_id": 1,
    "chunk_content": "table follows sql select d0lt_datasourcetest"
  },
  {
    "filename": "mysql.mdx",
    "path": "docs/integrations/data-integrations/mysql.mdx",
    "chunk_id": 0,
    "chunk_content": "title mysql sidebartitle mysql documentation describes integration mindsdb mysqlhttpswwwmysqlcom fast reliable scalable opensource database integration allows mindsdb access data mysql enhance mysql ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect mysql mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection mysql mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersmysql_handler engine sql create database mysql_conn engine mysql parameters host hostname port 3306 database dbname user username password password sql create database mysql_datasource engine mysql parameters url mysqlusernamehostname3306 required connection parameters include following user username mysql database password password mysql database host hostname ip address url mysql server port port number connecting mysql server database name mysql database connect url specify connection mysql server using urilike string alternative connection option optional connection parameters include following ssl boolean parameter indicates whether ssl encryption enabled connection set true enable ssl enhance connection security set false use default nonencrypted connection ssl_ca specifies path certificate authority ca file pem format ssl_cert specifies path ssl certificate file certificate signed trusted ca specified ssl_ca file selfsigned certificate trusted server ssl_key specifies path private key file pem format usage following usage examples utilize connection mysql made via create database"
  },
  {
    "filename": "mysql.mdx",
    "path": "docs/integrations/data-integrations/mysql.mdx",
    "chunk_id": 1,
    "chunk_content": "statement named mysql_conn retrieve data specified table providing integration table name sql select mysql_conntable_name limit 10 tip next steps follow tutorialhttpsdocsmindsdbcomusecasesdata_enrichmenttextsummarizationinsidemysqlwithopenai see use case examples tip troubleshooting warning database connection error symptoms failure connect mindsdb mysql database checklist 1 ensure mysql server running accessible 2 confirm host port user password correct try direct mysql connection 3 test network connection mindsdb host mysql server warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces reserved words special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning"
  },
  {
    "filename": "trino.mdx",
    "path": "docs/integrations/data-integrations/trino.mdx",
    "chunk_id": 0,
    "chunk_content": "title trino sidebartitle trino implementation trino data handler mindsdb trinohttpstrinoio opensource distributed sql query engine designed query large data sets distributed one heterogeneous data sources prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect trino mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access trino implementation handler implemented using pyhive collection python dbapi sqlalchemy interfaces presto hive required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection optional arguments follows auth authentication method currently basic supported http_scheme takes value httpby default set https well catalog catalog schema schema name defines default withclause properties tables parameter experimental might changed removed future release usage order make use handler connect trino database mindsdb following syntax used sql create database trino_datasource engine trino parameters host 127001 port 443 auth basic http_scheme https user trino password password catalog default schema test transactional true use established connection query table follows sql select trino_datasourcedemo_table"
  },
  {
    "filename": "databricks.mdx",
    "path": "docs/integrations/data-integrations/databricks.mdx",
    "chunk_id": 0,
    "chunk_content": "title databricks sidebartitle databricks documentation describes integration mindsdb databrickshttpswwwdatabrickscom worlds first data intelligence platform powered generative ai integration allows mindsdb access data stored databricks workspace enhance ai capabilities tip data source integration threadsafe utilizing connection pool thread assigned connection handling requests parallel threads retrieve connections pool needed tip prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect databricks mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies note databricks cluster attempting connect terminated executing queries given attempt start cluster therefore first query may take minutes execute avoid delays ensure databricks cluster running executing queries note connection establish connection databricks workspace mindsdb executing following sql command sql create database databricks_datasource engine databricks parameters server_hostname adb12345678901234567azuredatabricksnet http_path sqlprotocolv1o12345678901234561234567890test123 access_token dapi1234567890ab1cde2f3ab456c7d89efa schema example_db required connection parameters include following server_hostname server hostname cluster sql warehouse http_path http path cluster sql warehouse access_token databricks personal access token workspace tip refer instructions given httpsdocsdatabrickscomenintegrationscomputedetailshtml httpsdocsdatabrickscomendevtoolspythonsqlconnectorhtmlauthentication find connection parameters mentioned compute resource tip optional connection parameters include following session_configuration additional key value pairs set spark session configuration parameters provided json string http_headers additional key value pairs set http headers every rpc request client makes provided json string catalog catalog use connection default hive_metastore"
  },
  {
    "filename": "databricks.mdx",
    "path": "docs/integrations/data-integrations/databricks.mdx",
    "chunk_id": 1,
    "chunk_content": "schema schema database use connection default default usage retrieve data specified table providing integration name catalog schema table name sql select databricks_datasourcecatalog_nameschema_nametable_name limit 10 note catalog schema names need provided table queried specified default catalog schema note run databricks sql queries directly connected databricks workspace sql select databricks_datasource native query goes select city car_model rank partition car_model order quantity rank dealer qualify rank 1 note examples utilize databricks_datasource datasource name defined create database command note troubleshooting guide warning database connection error symptoms failure connect mindsdb databricks workspace checklist 1 make sure databricks workspace active 2 confirm server hostname http path access token correctly provided catalog schema provided ensure correct well 3 ensure stable network mindsdb databricks workspace warning warning sql statements running tables reasonable size taking longer expected symptoms sql queries taking longer expected execute checklist 1 ensure databricks cluster running executing queries 2 check network connection mindsdb databricks workspace warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing special characters checklist 1 ensure table names special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning"
  },
  {
    "filename": "google-bigquery.mdx",
    "path": "docs/integrations/data-integrations/google-bigquery.mdx",
    "chunk_id": 0,
    "chunk_content": "title google bigquery sidebartitle google bigquery documentation describes integration mindsdb google bigqueryhttpscloudgooglecombigqueryhlen fully managed aiready data analytics platform helps maximize value data integration allows mindsdb access data stored bigquery warehouse enhance ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect bigquery mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection bigquery warehouse mindsdb executing following sql command sql create database bigquery_datasource engine bigquery parameters project_id bgtest1111 dataset mydataset service_account_keys tmpkeysjson required connection parameters include following project_id globally unique identifier project google cloud bigquery located dataset default dataset connect optional connection parameters include following service_account_keys full path service account key file service_account_json content json file defined service_account_keys parameter note one service_account_keys service_account_json provided establish connection bigquery note usage retrieve data specified table default dataset providing integration name table name sql select bigquery_datasourcetable_name limit 10 retrieve data specified table different dataset providing integration name dataset name table name sql select bigquery_datasourcedataset_nametable_name limit 10 run sql supported bigquery dialect directly connected bigquery database sql select bigquery_datasource native query goes select t1 t1a select t2a t2 system_time t1timestamp_column note examples utilize bigquery_datasource datasource name defined create database command note troubleshooting guide warning database connection error"
  },
  {
    "filename": "google-bigquery.mdx",
    "path": "docs/integrations/data-integrations/google-bigquery.mdx",
    "chunk_id": 1,
    "chunk_content": "symptoms failure connect mindsdb bigquery warehouse checklist 1 make sure google cloud account active google bigquery service enabled 2 confirm project id dataset service account credentials correct try direct bigquery connection using client like dbeaver 3 ensure stable network mindsdb google bigquery warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks examples _ incorrect select _ integrationtravel data _ incorrect select _ integrationtravel data _ correct select _ integrationtravel data warning"
  },
  {
    "filename": "clickhouse.mdx",
    "path": "docs/integrations/data-integrations/clickhouse.mdx",
    "chunk_id": 0,
    "chunk_content": "title clickhouse sidebartitle clickhouse documentation describes integration mindsdb clickhousehttpsclickhousecomdocsenintro highperformance columnoriented sql database management system dbms online analytical processing olap integration allows mindsdb access data clickhouse enhance clickhouse ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect clickhouse mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection clickhouse mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersclickhouse_handler engine sql create database clickhouse_conn engine clickhouse parameters host 127001 port 8443 user root password mypass database test_data protocol https required connection parameters include following host hostname ip address clickhouse server port tcpip port clickhouse server user username used authenticate clickhouse server password password authenticate user clickhouse server database defaults default database name use connecting clickhouse server protocol defaults native optional parameter supported values native http https usage following usage examples utilize connection clickhouse made via create database statement named clickhouse_conn retrieve data specified table providing integration table name sql select clickhouse_conntable_name limit 10 troubleshooting warning database connection error symptoms failure connect mindsdb clickhouse database checklist 1 ensure clickhouse server running accessible 2 confirm host port user password correct try direct mysql connection 3 test network connection mindsdb host clickhouse server warning warning slow connection"
  },
  {
    "filename": "clickhouse.mdx",
    "path": "docs/integrations/data-integrations/clickhouse.mdx",
    "chunk_id": 1,
    "chunk_content": "initialization symptoms connecting clickhouse server takes exceptionally long time connections hang without completing checklist 1 ensure using appropriate protocol http https native clickhouse setup misconfigurations lead significant delays 2 ensure firewalls security groups cloud environments properly configured allow traffic necessary ports 8123 http 9000 native warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces reserved words special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtravel data incorrect select integrationtravel data correct select integrationtravel data warning"
  },
  {
    "filename": "ibm-informix.mdx",
    "path": "docs/integrations/data-integrations/ibm-informix.mdx",
    "chunk_id": 0,
    "chunk_content": "title ibm informix sidebartitle ibm informix implementation ibm informix data handler mindsdb ibm informixhttpswwwibmcomproductsinformix product family within ibms information management division centered several relational database management system rdbms offerings informix server supports objectrelational models extensions data types part sql standard widely used json bson time series spatial extensions provide data type support language extensions permit highperformance domainspecific queries efficient storage data sets based semistructured time series spatial data prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect ibm informix mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access ibm informix implementation handler implemented using ifxpyifxpydbi python library allows use python code run sql commands informix database required arguments establish connection follows user username associated database password password authenticate access host hostname ip address server port port tcpip connection made database database name connected schema_name schema name get tables server name server want connect logging_enabled defines whether logging enabled defaults true provided usage order make use handler connect informix database mindsdb following syntax used sql create database informix_datasource engineinformix parameters server server host 127001 port 9091 user informix password in4mix database stores_demo schema_name love loging_enabled false use established connection query table follows sql"
  },
  {
    "filename": "ibm-informix.mdx",
    "path": "docs/integrations/data-integrations/ibm-informix.mdx",
    "chunk_id": 1,
    "chunk_content": "select informix_datasourceitems note integration uses ifxpy development stage install using pip install ifxpy however doesnt work higher versions python therefore build source accordiongroup accordion titleon linux 1 code downloads extracts onedbodbc driver used make connection bash cd home mkdir informix cd informix mkdir p homeinformixcli wget httpshclonedbgithubioodbconedblinux64odbcdrivertar sudo tar xvf onedblinux64odbcdrivertar c homeinformixcli rm onedblinux64odbcdrivertar 2 add enviroment variables bashrc file bash export informixdirhomeinformixhomeinformixclionedbodbcdriver export ld_library_pathld_library_pathinformixdirlibinformixdirlibesqlinformixdirlibcli 3 code clones ifxpy repo builds wheel installs bash pip install wheel mkdir temp cd temp git clone httpsgithubcomopeninformixifxpygit cd ifxpyifxpy python setuppy bdist_wheel pip install findlinksdist ifxpy cd cd cd rm rf temp accordion accordion titleon windows 1 code downloads extracts onedbodbc driver used make connection bash cd home mkdir informix cd informix mkdir homeinformixcli wget httpshclonedbgithubioodbconedbwin64odbcdriverzip tar xvf onedbwin64odbcdriverzip c homeinformixcli del onedbwin64odbcdriverzip 2 add enviroment variable bash set informixdirhomeinformixhomeinformixclionedbodbcdriver 3 add informixdirbin path environment variable 4 code clones ifxpy repo builds wheel installs bash pip install wheel mkdir temp cd temp git clone httpsgithubcomopeninformixifxpygit cd ifxpyifxpy python setuppy bdist_wheel pip install findlinksdist ifxpy cd cd cd rmdir temp accordion accordiongroup note"
  },
  {
    "filename": "amazon-s3.mdx",
    "path": "docs/integrations/data-integrations/amazon-s3.mdx",
    "chunk_id": 0,
    "chunk_content": "title amazon s3 sidebartitle amazon s3 documentation describes integration mindsdb amazon s3httpsdocsawsamazoncomamazons3latestuserguidewelcomehtml object storage service offers industryleading scalability data availability security performance tip data source integration threadsafe utilizing connection pool thread assigned connection handling requests parallel threads retrieve connections pool needed tip prerequisites proceeding ensure mindsdb installed locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop connection establish connection amazon s3 bucket mindsdb executing following sql command sql create database s3_datasource engine s3 parameters aws_access_key_id aqaxeqk89ox07ys34op aws_secret_access_key wjalrxutnfemik7mdengbpxrficyexamplekey bucket mybucket note note sample parameter values provided reference replace connection parameters note required connection parameters include following aws_access_key_id aws access key identifies user iam role aws_secret_access_key aws secret access key identifies user iam role optional connection parameters include following aws_session_token aws session token identifies user iam role becomes necessary using temporary security credentials bucket name amazon s3 bucket provided available buckets queried however affect performance especially listing available objects usage retrieve data specified object file s3 bucket providing integration name object key sql select s3_datasourcemyfilecsv limit 10 tip bucket name provided create database command querying limited bucket bucket name ommitted object key shown example however bucket name provided object key must include bucket name s3_datasourcemybucketmyfoldermyfilecsv wrap object key backticks avoid issues parsing sql statements provided"
  },
  {
    "filename": "amazon-s3.mdx",
    "path": "docs/integrations/data-integrations/amazon-s3.mdx",
    "chunk_id": 1,
    "chunk_content": "especially important object key contains spaces special characters prefixes myfoldermyfilecsv moment supported file formats csv tsv json parquet tip note examples utilize s3_datasource datasource name defined create database command note special files table used list objects available specified bucket buckets bucket name provided sql select s3_datasourcefiles limit 10 content files also retrieved explicitly requesting content column column empty default avoid unnecessary data transfer sql select path content s3_datasourcefiles limit 10 tip table return objects regardless file format however supported file formats mentioned queried tip troubleshooting guide warning database connection error symptoms failure connect mindsdb amazon s3 bucket checklist 1 make sure amazon s3 bucket exists 2 confirm provided aws credentials correct try making direct connection s3 bucket using aws cli 3 ensure stable network mindsdb aws warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing object names containing spaces special characters prefixes checklist 1 ensure object names spaces special characters prefixes enclosed backticks 2 examples incorrect select integrationtraveltravel_datacsv incorrect select integrationtraveltravel_datacsv correct select integrationtraveltravel_datacsv warning"
  },
  {
    "filename": "cratedb.mdx",
    "path": "docs/integrations/data-integrations/cratedb.mdx",
    "chunk_id": 0,
    "chunk_content": "title cratedb sidebartitle cratedb implementation cratedb data handler mindsdb cratedbhttpscrateio distributed sql database management system integrates fully searchable documentoriented data store opensource written java based sharednothing architecture designed high scalability cratedb includes components lucene elasticsearch netty prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect cratedb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access cratedb implementation handler implemented using crate python library allows use python code run sql commands cratedb required arguments establish connection follows user username associated database password password authenticate access host hostname ip adress server port port connection made schema_name schema name get tables defaults doc usage order make use handler connect cratedb database mindsdb following syntax used sql create database crate_datasource engine crate parameters user crate password host 127001 port 4200 schema_name doc use established connection query table follows sql select crate_datasourcedemo"
  },
  {
    "filename": "opengauss.mdx",
    "path": "docs/integrations/data-integrations/opengauss.mdx",
    "chunk_id": 0,
    "chunk_content": "title opengauss sidebartitle opengauss implementation opengauss data handler mindsdb opengausshttpsopengaussorgen opensource relational database management system released mulan psl v2 kernel built huaweis years experience database field continuously provides competitive features tailored enterprisegrade scenarios prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect opengauss mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access opengauss implementation handler implemented extending postgresql data handler required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name usage order make use handler connect opengauss database mindsdb following syntax used sql create database opengauss_datasource engine opengauss parameters host 127001 port 5432 database opengauss user mindsdb password password use established connection query table follows sql select opengauss_datasourcedemo_table limit 10"
  },
  {
    "filename": "sap-hana.mdx",
    "path": "docs/integrations/data-integrations/sap-hana.mdx",
    "chunk_id": 0,
    "chunk_content": "title sap hana sidebartitle sap hana documentation describes integration mindsdb sap hanahttpswwwsapcomproductstechnologyplatformhanawhatissaphanahtml multimodel database columnoriented inmemory design stores data memory instead keeping disk integration allows mindsdb access data sap hana enhance sap hana ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect sap hana mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection sap hana mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlershana_handler engine sql create database sap_hana_datasource engine hana parameters address 123e4567e89b12d3a456426614174000hanatrialus10hanacloudondemandcom port 443 user demo_user password demo_password encrypt true required connection parameters include following address hostname ip address url sap hana database port port number connecting sap hana database user username sap hana database password password sap hana database optional connection parameters include following database name database connect parameter used sap hana cloud schema database schema use defaults users default schema encrypt setting enable disable encryption defaults true usage retrieve data specified table providing integration schema table names sql select sap_hana_datasourceschema_nametable_name limit 10 run teradata sql queries directly connected teradata database sql select sap_hana_datasource native query goes select customer year sumsales t1 group rollupcustomer year select customer year sumsales t1 group grouping sets customer year customer union select null"
  },
  {
    "filename": "sap-hana.mdx",
    "path": "docs/integrations/data-integrations/sap-hana.mdx",
    "chunk_id": 1,
    "chunk_content": "null sumsales t1 note examples utilize sap_hana_datasource datasource name defined create database command note troubleshooting warning database connection error symptoms failure connect mindsdb sap hana database checklist 1 make sure sap hana database active 2 confirm address port user password correct try direct connection using client like dbeaver 3 ensure stable network mindsdb sap hana warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing table names containing spaces special characters checklist 1 ensure table names spaces special characters enclosed backticks 2 examples incorrect select integrationtraveldata incorrect select integrationtraveldata correct select integrationtraveldata warning"
  },
  {
    "filename": "apache-druid.mdx",
    "path": "docs/integrations/data-integrations/apache-druid.mdx",
    "chunk_id": 0,
    "chunk_content": "title apache druid sidebartitle apache druid implementation druid data handler mindsdb apache druidhttpsdruidapacheorgdocslatestdesign realtime analytics database designed fast sliceanddice analytics _olap_ queries large data sets often druid powers use cases realtime ingestion fast query performance high uptime important prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect apache druid mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access apache druid implementation handler implemented using pydruid library python api apache druid required arguments establish connection follows host host name ip address apache druid database port port apache druid running path query path scheme uri schema parameter optional defaults http user username used authenticate apache druid parameter optional password password used authenticate apache druid parameter optional usage order make use handler connect apache druid mindsdb following syntax used sql create database druid_datasource engine druid parameters host localhost port 8888 path druidv2sql scheme http use established connection query table follows sql select druid_datasourceexample_tbl"
  },
  {
    "filename": "scylladb.mdx",
    "path": "docs/integrations/data-integrations/scylladb.mdx",
    "chunk_id": 0,
    "chunk_content": "title scylladb sidebartitle scylladb implementation scylladb data handler mindsdb scylladbhttpswwwscylladbcom opensource distributed nosql widecolumn data store purposefully designed offer compatibility apache cassandra outperforming higher throughputs reduced latencies comprehensive understanding scylladb visit scylladbs official website prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect scylladb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access scylladb implementation scylladb handler mindsdb developed using scylladriver library python required arguments establish connection follows host host name ip address scylladb port connection port user authentication username optional required authentication enabled password authentication password optional required authentication enabled keyspace specific keyspace toplevel container tables connect protocol_version optional defaults 4 secure_connect_bundle optional needed connections datastax astra usage set connection mindsdb scylla server utilize following sql syntax sql create database scylladb_datasource engine scylladb parameters user usermindsdbcom password pass host 127001 port 9042 keyspace test_data tip protocol version set 4 default wish modify simply include protocol_version 5 within parameters dictionary query tip connection established execute queries keyspace demonstrated sql select scylladb_datasourcekeystoreexample_table limit 10"
  },
  {
    "filename": "cockroachdb.mdx",
    "path": "docs/integrations/data-integrations/cockroachdb.mdx",
    "chunk_id": 0,
    "chunk_content": "title cockroachdb sidebartitle cockroachdb implementation cockroachdb data handler mindsdb cockroachdbhttpswwwcockroachlabscomdocs architected complex high performant distributed writes delivers scaleout read capability cockroachdb delivers simple relational sql transactions obscures complexity away developers wirecompatible postgresql provides familiar easy interface developers prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect cockroachdb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access cockroachdb implementation cockroachdb wirecompatible postgresql therefore implementation extends postgresql handler required arguments establish connection follows host host name ip address cockroachdb database name database connect user user authenticate cockroachdb port port use connecting password password authenticate user order make use handler connect cockroachdb server mindsdb following syntax used sql create database cockroachdb engine cockroachdb parameters host localhost database dbname user admin password password port 5432 usage use established connection query table follows sql select cockroachdbpublicdb"
  },
  {
    "filename": "yugabytedb.mdx",
    "path": "docs/integrations/data-integrations/yugabytedb.mdx",
    "chunk_id": 0,
    "chunk_content": "title yugabytedb sidebartitle yugabytedb implementation yugabytedb data handler mindsdb yugabytedbhttpswwwyugabytecom highperformance cloudnative distributed sql database aims support postgresql features best fit cloudnative oltp ie realtime businesscritical applications need absolute data correctness require least one following scalability high tolerance failures globallydistributed deployments prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect yugabytedb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access yugabytedb implementation handler implemented using psycopg2 python library allows use python code run sql commands yugabytedb database required arguments establish connection follows user database user password database password host host name ip address url port port used make tcpip connection database database name schema schema table belongs usage order make use handler connect yugabytedb database mindsdb following syntax used sql create database yugabyte_datasource engine yugabyte parameters user admin password 1234 host 127001 port 5433 database yugabyte schema your_schema_name use established connection query table follows sql select yugabyte_datasourcedemo note note using yugabytedb cloud mindsdb cloud website need add 3 static ips mindsdb cloud allow ip list accessing publicly 1822020595 31915246 521491162 publichttpsgithubproductionuserasset6210dfs3amazonawscom756535802389035481b054591f5db4a6da3d0d048671e4cfapng note"
  },
  {
    "filename": "datastax.mdx",
    "path": "docs/integrations/data-integrations/datastax.mdx",
    "chunk_id": 0,
    "chunk_content": "title datastax sidebartitle datastax implementation datastax data handler mindsdb datastaxhttpswwwdatastaxcom astra db cloud databaseasaservice based apache cassandra datastax also offers datastax enterprise dse onpremises database built apache cassandra astra streaming messaging event streaming cloud service based apache pulsar prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect datastax mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access datastax implementation datastax astra db apicompatible apache cassandra scylladb therefore implementation extends scylladb handler using scylladriver python library required arguments establish connection follows user user authenticate password password authenticate user secure_connect_bundle path secure_connect_bundle zip file usage order make use handler connect astra db database mindsdb following syntax used sql create database astra_connection engine astra parameters user user password pass secure_connect_bundle homedownloadsfilezip reference bundle datastax s3 sql create database astra_connection engine astra parameters user user password pass secure_connect_bundle httpsdatastaxclusterconfigprods3useast2amazonawscom32312b9eb4e09a641213eaesa121secureconnectdemozipxamzalgorithmaws4hmacsha256xamzcredentialak use established connection query table follows sql select astra_connectionkeystoreexample_table limit 10"
  },
  {
    "filename": "web-crawler.mdx",
    "path": "docs/integrations/app-integrations/web-crawler.mdx",
    "chunk_id": 0,
    "chunk_content": "title web crawler sidebartitle web crawler section present use web crawler within mindsdb web crawler automated script designed systematically browse index content internet within mindsdb utilize web crawler efficiently collect data various websites prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 use web crawler mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection handler require connection parameters initialize web crawler sql create database my_web engine web tip query creates database called my_web database default table called crawler use crawl data given urlurls tip usage note specifying limit clause required crawl pages site consider setting limit high value 10000 exceeds expected number pages aware setting higher limit may result longer response times note get websites content following usage examples demonstrate retrieve content docsmindsdbcom sql select my_webcrawler url docsmindsdbcom limit 1 also retrieve content internal pages following query fetches content 10 internal pages sql select my_webcrawler url docsmindsdbcom limit 10 order get content multiple websites use union operator sql select my_webcrawler url docsmindsdbcom limit 5 union select my_webcrawler url docspythonorg limit 5 get pdf content mindsdb accepts file uploadssqlcreatefile csv xlsx xls sheet json parquet however also configure web crawler fetch data pdf files accessible via urls"
  },
  {
    "filename": "web-crawler.mdx",
    "path": "docs/integrations/app-integrations/web-crawler.mdx",
    "chunk_id": 1,
    "chunk_content": "sql select my_webcrawler url linktopdffile limit 1 configuring web handler specific domains web handler configured interact specific domains using web_crawling_allowed_sites setting configjson file feature allows restrict handler crawl process content domains specify enhancing security control web interactions configure simply list allowed domains web_crawling_allowed_sites key configjson example json web_crawling_allowed_sites httpsdocsmindsdbcom httpsanotherallowedsitecom troubleshooting warning web crawler encounters character encoding issues symptoms extracted text appears garbled contains strange characters instead expected text checklist 1 open github issue encounter bug repeatable error encoding report mindsdb githubhttpsgithubcommindsdbmindsdbissues repository opening issue warning warning web crawler times trying fetch content symptoms crawler fails retrieve data website resulting timeout errors checklist 1 check network connection ensure target site reachable warning"
  },
  {
    "filename": "hackernews.mdx",
    "path": "docs/integrations/app-integrations/hackernews.mdx",
    "chunk_id": 0,
    "chunk_content": "title hacker news sidebartitle hacker news section present connect hacker news mindsdb hacker newshttpsnewsycombinatorcom online platform community discussions related technology startups computer science entrepreneurship wide range topics interest tech hacker communities created combinator wellknown startup accelerator data hacker news including articles user comments utilized within mindsdb train ai models chatbots knowledge discussions shared hacker news connection handler implemented using official hacker news api provides simple easytouse interface access hacker news api connection arguments required order make use handler connect hacker news mindsdb following syntax used sql create database my_hackernews engine hackernews creates database comes stories comments tables usage query articles like sql select my_hackernewsstories limit 2 fetch comments specific article sql select my_hackernewscomments item_id35662571 limit 1"
  },
  {
    "filename": "email.mdx",
    "path": "docs/integrations/app-integrations/email.mdx",
    "chunk_id": 0,
    "chunk_content": "title email sidebartitle email section present connect email accounts mindsdb connecting email account mindsdb utilize various ai models available within mindsdb summarize emails detect spam even automate email replies note please note currently connect gmail outlook accounts using integration note connection handler implemented using standard python libraries email imaplib smtplib email handler initialized following required parameters email stores email address used authentication password stores password used authentication additionally following optional parameters passed smtp_server used send emails defaults smtpgmailcom smtp_port used send emails defaults 587 imap_server used receive emails defaults imapgmailcom tip moment handler tested gmail outlook accounts use handler gmail account must create app password following instructionhttpssupportgooglecomaccountsanswer185833hlen use value password parameter default email handler connects gmail want use email providers outlook add values imap_server smtp_server parameters tip gmail connect gmail account mindsdb use create database statement sql create database email_datasource engine email parameters email youremailgmailcom password yourpassword creates database comes emails table outlook connect outlook account mindsdb use create database statement sql create database email_datasource engine email parameters email youremailoutlookcom password yourpassword smtp_server smtpoffice365com smtp_port 587 imap_server outlookoffice365com creates database comes emails table usage query emails like sql select email_datasourceemails apply filters like sql select id body subject to_field from_field"
  },
  {
    "filename": "email.mdx",
    "path": "docs/integrations/app-integrations/email.mdx",
    "chunk_id": 1,
    "chunk_content": "datetime email_datasourceemails subject mindsdb order id limit 5 write emails like sql insert email_datasourceemailsto_field subject body values toemailoutlookcom mindsdb hello mindsdb"
  },
  {
    "filename": "instatus.mdx",
    "path": "docs/integrations/app-integrations/instatus.mdx",
    "chunk_id": 0,
    "chunk_content": "title instatus sidebartitle instatus section present connect instatus mindsdb instatushttpsinstatuscom cloudbased status page software enables users communicate status information using incidents maintenances serves saas platform creating status pages services instatus handler mindsdb offers interface connect instatus via apis retrieve status pages connection initialize instatus handler following parameter api_key instatus api key authentication obtain instatus developer dashboardhttpsdashboardinstatuscomdeveloper start creating database new instatus engine using following sql command sql create database mindsdb_instatus display name database engine instatus name mindsdb handler parameters api_key yourinstatusapikey instatus api key use authentication usage get status page use select statement sql select id name status subdomain mindsdb_instatusstatus_pages id statuspageid limit 10 create new status page use insert statement sql insert mindsdb_instatusstatus_pages email name subdomain components logourl faviconurl websiteurl language uselargeheader brandcolor okcolor disruptedcolor degradedcolor downcolor noticecolor unknowncolor googleanalytics subscribebysms smsservice twiliosid twiliotoken twiliosender nexmokey nexmosecret nexmosender htmlinmeta htmlaboveheader htmlbelowheader htmlabovefooter htmlbelowfooter htmlbelowsummary cssglobal launchdate dateformat dateformatshort timeformat values yournamegmailcom mindsdb mindsdbinstatus website app api httpsinstatuscomsamplepng httpsinstatuscomfavicon32x32png httpsinstatuscom en true 111 33b17e ff8c03 ecc94b dc123d 70808f dfe0e1 ua000000001 true twilio your_twilio_sid your_twilio_token your_twilio_sender null null null null null null null null null null mmmmmm yyyy mmm yyyy p tip following fields required inserting new status pages email eg"
  },
  {
    "filename": "instatus.mdx",
    "path": "docs/integrations/app-integrations/instatus.mdx",
    "chunk_id": 1,
    "chunk_content": "yournamegmailcom name eg mindsdb subdomain eg mindsdbdocs components eg website app api fields optional tip update existing status page use update statement sql update mindsdb_instatusstatus_pages set name mindsdb status logourl httpsinstatuscomsamplepng faviconurl httpsinstatuscomfavicon32x32png websiteurl httpsinstatuscom language en translations name fr nasa id statuspageid"
  },
  {
    "filename": "twitter.mdx",
    "path": "docs/integrations/app-integrations/twitter.mdx",
    "chunk_id": 0,
    "chunk_content": "title twitter sidebartitle twitter section present connect twitter accounts mindsdb twitterhttpstwittercom widely recognized social media platform microblogging service allows users share short messages called tweets twitter handler enables fetch tweets create replies utilizing ai models wthin mindsdb furthermore automate process fetching tweets preparing replies sending replies twitter prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect twitter mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access twitter connection connect twitter account mindsdb need twitter developer account tip please note requires paid developer account recommend use elevated accesshttpsdevelopertwittercomensupporttwitterapideveloperaccount allowing pull 2m tweets avoid _parameters authentication issue_ error might get sometimes check stepbystep guidehttpsmediumcomskillcatesetuptwitterapitopull2mtweetsmonth44d004c6f7ce describing apply elevated access tip dont already twitter developer account follow steps video apply one accordion titlehow apply twitter developer account icontwitter icontypethin begin apply twitter developer accounthttpsdevelopertwittercomapplyforaccess div styleposition relative paddingbottom 59602649006622514 height 0iframe srchttpswwwyoutubecomembedqve7pec0suq titleyoutube video player frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia fullscreen gyroscope pictureinpicture webshare styleposition absolute top 0 left 0 width 100 height 100iframediv presented questions use twitter api twitter data use answers similar ones tweak fit exact use case thorough answers likely account get approved intended usage words blog want educate users use twitter api"
  },
  {
    "filename": "twitter.mdx",
    "path": "docs/integrations/app-integrations/twitter.mdx",
    "chunk_id": 1,
    "chunk_content": "mindsdb read tweets mention use mindsdb machine learning generate responses plan post tweets 23 times day keep using twitter like normally would planning analyze twitter data plan build machine learning algorithms based twitter data interested sentiment analysis topic analysis potentially extract tweet text favorite count retweet count hashtags mentions app use tweet retweet like follow direct message functionality use twitter api post responses tweets mention word filters make sure never share offensive potentially controversial subjects plan display tweets aggregate data twitter content outside twitter plan share aggregate data examples users upcoming blog dont intend create automated dashboard consumes lot twitter api calls every api call done locally automated simple web server aggregate data educational purposes product service analysis make twitter content derived information available government entity answer one accordion already twitter developer account need generate api keys following instructions heading twitter developer websitehttpsdevelopertwittercomen accordion titlehow generate api keys icontwitter icontypethin create application readwrite permissions activated open developer portalhttpsdevelopertwittercomenportalprojectsandapps select add app button create new app select create new button select production give name copy populate following create database statement bearer token value bearer_token parameter api key value consumer_key parameter api key secret value consumer_secret parameter setup user authentication settings click"
  },
  {
    "filename": "twitter.mdx",
    "path": "docs/integrations/app-integrations/twitter.mdx",
    "chunk_id": 2,
    "chunk_content": "setup user authentication settings permissions select read write type app select web app automated app bot app info provide url callback url website url use url page click save generate access tokens back app settings click keys tokens generate access token access token secret populate create database statement access token value access_token parameter access token secret value access_token_secret parameter accordion tokens keys connect twitter account mindsdb sql create database my_twitter engine twitter parameters bearer_token twitter bearer token consumer_key twitter consumer key consumer_secret twitter consumer key secret access_token twitter access token access_token_secret twitter access token secret usage my_twitter database contains table called tweets default search tweets containing mindsdb keyword sql select id created_at author_username text my_twittertweets query mindsdb mindsdb isretweet isreply created_at 20230216 limit 20 warning please note see recent tweets past seven days created_at column condition skipped provided date earlier seven days warning alternatively use twitter native query sql select my_twitter search_recent_tweets query mindsdb mindsdb isretweet isreply start_time 20230316t000000000z max_results 2 tip learn native queries mindsdb visit docs heresqlnativequeries tip write tweets sql insert my_twittertweets reply_to_tweet_id text values 1626198053446369280 mindsdb great super simple build ml powered apps 1626198053446369280 holy mindsdb best thing invented developers ml info information available actions development"
  },
  {
    "filename": "twitter.mdx",
    "path": "docs/integrations/app-integrations/twitter.mdx",
    "chunk_id": 3,
    "chunk_content": "plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlerstwitter_handlerreadmemd info tip whats next check tutorial create twitter chatbotsqltutorialstwitterchatbot see one interesting applications integration tip"
  },
  {
    "filename": "slack.mdx",
    "path": "docs/integrations/app-integrations/slack.mdx",
    "chunk_id": 0,
    "chunk_content": "title slack sidebartitle slack documentation describes integration mindsdb slackhttpsslackcom cloudbased collaboration platform integration allows mindsdb access data slack enhance slack ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect slack mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access slack connection establish connection slack mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersslack_handler engine sql create database slack_datasource engine slack parameters token values required parameter app_token values optional parameter slack handler initialized following parameters token slack bot token use authentication app_token slack app token use authentication note please note app_token optional parameter without providing need integrate app slack channel note method 1 chatbot responds direct messages slack app one way connect slack use bot app tokens following instructions youll set slack app able message slack app directly chat bot note want use slack create chatbotagentschatbot syntax use method connecting slack mindsdb note accordion titleset slack app generate tokens set slack app generate slack bot token slack app token 1 follow linkhttpsapislackcomapps sign slack account 2 create new app scratch select existing app please note following instructions support apps created scratch apps created app manifest please follow slack docs herehttpsapislackcomreferencemanifests 3 go"
  },
  {
    "filename": "slack.mdx",
    "path": "docs/integrations/app-integrations/slack.mdx",
    "chunk_id": 1,
    "chunk_content": "basic information settings applevel tokens click generate token scopes name token socket add connectionswrite scope copy save xapp token youll need publish chatbot 4 go socket mode settings toggle button enable socket mode 5 go oauth permissions features add following bot token scopes app_mentionsread channelshistory channelsread chatwrite groupshistory groupsread optional imhistory imread imwrite mpimread optional usersprofileread usersread optional oauth tokens workspace section click install workspace allow copy save xoxb token youll need publish chatbot 6 go app home features click checkbox allow users send slash commands messages messages tab 7 go event subscriptions features toggle button enable events subscribe bot events click add bot user event add app_mention messageim click save changes 8 use tokens points 3 5 initialize slack handler mindsdb accordion note connection method enables chat directly app via slack alternatively connect app slack channel go channel want use bot rightclick channel select view channel details select integrations click add app note connect slack mindsdb sql create database slack_datasource engine slack parameters token xoxb app_token xapp comes conversations messages tables method 2 chatbot responds defined slack channel another way connect slack use bot token following instructions youll set slack app integrate one channels directly chat bot accordion titleset"
  },
  {
    "filename": "slack.mdx",
    "path": "docs/integrations/app-integrations/slack.mdx",
    "chunk_id": 2,
    "chunk_content": "slack app generate tokens set slack app generate slack bot token 1 follow linkhttpsapislackcomapps sign slack account 2 create new app scratch select existing app please note following instructions support apps created scratch apps created app manifest please follow slack docs herehttpsapislackcomreferencemanifests 3 go oauth permissions section 4 scopes section add bot token scopes necessary application add later well channelshistory channelsread chatwrite groupsread imread mpimread usersread 5 install bot workspace 6 oauth tokens workspace section copy bot user oauth token value 7 open slack application add appbot one channels go channel want use bot rightclick channel select view channel details select integrations click add app 8 use token step 6 initialize slack handler mindsdb use channel name query write messages accordion connect slack mindsdb sql create database slack_datasource engine slack parameters token xoxb usage warning following usage applies connection method 2 used connect slack see usage connection method 1 via create chatbot syntaxsqltutorialscreatechatbot warning retrieve data specified table providing integration table names sql select slack_datasourcetable_name limit 10 supported tables slack integration supports following tables conversations table conversations virtual table used query conversations channels dms groups connected slack workspace sql retrieve conversations workspace select slack_datasourceconversations retrieve specific conversation using id select"
  },
  {
    "filename": "slack.mdx",
    "path": "docs/integrations/app-integrations/slack.mdx",
    "chunk_id": 3,
    "chunk_content": "slack_datasourceconversations channel_id channelid retrieve specific conversation using name select slack_datasourceconversations name channelname messages table messages virtual table used query post update delete messages specific conversations within connected slack workspace sql retrieve messages specific conversation channel_id required parameter found conversations table select slack_datasourcemessages channel_id channelid post new message channel_id text required parameters insert slack_datasourcemessages channel_id text valueschannelid hello sql update botposted message channel_id ts text required parameters update slack_datasourcemessages set text updated message content channel_id channelid ts timestamp delete botposted message channel_id ts required parameters delete slack_datasourcemessages channel_id channelid ts timestamp tip also find channel id rightclicking conversation slack selecting view conversation details view channel details copying channel id bottom tab tip threads table threads virtual table used query post messages threads within connected slack workspace sql retrieve messages specific thread channel_id thread_ts required parameters thread_ts timestamp parent message found messages table select slack_datasourcethreads channel_id channelid thread_ts threadts post message thread insert slack_datasourcethreads channel_id thread_ts text valueschannelid threadts replying thread users table users virtual table used query user information connected slack workspace sql retrieve users workspace select slack_datasourceusers retrieve specific user name select slack_datasourceusers name john doe rate limit considerations slack api enforces rate limits data retrieval therefore querying tables default"
  },
  {
    "filename": "slack.mdx",
    "path": "docs/integrations/app-integrations/slack.mdx",
    "chunk_id": 4,
    "chunk_content": "first 1000 999 messages records returned retrieve records use limit clause sql queries example sql select slack_datasourceconversations limit 2000 using limit clause query additional records may encounter slack api rate limits next steps follow tutorialusecasesai_agentsbuild_ai_agents build ai agent mindsdb"
  },
  {
    "filename": "reddit.mdx",
    "path": "docs/integrations/app-integrations/reddit.mdx",
    "chunk_id": 0,
    "chunk_content": "title reddit sidebartitle reddit section present connect reddit mindsdb reddithttpswwwredditcom social media platform online community registered users engage discussions share content participate various communities called subreddits data reddit utilized within mindsdb train ai models chatbots prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect reddit mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access reddit connection handler implemented using praw python reddit api wrapperhttpsprawreadthedocsioenlatest library python package provides simple easytouse interface access reddit api required arguments establish connection follows client_id reddit api client id client_secret reddit api client secret user_agent user agent string identify application tip get reddit credentials 1 go reddit app preferences httpswwwredditcomprefsapps httpsoldredditcomprefsapps 2 scroll bottom page click create another app 3 fill form name description redirect url app click create app 4 able see personal user script secret name app store environment variables client_id client_secret user_agent respectively tip order make use handler connect reddit app mindsdb following syntax used sql create database my_reddit engine reddit parameters client_id your_client_id client_secret your_client_secret user_agent your_user_agent creates database comes two tables submission comment usage fetch data reddit like sql select my_redditsubmission subreddit machinelearning sort_type top specifies sorting type subreddit possible values include"
  },
  {
    "filename": "reddit.mdx",
    "path": "docs/integrations/app-integrations/reddit.mdx",
    "chunk_id": 1,
    "chunk_content": "hot new top controversial gilded wiki mod rising items 5 specifies number items fetch subreddit also fetch comments particular postsubmission like sql select my_redditcomment submission_id 12gls93 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersreddit_handlerreadmemd info"
  },
  {
    "filename": "google-calendar.mdx",
    "path": "docs/integrations/app-integrations/google-calendar.mdx",
    "chunk_id": 0,
    "chunk_content": "title google calendar sidebartitle google calendar section present connect google calendar mindsdb google calendarhttpscalendargooglecomcalendar online calendar service application developed google allows users create manage share events appointments well schedule organize personal work team activities data google calendar utilized within mindsdb train ai models make predictions automate time management ai prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect google calendar mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access google calendar connection required arguments establish connection follows credentials_file path json file stores credentials google account tip please note google account enabled google calendar required find information herehttpsdevelopersgooglecomcalendarapiquickstartpython tip order make use handler connect google calendar app mindsdb following syntax used sql create database my_calendar engine google_calendar parameters credentials_file pathtoyourfilecredentialsjson tip need google account order use integration get credentials file 1 create google cloud platform gcp project 11 go gcp console httpsconsolecloudgooglecom 12 havent created project youll prompted 13 give new project name 14 click create create new project 2 enable google calendar api 21 gcp console select project 22 navigate apis services library 23 search bar search google calendar api 24 click google calendar api click enable 3 create credentials google calendar"
  },
  {
    "filename": "google-calendar.mdx",
    "path": "docs/integrations/app-integrations/google-calendar.mdx",
    "chunk_id": 1,
    "chunk_content": "api 31 navigate apis services credentials 32 click create credentials button choose oauth client id 33 havent configured oauth consent screen youll prompted make sure choose external user type add necessary scopes make sure save changes create oauth client id choose desktop app application type give name 34 click create 4 download json file 41 creating credentials click download button icon arrow pointing right side client id download json file use location credentials_file param tip usage creates database comes calendar table use google calendar data like searching events sql select id created_at author_username text my_calendarevents start_time 20230216 end_time 20230409 limit 20 creating events sql insert my_calendareventsstart_time end_time summary description location attendees reminders timezone values 20230216 100000 20230216 110000 mindsdb meeting discussing future mindsdb mindsdb hq europeathens updating one events sql update my_calendarevents set summary mindsdb meeting description discussing future mindsdb location mindsdb hq attendees reminders event_id 1 event_id 10 used update events given range deleting one events sql delete my_calendarevents id 1 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersgoogle_calendar_handlerreadmemd info"
  },
  {
    "filename": "pypi.mdx",
    "path": "docs/integrations/app-integrations/pypi.mdx",
    "chunk_id": 0,
    "chunk_content": "title pypi sidebartitle pypi section present connect pypi mindsdb pypihttpspypiorg host maintaining storing python packages good place publishing python packages different versions releases data pypi utilized within mindsdb train models make predictions python packages connection handler implemented using standard python requests library used connect restful service pypistatsorghttpspypistatsorg serving connection arguments required initialize handler connect pypi using mindsdb following create database statement used sql create database pypi_datasource engine pypi usage use following queries view statistics python packages mindsdb example overall downloads including mirrors sql select pypi_datasourceoverall packagemindsdb mirrorstrue overall downloads cpython27 sql select pypi_datasourcepython_minor packagemindsdb version27 recent downloads sql select pypi_datasourcerecent packagemindsdb recent downloads last day sql select pypi_datasourcerecent packagemindsdb periodday downloads linuxbased distributions sql select date downloads pypi_datasourcesystem packagemindsdb oslinux tip table takes required package argument clause name package want query tip supported tables following tables supported pypi handler overall daily download quantities packages recent recent download quantities packages python_major daily download quantities packages grouped python major version python_minor daily download quantities packages grouped python minor version system daily download quantities packages grouped operating system"
  },
  {
    "filename": "gmail.mdx",
    "path": "docs/integrations/app-integrations/gmail.mdx",
    "chunk_id": 0,
    "chunk_content": "title gmail sidebartitle gmail section present connect gmail accounts mindsdb gmailhttpsgmailcom widely used popular email service developed google connecting gmail account mindsdb utilize various ai models available within mindsdb summarize emails detect spam even automate email replies warning please note currently connect gmail account local mindsdb installation providing path credentials file stored locally want connect gmail account mindsdb cloud upload credentials file instance s3 bucket provide link parameter warning prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect gmail mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access gmail connection required arguments establish connection follows credentials_file local path credentialsjson credentials_url case file uploaded s3 follow instructions generate credentials file scopes define level access granted optional default uses httpsgmailcompose httpsgmailreadonly scopes order make use handler connect google calendar app mindsdb following syntax used sql create database mindsdb_gmail engine gmail parameters credentials_file mindsdbintegrationshandlersgmail_handlercredentialsjson scopes httpsgmailcompose httpsgmailreadonly also connect giving credentials file s3 pre signed urlhttpsdocsawsamazoncomamazons3latestuserguideshareobjectpresignedurlhtml need pass credentials_file parameter pre signed urlhttpsdocsawsamazoncomamazons3latestuserguideshareobjectpresignedurlhtml example sql create database mindsdb_gmail engine gmail parameters credentials_url httpss3amazonawscomyour_bucketcredentialsjsonresponsecontentdispositioninlinexamzsecuritytoken12312 scopes scope_1 scope_2 optional scopes default httpsgmailcompose httpsgmailreadonly scopes used tip need google account order use integration get credentials file 1 create"
  },
  {
    "filename": "gmail.mdx",
    "path": "docs/integrations/app-integrations/gmail.mdx",
    "chunk_id": 1,
    "chunk_content": "google cloud platform gcp project 11 go gcp console httpsconsolecloudgooglecom 12 havent created project youll prompted 13 give new project name 14 click create create new project 2 enable gmail api 21 gcp console select project 22 navigate apis services library 23 search bar search gmail 24 click gmail api click enable 3 create credentials gmail api 31 navigate apis services credentials 32 click create credentials button choose oauth client id 33 havent configured oauth consent screen youll prompted make sure choose external user type select necessary scopes make sure save changes create oauth client id choose web application application type give name 34 add following mindsdb url authorized redirect uris local installation add httplocalhostverifyauth cloud add httpcloudmindsdbcomverifyauth 35 click create 4 download json file 41 creating credentials click download button icon arrow pointing right side client id download json file use location credentials_file param tip usage creates database called mindsdb_gmail database ships table called emails use search emails well write emails use gmail data like searching email sql select mindsdb_gmailemails query alert fromgooglecom label_ids inboxunread limit 20 writing emails sql insert mindsdb_gmailemails thread_id message_id to_email subject body values 187cbdd861350934d 8e54ccfdabd0756ba12ef7bc95ebc75bspark testexample2com trying mindsdb seems awesome must try whenever example"
  },
  {
    "filename": "gmail.mdx",
    "path": "docs/integrations/app-integrations/gmail.mdx",
    "chunk_id": 2,
    "chunk_content": "1 automating email replies know pull emails database write emails make use openapi engine write email replies first create openai engine passing openai api key sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey create model using engine sql create model mindsdbgpt_model predict response using engine openai_engine max_tokens 500 api_key your_api_key model_name gpt35turbo prompt_template input message body from_user sender less 500 characters write email response sender following format start proper salutation respond short message casual tone sign email name mindsdb example 2 detecting spam emails check email spam using one hugging face pretrained models sql create model mindsdbspam_classifier predict pred using engine huggingface task textclassification model_name mrm8488berttinyfinetunedsmsspamdetection input_column text_spammy labels ham spam create view contains snippet body email sql create view mindsdbemails_text select snippet text_spammy mindsdb_gmailemails finally use model classify emails spam ham sql select hpred hpred_explain ttext_spammy input_text mindsdbemails_text join mindsdbspam_classifier h info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersgmail_handlerreadmemd info"
  },
  {
    "filename": "microsoft-teams.mdx",
    "path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "chunk_id": 0,
    "chunk_content": "title microsoft teams sidebartitle microsoft teams documentation describes integration mindsdb microsoft teamshttpswwwmicrosoftcomenusmicrosoftteamsgroupchatsoftware ultimate messaging app organization integration allows mindsdb create chatbots enhanced ai capabilities respond messages microsoft teams prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect microsoft teams mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection establish connection microsoft teams mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlersms_teams_handler engine sql create database teams_conn engine teams parameters client_id 1234567890abcdef1234567890abcdef client_secret a1b2c3d4e5f6g7h8i9j0k1l2m3n4o5p6 required connection parameters include following client_id client id registered microsoft entra id application client_secret client secret registered microsoft entra id application note microsoft entra id previously known azure active directory azure ad note set microsoft teams app mindsdb chatbot follow instructions given set microsoft teams app act chatbot steps step titleregister bot microsoft bot framework portal follow linkhttpsdevbotframeworkcombotsnew microsoft bot framework portal sign microsoft account fill display name bot handle optionally long description leave messaging endpoint field empty set app type multi tenant click create microsoft app id password open new tab azure portal step step titleregister application azure portal click new registration fill name select accounts organizational directory azure ad directory multitenant option supported account types click register save application client id"
  },
  {
    "filename": "microsoft-teams.mdx",
    "path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "chunk_id": 1,
    "chunk_content": "later use click certificates secrets manage click new client secret fill description select appropriate expires period click add copy save client secret secure location tip already existing app registration use instead creating new one skip steps tip step step titleconfigure chatbot mindsdb editor open mindsdb editor create connection microsoft teams using client id client secret obtained previous steps use create database statement show using connection create chatbot create chatbotagentschatbot syntax shown usage section run show chatbots command record webhook_token chatbot created step step titlecomplete bot setup microsoft bot framework portal navigate back microsoft bot framework portal fill messaging endpoint following format mindsdb_urlapiwebhookschatbotswebhook_token tip mindsdb_url placeholder replaced url mindsdb instance running note running mindsdb locally need exposed internet using service like ngrokhttpsngrokcom tip fill microsoft app id using client id obtained previous steps agree terms click register add featured channel click microsoft teams select microsoft teams solution click save agree terms prompted step step titlecreate application via developer portal microsoft teams navigate microsoft teams apps tab search developer portal app add workspace open developer portal click apps new app fill name app click add navigate basic information fill required fields short description long description developer company name website privacy policy terms"
  },
  {
    "filename": "microsoft-teams.mdx",
    "path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "chunk_id": 2,
    "chunk_content": "use application client id may also provide additional information prefer tip please note fields required bot usable microsoft teams urls provided website privacy policy terms use fields must valid urls tip navigate app features select bot choose select existing bot option select bot created earlier microsoft bot framework portal select scopes bot required used click save finally navigation pane click publish publish org step steps info bot made available users organization need ask administrator approve submission linkhttpsadminteamsmicrosoftcompoliciesmanageapps approved users find built org section apps tab either chat bot directly add channel chat bot channel type bot_name followed message waiting approval test chatbot clicking preview teams developer portal info usage integration used create chatbots microsoft teams via create chatbotagentschatbot syntax currently used data source workloads sql create chatbot teams_chatbot using database teams_datasource agent ai_agent is_running true learn agents chatbots heremindsdb_sqlagentschatbot follow tutorialusecasesai_agentsbuild_ai_agents build chatbot troubleshooting guide warning response bot symptoms bot respond messages checklist 1 ensure bot correctly set microsoft bot framework portal 2 ensure client id client secret used create connection correct valid 3 ensure messaging endpoint correctly set microsoft bot framework portal correct webhook token 4 confirm bot added microsoft teams workspace warning warning bot available users symptoms bot"
  },
  {
    "filename": "microsoft-teams.mdx",
    "path": "docs/integrations/app-integrations/microsoft-teams.mdx",
    "chunk_id": 3,
    "chunk_content": "available users organization checklist 1 ensure bot published organization microsoft bot framework portal 2 ensure bot approved administrator warning"
  },
  {
    "filename": "microsoft-onedrive.mdx",
    "path": "docs/integrations/app-integrations/microsoft-onedrive.mdx",
    "chunk_id": 0,
    "chunk_content": "title microsoft one drive sidebartitle microsoft one drive documentation describes integration mindsdb microsoft onedrivehttpswwwmicrosoftcomenusmicrosoft365onedriveonlinecloudstorage cloud storage service lets back access edit share sync files device prerequisites 1 proceeding ensure mindsdb installed locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 register application azure portalhttpsportalazurecom navigate azure portalhttpsportalazurecomhome sign microsoft account locate microsoft entra id service click click app registrations click new registration enter name application select accounts organizational directory option supported account types field keep redirect uri field empty click register click api permissions click add permission select microsoft graph click delegated permissions search filesread permission select click add permissions request administrator grant consent permissions administrator click grant admin consent organization click yes copy application client id record client_id parameter copy directory tenant id record tenant_id parameter click certificates secrets click new client secret enter description client secret select expiration period click add copy generated client secret record client_secret parameter click authentication click add platform select web enter url mindsdb deployed followed verifyauth redirect uris field example running mindsdb locally httpslocalhost47334 enter httpslocalhost47334verifyauth redirect uris field connection establish connection microsoft onedrive mindsdb executing following sql command sql create database one_drive_datasource engine one_drive parameters client_id 1234567890abcdef1234567890abcdef client_secret abcd1234efgh5678ijkl9012mnop3456qrst7890uvwx tenant_id abcdef1234567890abcdef1234567890 note note sample parameter"
  },
  {
    "filename": "microsoft-onedrive.mdx",
    "path": "docs/integrations/app-integrations/microsoft-onedrive.mdx",
    "chunk_id": 1,
    "chunk_content": "values provided reference replace connection parameters note required connection parameters include following client_id client id registered application client_secret client secret registered application tenant_id tenant id registered application usage retrieve data specified file microsoft onedrive providing integration name file name sql select one_drive_datasourcemyfilecsv limit 10 tip wrap object key backticks avoid issues parsing sql statements provided especially important file name contains spaces special characters prefixes myfoldermyfilecsv moment supported file formats csv tsv json parquet tip note examples utilize one_drive_datasource datasource name defined create database command note special files table used list files available microsoft onedrive sql select one_drive_datasourcefiles limit 10 content files also retrieved explicitly requesting content column column empty default avoid unnecessary data transfer sql select path content one_drive_datasourcefiles limit 10 tip table return objects regardless file format however supported file formats mentioned queried tip troubleshooting guide warning database connection error symptoms failure connect mindsdb microsoft onedrive checklist 1 ensure client_id client_secret tenant_id parameters correctly provided 2 ensure registered application required permissions 3 ensure generated client secret expired warning warning sql statement parsed mindsdb_sql symptoms sql queries failing recognizing object names containing spaces special characters prefixes checklist 1 ensure object names spaces special characters prefixes enclosed backticks 2 examples incorrect"
  },
  {
    "filename": "microsoft-onedrive.mdx",
    "path": "docs/integrations/app-integrations/microsoft-onedrive.mdx",
    "chunk_id": 2,
    "chunk_content": "select integrationtraveltravel_datacsv incorrect select integrationtraveltravel_datacsv correct select integrationtraveltravel_datacsv warning"
  },
  {
    "filename": "strapi.mdx",
    "path": "docs/integrations/app-integrations/strapi.mdx",
    "chunk_id": 0,
    "chunk_content": "title strapi sidebartitle strapi strapihttpsstrapiio popular opensource headless content management system cms empowers developers work preferred tools frameworks providing content editors userfriendly interface manage distribute content across various platforms strapi handler mindsdb handler enables sqlbased querying strapi collections documentation provides brief overview features initialization parameters example usage connection use strapi handler initialize following parameters host strapi server host port strapi server port typically 1337 api_token strapi server api token authentication plural_api_ids list plural api ids collections get started create strapi engine database following sql command sql create database myshop display name database engine strapi name mindsdb handler parameters host strapihost host ip address url port strapiport common port 1337 api_token yourstrapiapitoken api token strapi server plural_api_ids pluralapiid plural api ids collections usage retrieve data collection sql select myshopcollectionname filter data based specific criteria sql select myshopcollectionname id idvalue insert new data collection sql insert myshopcollectionname fieldname1 fieldname2 values value1 value2 tip note able insert data collection create permission tip modify existing data collection sql update myshopcollectionname set fieldname1 value1 fieldname2 value2 id idvalue tip note able update data collection update permission tip"
  },
  {
    "filename": "salesforce.mdx",
    "path": "docs/integrations/app-integrations/salesforce.mdx",
    "chunk_id": 0,
    "chunk_content": "title salesforce sidebartitle salesforce documentation describes integration mindsdb salesforcehttpswwwsalesforcecom worlds trusted customer relationship management crm platform integration allows mindsdb access data salesforce enhance ai capabilities prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockerhttpsdocsmindsdbcomsetupselfhosteddocker docker desktophttpsdocsmindsdbcomsetupselfhosteddockerdesktop 2 connect salesforce mindsdb install required dependencies following instructionhttpsdocsmindsdbcomsetupselfhosteddockerinstalldependencies connection establish connection salesforce mindsdb executing following sql command providing handler namehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssalesforce_handler engine sql create database salesforce_datasource engine salesforce parameters username demoexamplecom password demo_password client_id 3mvg9lkcponinvbipjjdw1j9llm82hnzz9yh7zjny client_secret 5a52c1a1e21df9012iodc9isnxxaadda9 required connection parameters include following username username salesforce account password password salesforce account client_id client id consumer key connected app salesforce client_secret client secret consumer secret connected app salesforce tip create connected app salesforce obtain client id client secret follow steps given 1 log salesforce account 2 go setup apps app manager 3 click new connected app 4 fill required details esure enable oauth settings checkbox checked set callback url wherever mindsdb deployed followed verifyauth eg httplocalhost47334verifyauth choose appropriate oauth scopes 5 click save 6 copy consumer key client id consumer secret client secret connected app details consumer key secret 7 go setup apps connected apps manage connected apps 8 click connected app name 9 click edit policies 10 oauth policies ensure permitted users"
  },
  {
    "filename": "salesforce.mdx",
    "path": "docs/integrations/app-integrations/salesforce.mdx",
    "chunk_id": 1,
    "chunk_content": "set users may selfauthorize ip relaxation set relax ip restrictions 11 click save 12 go setup identity oauth openid connect settings 13 ensure allow oauth usernamepassword flows checkbox checked tip usage retrieve data specified table providing integration table names sql select salesforce_datasourcetable_name limit 10 run soqlhttpsdevelopersalesforcecomdocsatlasenussoql_soslmetasoql_soslsforce_api_calls_soqlhtm queries directly connected salesforce account sql select salesforce_datasource native query goes select name accountname accountindustry contact accountindustry technology limit 5 note examples utilize salesforce_datasource datasource name defined create database command note supported tables salesforce integration supports following tables account table containing account information contact table containing contact information people business opportunity table containing sales opportunities lead table containing potential sales leads task table containing tasks activities event table containing calendar events user table containing user information product2 table containing product information pricebook2 table containing price book information pricebookentry table containing price book entries order table containing order information orderitem table containing order items case table containing customer service cases campaign table containing marketing campaigns campaignmember table containing campaign members contract table containing contract information asset table containing asset information"
  },
  {
    "filename": "gitlab.mdx",
    "path": "docs/integrations/app-integrations/gitlab.mdx",
    "chunk_id": 0,
    "chunk_content": "title gitlab sidebartitle gitlab section present connect gitlab repository mindsdb gitlabhttpsaboutgitlabcom devsecops platform data gitlab including issues mrs utilized within mindsdb make relevant predictions automate issuemr creation prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect gitlab mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access gitlab connection handler implemented using pythongitlabhttpsgithubcompythongitlabpythongitlab library pythongitlab python library wraps gitlab api gitlab handler initialized following parameters repository required name gitlab repository connect api_key optional gitlab api key use authentication connect mindsdb gitlab repository sql create database mindsdb_gitlab engine gitlab parameters repository gitlaborggitlab api_key api_key optional gitlab api key usage mindsdb_gitlab connection contains two tables issues merge_requests use established connection query table sql select mindsdb_gitlabissues run advanced queries fetch specific issues defined order sql select number state creator assignee title created labels mindsdb_gitlabissues stateopened order created asc creator desc limit 10 goes merge requests sql select number state creator reviewers title created has_conflicts mindsdb_gitlabmerge_requests statemerged order created asc creator desc limit 10 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersgitlab_handlerreadmemd info"
  },
  {
    "filename": "paypal.mdx",
    "path": "docs/integrations/app-integrations/paypal.mdx",
    "chunk_id": 0,
    "chunk_content": "title paypal sidebartitle paypal section present connect paypal mindsdb paypalhttpswwwbankratecomfinancecreditcardsguidetousingpaypal online payment system makes paying things online sending receiving money safe secure data paypal utilized within mindsdb train models make predictions transactions connection handler implemented using paypalpythonsdkhttpsgithubcompaypalpaypalpythonsdk python sdk paypal restful apis required arguments establish connection follows mode mode paypal api sandbox live client_id client id paypal api client_secret client secret paypal api connect paypal using mindsdb following create database statement used sql create database paypal_datasource engine paypal parameters mode yourpaypalmode client_id yourpaypalclientid client_secret yourpaypalclientsecret tip check guidehttpsdeveloperpaypalcomapirest create client credentials paypal tip usage query paypal follows payments sql select paypal_datasourcepayments invoices sql select paypal_datasourceinvoices subscriptions sql select paypal_datasourcesubscriptions orders sql select paypal_datasourceorders payouts sql select paypal_datasourcepayouts also run advanced queries data payments sql select intent cart paypal_datasourcepayments state approved order id limit 5 invoices sql select invoice_number total_amount paypal_datasourceinvoices status paid order total_amount desc limit 5 subscriptions sql select id state name paypal_datasourcesubscriptions state created limit 5 orders sql select id state amount paypal_datasourceorders state approved order total_amount desc limit 5 payouts sql select payout_batch_id amount_currency amount_value paypal_datasourcepayouts order amount_value desc limit 5 supported tables following tables supported paypal handler payments payments made invoices invoices created subscriptions subscriptions created"
  },
  {
    "filename": "paypal.mdx",
    "path": "docs/integrations/app-integrations/paypal.mdx",
    "chunk_id": 1,
    "chunk_content": "orders orders created payouts payouts made"
  },
  {
    "filename": "symbl.mdx",
    "path": "docs/integrations/app-integrations/symbl.mdx",
    "chunk_id": 0,
    "chunk_content": "title symbl sidebartitle symbl documentation describes integration mindsdb symblhttpssymblai platform stateoftheart taskspecific llms enables businesses analyze multiparty conversations scale integration allows mindsdb process conversation data extract insights prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect symbl mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies tip please note order successfully install dependencies symbl necessary install portaudio linux packages docker container first run following commands 1 start interactive shell container bash docker exec mindsdb_container sh havent specified name spinning mindsdb container docker run find running docker ps note using docker desktop navigate containers locate multicontainer application running extension click mindsdb_service container click exec tab start interactive shell note 2 install required packages bash aptget update aptget install libportaudio2 libportaudiocpp0 portaudio19dev python3dev buildessential rm rf varlibaptlists tip connection establish connection symbl mindsdb executing following sql command sql create database mindsdb_symbl engine symbl parameters app_id app_id app_secretapp_secret required connection parameters include following app_id symbl app identifier app_secret symbl app secret usage first process conversation data get conversation id via get_conversation_id table sql select mindsdb_symblget_conversation_id audio_urlhttpssymbltestdatas3useast2amazonawscomnewphonecallmp3 next use conversation id get results supported tables sql select mindsdb_symblget_messages conversation_id5682305049034752 supported tables include get_topics get_questions get_analytics get_action_items note examples utilize mindsdb_symbl"
  },
  {
    "filename": "symbl.mdx",
    "path": "docs/integrations/app-integrations/symbl.mdx",
    "chunk_id": 1,
    "chunk_content": "datasource name defined create database command note"
  },
  {
    "filename": "youtube.mdx",
    "path": "docs/integrations/app-integrations/youtube.mdx",
    "chunk_id": 0,
    "chunk_content": "title youtube sidebartitle youtube section present connect youtube mindsdb youtubehttpswwwyoutubecom popular online videosharing platform social media website users upload view share interact videos created individuals organizations around world prerequisites proceeding ensure following prerequisites met 1 install mindsdb system obtain access cloud options 2 use youtube mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies connection two ways connect youtube mindsdb 1 limited permissions option provides mindsdb readonly access youtube including viewing comments data 2 elevated permissions option provides mindsdb full access youtube including viewing comments data posting replies comments option 1 limited permissions establish connection youtube mindsdb executing sql command following google authorization link provided output sql create database mindsdb_youtube engine youtube parameters youtube_api_token youryoutubeapikeytoken tip alternatively connect youtube mindsdb via form click add button choose new datasource search youtube follow instructions form providing connection name youtube api token click test connection button connection established click save continue button tip required connection parameters include following youtube_api_token google api key used authentication check guidehttpsbloghubspotcomwebsitehowtogetyoutubeapikey create api key access youtube data option 2 elevated permissions establish connection youtube mindsdb executing sql command following google authorization link provided output sql create database mindsdb_youtube_oauth engine youtube parameters credentials_file pathtocredentialsjsonfile alternatively use credentials_url parameter tip alternatively connect youtube"
  },
  {
    "filename": "youtube.mdx",
    "path": "docs/integrations/app-integrations/youtube.mdx",
    "chunk_id": 1,
    "chunk_content": "mindsdb via form click add button choose new datasource search youtube follow instructions form providing connection name credentials file url click test connection button complete authorization process popup window connection established click save continue button tip required connection parameters include one following credentials_file path file generated google cloud console described credentials_url url file generated google cloud console described accordion titlehow generate credentials file iconyoutube icontypethin 1 open google cloud console 2 create new project 3 inside project go apis services go enabled apis services click enable apis services top bar search youtube data api v3 enable go oauth consent screen choose user type external click create provide app name support email scopes add following scopes authyoutube authyoutubeforcessl authyoutubepartner next add test users save continue go credentials click create credentials top bar choose oauth client id choose type web application provide name authorized redirect uris add following httplocalhostverifyauth httpscloudmindsdbcomverifyauth http12700147334verifyauth click create download json file required connect youtube mindsdb accordion usage use established connection query comments table query one videos comments sql select mindsdb_youtubecomments video_id rawfgq20ofa one channelss comments sql select mindsdb_youtubecomments channel_iduc include ordering limiting output data sql select mindsdb_youtubecomments video_id rawfgq20ofa order display_name asc limit 5 use established connection"
  },
  {
    "filename": "youtube.mdx",
    "path": "docs/integrations/app-integrations/youtube.mdx",
    "chunk_id": 2,
    "chunk_content": "query channels table sql select mindsdb_youtubechannels channel_iduc channel_id column mandatory clause use established connection query videos table sql select mindsdb_youtubevideos video_idid video_id column mandatory clause connection option 2 insert replies comments sql insert mindsdb_youtube_oauthcomments comment_id reply values comment_id reply message"
  },
  {
    "filename": "sendinblue.mdx",
    "path": "docs/integrations/app-integrations/sendinblue.mdx",
    "chunk_id": 0,
    "chunk_content": "title sendinblue sidebartitle sendinblue section present connect sendinblue mindsdb brevo formerly sendinbluehttpswwwbrevocom allinone platform automate marketing campaigns email sms whatsapp chat data sendinblue used understand impact email marketing prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect sendinblue mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access sendinblue connection handler implemented using sibapiv3sdkhttpsgithubcomsendinblueapiv3pythonlibrary library python library wraps sendinblue apis required arguments establish connection follows api_key required sendinblue api key use authentication tip check guidehttpsdevelopersbrevocomdocs create sendinblue api key recommended use api key avoid api rate limit exceeded error tip connect sendinblue mindsdb sql create database sib_datasource engine sendinblue parameters api_key xkeysib usage use established connection query database sql select sib_datasourceemail_campaigns run advanced queries sql select id name sib_datasourceemail_campaigns status sent order name limit 5 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlerssendinblue_handlerreadmemd info"
  },
  {
    "filename": "binance.mdx",
    "path": "docs/integrations/app-integrations/binance.mdx",
    "chunk_id": 0,
    "chunk_content": "title binance sidebartitle binance section present connect binance mindsdb binancehttpswwwbinancecomen one worlds largest cryptocurrency exchanges online platform buy sell trade wide variety cryptocurrencies binance offers range services beyond trading including staking lending various financial products related cryptocurrencies binance provides realtime trade data utilized within mindsdb make realtime forecasts connection handler integrates binance apihttpsbinancedocsgithubioapidocsspotenchangelog make aggregate trade kline data available use model training predictions since parameters required connect binance using mindsdb use statement sql create database my_binance engine binance usage select data default aggregate data klines latest 1000 trading intervals length one minute 1m returned sql select my_binanceaggregated_trade_data symbol btcusdt accordion titleresponse iconbinance icontypethin sample output data symbol open_time open_price high_price low_price close_price volume close_time quote_asset_volume number_of_trades taker_buy_base_asset_volume taker_buy_quote_asset_volume btcusdt 1678338600 2175265000 2176133000 2175153000 217567000 1038614100 1678338659999 225965620520700 3655 5525763000 120221960971860 symbol trading pair btc usdt example open_time start time interval seconds since unix epoch default interval 1m open_price price base asset beginning trading interval high_price highest price base asset trading interval low_price lowest price base asset trading interval close_price price base asset end trading interval volume total amount base asset traded interval close_time end time interval seconds since unix epoch quote_asset_volume total amount quote asset usdt case traded interval number_of_trades"
  },
  {
    "filename": "binance.mdx",
    "path": "docs/integrations/app-integrations/binance.mdx",
    "chunk_id": 1,
    "chunk_content": "total number trades made interval taker_buy_base_asset_volume much base asset volume contributed taker buy orders taker_buy_quote_asset_volume much quote asset volume contributed taker buy orders accordion get customized response pass open_time close_time interval sql select my_binanceaggregated_trade_data symbol btcusdt open_time 20230101 close_time 20230103 080000 interval 1s limit 10000 note supported intervals listed herehttpsbinancedocsgithubioapidocsspotenklinecandlestickdata note train model create time series model using 10000 trading intervals past duration 1m sql create model mindsdbbtc_forecast_model my_binance select aggregated_trade_data symbol btcusdt close_time 20230101 interval 1m limit 10000 predict open_price order open_time window 100 horizon 10 note accuracy limit set higher value eg 100000 note making predictions first lets create view recent btcusdt aggregate trade data sql create view recent_btcusdt_data select my_binanceaggregated_trade_data symbol btcusdt lets predict future price btc sql select recent_btcusdt_data join mindsdbbtc_forecast_model mopen_time latest give predicted btc price next 10 minutes horizon set 10 terms usdt"
  },
  {
    "filename": "stripe.mdx",
    "path": "docs/integrations/app-integrations/stripe.mdx",
    "chunk_id": 0,
    "chunk_content": "title stripe sidebartitle stripe section present connect stripe mindsdb stripehttpsstripecom financial technology company provides set software payment processing solutions businesses individuals accept payments internet stripe one leading payment gateway online payment processing platforms data stripe utilized within mindsdb train ai models chatbots based customers products payment intents make relevant predictions forecasts prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect stripe mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access stripe connection handler implemented using stripepythonhttpsgithubcomstripestripepython python library stripe api one parameter required set connection stripe api_key stripe api key tip find api keys stripe dashboard read morehttpsstripecomdocskeys tip connect stripe using mindsdb following create database statement used sql create database stripe_datasource engine stripe parameters api_key sk_ usage query data stripe account customers example follows sql select stripe_datasourcecustomers run advanced queries fetch specific customers defined order sql select name email stripe_datasourcecustomers currency inr order name limit 5 supported tables following tables supported stripe handler customers products payment_intents"
  },
  {
    "filename": "plaid.mdx",
    "path": "docs/integrations/app-integrations/plaid.mdx",
    "chunk_id": 0,
    "chunk_content": "title plaid sidebartitle plaid section present connect plaid mindsdb plaidhttpsplaidcom financial technology company offers platform set apis facilitate integration financial services data applications websites services primarily focus enabling developers connect access financial accounts data various financial institutions data plaid utilized within mindsdb train ai models make financial forecasts prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect plaid mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access plaid connection required arguments establish connection follows client_id secret access_token plaid_env tip get client_id secret access_token values herehttpsdashboardplaidcomteamkeys sign plaid account herehttpsplaidcomdocsapiitemsitempublic_tokenexchange generate access_token value tip order make use handler connect plaid app mindsdb following syntax used sql create database my_plaid engine plaid parameters client_id your_client_id secret your_secret access_token your_public_key plaid_env env creates database comes two tables transactions balance usage query data like sql select id merchant_name authorized_date amount payment_channel my_plaidtransactions start_date 20220101 end_date 20230411 limit 20 want use functions provided plaid api use native queries syntax like sql select my_plaid get_transactions start_date 20220101 end_date 20220201 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersplaid_handlerreadmemd info"
  },
  {
    "filename": "newsapi.mdx",
    "path": "docs/integrations/app-integrations/newsapi.mdx",
    "chunk_id": 0,
    "chunk_content": "title news api sidebartitle news api section present connect news api mindsdb news apihttpsnewsapiorg simple http rest api searching retrieving live articles web data news api utilized within mindsdb model training predictions prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect news api mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access news api connection handler implemented using newsapipythonhttpsnewsapiorgdocsclientlibrariespython library required arguments establish connection follows api_key news api key use authentication tip check guidehttpsnewsapiorgdocsauthentication create api key recommended use api key avoid api rate limit exceeded error tip connect news api mindsdb sql create database newsapi engine newsapi parameters api_key api key usage simple search recent articles sql select newsapiarticle query python advanced search recent articles per specific sources dates sql select newsapiarticle query python sourcesbbcnews publishedat 20210323 publishedat 20230423 limit 4 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersnewsapi_handlerreadmemd info"
  },
  {
    "filename": "google-analytics.mdx",
    "path": "docs/integrations/app-integrations/google-analytics.mdx",
    "chunk_id": 0,
    "chunk_content": "title google analytics sidebartitle google analytics section present connect google analytics mindsdb google analyticshttpsanalyticsgooglecom web analytics service offered google tracks reports website traffic also mobile app traffic events data google analytics utilized within mindsdb train ai models make predictions automate user engagement events ai prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect google analytics mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access google analytics connection required arguments establish connection follows credentials_file optional path json file stores credentials google account credentials_json optional content json file stores credentials google account property_id required property id google analytics website herehttpsdevelopersgooglecomanalyticsdevguidesreportingdatav1propertyid information get property id one credentials_file credentials_json chosen tip please note google account enabled google analytics admin api required find information herehttpsdevelopersgooglecomanalyticsdevguidesconfigadminv1quickstartclientlibrariesbrbr also active website connected google analytics required find information herehttpssupportgooglecomanalyticsanswer9304153hlen tip make use handler connect google analytics app mindsdb following syntax used sql create database my_ga engine google_analytics parameters credentials_file pathtoyourfilecredentialsjson property_id your_property_id tip need google account order use integration get credentials file 1 create google cloud platform gcp project 11 go gcp console httpsconsolecloudgooglecom 12 havent created project youll prompted 13 give new project name 14 click create create new project"
  },
  {
    "filename": "google-analytics.mdx",
    "path": "docs/integrations/app-integrations/google-analytics.mdx",
    "chunk_id": 1,
    "chunk_content": "2 enable google analytics admin api 21 gcp console select project 22 navigate apis services library 23 search bar search google analytics admin api 24 click google analytics admin api click enable 3 create credentials google analytics admin api 31 navigate apis services credentials 32 click create credentials button choose service account 33 enter unique service account id 34 click done 35 copy service account created find service accounts 36 click service account created navigate keys 37 click add key create new key 38 choose json click create 39 credentials file downloaded directly locate file use location credentials_file param 4 add service account google analytics property 41 google analytics admin console select account property want grant access 42 navigate admin panel 43 navigate account account access management 44 click icon add new user 45 enter service account copied step 35 email address 46 assign appropriate permissions service account minimum youll need grant edit permissions 47 click add button add service account user specified permissions tip usage creates database comes conversion_events table use google analytics data like searching conversion events sql select event_name custom countingmethod my_gaconversion_events creating conversion event sql insert my_gaconversion_events event_name countingmethod values mindsdb_event 2 updating one conversion event"
  },
  {
    "filename": "google-analytics.mdx",
    "path": "docs/integrations/app-integrations/google-analytics.mdx",
    "chunk_id": 2,
    "chunk_content": "sql update my_gaconversion_events set countingmethod 1 name mindsdb_event_name deleting one conversion event sql delete my_gaconversion_events name mindsdb_event_name info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersgoogle_analytics_handler info"
  },
  {
    "filename": "shopify.mdx",
    "path": "docs/integrations/app-integrations/shopify.mdx",
    "chunk_id": 0,
    "chunk_content": "title shopify sidebartitle shopify section present connect shopify mindsdb shopifyhttpswwwshopifycom ecommerce platform enables businesses create manage online stores one leading ecommerce solutions providing wide range tools services help entrepreneurs businesses sell products services online data shopify utilized within mindsdb train ai models chatbots using products customers orders data make predictions relevant businesses connection required arguments establish connection follows shop_url required url shopify store access_token required access token use authentication tip create shopify access tokenhttpswwwyoutubecomwatchv4f_aic5otnct302s tip optionally want access customer reviews provide following parameters yotpo_app_key token needed access customer reviews via yotpo product reviews app yotpo_access_token token needed access customer reviews via yotpo product reviews app tip want query customer reviews use yotpo product reviewshttpsappsshopifycomyotposocialreviews app available shopify steps follow 1 install yotpo product reviewshttpsappsshopifycomyotposocialreviews app shopify store 2 generate yotpo_app_key following instructionhttpssupportyotpocomdocsfindingyouryotpoappkeyandsecretkey retrieving app key learn yotpo authentication herehttpsapidocsyotpocomreferenceyotpoauthentication 3 generate yotpo_access_token following instructionhttpsdevelopyotpocomreferencegenerateatoken tip connect shopify account mindsdb must first create new handler instance following query sql create database shopify_datasource engine shopify parameters shop_url yourshopnamemyshopifycom access_token shppa_ usage created database query following tables products table customers table orders table customerreviews table requires yotpo product reviewshttpsappsshopifycomyotposocialreviews app installed shopify account inventorylevel table location table carrierservice table shippingzone table saleschannel table"
  },
  {
    "filename": "shopify.mdx",
    "path": "docs/integrations/app-integrations/shopify.mdx",
    "chunk_id": 1,
    "chunk_content": "products table query table sql select shopify_datasourceproducts also run advanced queries filter products status like sql select id title shopify_datasourceproducts status active order id limit 5 insert new data run insert statement providing following values title body_html vendor product_type tags status update existing data run update statement delete data run delete statement customers table query table sql select shopify_datasourcecustomers insert new data run statement sql insert shopify_datasourcecustomersfirst_name last_name email phone values john doe johndoeexamplecom 10001112222 update existing data run update statement delete data run delete statement orders table query table sql select shopify_datasourceorders insert new data run insert statement update existing data run update statement delete data run delete statement customerreviews table query table sql select shopify_datasourcecustomer_reviews inventorylevel table query table sql select shopify_datasourceinventory_level location table query table sql select shopify_datasourcelocations carrierservice table query table sql select shopify_datasourcecarrier_service insert new data run insert statement providing following values name callback_url service_discovery update existing data run update statement delete data run delete statement shippingzone table query table sql select shopify_datasourceshipping_zone saleschannel table query table sql select shopify_datasourcesales_channel info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersshopify_handlerreadmemd info"
  },
  {
    "filename": "confluence.mdx",
    "path": "docs/integrations/app-integrations/confluence.mdx",
    "chunk_id": 0,
    "chunk_content": "title confluence sidebartitle confluence section present connect confluence mindsdb confluencehttpswwwatlassiancomsoftwareconfluence popular collaboration documentation tool developed atlassian software company known suite productivity project management software confluence designed help teams organizations collaborate share information create manage various types content data confluence utilized within mindsdb train ai models make predictions prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect confluence mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access confluence connection handler implemented using atlassianpythonapi library provides simple convenient way interact atlassian products required arguments establish connection follows url confluencehosted url instance confluence_api_token token used authenticate tip please follow linkhttpsdocssearchunifycomcontentcontentsourcesatlassianjiraconfluenceauthenticationcreateapitokenhtm generate token accessing confluence api tip order make use handler connect confluence app mindsdb following syntax used sql create database mindsdb_confluence engine confluence parameters url your_confluence_url username your_username passwordaccess_token creates database comes pages table usage query data like sql select id key name type mindsdb_confluencepages type personal order id asc name desc limit 10 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersconfluence_handlerreadmemd info"
  },
  {
    "filename": "mediawiki.mdx",
    "path": "docs/integrations/app-integrations/mediawiki.mdx",
    "chunk_id": 0,
    "chunk_content": "title mediawiki sidebartitle mediawiki section present connect mediawiki mindsdb mediawikihttpswwwmediawikiorgwikimediawiki free opensource wiki software platform designed enable creation management wikis originally developed continues power wikipedia mediawiki highly customizable used create wide range collaborative websites knowledge bases data mediawiki utilized within mindsdb train ai models chatbots using wide range available information prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect mediawiki mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access mediawiki connection handler implemented using mediawikiapihttpsgithubcomlehinevychmediawikiapi python wrapper mediawiki api connection arguments required initialize handler connect mediawiki api mindsdb following create database statement used sql create database mediawiki_datasource engine mediawiki usage query mediawiki api follows sql select mediawiki_datasourcepages run advanced queries fetch specific pages defined order sql select mediawiki_datasourcepages title barack order pageid limit 5"
  },
  {
    "filename": "github.mdx",
    "path": "docs/integrations/app-integrations/github.mdx",
    "chunk_id": 0,
    "chunk_content": "title github sidebartitle github section present connect github repository mindsdb githubhttpsgithubcom webbased platform service primarily used version control collaborative software development provides platform developers teams host review manage source code software projects data github including issues prs utilized within mindsdb make relevant predictions automate issuepr creation prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect github mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access github connection handler implemented using pygithub library python library wraps github api v3 required arguments establish connection follows repository github repository name api_key optional github api key use authentication github_url optional github url connect github enterprise instance tip check guidehttpsdocsgithubcomenauthenticationkeepingyouraccountanddatasecuremanagingyourpersonalaccesstokens create github api key recommended use api key avoid api rate limit exceeded error tip connect mindsdb github repository sql create database mindsdb_github engine github parameters repository mindsdbmindsdb usage mindsdb_github connection contains two tables issues pull_requests query issues sql select mindsdb_githubissues run advanced queries fetch specific issues defined order sql select number state creator assignees title labels mindsdb_githubissues state open limit 10 goes pull requests sql select number state title creator head commits mindsdb_githubpull_requests state open limit 10 info information available actions development plans visit pagehttpsgithubcommindsdbmindsdbblobmainmindsdbintegrationshandlersgithub_handlerreadmemd info"
  },
  {
    "filename": "dockerhub.mdx",
    "path": "docs/integrations/app-integrations/dockerhub.mdx",
    "chunk_id": 0,
    "chunk_content": "title docker hub sidebartitle docker hub section present connect docker hub repository mindsdb docker hubhttpshubdockercom worlds easiest way create manage deliver teams container applications data docker hub utilized within mindsdb train models make predictions docker hub repositories prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect docker hub mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access docker hub connection handler implemented using requests library makes http calls httpsdocsdockercomdockerhubapilatesttagresources required arguments establish connection follows username username used login dockerhub password password used login dockerhub tip read creating account herehttpshubdockercom tip connect docker hub using mindsdb sql create database dockerhub_datasource engine dockerhub parameters username username password password usage query docker hub follows sql select dockerhub_datasourcerepo_images_summary namespacedocker repositorytrustedregistrynginx tip namespace repository parameters required clause tip"
  },
  {
    "filename": "intercom.mdx",
    "path": "docs/integrations/app-integrations/intercom.mdx",
    "chunk_id": 0,
    "chunk_content": "title intercom sidebartitle intercom intercomhttpsintercomcom software company provides customer messaging engagement tools businesses offer products services customer support marketing sales allowing companies communicate customers various channels like chat email connection get started intercom api need initialize api handler required access token authentication follows access_token intercom access token authentication tip check guidehttpsdevelopersintercomcomdocsbuildanintegrationlearnmoreauthentication get intercom access token order access intercom data tip create database using intercom engine use sqllike syntax shown sql create database myintercom engine intercom parameters access_token yourintercomaccesstoken usage retrieve data intercom using select statement example sql select myintercomarticles filter data based specific criteria using clause heres example sql select myintercomarticles id articleid create new article intercom use insert statement heres example sql insert myintercomarticles title description body author_id state parent_id parent_type values thanks everything description article body article 6840572 published 6801839 collection update existing records intercom using update statement instance sql update myintercomarticles set title christmas body pnew gifts store jolly seasonp id articleid"
  },
  {
    "filename": "couchbase.mdx",
    "path": "docs/integrations/vector-db-integrations/couchbase.mdx",
    "chunk_id": 0,
    "chunk_content": "title couchbase sidebartitle couchbase implementation couchbase vector store data handler mindsdb couchbasehttpswwwcouchbasecom opensource distributed multimodel nosql documentoriented database software package optimized interactive applications applications may serve many concurrent users creating storing retrieving aggregating manipulating presenting data prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect couchbase mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access couchbase implementation order make use handler connect couchbase server mindsdb following syntax used note example uses default travelsample bucket enabled couchbase ui predefined scope documents sql create database couchbase_vectorsource enginecouchbasevector parameters connection_string couchbaselocalhost bucket travelsample user admin password password scope inventory handler implemented using couchbase library python driver couchbase required arguments establish connection follows connection_string connection string endpoint couchbase server bucket bucket name use connecting couchbase server user user authenticate couchbase server password password authenticate user couchbase server scope scopes level data organization within bucket omitted default _default note connection string expects either couchbases couchbase protocol tip using couchbase capella find connection_string connect tab also required whitelist machines running mindsdb database credentials need created user steps also taken connect tab tip usage use established connection create collection table context mindsdb couchbase insert data creating tables use established"
  },
  {
    "filename": "couchbase.mdx",
    "path": "docs/integrations/vector-db-integrations/couchbase.mdx",
    "chunk_id": 1,
    "chunk_content": "connection create collection table context mindsdb couchbase insert data sql create table couchbase_vectorsourcetest_embeddings select embeddings mysql_datasourcetest_embeddings note mysql_datasource another mindsdb data source created connecting mysql database test_embeddings table mysql_datasource data source contains embeddings want store couchbase note querying searching query collection table shown sql select couchbase_vectorsourcetest_embeddings filter data collection table metadata use following query sql select couchbase_vectorsourcetest_embeddings id some_id perform vector search following query used sql select couchbase_vectorsourcetest_embeddings embeddings select embeddings mysql_datasourcetest_embeddings limit 1 deleting records delete documents using delete like sql sql delete couchbase_vectorsourcetest_embeddings metadatatest test1 dropping connection drop connection use command sql drop database couchbase_vectorsource"
  },
  {
    "filename": "weaviate.mdx",
    "path": "docs/integrations/vector-db-integrations/weaviate.mdx",
    "chunk_id": 0,
    "chunk_content": "title weaviate sidebartitle weaviate implementation weaviate mindsdb weaviate opensource vector database allows store data objects vector embeddings favorite mlmodels scale seamlessly billions data objects prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect weaviate mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access weaviate implementation handler uses weaviateclient python library connect weaviate instance required arguments establish connection weaviate_url url weaviate database weaviate_api_key api key authenticate weaviate case cloud instance persistence_directory directory used case local storage creating connection order make use handler connect weaviate server mindsdb following syntax used sql create database weaviate_datasource engine weaviate parameters weaviate_url httpssampleweaviatenetwork weaviate_api_key apikey sql create database weaviate_datasource engine weaviate parameters weaviate_url httpslocalhost8080 sql create database weaviate_datasource engine weaviate parameters persistence_directory db_path dropping connection drop connection use command sql drop database weaviate_datasource creating tables insert data preexisting table use create sql create table weaviate_datascourcetest select sqlitedbtest weaviate currently doesnt support json field creates another table metadata field reference created original table points metadata entry weaviate follows graphql conventions classes table schemas start capital letter properties start lowercase letter whenever create table tables name gets capitalized dropping collections drop weaviate table use command sql drop table weaviate_datasourcetablename querying"
  },
  {
    "filename": "weaviate.mdx",
    "path": "docs/integrations/vector-db-integrations/weaviate.mdx",
    "chunk_id": 1,
    "chunk_content": "selecting query database using search vector use search_vector embeddings clause sql select weaviate_datasourcetest search_vector 30 10 20 45 limit 10 basic query sql select weaviate_datasourcetest use clause dynamic fields like normal sql sql select weaviate_datasourcecreatetest category science deleting records delete entries using delete like sql sql delete weaviate_datasourcetest id 1 2 3 update supported mindsdb vector database"
  },
  {
    "filename": "pgvector.mdx",
    "path": "docs/integrations/vector-db-integrations/pgvector.mdx",
    "chunk_id": 0,
    "chunk_content": "title pgvector sidebartitle pgvector implementation pgvector mindsdb pgvector opensource vector similarity search postgres store vectors rest data supports exact approximate nearest neighbor search l2 distance inner product cosine distance language postgres client plus acid compliance pointintime recovery joins great features postgres implementation handler uses pgvector python library make use vector data type postgres created pgvector extension required arguments establish connection regular postgres connection host host name ip address postgres instance port port use connecting database database connect user user connect password password use connecting usage installing pgvector extension postgres installed run following commands install pgvector extension cd tmp git clone branch v044 httpsgithubcompgvectorpgvectorgit cd pgvector make make install installing pgvector python library ensure install requirementstxt pgvector_handler folder creating database connection mindsdb create database connection like would regular postgres database difference need specify engine pgvector sql create database pvec engine pgvector parameters host 127001 port 5432 database postgres user user password password insert data new collection like sql create table pvecembed select embeddings mysql_demo_dbtest_embeddings create ml_engine openai openai using api_key youropenaiapikey create model openai_emb predict embedding using engine openai model_nametextembeddingada002 mode embedding question_column review create table pvecitemstest select membedding embeddings treview content mysql_demo_dbamazon_reviews join openai_emb query collection within pgvector follows sql"
  },
  {
    "filename": "pgvector.mdx",
    "path": "docs/integrations/vector-db-integrations/pgvector.mdx",
    "chunk_id": 1,
    "chunk_content": "select pvecembed limit 5 select pvecitemstest limit 5 query semantic search like sql select pvec3items_test embeddings select mindsdbembedding limit 5"
  },
  {
    "filename": "pinecone.mdx",
    "path": "docs/integrations/vector-db-integrations/pinecone.mdx",
    "chunk_id": 0,
    "chunk_content": "title pinecone sidebartitle pinecone implementation pinecone mindsdb pinecone vector database fullymanaged developerfriendly easily scalable prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect pinecone mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access pinecone implementation handler uses pineconeclient python library connect pinecone environment required arguments establish connection api_key api key found pinecone account optional arguments used create table statements dimension dimensions vectors stored index default8 metric distance metric used similarity search defaultcosine spec spec index created dictionary contain following keys cloud cloud provider use defaultaws region region use defaultuseast1 note creation serverless indexes supported moment running create table statements note limitations drop table support support namespaceshttpsdocspineconeiodocsnamespaces display scoredistance support creatingreading sparse values content column supported since exist pinecone usage order make use handler connect environment use following syntax sql create database pinecone_dev engine pinecone parameters api_key query pinecone indexes temp following examples based id search_vector sql select pinecone_devtemp id abc limit 1 sql select pinecone_devtemp search_vector 12345678 using subqueries make sure result single row since use multiple search vectors allowed sql select pinecone_databasetemp search_vector select embeddings sqlitetesterdbtest id 10 optionally filter based metadata sql select pinecone_devtemp id abc metadatahello 100 delete records"
  },
  {
    "filename": "pinecone.mdx",
    "path": "docs/integrations/vector-db-integrations/pinecone.mdx",
    "chunk_id": 1,
    "chunk_content": "using id metadata like sql delete pinecone_devtemp id abc note deletion metadata supported starter tier sql delete pinecone_devtemp metadatatbd true insert data new collection like sql create table pinecone_devtemp select mysql_demo_dbtemp limit 10 update records use insert statement conflicting id pinecone index record updated new values might take bit see reflected sql insert pinecone_testtesttable idcontentmetadataembeddings values id1 test test test 10 20 30 40 50 60 70 80"
  },
  {
    "filename": "chromadb.mdx",
    "path": "docs/integrations/vector-db-integrations/chromadb.mdx",
    "chunk_id": 0,
    "chunk_content": "title chromadb sidebartitle chromadb section present connect chromadb mindsdb chromadbhttpswwwtrychromacom opensource embedding database chroma makes easy build llm apps making knowledge facts skills pluggable llms prerequisites proceeding ensure following prerequisites met 1 install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop 2 connect chromadb mindsdb install required dependencies following instructionsetupselfhosteddockerinstalldependencies 3 install ensure access chromadb connection handler implemented using chromadb python library required arguments establish connection host host name ip address chromadb instance port tcpip port chromadb instance persist_directory directory use persisting data tip host port arguments provided want connect remote chromadb instance otherwise persist_directory argument provided create inmemory chromadb instance tip connect remote chromadb instance following create database used sql create database chromadb_datasource engine chromadb parameters host your_host port your_port alternateively connect inmemory chromadb instance following create database used sql create database chromadb_datasource engine chromadb parameters persist_directory your_persist_directory usage use established connection create collection table context mindsdb chromadb insert data sql create table chromadb_datasourcetest_embeddings select embeddingssource fda metadata mysql_datasourcetest_embeddings note mysql_datasource another mindsdb data source created connecting mysql database test_embeddings table mysql_datasource data source contains embeddings want store chromadb note query collection table shown sql select chromadb_datasourcetest_embeddings filter data collection table metadata use following query sql select chromadb_datasourcetest_embeddings metadatasource fda conduct"
  },
  {
    "filename": "chromadb.mdx",
    "path": "docs/integrations/vector-db-integrations/chromadb.mdx",
    "chunk_id": 1,
    "chunk_content": "similarity search following query used sql select chromadb_datasourcetest_embeddings search_vector select embeddings mysql_datasourcetest_embeddings limit 1"
  },
  {
    "filename": "pdf.mdx",
    "path": "docs/integrations/files/pdf.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload pdf files mindsdb sidebartitle pdf upload pdf files size mindsdb runs locally via dockersetupselfhosteddocker pipcontributeinstall note mindsdb supports searchable pdfs opposed scanned pdfs stored form table inside mindsdb upload files follow steps upload file 1 click add dropdown choose upload file p aligncenter img srcassetsfilesupload_filepng p 2 upload file provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_computerpng p query files query data within mindsdb query content file uploaded name my_file sql select filesmy_file"
  },
  {
    "filename": "json.mdx",
    "path": "docs/integrations/files/json.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload json files mindsdb sidebartitle json upload json files size mindsdb runs locally via dockersetupselfhosteddocker pipcontributeinstall json files converted table json file structure allows otherwise json files stored similarly text files upload files follow steps upload file 1 click add dropdown choose upload file p aligncenter img srcassetsfilesupload_filepng p 2 upload file provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_computerpng p 3 alternatively upload file link provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_urlpng p query files query data within mindsdb query content file uploaded name my_file sql select filesmy_file"
  },
  {
    "filename": "txt.mdx",
    "path": "docs/integrations/files/txt.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload txt files mindsdb sidebartitle txt upload txt files size mindsdb runs locally via dockersetupselfhosteddocker pipcontributeinstall txt files divided chunks stored multiple table cells mindsdb uses textloader langchainhttpsapipythonlangchaincomenlatestdocument_loaderslangchain_communitydocument_loaderstexttextloaderhtml load txt files upload files follow steps upload file 1 click add dropdown choose upload file p aligncenter img srcassetsfilesupload_filepng p 2 upload file provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_computerpng p query files query data within mindsdb query content file uploaded name my_file sql select filesmy_file"
  },
  {
    "filename": "csv-xlsx-xls.mdx",
    "path": "docs/integrations/files/csv-xlsx-xls.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload csv xlsx xls files mindsdb sidebartitle csv xlsx xls upload csv xlsx xls files size mindsdb runs locally via dockersetupselfhosteddocker pipcontributeinstall csv xlsx xls files stored form table inside mindsdb upload files follow steps upload file 1 click add dropdown choose upload file p aligncenter img srcassetsfilesupload_filepng p 2 upload file provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_computerpng p 3 alternatively upload file link provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_urlpng p query files csv xlsx xls files may contain one sheets query data within mindsdb query list available sheets file uploaded name my_file sql select filesmy_file query content one sheets listed command sql select filesmy_filemy_sheet"
  },
  {
    "filename": "parquet.mdx",
    "path": "docs/integrations/files/parquet.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload parquet files mindsdb sidebartitle parquet upload parquet files size mindsdb runs locally via dockersetupselfhosteddocker pipcontributeinstall parquet files stored form table inside mindsdb upload files follow steps upload file 1 click add dropdown choose upload file p aligncenter img srcassetsfilesupload_filepng p 2 upload file provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_computerpng p 3 alternatively upload file link provide name used access within mindsdb p aligncenter img srcassetsfilesupload_file_from_urlpng p query files query data within mindsdb query content file uploaded name my_file sql select filesmy_file"
  },
  {
    "filename": "mindsdb-with-php.mdx",
    "path": "docs/faqs/mindsdb-with-php.mdx",
    "chunk_id": 0,
    "chunk_content": "title interact mindsdb php sidebartitle mindsdb php get started mindsdb need install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop ways interact mindsdb php code 1 connect mindsdb using php data objectshttpswwwphpnetmanualenbookpdophp execute statements directly mindsdb pdoquery method 2 use rest apirestoverview endpoints interact mindsdb directly php"
  },
  {
    "filename": "persist-predictions.mdx",
    "path": "docs/faqs/persist-predictions.mdx",
    "chunk_id": 0,
    "chunk_content": "title persist predictions sidebartitle persisting predictions mindsdb provides range options persisting predictions forecasts lets explore possibilities save prediction results note reasons save predictions every time want get predictions need query model usually joined input data table like sql select inputproduct_name inputreview outputsentiment mysql_demo_dbamazon_reviews input join sentiment_classifier output however querying model returns result set persistent default future use recommended persist result set instead querying model data mindsdb enables save predictions view table download csv file note creating view creating model save prediction results view sql create view review_sentiment querying predictions select inputproduct_name inputreview outputsentiment mysql_demo_dbamazon_reviews input join sentiment_classifier output limit 10 review_sentiment view stores sentiment predictions made customer reviews tip comprehensive tutorialnlpsentimentanalysisinsidemysqlwithopenai predict sentiment customer reviews using openai tip creating table creating model save predictions database table sql create table local_postgresquestion_answers querying predictions select inputarticle_title inputquestion outputanswer mysql_demo_dbquestions input join question_answering_model output limit 10 local_postgres database postgresql database connected mindsdb user write access question_answers table stores prediction results tip comprehensive tutorialnlpquestionansweringinsidemysqlwithopenai answer questions using openai tip downloading csv file executing select statement download output csv file p aligncenter img srcassetsfaqs_downloadcsvpng p click export button choose csv option"
  },
  {
    "filename": "benefits.mdx",
    "path": "docs/faqs/benefits.mdx",
    "chunk_id": 0,
    "chunk_content": "title benefits mindsdb sidebartitle benefits mindsdb mindsdb facilitates development aipowered apps bridging gap data ai thanks numerous integrations data sources including databases vector stores applications ai frameworks including llms automl mix match available integrations create custom ai workflows mindsdb prominent benefits using mindsdb 1 unified ai deployment managementbrbr mindsdb integrates directly database warehouse stream eliminates need build maintain custom complex data pipelines separate systems aiml deployment 2 automated ai workflowsbrbr mindsdb automates entire ai workflow execute timebased eventbased triggers need build custom automation logic get predictions move data retrain models 3 turn every developer ai engineerbrbr mindsdb enables developers leverage existing sql skills accelerating adoption ai across teams departments turning every developer ai engineer 4 enhanced scalability performancebrbr whether private cloud using mindsdbs managed service mindsdb enables handle largescale aiml workloads efficiently mindsdb scale meet demands use case ensuring optimal performance responsiveness"
  },
  {
    "filename": "disposable-email-doman-and-openai.mdx",
    "path": "docs/faqs/disposable-email-doman-and-openai.mdx",
    "chunk_id": 0,
    "chunk_content": "title disposable email domains openai sidebartitle disposable email domains openai disposable email domains cant make use openai therefore users encounter errors using mindsdbs integration openai check email domain disposable verify quickemailverificationhttpsquickemailverificationcomtoolsdisposableemailaddressdetector verifyemailiohttpsverifymailiodomainipnuccom"
  },
  {
    "filename": "missing-required-cpu-features.mdx",
    "path": "docs/faqs/missing-required-cpu-features.mdx",
    "chunk_id": 0,
    "chunk_content": "title missing required cpu features sidebartitle missing required cpu features depending operating system setup may encounter runtime warning starting mindsdb bash runtimewarning missing required cpu features following required cpu features detected avx2 fma bmi1 bmi2 lzcnt solution install polarsltscpu package environment mindsdb runs apple arm machine eg m1 warning likely due running python rosetta troubleshoot install native version python run rosetta x8664 emulation"
  },
  {
    "filename": "syntax.mdx",
    "path": "docs/mindsdb_sql/syntax.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb sql syntax sidebartitle mindsdb sql syntax general mindsdb sql attempts follow syntax conventions mysql postgresql following sections describe common conventions mindsdb sql notation adopt following notation describe commands reserved words capitalized example select square brackets indicate optional clauses curly brackets indicate logical choices options separated examples x z red denotes required clauses choices identifiers readability statements enclosed red square curly brackets black square brackets separator followed three ellipses denote repetition previous item separated separator example identifier denotes comma separated list identifiers note parentheses reserved characters mindsdb sql part notation present parentheses required unless explicitly wrapped square brackets note singledouble quotes backticks identifiers databases tables column names special characters reserved words must enclosed backticks sql select select selectid 100 select selectdatabase selectdatabaseid 100 string values represented single double quotes sql select table_name table_namecolumn_name string select table_name table_namecolumn_name string parentheses sql statements nested parentheses sql select select table_name table_namecolumn_name string"
  },
  {
    "filename": "overview.mdx",
    "path": "docs/mindsdb_sql/overview.mdx",
    "chunk_id": 0,
    "chunk_content": "title sql api sidebartitle overview icon database mindsdb enhances standard sql providing ai building blocks section introduces custom sql syntax provided mindsdb bring data ai together follow steps get started steps step titleconnect data source use create databasemindsdb_sqlsqlcreatedatabase connect data source mindsdbbrbr explore available data sources hereintegrationsdataoverview step step titleconfigure ai engine use create ml_enginemindsdb_sqlsqlcreatemlengine configure engine choicebrbr explore available ai engines hereintegrationsaioverview step step titlecreate deploy aiml model use create modelmindsdb_sqlsqlcreatemodel create train deploy aiml models within mindsdb step step titlequery predictions query single predictionmindsdb_sqlsqlgetsingleprediction batch predictionsmindsdb_sqlsqlgetbatchpredictions joining data models step step titleautomate customized workflows use jobmindsdb_sqlsqlcreatejobs triggermindsdb_sqlsqlcreatetrigger agentmindsdb_sqlagentsagent automate workflows step steps"
  },
  {
    "filename": "custom_functions.mdx",
    "path": "docs/mindsdb_sql/functions/custom_functions.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload custom functions sidebartitle custom functions custom functions provide advanced means manipulating data users upload custom functions written python mindsdb apply data works upload custom functions via mindsdb editor clicking add upload custom functions like p aligncenter img srcassetsupload_custom_functionpng p form needs filled order bring custom functions mindsdb p aligncenter img srcassetsupload_custom_function_empty_formpng p lets briefly go files need uploaded python file stores implementation custom functions sample format py def function_name_1atype btype type implementation goes return x def function_name_2atype btype ctype type implementation goes return x note note input output types set str used default note accordion titleexample py def add_integersaint bint int return ab accordion optional requirements file requirementstxt stores dependencies along versions sample format sql dependency_package_1 version dependency_package_2 version dependency_package_3 verion version accordion titleexample sql pandas scikitlearn accordion upload files please provide name storage collection lets look example example upload custom functions p aligncenter img srcassetsupload_custom_function2png p upload functionspy file stores implementation functions requirementstxt file stores dependencies named storage collection custom_functions use functions sql select functionsadd_integerssqft 1 added_one sqft example_dbhome_rentals limit 1 output sql added_one sqft 918 917"
  },
  {
    "filename": "llm_function.mdx",
    "path": "docs/mindsdb_sql/functions/llm_function.mdx",
    "chunk_id": 0,
    "chunk_content": "title llm function sidebartitle llm function mindsdb provides llm function lets users incorporate llmgenerated output directly data queries prerequisites use llm function mindsdb choosing one available model providers define following environment variables accordiongroup accordion titleopenai environment variables openai provider llm_function_model_name llm_function_temperature llm_function_max_retries llm_function_max_tokens llm_function_base_url openai_api_key llm_function_api_organization llm_function_request_timeout note values stored environment variables specific provider accordion accordion titleanthropic environment variables anthropic provider llm_function_model_name llm_function_temperature llm_function_max_tokens llm_function_top_p llm_function_top_k llm_function_default_request_timeout llm_function_api_key llm_function_base_url note values stored environment variables specific provider accordion accordion titleanyscale environment variables anyscale provider llm_function_model_name llm_function_temperature llm_function_max_retries llm_function_max_tokens llm_function_base_url llm_function_api_key llm_function_proxy llm_function_request_timeout note values stored environment variables specific provider accordion accordion titlelitellm environment variables litellm provider llm_function_model_name llm_function_temperature llm_function_api_base llm_function_max_retries llm_function_max_tokens llm_function_top_p llm_function_top_k note values stored environment variables specific provider accordion accordion titleollama environment variables ollama provider llm_function_base_url llm_function_model_name llm_function_temperature llm_function_top_p llm_function_top_k llm_function_request_timeout llm_function_format llm_function_headers llm_function_num_predict llm_function_num_ctx llm_function_num_gpu llm_function_repeat_penalty llm_function_stop llm_function_template note values stored environment variables specific provider accordion accordion titlenvidia nims environment variables nvidia nims provider llm_function_base_url llm_function_model_name llm_function_temperature llm_function_top_p llm_function_request_timeout llm_function_format llm_function_headers llm_function_num_predict llm_function_num_ctx llm_function_num_gpu llm_function_repeat_penalty llm_function_stop llm_function_template llm_function_nvidia_api_key note values stored environment variables specific provider accordion accordion titlemindsdb environment variables mindsdb provider llm_function_model_name llm_function_project_name use mindsdb provider create model project within mindsdb use name llm_function_model_name environment variable project"
  },
  {
    "filename": "llm_function.mdx",
    "path": "docs/mindsdb_sql/functions/llm_function.mdx",
    "chunk_id": 1,
    "chunk_content": "name llm_function_project_name environment variable accordion accordiongroup usage use llm function simply ask question get answer sql select llmhow many planets solar system output sql llm 8 planets solar system moreover llm function data swiftly complete tasks text generation summarization sql select comment llmdescribe comments category one word comment category example_dbuser_comments output sql comment category hate tacos dislike want dance desire baking big deal opinion"
  },
  {
    "filename": "case-when.mdx",
    "path": "docs/mindsdb_sql/sql_support/case-when.mdx",
    "chunk_id": 0,
    "chunk_content": "title case statement sidebartitle case mindsdb supports standard sql syntax including case statement case statement used conditional logic within queries evaluates conditions returns specific values based whether condition true false allowing conditional output within select clauses sql select case a1 ab 12b2 0 ab2 bc3 ab b else c end table_name"
  },
  {
    "filename": "aggregate-functions.mdx",
    "path": "docs/mindsdb_sql/sql_support/aggregate-functions.mdx",
    "chunk_id": 0,
    "chunk_content": "title sql aggregate functions sidebartitle aggregate functions mindsdb supports standard sql syntax including sql aggregate functions sql aggregate functions perform calculations set values return single result making useful summarizing analyzing data across multiple rows common aggregate functions include count sum avg min max functions used group organize results specific categories sql select year sumsalary annual_salary salaries group year"
  },
  {
    "filename": "cte.mdx",
    "path": "docs/mindsdb_sql/sql_support/cte.mdx",
    "chunk_id": 0,
    "chunk_content": "title common table expressions sidebartitle ctes mindsdb supports standard sql syntax including common table expressions ctes ctes used create temporary named result sets simplify complex queries enhance readability allow modular query design breaking large queries manageable parts sql table_name1 select columns table1 t1 join table2 t2 t1col t2col table_name2 select columns table1 t1 join table2 t2 t1col t2col select columns table_name1 t1 join table_name2 t2 t1col t2col"
  },
  {
    "filename": "window-functions.mdx",
    "path": "docs/mindsdb_sql/sql_support/window-functions.mdx",
    "chunk_id": 0,
    "chunk_content": "title sql window functions sidebartitle window functions mindsdb supports standard sql syntax including sql window functions window functions sql perform calculations across set table rows related current row without collapsing rows single result like aggregate functions functions useful ranking calculating running totals working moving averages common window functions include row_number rank dense_rank ntilen lag lead sum avg sql select b cd avga partition b table_name"
  },
  {
    "filename": "chatbot.mdx",
    "path": "docs/mindsdb_sql/agents/chatbot.mdx",
    "chunk_id": 0,
    "chunk_content": "title chatbot sidebartitle chatbot within mindsdb chatbots agentsmindsdb_sqlagentsagent connected chat interface creating chatbot requires either ai agentmindsdb_sqlagentsagent llm connection chat app like slackintegrationsappintegrationsslack ms teamsintegrationsappintegrationsmicrosoftteams p aligncenter img srcassetschatbot_diagrampng p chatbot ai agent ai agentsmindsdb_sqlagentsagent customized ai models answer questions data connect data agents form skillsmindsdb_sqlagentsagentcreateskills create chatbot integrates ai agent connected chat interface conversation data sql create chatbot my_chatbot using database my_slack created create database my_slack agent my_agent created create agent my_agent is_running true default true parameters include following database stores connection chat app like slackintegrationsappintegrationsslack ms teamsintegrationsappintegrationsmicrosoftteams created create database statement agent ai agentmindsdb_sqlagentsagent created create agent command consists ai model set skills defined data sets provided inference time via ragbased techniques is_running indicates whether start chatbot upon creation tip tips using slack integration 1 want use slack create chatbotagentschatbot syntax use method connecting slack mindsdbintegrationsappintegrationsslackmethod1chatbotrespondsindirectmessagestoaslackapp 2 want connect chatbot multiple slack channels open slack application add appbot one channels go channel want use bot rightclick channel select view channel details select integrations click add app tip chatbot llm alternatively create chatbot equivalent embedding llm choice chat app like slackintegrationsappintegrationsslack ms teamsintegrationsappintegrationsmicrosoftteams create chatbot integrates llm connected chat interface sql create chatbot my_chatbot using database my_slack created create"
  },
  {
    "filename": "chatbot.mdx",
    "path": "docs/mindsdb_sql/agents/chatbot.mdx",
    "chunk_id": 1,
    "chunk_content": "database my_slack model my_model created create model my_model is_running true default true parameters include following database stores connection chat app like slackintegrationsappintegrationsslack ms teamsintegrationsappintegrationsmicrosoftteams created create database statement model conversational model created create model command using langchain engineintegrationsaiengineslangchain is_running indicates whether start chatbot upon creation delete chatbot sql drop chatbot my_chatbot query chatbots sql show chatbots select chatbots example following example hereagentsagentexample lets create chatbot utilizing already created agent start connecting chat app mindsdb follow instructionintegrationsappintegrationsslack connect slack mindsdb follow instructionintegrationsappintegrationsmicrosoftteams connect ms teams mindsdb next create chatbot sql create chatbot text_to_sql_chatbot using database my_slack must created create database agent text_to_sql_agent must created create agent is_running true tip follow tutorialusecasesai_agentsbuild_ai_agents build chatbot tip"
  },
  {
    "filename": "knowledge-bases.mdx",
    "path": "docs/mindsdb_sql/agents/knowledge-bases.mdx",
    "chunk_id": 0,
    "chunk_content": "title knowledge base sidebartitle knowledge base knowledge base batteriesincluded rag system create insert data well query table internally knowledge bases use vector store embedding model default uses chromadb vector store openai embedding model requires openai api key set evironment variable define vector store embedding model examples p aligncenter img srchttpsdocsgooglecomdrawingsde2pacx1vtvmuqhofxlf3ncl0nwwgdpji7hj19f5xu8ed31ntyvlstm3pom9zzkwcrewzvxjrohl2raifkwlsppubw1342h681 p syntax general syntax creating knowledge base sql create knowledge_base my_kb using model embedding_model optional storage vector_databasestorage_table optional metadata_columns date creator optional content_columns review content optional id_column index optional parameters optional model embedding model created within mindsdb create model embedding_model provide model parameter openais textembeddingada002 model used default provided openai_api_key environment variable defined storage vector database stores embedded data defaults chromadb connect vector store mindsdb create database vector_database metadata_columns list column names stored metadata column knowledge base set metadata column used content_columns list column names stored content column knowledge base set columns stored content column id_column column name stored id column knowledge base uniquely identitify data note knowledge base comprises id content metadata columns store data defined parameters note examples section presents examples create knowledge bases insert data storage form embeddings knowledge base openai embedding model note using openais embedding model requires openai api key first create engine model accessed"
  },
  {
    "filename": "knowledge-bases.mdx",
    "path": "docs/mindsdb_sql/agents/knowledge-bases.mdx",
    "chunk_id": 1,
    "chunk_content": "sql create ml_engine embedding langchain_embedding next create embedding model providing openai api key sql create model embedding_model predict embeddings using engine embedding class openai model textembeddingada002 openai_api_key skxxx input_columns content analyze data want insert knowledge base sql select example_dbhome_rentals output sql number_of_rooms number_of_bathrooms sqft location days_on_market neighborhood rental_price created_at 2 1 917 great 13 berkeley_hills 3901 20240224 022821746167 0 1 194 great 10 berkeley_hills 2042 20240219 061059693052 1 1 543 poor 18 westbrae 1871 20240212 075345914146 decide columns use content ones metadata example use days_on_market neighborhood columns metadata location rental_price columns content embedding model create knowledge base passing embedding model defining content metadata columns sql create knowledge_base kb_custom_model using model embedding_model metadata_columns days_on_market neighborhood content_columns location rental_price successful creation knowledge base insert data store form embeddings sql insert kb_custom_model select example_dbhome_rentals finally verify data inserted knowledge base querying sql select kb_custom_model output sql id content metadata 1 days_on_market 13 locationgreatrental_price3901 neighborhood berkeley_hills 2 days_on_market 10 locationgreatrental_price2042 neighborhood berkeley_hills 3 days_on_market 18 locationpoorrental_price1871 neighborhood westbrae knowledge base hugging face embedding model example uses open source embedding model first create engine model accessed sql create ml_engine embedding langchain_embedding next create embedding model sql create model embedding_model predict embedding using engine langchain_embedding class"
  },
  {
    "filename": "knowledge-bases.mdx",
    "path": "docs/mindsdb_sql/agents/knowledge-bases.mdx",
    "chunk_id": 2,
    "chunk_content": "huggingfaceembeddings model_name sentencetransformersallmpnetbasev2 embedding model create knowledge base passing embedding model sql create knowledge_base kb_custom_model using model embedding_model successful creation knowledge base insert data store form embeddings sql insert kb_custom_model select review content example_dbamazon_reviews finally verify data inserted knowledge base querying sql select kb_custom_model knowledge base custom vector store example shows create knowledge base custom vector database first connect vector database chromadb sql create database chroma_dev_local engine chromadb parameters persist_directory persist path create index vector store insert one example point sql create table chroma_dev_localworld_news_index select content embeddings embedding_model content lorem ipsum dolor sit amet consectetur adipiscing elit sed eiusmod tempor incididunt ut labore et dolore magna aliqua tristique sollicitudin nibh sit amet commodo nulla risus sed vulputate odio ut enim blandit volutpat suspendisse ultrices gravida dictum fusce ut placerat orci eget nulla facilisi etiam dignissim diam aenean euismod elementum nisi quis eleifend quam ac placerat vestibulum lectus mauris ultrices eros sed turpis tincidunt id aliquet risus feugiat ante metus pellentesque habitant morbi tristique senectus et netus imperdiet massa tincidunt nunc pulvinar sapien et ligula leo vitae turpis massa sed elementum tempus egestas aliquam malesuada bibendum arcu vitae elementum curabitur sit amet tellus cras adipiscing enim ut tellus elementum sagittis vitae"
  },
  {
    "filename": "knowledge-bases.mdx",
    "path": "docs/mindsdb_sql/agents/knowledge-bases.mdx",
    "chunk_id": 3,
    "chunk_content": "et leo donec pretium vulputate sapien nec next create knowledge base passing vector database connection sql create knowledge_base world_news_kb using storage chroma_dev_localworld_news_index automate data sync knowledge base example shows set job inserts newly available data database knowledge base automate adding content knowledge base every time new data becomes available sql create job keep_knowledge_base_up_to_date insert world_news_kb select id text content world_news_with_ids id last every second note last keyword enables quey select newly added data learn last keyword heremindsdb_sqlsqlcreatejobslast note query knowledge base sql select world_news_kb select world_news_kb content youtube limit 1 select world_news_kb content canada google limit 1"
  },
  {
    "filename": "agent.mdx",
    "path": "docs/mindsdb_sql/agents/agent.mdx",
    "chunk_id": 0,
    "chunk_content": "title agent sidebartitle agent mindsdb create deploy ai agents comprise ai models customizable skills knowledge bases texttosql p aligncenter img srcassetsagent_diagrampng p ai agents use conversational model like openai langchain utilizing tools skillshttpspythonlangchaincomdocsmodulesagentstools respond user input users customize ai agents prompts fit use cases chatbotagentschatbot thought agent connected messaging interface work ai agents create skills start setting skills create manage skills using sql api creating inserting updating deleting knowledge base sql create knowledge base my_knowledge_base using model embedding_model_name parameter optional provided suitable embedding model chosen task storage vector_databasestorage_table parameter optional provided default chromadb used storage inserts new data rows generates id row id provided insert my_knowledge_base select text content datasourcedata_table inserts new data rows updates existing ones id value matches insert my_knowledge_base select id text content datasourcedata_table view content knowledge base example look generated id values select my_knowledge_base drop knowledge base my_knowledge_base creating updating deleting knowledge_base skill sql create skill kb_skill using type knowledge_base source my_knowledge_base must created create knowledge base description data data description help agent know use knowledge base update skill kb_skill set source new_knowledge_base must created create knowledge base drop skill kb_skill creating updating deleting text2sql skill sql create skill text_to_sql_skill using type text2sql database example_db must"
  },
  {
    "filename": "agent.mdx",
    "path": "docs/mindsdb_sql/agents/agent.mdx",
    "chunk_id": 1,
    "chunk_content": "created create database tables sales_data list tables passed skill description sales data update skill text_to_sql_skill set database new_example_db must created create database tables sales_data list tables passed skill drop skill text_to_sql_skill query skills using command sql show knowledge_bases show skills create agent agent created deleted queried updated using sql api creating ai agent sql create agent my_agent using model chatbot_agent must conversational model created create model example section skills test_skill must created create skill alternatively create agent define model used agent agent creation time based model providers defined hereintegrationsaiengineslangchainopenai sql create agent my_agent using skillstext_to_sql_skill must created create skill provideropenai choose one available model providers openai anthropic anyscale ollama litellm mindsdb modelgpt4 define model provider prompt_templateanswer user input helpful way using tools provide instruction model verbosetrue max_tokens100 updating ai agent sql update agent my_agent set model new_chatbot_agent must conversational model created create model skills_to_remove test_skill skills_to_add production_skill must created create skill querying ai agent sql select my_agent question insert question user_column parameter defined creating conversational model agent example section deleting ai agent sql drop agent my_agent query agents using command sql show agents project_name select agents example agents texttosql skills start creating conversational large language model used agent sql create"
  },
  {
    "filename": "agent.mdx",
    "path": "docs/mindsdb_sql/agents/agent.mdx",
    "chunk_id": 2,
    "chunk_content": "model my_model predict answer using engine langchain openai_api_key yourmodelapikey model_namegpt4 mode conversational user_column question assistant_column answer max_tokens100 temperature0 verbosetrue prompt_templateanswer user input helpful way tip agents access models via langchain integration mindsdbintegrationsaiengineslangchain check link find available models tip connect data source used creating skill sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo select example_dbsales_data create skill using one tables connected data source sql create skill text_to_sql_skill using type text2sql database example_db tables car_sales description car sales data model skill lets create agent sql create agent text_to_sql_agent using model my_model skills text_to_sql_skill query agent sql select text_to_sql_agent question many cars sold 2017 column defined user_column parameter create model next step would connect chat app like slack mindsdb create chatbot utilizing agent tip learn chatbots hereagentschatbot tip agents knowledge bases skills example lets create embedding model using openaiintegrationsaienginesopenai langchainhttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerslangchain_embedding_handler engine knowledge base note step optional knowledge bases provide default embedding model sql create ml_engine openai_engine openai using openai_api_key youropenaiapikey create model embedding_model predict embeddings using engine openai_engine modeembedding model_nametextembeddingada002 question_column content lets create knowledge base uses embedding model default storage vector database chromadb sql create knowledge base my_knowledge_base using model embedding_model optional insert"
  },
  {
    "filename": "agent.mdx",
    "path": "docs/mindsdb_sql/agents/agent.mdx",
    "chunk_id": 3,
    "chunk_content": "data knowledge base select sql insert my_knowledge_base content values drink tea select my_knowledge_base use knowledge base create skill agent sql create skill kb_skill using type knowledge_base source my_knowledge_base must created create knowledge base description data data description help agent know use knowledge base assign skill agent created example query sql update agent text_to_sql_agent set skills_to_add kb_skill select text_to_sql_agent questions data"
  },
  {
    "filename": "get-single-prediction.mdx",
    "path": "docs/mindsdb_sql/sql/get-single-prediction.mdx",
    "chunk_id": 0,
    "chunk_content": "title get single prediction sidebartitle get single prediction description select statement fetches predictions model table data returned fly result set persisted ways save predictions data save predictions view using create viewsqlcreateview statement please note view saved query store data like table another way create table using create tablesqlcreatetable statement insert predictions existing table using insert intosqlapiinsert statement syntax syntax fetching single prediction model table sql select target_name target_name_explain mindsdbpredictor_name column_name value column_name value warning grammar matters points keep mind writing queries mindsdbbr nbspnbspnbsp1 column_name value pairs may joined keywordsbr nbspnbspnbsp2 use quotations numerical valuesbr nbspnbspnbsp3 use single quotes strings nbspnbspnbsp4 tables column names case sensitive warning execution get sql target_name target_name_explain predicted_value predicted_value 4394 confidence 099 anomaly null truth null confidence_lower_bound 4313 confidence_upper_bound 4475 name description target_name name column predicted target_name_explain object json type contains predicted_value additional information confidence anomaly truth confidence_lower_bound confidence_upper_bound predictor_name name model used make prediction column_name value clause used pass data values prediction made example lets predict rental_price value using home_rentals_model model property sqft823 locationgood neighborhooddowntown days_on_market10 sql select sqft location neighborhood days_on_market rental_price rental_price_explain mindsdbhome_rentals_model1 sqft823 locationgood neighborhooddowntown days_on_market10 execution get sql sqft location neighborhood days_on_market rental_price rental_price_explain 823 good downtown 10 4394 predicted_value 4394"
  },
  {
    "filename": "get-single-prediction.mdx",
    "path": "docs/mindsdb_sql/sql/get-single-prediction.mdx",
    "chunk_id": 1,
    "chunk_content": "confidence 099 anomaly null truth null confidence_lower_bound 4313 confidence_upper_bound 4475"
  },
  {
    "filename": "get-batch-predictions.mdx",
    "path": "docs/mindsdb_sql/sql/get-batch-predictions.mdx",
    "chunk_id": 0,
    "chunk_content": "title get batch predictions sidebartitle get batch predictions description select statement fetches predictions model table data returned fly result set persisted ways save predictions data save predictions view using create viewsqlcreateview statement please note view saved query store data like table another way create table using create tablesqlcreatetable statement insert predictions existing table using insert intosqlapiinsert statement syntax syntax making batch predictions joining one data source tables one model tables sql select t1column t2column m1target m2target integration_nametable_name1 t1 join integration_nametable_name2 t2 t1column t2column join join mindsdbmodel_name1 m1 join mindsdbmodel_name2 m2 join t1input_data m1expected_argument m1parameter value m2parameter value data tables provide input models integration_nametable_name1 integration_nametable_name2 ai tables mindsdbmodel_name1 mindsdbmodel_name2 note provide input models data tables also clause tip querying predictions specify partition_size parameter split data partitions run prediction different workers note ml task queuesetupcustomconfigoverviewofconfigparameters needs enabled use parameter use partition_size parameter provide using clause specifying partition size like using partition_size100 tip tip follow doc pagegenerativeaitables learn ai tables tip example lets make bulk predictions predict rental_price value using home_rentals_model model joined data source table sql select tsqft tlocation tneighborhood tdays_on_market trental_price real_price mrental_price predicted_rental_price example_dbdemo_datahome_rentals join mindsdbhome_rentals_model limit 5 execution get sql sqft location neighborhood days_on_market real_price predicted_rental_price 917 great berkeley_hills"
  },
  {
    "filename": "get-batch-predictions.mdx",
    "path": "docs/mindsdb_sql/sql/get-batch-predictions.mdx",
    "chunk_id": 1,
    "chunk_content": "13 3901 3886 194 great berkeley_hills 10 2042 2007 543 poor westbrae 18 1871 1865 503 good downtown 10 3026 3020 1066 good thowsand_oaks 13 4774 4748 tip follow doc pagegenerativeaitablesworkingwithgenerativeaitables see examples joining multiple data table multiple models tip"
  },
  {
    "filename": "query-jobs.mdx",
    "path": "docs/mindsdb_sql/sql/query-jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title query jobs sidebartitle query jobs querying jobs view jobs project sql show jobs project projectname select projectnamejobs execution get sql name project run_start run_end next_run_at schedule_str query drop_model mindsdb 20230401 000000000000 null 20230401 000000000000 null drop model mindsdbhome_rentals_model retrain_model_and_save_predictions mindsdb 20230215 191943210122 20230401 000000000000 20230215 191943210122 every 2 days retrain mindsdbhome_rentals_model using join_learn_process true insert my_integrationrentals select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals save_predictions mindsdb 20230215 191948545580 null 20230215 191948545580 every hour create table my_integrationresult_start_datetime select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals projects sql show jobs select information_schemajobs querying jobs history query history jobs similar querying jobs find information error job didnt execute successfully view jobs history current project sql select logjobs_history project mindsdb execution get sql name project run_start run_end next_run_at error query retrain_model_and_save_predictions mindsdb 20230215 191943210122 20230401 000000000000 20230215 191943210122 null retrain mindsdbhome_rentals_model using join_learn_process true insert my_integrationrentals select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals save_predictions mindsdb 20230215 191948545580 null 20230215 191948545580 null create table my_integrationresult_start_datetime select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals please note drop_model job jobs_history table didnt start yet"
  },
  {
    "filename": "list-data-handlers.mdx",
    "path": "docs/mindsdb_sql/sql/list-data-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data handlers sidebartitle list data handlers description show handlers command lists available handlers clause filter handlers type data ml syntax syntax sql show handlers type data"
  },
  {
    "filename": "show-databases.mdx",
    "path": "docs/mindsdb_sql/sql/show-databases.mdx",
    "chunk_id": 0,
    "chunk_content": "title list data sources sidebartitle list data sources description show databases statement lists connected data sources mindsdb access syntax list connected data sources sql show databases"
  },
  {
    "filename": "list-ml-handlers.mdx",
    "path": "docs/mindsdb_sql/sql/list-ml-handlers.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml handlers sidebartitle list ml handlers description show handlers command lists available handlers clause filter handlers type data ml syntax syntax sql show handlers type ml"
  },
  {
    "filename": "query-triggers.mdx",
    "path": "docs/mindsdb_sql/sql/query-triggers.mdx",
    "chunk_id": 0,
    "chunk_content": "title query triggers sidebartitle query triggers description triggers enable users define eventbased actions example table updated run query update predictions info currently create triggers following data sources mongodbhttpsdocsmindsdbcomintegrationsdataintegrationsmongodb slackhttpsdocsmindsdbcomintegrationsappintegrationsslack solacehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssolace_handler info syntax syntax querying triggers sql show triggers"
  },
  {
    "filename": "show-ml-engines.mdx",
    "path": "docs/mindsdb_sql/sql/show-ml-engines.mdx",
    "chunk_id": 0,
    "chunk_content": "title list ml engines sidebartitle list ml engines description show ml_engines statement lists created ml engines used create models syntax list created ml engines sql show ml_engines"
  },
  {
    "filename": "native-queries.mdx",
    "path": "docs/mindsdb_sql/sql/native-queries.mdx",
    "chunk_id": 0,
    "chunk_content": "title native queries sidebartitle native queries underlying database engine mindsdb mysql however run queries native database engine within mindsdb connect database mindsdb run queries native database must first connect database mindsdb using create database statement sql create database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo connect example_db database postgresql database run queries native database postgresql database connected run postgresqlnative queries querying run postgresqlnative code must nest within select statement like sql select example_db select model year price transmission mileage fueltype mpg miles per galon roundcastmpg 23521458 numeric 1 kml kilometers per liter date_partyear current_dateyear years_old age car count partition model year units_to_sell many units certain model sold year roundcasttax decimal price 3 tax_div_price value tax divided price car demo_dataused_car_price execution get sql modelyearpricetransmissionmileagefueltypempg kml years_oldunits_to_selltax_div_price a1 20109990 automatic 38000 petrol 53322712 1 0013 a1 20116995 manual 65000 petrol 53322711 5 0018 a1 20116295 manual 107000 petrol 53322711 5 002 a1 20114250 manual 116000 diesel 70630 11 5 0005 a1 20116475 manual 45000 diesel 70630 11 5 0 first line select example_db informs mindsdb select postgresql database nest postgresql code within brackets creating views create view based native query sql create view cars example_db"
  },
  {
    "filename": "native-queries.mdx",
    "path": "docs/mindsdb_sql/sql/native-queries.mdx",
    "chunk_id": 1,
    "chunk_content": "select model year price transmission mileage fueltype mpg miles per galon roundcastmpg 23521458 numeric 1 kml kilometers per liter date_partyear current_dateyear years_old age car count partition model year units_to_sell many units certain model sold year roundcasttax decimal price 3 tax_div_price value tax divided price car demo_dataused_car_price execution get sql query ok 0 rows affected xxxx sec"
  },
  {
    "filename": "show-models.mdx",
    "path": "docs/mindsdb_sql/sql/show-models.mdx",
    "chunk_id": 0,
    "chunk_content": "title list models sidebartitle list models description show models statement lists models created mindsdb syntax list models sql show models alternatively query models table sql select project_namemodels"
  },
  {
    "filename": "list-projects.mdx",
    "path": "docs/mindsdb_sql/sql/list-projects.mdx",
    "chunk_id": 0,
    "chunk_content": "title list projects sidebartitle list projects description show databases command lists available data sources projects clause filters projects syntax syntax sql show databases type project alternatively use full keyword get information sql show full databases type project"
  },
  {
    "filename": "jobs.mdx",
    "path": "docs/mindsdb_sql/sql/drop/jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove job sidebartitle remove job description drop job statement deletes job syntax syntax deleting job sql drop job exists project_namejob_name project_name value optional job_name value indicates job deleted lets look examples sql drop job my_projectretrain_and_save_job drop retrain_and_save_job resides my_project project another example sql drop job create_table_job drop create_table_job job resides current project learn projects mindsdb visit docs heresqlproject"
  },
  {
    "filename": "file.mdx",
    "path": "docs/mindsdb_sql/sql/drop/file.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove file sidebartitle remove file description drop table statement also used delete file syntax syntax sql drop table filesfile_name execution get sql query successfully completed note please note uploaded files tables well remove uploaded file use drop table statement note"
  },
  {
    "filename": "ml-engine.mdx",
    "path": "docs/mindsdb_sql/sql/drop/ml-engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove ml engine sidebartitle remove ml engine description drop ml_engine statement deletes ml engine syntax syntax sql drop ml_engine exists ml_engine_name execution get sql query successfully completed"
  },
  {
    "filename": "model.mdx",
    "path": "docs/mindsdb_sql/sql/drop/model.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove model sidebartitle remove model description drop model statement deletes model table syntax syntax sql drop model exists predictor_name execution get sql query ok 0 rows affected 0058 sec name description predictor_name name model deleted example lets list available predictor tables sql show models execution get sql name other_model home_rentals_model delete home_rentals_model table sql drop model home_rentals_model execution get sql query ok 0 rows affected 0058 sec check deletion successful querying mindsdbmodels table sql show models execution get sql name other_model"
  },
  {
    "filename": "project.mdx",
    "path": "docs/mindsdb_sql/sql/drop/project.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove project sidebartitle remove project description drop project statement deletes project syntax syntax sql drop project exists project_name execution get sql query successfully completed"
  },
  {
    "filename": "database.mdx",
    "path": "docs/mindsdb_sql/sql/drop/database.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove data source sidebartitle remove data source description drop database statement deletes database syntax syntax sql drop database exists database_name execution get sql query successfully completed"
  },
  {
    "filename": "table.mdx",
    "path": "docs/mindsdb_sql/sql/drop/table.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove table sidebartitle remove table description drop table statement deletes table file warning please note feature yet implemented tables connected data sources warning syntax syntax sql drop table table_name files sql drop table filesfile_name execution get sql query successfully completed note please note uploaded files tables well remove uploaded file use drop table statement note"
  },
  {
    "filename": "view.mdx",
    "path": "docs/mindsdb_sql/sql/drop/view.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove view sidebartitle remove view description drop view statement deletes view syntax syntax sql drop view exists view_name execution get sql query successfully completed"
  },
  {
    "filename": "trigger.mdx",
    "path": "docs/mindsdb_sql/sql/drop/trigger.mdx",
    "chunk_id": 0,
    "chunk_content": "title remove trigger sidebartitle remove trigger description triggers enable users define eventbased actions example table updated run query update predictions info currently create triggers following data sources mongodbhttpsdocsmindsdbcomintegrationsdataintegrationsmongodb slackhttpsdocsmindsdbcomintegrationsappintegrationsslack solacehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssolace_handler info syntax syntax removing trigger sql drop trigger trigger_name"
  },
  {
    "filename": "jobs.mdx",
    "path": "docs/mindsdb_sql/sql/create/jobs.mdx",
    "chunk_id": 0,
    "chunk_content": "title jobs sidebartitle create job mindsdb enables automate pipeline using jobs grant power schedule query frequency additionally introduces keyword stronglastlaststrong offering capability job act solely new data essentially treating data source stream p aligncenter img srchttpsdocsgooglecomdrawingsde2pacx1vt_q7r0x4hhssxhampj2rgftfrp_sk6gjc9kz4cg99ahi94ydh2dpttax7za54iu5me4zs4jwmw_ofpubw955h456 p description create job statement lets schedule execution queries providing relevant parameters start date end date repetition frequency syntax create job syntax sql create job exists project_namejob_name statement_1 statement_2 start date end date every number period statement_1 statement_2 expression description project_namejob_name name job preceded optional project name job created provide project_name value job created default mindsdb project statement_1 statement_2 one statements separated executed job start date optional date job starts periodical onetime execution set current system date end date optional date job ends periodical onetime execution set repetition rules set job repeats forever every number period optional repetition rules job set job runs considering end date value number value set defaults 1 statement_1 statement_2 optional last statement returns one rows job execute info available date formats supported date formats ymd hms ymd please note default time zone utc info info available period values supported period values minute minutes min hour hours day days week weeks month months info query jobs execution history like sql"
  },
  {
    "filename": "jobs.mdx",
    "path": "docs/mindsdb_sql/sql/create/jobs.mdx",
    "chunk_id": 1,
    "chunk_content": "show jobs select project_namejobs name job_name select logjobs_history project mindsdb name job_name last mindsdb provides custom last keyword enables fetch data inserted last time queried convenient way select newly added data rows running job imagine fruit_data table contains following sql id name 1 apple 2 orange run select query last keyword first time itll give empty output sql select id name fruit_data id last query returns sql id name null null tip want specify concrete value last first execution query use coalescelast value function sql select id name fruit_data id coalescelast 1 result executing following query first run sql select id name fruit_data id 1 query subsequent run sql select id name fruit_data id last tip imagine inserted new record fruit_data table sql id name 1 apple 2 orange 3 pear run select query last keyword youll get newly added record output sql select id name fruit_data id last query returns sql id name 3 pear point whenever add new records fruit_data table itll returned next run select query last keyword add new records query runs output null want clear context last keyword editor run set context 0 set context null conditional jobs create conditional job execute periodically new data"
  },
  {
    "filename": "jobs.mdx",
    "path": "docs/mindsdb_sql/sql/create/jobs.mdx",
    "chunk_id": 2,
    "chunk_content": "available sql create job conditional_job finetune model model_name select datasourcetable_name incremental_column last every 1 min select datasourcetable_name incremental_column last job triggered every minute execute task ie finetuning model new data available examples example 1 retrain model example create job current project retrain home_rentals_model model insert predictions rentals table sql create job retrain_model_and_save_predictions retrain mindsdbhome_rentals_model using join_learn_process true insert my_integrationrentals select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals end 20230401 000000 every 2 days tip please note join_learn_process parameter using clause retrainsqlapiretrain statement ensures retraining process completes inserting predictions table general parameter used prevent several retrain processes running simultaneously tip retrain_model_and_save_predictions job starts execution current system date ends 1st april 2023 job executed every 2 days example 2 save predictions example job creates table named result_start_datetime inserts predictions sql create job save_predictions create table my_integrationresult_start_datetime select mrental_price mrental_price_explain mindsdbhome_rentals_model join example_dbdemo_datahome_rentals every hour tip please note uniqueness created table name ensured using start_datetime variable replaced runtime date time current run use following variables purpose previous_start_datetime replaced date time previous run job start_datetime replaced date time current job run start_date replaced date current job run tip save_predictions job starts execution current system date repeats every 2 hours manually disabled example 3 drop model example"
  },
  {
    "filename": "jobs.mdx",
    "path": "docs/mindsdb_sql/sql/create/jobs.mdx",
    "chunk_id": 3,
    "chunk_content": "create job drop home_rentals_model model scheduled 1st april 2023 sql create job drop_model drop model mindsdbhome_rentals_model start 20230401 job runs 1st april 2023 example 4 twitter chatbot easily create chatbot respond tweets using jobs get connect twitter account mindsdb following instructions heredataintegrationstwitter tip follow tutorial create twitter chatbotsqltutorialstwitterchatbot learn details tip lets create job runs every hour checks new tweets responds using openai model sql create job mindsdbgpt4_twitter_job insert tweets output joining model new tweets insert my_twitter_v2tweets in_reply_to_tweet_id text select tid in_reply_to_tweet_id rresponse text my_twittertweets join mindsdbsnoopstein_model r tquery snoopstein snoop_stein snoopstein snoop_stein isretweet fromsnoop_stein tcreated_at last limit 10 every hour selectsqlapiselect statement joins data table model table get responses newly posted tweets thanks last keyword insert intosqlapiinsert statement writes responses tweets table my_twitter integration tip learn openai integration mindsdb visit docs herenlpnlpmindsdbopenai tip additional configuration template configjson file pass parameter starting local mindsdb instance bash jobs disable true check_interval 30 disable parameter defines whether scheduler active true false default mindsdb editor scheduler active check_interval parameter defines interval seconds consecutive checks scheduler table default mindsdb editor 30 seconds modify default configuration local mindsdb installation creating configjson file starting mindsdb file parameter find detailed instructions heresetupcustomconfigstartingmindsdbwithextendedconfiguration"
  },
  {
    "filename": "file.mdx",
    "path": "docs/mindsdb_sql/sql/create/file.mdx",
    "chunk_id": 0,
    "chunk_content": "title upload file sidebartitle upload file follow steps upload file mindsdb note note trailing whitespaces column names erased upon uploading file mindsdb note 1 access mindsdb editor 2 navigate add data section clicking add data button located top right corner p aligncenter img srcassetssqladdfiletomindsdbcloud3png p 3 choose files tab p aligncenter img srcassetssqladdfiletomindsdbcloudpng p 4 choose import file option 5 upload file house_salescsv name table used store file data house_sales click save continue button p aligncenter img srcassetssqladdfiletomindsdbcloud2png p configuring url file upload specific domains file uploader configured interact specific domains using file_upload_domains setting configjson file feature allows restrict handler upoad process files domains specify enhancing security control web interactions configure simply list allowed domains file_upload_domains key configjson example json file_upload_domains s3amazonawscom drivegooglecom whats next ready create predictor file make sure check guidesqlcreatemodel"
  },
  {
    "filename": "ml-engine.mdx",
    "path": "docs/mindsdb_sql/sql/create/ml-engine.mdx",
    "chunk_id": 0,
    "chunk_content": "title configure ml engine sidebartitle configure ml engine create machine learning ml engines based ml handlers available mindsdb cant find ml handler interest always contribute building new ml handlercontributemlhandlers description create ml_engine command creates ml engine uses one available ml handlers syntax creating ml engine make sure ml handler interest available querying ml handlers sql select information_schemahandlers show handlers info cant find ml handler interest contribute building new ml handlerhttpsdocsmindsdbcomcontributeml_handlers please note process contributing new ml engines ml engines andor tests run correctly dependencies listed requirementstxt file installed beforehand info find ml handler interest create ml engine using command sql create ml_engine exists ml_engine_name handler_name using argument_key argument_value please replace ml_engine_name handler_name optionally argument_key argument_value real values note please use ml_engine_name handler_name avoid issue dropping ml engine note verify ml engine successfully created run command sql select information_schemaml_engines show ml_engines want drop ml engine run command sql drop ml_engine ml_engine_name example lets check ml handlers currently available sql show handlers execution get sql name title description version connection_args import_success import_error ray_serve rayserve mindsdb handler ray serve 001 null true null neuralforecast neuralforecast mindsdb handler nixtlas neuralforecast package 001 null true null autosklearn autosklearn mindsdb handler autosklearn 002 null false module"
  },
  {
    "filename": "ml-engine.mdx",
    "path": "docs/mindsdb_sql/sql/create/ml-engine.mdx",
    "chunk_id": 1,
    "chunk_content": "named autosklearn mlflow mlflow mindsdb handler mlflow 002 null false module named mlflow openai openai mindsdb handler openai 001 null true null merlion merlion mindsdb handler merlion 001 null false object__init__ takes exactly one argument instance initialize byom byom mindsdb handler byom 001 code type path description path model code modules type path description path model requirements true null ludwig ludwig mindsdb handler ludwig automl 002 null false module named dask lightwood lightwood null 100 null true null huggingface_api hugging face api mindsdb handler autosklearn 002 null false module named hugging_py_face statsforecast statsforecast mindsdb handler nixtlas statsforecast package 000 null true null huggingface hugging face mindsdb handler higging face 001 null true null tpot tpot mindsdb handler tpot 002 null false module named tpot langchain langchain mindsdb handler langchain 001 null true null autokeras autokeras mindsdb handler autokeras automl 001 null false module named autokeras create ml engine using openai handler providing openai api key using clause sql create ml_engine my_openai_engine openai using openai_api_key opanai api key execution get sql query successfully completed lets verify ml engine exists sql show ml_engines execution get sql name handler connection_data lightwood lightwood keypasswordvalue huggingface huggingface keypasswordvalue openai openai keypasswordvalue my_openai_engine openai keyopenai_api_keypasswordvalue please"
  },
  {
    "filename": "ml-engine.mdx",
    "path": "docs/mindsdb_sql/sql/create/ml-engine.mdx",
    "chunk_id": 2,
    "chunk_content": "note using clause optional depends ml handler whether requires arguments created openai engine provided api key creating ml engine create model like sql create model my_model predict answer using engine my_openai_engine prompt_template ask question model using clause specifies ml engine used creating new model"
  },
  {
    "filename": "model.mdx",
    "path": "docs/mindsdb_sql/sql/create/model.mdx",
    "chunk_id": 0,
    "chunk_content": "title create train deploy model sidebartitle create train deploy model description create model statement creates trains machine learning ml model info please note create model statement equivalent create model statement transitioning create model statement create model statement still works info syntax overview full syntax sql create replace model exists project_namepredictor_name integration_name project_name select sequential_column partition_column column_name integration_name project_nametable_name join model_name predict target_column order sequential_column group partition_column window int horizon int using engine engine_name tag tag_name expressions description project_name name project model created default mindsdb project used predictor_name name model created integration_name name integration created using create databasesqlcreatedatabases statement file uploadsqlcreatefile select column_name table_name selecting data used training validation target_column column predicted order sequential_column used time series models column time series ordered date anything defines sequence events group partition_column used time series models optional column rows make partition grouped example want forecast inventory items store partition data product_id distinct product_id time series window int used time series models number rows look back making prediction comes rows ordered column defined order split groups columns defined group window 10 syntax could interpreted always use previous 10 rows horizon int used time series models optional defines number future predictions 1 default however horizon parameter"
  },
  {
    "filename": "model.mdx",
    "path": "docs/mindsdb_sql/sql/create/model.mdx",
    "chunk_id": 1,
    "chunk_content": "besides defining number predictions impact training procedure using lightwood ml backend example different mixers selected depending whether horizon value one greater one engine_name optionally provide ml engine based model created tag_name optionally provide tag visible training_options column mindsdbmodels table regression models syntax regression models sql create model project_namepredictor_name integration_name select column_name table_name predict target_column using engine engine_name tag tag_name note please note clause mandatory note target_column predicted numerical value prediction values limited defined set values number given range numbers classification models syntax classification models sql create model project_namepredictor_name integration_name select column_name table_name predict target_column using engine engine_name tag tag_name note please note clause mandatory note target_column predicted string value prediction values limited defined set values yes time series models syntax time series models sql create model project_namepredictor_name integration_name select sequential_column partition_column other_column target_column table_name predict target_column order sequential_column group partition_column window int horizon int using engine engine_name tag tag_name note please note clause mandatory note due nature time series forecasting need use joinsqlapijoin statement join data table model table get predictions nlp models syntax using external models within mindsdb sql create model project_namemodel_name predict pred using engine engine_name task task_name model_name hub_model_name input_column input_column_name labels label1 label2 note please note"
  },
  {
    "filename": "model.mdx",
    "path": "docs/mindsdb_sql/sql/create/model.mdx",
    "chunk_id": 2,
    "chunk_content": "dont need define clause instead input_column defined using clause note allows bring external model example hugging face model hub use within mindsdb large language models llm mindsdb integrates numerous llm providers listed hereintegrationsaioverviewlargelanguagemodels commonly llms support prompt_template parameter stores messageinstruction model sql create model project_namemodel_name predict answer using engine llm_engine_name prompt_template answer users questions helpful way questions prompt_template parameter instructs model output generated include variables enclosed double curly braces replaced data values upon joining model input data example regression models example regression models uses data database sql create model mindsdbhome_rentals_model example_db select demo_datahome_rentals predict rental_price using engine lightwood tag home rentals model execution get sql query ok 0 rows affected xxxx sec visit tutorial regression modelssqltutorialshomerentals see full example classification models example classification models uses data file sql create model mindsdbcustomer_churn_predictor files select churn predict churn using engine lightwood tag customers model execution get sql query ok 0 rows affected xxxx sec visit tutorial classification modelssqltutorialscustomerchurn see full example time series models example time series models uses data file sql create model mindsdbhouse_sales_predictor files select house_sales predict order saledate group bedrooms target column predicted stores one row per quarter window 8 using data last two years make forecasts last 8"
  },
  {
    "filename": "model.mdx",
    "path": "docs/mindsdb_sql/sql/create/model.mdx",
    "chunk_id": 3,
    "chunk_content": "rows horizon 4 making forecasts next year next 4 rows execution get sql query ok 0 rows affected xxxx sec visit tutorial time series modelssqltutorialshousesalesforecasting see full example nlp models example hugging face model sql create model mindsdbspam_classifier predict pred using engine huggingface task textclassification model_name mrm8488berttinyfinetunedsmsspamdetection input_column text_spammy labels ham spam execution get sql query ok 0 rows affected xxxx sec large language models llm example using openai engine sql create model sentiment_classifier predict sentiment using engine openai_engine prompt_template analyze customer reviews assign sentiment positive negative neutral review note prompt_template parameter stores instructions model follow generate output visit page bring hugging face models mindsdbcustommodelhuggingface details brbr tip checking model status run create model statement check status training process querying mindsdbmodels table sql describe predictor_name execution get sql name status accuracy predict update_statusmindsdb_versionerrorselect_data_querytraining_options predictor_name generating training complete number depending accuracy metric column_to_be_predicted up_to_date 22750 tip"
  },
  {
    "filename": "project.mdx",
    "path": "docs/mindsdb_sql/sql/create/project.mdx",
    "chunk_id": 0,
    "chunk_content": "title create project sidebartitle create project description mindsdb introduces projects natural way keep artifacts models views separate according predictive task solve learn mindsdb projects heresqlproject syntax syntax creating project sql create project exists project_name"
  },
  {
    "filename": "database.mdx",
    "path": "docs/mindsdb_sql/sql/create/database.mdx",
    "chunk_id": 0,
    "chunk_content": "title connect data source sidebartitle connect data source description mindsdb lets connect favorite databases data warehouses data lakes etc via create database command mindsdb sql api supports creating connections integrations passing connection parameters specific per integration find supported integrationssupportedintegrations chapter note mindsdb doesnt store copy data instead fetches data directly connected sources time make query ensuring changes data instantly reflected means data remains original location mindsdb always works uptodate information note syntax lets review syntax create database command sql create database exists datasource_name engine engine_name parameters key value execution get sql query ok 0 rows affected xxxx sec name description datasource_name identifier data source created engine_name engine selected depending database connection parameters key value object connection parameters specific engine note sql commands resulting output please note keywordsstatements enclosed within square brackets optional also default engine mindsdb provided otherwise yields following sql commands result output sql create database db create database db engine mindsdb create database db engine mindsdb create database db engine mindsdb create database db using engine mindsdb note whats available installation info query available data handlers used create database connections sql select information_schemahandlers type data alternatively sql show handlers type data query connected databases sql select information_schemadatabases alternatively"
  },
  {
    "filename": "database.mdx",
    "path": "docs/mindsdb_sql/sql/create/database.mdx",
    "chunk_id": 1,
    "chunk_content": "sql show databases show full databases info example connecting data source example connect mysql database sql create database mysql_datasource engine mariadb parameters user root port 3307 password password host 127001 database my_database execution get sql query ok 0 rows affected 8878 sec listing linked databases list linked databases using command sql show databases execution get sql database information_schema mindsdb files mysql_datasource making local database available mindsdb connecting local database mindsdb cloud expose local database server publicly accessible easy accomplish using ngrok tunnelhttpsngrokcom free tier offers need get started installation instructions easy follow head downloads pagehttpsngrokcomdownload choose operating system follow instructions installation create free account ngrokhttpsdashboardngrokcomsignup get auth token use configure ngrok instance installed configured run following command obtain host port localhost portnumber bash ngrok tcp portnumber example assuming run postgresql database localhost5432 use following command bash ngrok tcp 5432 execution get bash session status online account myaccount plan free version 2340 region united states us web interface http1270014040 forwarding tcp4tcpngrokio15093 localhost 5432 access local database 4tcpngrokio15093 instead localhost5432 connect local database mindsdb gui use forwarding information host 4tcpngrokio port 15093 proceed create database connection mindsdb gui executing create database statement host port number obtained ngrok sql create database psql_datasource engine"
  },
  {
    "filename": "database.mdx",
    "path": "docs/mindsdb_sql/sql/create/database.mdx",
    "chunk_id": 2,
    "chunk_content": "postgres parameters user postgres port 15093 password password host 4tcpngrokio database postgres please note ngrok tunnel loses connection stopped canceled reconnect local database mindsdb create ngrok tunnel free tier ngrok changes host port values time launch program need reconnect database mindsdb cloud passing new host port values obtained ngrok resetting database connection drop previously connected data source using drop database statement sql drop database psql_datasource dropping data source reconnecting local database use predictors trained using previously connected data source however retrain predictors please ensure database connection name used creating predictor avoid failing retrain supported integrations list databases supported mindsdb keeps growing check database integrations heredataintegrationsalldataintegrations"
  },
  {
    "filename": "table.mdx",
    "path": "docs/mindsdb_sql/sql/create/table.mdx",
    "chunk_id": 0,
    "chunk_content": "title create table sidebartitle create table description create table statement creates table optionally fills data provided query may used materialize prediction results tables syntax use create table statement create empty table sql create table integration_nametable_name column_name data_type use create table statement create table fill data sql create table integration_nametable_name select create replace table statement sql create replace table integration_nametable_name select note note integration_name connection must created create databasemindsdb_sqlsqlcreatedatabase statement user write access note steps followed syntax executes subselect query get output data case create replace table statement integration_nametable_name table dropped recreating recreates integration_nametable_name table inside integration_name integration uses insert intosqlapiinsert statement insert output select query integration_nametable_name execution get sql query ok 0 rows affected xxxx sec example want save prediction results int1tbl1 table schema structure used throughout example bash int1 tbl1 mindsdb predictor_name int2 tbl2 name description int1 integration table stores prediction results resides tbl1 table stores prediction results predictor_name name model int2 integration data source table used inner select statement resides tbl2 data source table used inner select statement lets execute query sql create replace table int1tbl1 select int2tbl2 ta join mindsdbpredictor_name tb tadate 20151231 execution get sql query ok 0 rows affected xxxx sec"
  },
  {
    "filename": "view.mdx",
    "path": "docs/mindsdb_sql/sql/create/view.mdx",
    "chunk_id": 0,
    "chunk_content": "title create view sidebartitle create view description create view statement creates view great way data preparation mindsdb view saved select statement executed every time call view syntax syntax sql create view exists project_nameview_name select acolumn_name pmodel_column model_column integration_nametable_name join mindsdbpredictor_name p execution get sql query ok 0 rows affected xxxx sec name description project_name name project store view view_name name view acolumn_name columns data source table input model make predictions pmodel_column name target column predicted integration_nametable_name data source table name along integration resides predictor_name name model example query creates trains home_rentals_model model predict rental_price value inner select statement provides real estate listing data used train model sql create model mindsdbhome_rentals_model integration select house_rentals_data predict rental_price execution get sql query ok 0 rows affected xxxx sec joinsqlapijoin home_rentals_data table home_rentals_model model make predictions creating view using create view statement based select statement joining data model tables create ai table select statement joins data source table model table input data making predictions consists sqft number_of_bathrooms location columns joined rental_price column stores predicted values sql create view mindsdbhome_rentals_predictions select asqft anumber_of_bathrooms alocation prental_price price integrationhome_rentals_data join mindsdbhome_rentals_model p execution get sql query ok 0 rows affected xxxx sec note dataset training dataset joining"
  },
  {
    "filename": "view.mdx",
    "path": "docs/mindsdb_sql/sql/create/view.mdx",
    "chunk_id": 1,
    "chunk_content": "example used dataset integrationhome_rentals_data training model see create model statement joining model make predictions see create view statement doesnt happen like realworld scenarios normally use old data train model join new data model make predictions consider old_data dataset stores data years 20192021 new_data dataset stores data year 2022 train model old_data dataset like sql create model mindsdbdata_model integration select old_data predict column data_model model trained using old_data dataset join model new_data dataset make predictions like sql create view mindsdbdata_predictions select acolumn1 acolumn2 acolumn3 pcolumn predicted_column integrationnew_data join mindsdbdata_model p note using view examples use view 1 complex select view grouping example sql select type lastbedrooms mindsdbhouse_v group 1 2 creating predictor view sql create model house_sales_model mindsdb select house_v predict order saledate group bedrooms type window 1 horizon 4 3 using predictor view sql select mindsdbhouse_v join mindsdbhouse_sales_model house_vsaledate latest tip community check article created community article creating views mindsdbhttpsdevtorutamherecreatingviewswithmindsdb1mnf rutam prita mishrahttpscommunityopsiorutamhere tip"
  },
  {
    "filename": "trigger.mdx",
    "path": "docs/mindsdb_sql/sql/create/trigger.mdx",
    "chunk_id": 0,
    "chunk_content": "title create trigger sidebartitle create trigger description triggers enable users define eventbased actions example table updated run query update predictions info currently create triggers following data sources mongodbintegrationsdataintegrationsmongodb available mongodb atlas database slackintegrationsappintegrationsslack solacehttpsgithubcommindsdbmindsdbtreemainmindsdbintegrationshandlerssolace_handler postgresqlintegrationsdataintegrationspostgresql requires write access info syntax syntax creating trigger sql create trigger trigger_name integration_nametable_name columns column_name1 column_name2 sql_code creating trigger data source every time data source updated new data inserted sql_code provided statement executed create trigger either table sql create trigger trigger_name integration_nametable_name sql_code one columns table sql create trigger trigger_name integration_nametable_name columns column_name1 column_name2 sql_code example firstly connect slack mindsdb following instructionintegrationsappintegrationsslacksetupaslackappandgeneratetokens connect slack app channel sql create database mindsdb_slack engine slack parameters token xoxb app_token xapp create model used answer chat questions every time new messages arrive use openai engineintegrationsaienginesopenai use llmintegrationsaioverviewlargelanguagemodels sql create model chatbot_model predict answer using engine openai_engine prompt_template answer question text generate answers slack messages using model sql select stext question manswer chatbot_model join mindsdb_slackmessages schannel_id slackbotchannelid suser u07j30kpauf screated_at last lets analyze query select question slack connection answer generated model join model messages table clause provide channel name appbot integrated exclude messages sent appbot find user id appbot querying mindsdb_slackusers table use last keyword ensure model generates answers newly"
  },
  {
    "filename": "trigger.mdx",
    "path": "docs/mindsdb_sql/sql/create/trigger.mdx",
    "chunk_id": 1,
    "chunk_content": "sent messages finally create trigger insert answer generated model every time new messages sent channel sql create trigger slack_trigger mindsdb_slackmessages insert mindsdb_slackmessages channel_id text select slackbotchannelid channel_id answer text chatbot_model join table_delta suser slackbotid prevent bot replying messages schannel_id slackbotchannelid lets analyze statement create trigger named slack_trigger trigger created mindsdb_slackmessages table therefore every time data added updated trigger execute code provide code executed trigger every time triggering event takes place insert answer generated model messages table table_delta stands table trigger created exclude messages sent appbot find user id appbot querying mindsdb_slackusers table"
  },
  {
    "filename": "join.mdx",
    "path": "docs/mindsdb_sql/sql/api/join.mdx",
    "chunk_id": 0,
    "chunk_content": "title join models tables sidebartitle join models tables description join clause combines rows database table model table column defined implementation used make batch predictions shown examples syntax syntax lets join multiple data tables multiple models get predictions sql select d1column_name d2column_name m1column_name m2column_name integration_nametable_name_1 d1 join integration_nametable_name_2 d2 join join project_namemodel_name_1 m1 join project_namemodel_name_2 m2 join d1input_data m1expected_argument name description integration_nametable_name_1 name data source table used input making predictions integration_nametable_name_2 optionally join arbitrary number data source tables project_namemodel_name_1 name model table used make predictions project_namemodel_name_2 optionally join arbitrary number models mapping input data model arguments input data contains column named question model requires argument named input map columns explained model expects receive input sql create model model_name predict answer using engine openai prompt_template provide answers input user input input data table following columns sql id question 1 many planets solar system 2 many stars solar system want get answers questions using model need join input data table model map question column onto input argument sql select input_table join model_name dquestion minput example 1 lets join home_rentals table home_rentals_model model using statement sql select trental_price real_price mrental_price predicted_price tnumber_of_rooms tnumber_of_bathrooms tsqft tlocation tdays_on_market example_dbdemo_datahome_rentals join mindsdbhome_rentals_model limit 20 execution get sql"
  },
  {
    "filename": "join.mdx",
    "path": "docs/mindsdb_sql/sql/api/join.mdx",
    "chunk_id": 1,
    "chunk_content": "real_price predicted_price number_of_rooms number_of_bathrooms sqft location days_on_market 3901 3886 2 1 917 great 13 2042 2007 0 1 194 great 10 1871 1865 1 1 543 poor 18 3026 3020 2 1 503 good 10 4774 4748 3 2 1066 good 13 4382 4388 3 2 816 poor 25 2269 2272 0 1 461 great 6 2284 2272 1 1 333 great 6 5420 5437 3 2 1124 great 9 5016 4998 3 2 1204 good 7 1421 1427 0 1 538 poor 43 3476 3466 2 1 890 good 6 5271 5255 3 2 975 great 6 3001 2993 2 1 564 good 13 4682 4692 3 2 953 good 10 1783 1738 1 1 493 poor 24 1548 1543 1 1 601 poor 47 1492 1491 0 1 191 good 12 2431 2419 0 1 511 great 1 4237 4257 3 2 916 poor 36 example 2 lets query time series model using statement sql select msaledate date mma forecast mindsdbhouse_sales_model join example_dbdemo_datahouse_sales tsaledate latest ttype house limit 4 execution get sql date forecast 2019123151750631349071994 201912316278226592658638 201912319534269545788583 201912317672524205039773 tip follow doc pagegenerativeaitablesworkingwithgenerativeaitables see examples joining multiple data table multiple models tip"
  },
  {
    "filename": "join-on.mdx",
    "path": "docs/mindsdb_sql/sql/api/join-on.mdx",
    "chunk_id": 0,
    "chunk_content": "title join tables sidebartitle join tables description join statement combines two tables based specified columns functions standard join sql offering added capability combining data multiple data sources allowing users join data one data sources seamlessly syntax syntax sql select t1column_name t2column_name t3column_name datasource1table1 t1 join datasource2table2 t2 t1column_name t2column_name join datasource3table3 t3 t1column_name t3column_name query joins data three different datasources datasource1 datasource2 datasource3 allowing users execute federated queries accross multiple data sources tip nested joins mindsdb provides two categories joins one join statement combines data table model tablemindsdb_sqlsqlapijoin order fetch bulk predictions another regular join used throughout sql requires clause nest types joins follows sql select select project_namemodel_table join datasource_namedata_table t1 join select project_namemodel_table join datasource_namedata_table t2 t1column_name t2column_name tip example 1 lets use following data see different types joins work pets table stores pets sql pet_idname 1 moon 2 ripley 3 bonkers 4 star 5 luna 6 lake owners table stores pets owners sql owner_idname pet_id 1 amy 4 2 bob 1 3 harry 5 4 julia 2 5 larry 3 6 henry 0 join inner join join inner join command joins rows owners pets tables wherever match example pet named lake owner itll left sql select filesowners inner"
  },
  {
    "filename": "join-on.mdx",
    "path": "docs/mindsdb_sql/sql/api/join-on.mdx",
    "chunk_id": 1,
    "chunk_content": "join filespets p opet_id ppet_id execution get sql owner_idname pet_idpet_idname 1 amy 4 4 star 2 bob 1 1 moon 3 harry 5 5 luna 4 julia 2 2 ripley 5 larry 3 3 bonkers standard sql use clause filter output data sql select filesowners inner join filespets p opet_id ppet_id oname amy oname bob execution get sql owner_idname pet_idpet_idname 1 amy 4 4 star 2 bob 1 1 moon left join left join command joins rows two tables rows left table even ones match show left table owners table sql select filesowners left join filespets p opet_id ppet_id execution get sql owner_idname pet_idpet_idname 1 amy 4 4 star 2 bob 1 1 moon 3 harry 5 5 luna 4 julia 2 2 ripley 5 larry 3 3 bonkers 6 henry 0 nullnull right join right join command joins rows two tables rows right table even ones match show right table pets table sql select filesowners right join filespets p opet_id ppet_id execution get sql owner_idname pet_idpet_idname 2 bob 1 1 moon 4 julia 2 2 ripley 5 larry 3 3 bonkers 1 amy 4 4 star 3 harry 5 5 luna null null null6 lake full join full outer"
  },
  {
    "filename": "join-on.mdx",
    "path": "docs/mindsdb_sql/sql/api/join-on.mdx",
    "chunk_id": 2,
    "chunk_content": "join full outer join command joins rows two tables rows tables even ones match show sql select filesowners full outer join filespets p opet_id ppet_id execution get sql owner_idname pet_idpet_idname animal_id 1 amy 4 4 star 2 2 bob 1 1 moon 1 3 harry 5 5 luna 2 4 julia 2 2 ripley 1 5 larry 3 3 bonkers3 6 henry 0 nullnull null null nullnull6 lake 4 example 2 two tables joined subsequently lets use another table called animals sql animal_idname 1 dog 2 cat 3 hamster 4 fish join three tables sql select filesowners right join filespets p opet_id ppet_id join filesanimals panimal_id aanimal_id execution get sql owner_idname pet_idpet_idname animal_idanimal_idname 2 bob 1 1 moon 1 1 dog 4 julia 2 2 ripley 1 1 dog 5 larry 3 3 bonkers3 3 hamster 1 amy 4 4 star 2 2 cat 3 harry 5 5 luna 2 2 cat null null null6 lake 4 4 fish"
  },
  {
    "filename": "evaluate.mdx",
    "path": "docs/mindsdb_sql/sql/api/evaluate.mdx",
    "chunk_id": 0,
    "chunk_content": "title evaluate sidebartitle evaluate predictions description evaluate statement evaluates predictions based available metricshttpsscikitlearnorgstablemodulesmodel_evaluationhtml syntax syntax sql evaluate metric_name select real_value actual predicted_value prediction table_name name description metric_name name metric evaluated chosen herehttpsscikitlearnorgstablemodulesmodel_evaluationhtml real_value real value compared predicted value predicted_value value predicted model table_name table stores corresponding real predicted values example example calculates mean absolute error sql evaluate mean_absolute_error select column_name_that_stores_real_value actual column_name_that_stores_predicted_value prediction table"
  },
  {
    "filename": "select.mdx",
    "path": "docs/mindsdb_sql/sql/api/select.mdx",
    "chunk_id": 0,
    "chunk_content": "title query table sidebartitle query table description select statement fetches data table predictions model go example selecting data tables connected data sources learn select predictions model visit pagesqlapiselectpredictions syntax simple select integration example query contains tables one integration query executed integration database integration name cut table name sql select location maxsqft example_dbdemo_datahome_rentals group location limit 5 raw select integration also possible send native queriessqlnativequeries integration use syntax native given integration useful query parsed sql sql select integration_name native query goes example selecting mongo integration using mongoql syntax sql select mongo dbhouse_sales2findlimit1 complex queries 1 subselect data integration useful cases integration engine doesnt support functions example grouping shown case data raw select passed mindsdb subselect performs operations inside mindsdb sql select type maxbedrooms lastma mongo dbhouse_sales2findlimit300 group 1 2 unions possible use union union operators case every subselect union fetched merged one resultset mindsdb side sql select datatime date datatarget datasourcetable_name data union select modeltime date modeltarget target mindsdbmodel model join datasourcetable_name ttime latest tgroup value"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/mindsdb_sql/sql/api/retrain.mdx",
    "chunk_id": 0,
    "chunk_content": "title retrain model sidebartitle retrain model description retrain statement used retrain already trained predictors new data predictor updated leverage new data optimizing predictive capabilities retraining takes least much time training process predictor dataset used retrain new updated data addition old data syntax syntax sql retrain model project_namepredictor_name integration_name project_name select column_name integration_name project_nametable_name predict target_name using engine engine_name tag tag_name active 01 execution get sql query ok 0 rows affected 0058 sec expressions description project_name name project model resides predictor_name name model retrained integration_name optional name integration created using create databasesqlcreatedatabases statement file uploadsqlcreatefile select column_name table_name optional selecting data used training validation target_column optional column predicted engine_name optionally provide ml engine based model retrained tag_name optionally provide tag visible training_options column mindsdbmodels table active optional setting 0 causes retrained version inactive setting 1 causes retrained version active note model versions every time model retrained new version created incremented version number query model versions like sql select project_namemodels information managing model versions check docs heresqlapimanagemodelsversions note retrain model advised retrain predictor whenever update_status column value mindsdbmodels table set available update_status column value set available new version mindsdb available causes predictor become obsolete new data available table used train predictor find"
  },
  {
    "filename": "retrain.mdx",
    "path": "docs/mindsdb_sql/sql/api/retrain.mdx",
    "chunk_id": 1,
    "chunk_content": "whether need retrain model query mindsdbmodels table look update_status column possible values update_status column name description available indicates model updated updating indicates retraining process model takes place up_to_date indicates model date need retrained lets run query sql select name update_status mindsdbmodels name predictor_name execution get sql name update_status predictor_name up_to_date name description predictor_name name model retrained update_status column informing whether model needs retrained alternatively use describe command sql describe model predictor_name example lets look example using home_rentals_model model first check status predictor sql select name update_status mindsdbmodels name home_rentals_model execution get sql name update_status home_rentals_model available alternatively use describe command sql describe model home_rentals_model available value update_status column informs us retrain model sql retrain mindsdbhome_rentals_model execution get sql query ok 0 rows affected 0058 sec lets check status sql select name update_status mindsdbmodels name home_rentals_model execution get sql name update_status home_rentals_model updating retraining process completed sql select name update_status mindsdbmodels name home_rentals_model execution get sql name update_status home_rentals_model up_to_date"
  },
  {
    "filename": "delete.mdx",
    "path": "docs/mindsdb_sql/sql/api/delete.mdx",
    "chunk_id": 0,
    "chunk_content": "title delete table sidebartitle delete table description delete statement removes rows fulfill clause criteria syntax syntax sql delete integration_nametable_name column_name column_value_to_be_removed statement removes rows table_name table belongs integration_name integration wherever column_name column value equal column_value_to_be_removed another way filter rows using subquery sql delete integration_nametable_name column_name select column_value_to_be_removed some_integrationsome_table some_column some_value statement removes rows table_name table belongs integration_name integration wherever column_name column value equal one values returned subquery"
  },
  {
    "filename": "insert.mdx",
    "path": "docs/mindsdb_sql/sql/api/insert.mdx",
    "chunk_id": 0,
    "chunk_content": "title insert table sidebartitle insert table description insert statement inserts data table data comes subselect query commonly used input prediction results database table syntax syntax sql insert integration_nametable_name select please note destination table integration_nametable_name must exist contain columns data inserted steps followed syntax executes subselect query get output dataset uses insert statement insert output select query integration_nametable_name table execution get sql query ok 0 rows updated xxxxs example want save prediction results int1tbl1 table schema structure used throughout example bash int1 tbl1 mindsdb predictor_name int2 tbl2 name description int1 integration table stores prediction results resides tbl1 table stores prediction results predictor_name name model int2 integration data source table used inner select statement resides tbl2 data source table used inner select statement lets execute query sql insert int1tbl1 select int2tbl2 ta join mindsdbpredictor_name tb tadate 20151231 execution get sql query ok 0 rows updated xxxxs"
  },
  {
    "filename": "select-files.mdx",
    "path": "docs/mindsdb_sql/sql/api/select-files.mdx",
    "chunk_id": 0,
    "chunk_content": "title query file sidebartitle query file description select filesfile_name statement used select data file first upload file mindsdb editor following guidesqlcreatefile create modelsqlcreatemodel uploaded file syntax syntax sql select filesfile_name execution get sql column column column column value value value value name description file_name name file uploaded mindsdb editor following guidesqlcreatefile column name column file example uploaded file following guidesqlcreatefile query like table sql select fileshome_rentals limit 10 execution get sql number_of_rooms number_of_bathrooms sqft location days_on_market initial_price neighborhood rental_price 0 1 4848 great 10 2271 south_side 2271 1 1 674 good 1 2167 downtown 2167 1 1 554 poor 19 1883 westbrae 1883 0 1 529 great 3 2431 south_side 2431 3 2 1219 great 3 5510 south_side 5510 1 1 398 great 11 2272 south_side 2272 3 2 1190 poor 58 4463 westbrae 4123812 1 1 730 good 0 2224 downtown 2224 0 1 298 great 9 2104 south_side 2104 2 1 878 great 8 3861 south_side 3861 lets create predictor using uploaded file learn create model command heresqlcreatemodel sql create model mindsdbhome_rentals_model files select home_rentals predict rental_price execution get sql query ok 0 rows affected xxxx sec"
  },
  {
    "filename": "select-view.mdx",
    "path": "docs/mindsdb_sql/sql/api/select-view.mdx",
    "chunk_id": 0,
    "chunk_content": "title query view sidebartitle query view description select statement fetches data view resides inside project syntax syntax sql select project_nameview_name"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/mindsdb_sql/sql/api/describe.mdx",
    "chunk_id": 0,
    "chunk_content": "title describe model sidebartitle describe model description describe statement used display attributes existing model available options describe model depend underlying engine syntax retrieve general information model sql describe model_name sql describe model model_name command similar command sql select models name model_name one difference two commands describe outputs additional column stores available options describe model depending underlying engine examples lightwood models mindsdb uses lightwood engine default lets see describe models sql describe model home_rentals_model execution get sql tables name engine project active version status accuracy predict update_status mindsdb_version error select_data_query training_options tag infofeaturesmodeljsonai home_rentals_model lightwood mindsdb true 1 complete 0999 rental_price up_to_date 23440 null select demo_datahome_rentals target rental_price null tables output column lists available options describe model tabs tab titledescribe info sql describe model home_rentals_modelinfo command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodeaccuraciescodetd tdit lists accuracy function used evaluate model achieved scoretd tr tr tdcodecolumn_importancescodetd tdit lists featuretype columns assigns importance valuestd tr tr tdcodeoutputscodetd tdthe target columntd tr tr tdcodeinputscodetd tdall feature columnstd tr tbody table tab tab titledescribe features sql describe model home_rentals_modelfeatures command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodecolumncodetd tddata columns used create"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/mindsdb_sql/sql/api/describe.mdx",
    "chunk_id": 1,
    "chunk_content": "modeltd tr tr tdcodetypecodetd tddata type columntd tr tr tdcodeencodercodetd tdencoder type used columntd tr tr tdcoderolecodetd tdrole column feature targettd tr tbody table tab tab titledescribe model sql describe model home_rentals_modelmodel command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodenamecodetd tdname candidate modeltd tr tr tdcodeperformancecodetd tdaccuracy value 0 1 depending type modeltd tr tr tdcodetraining_timecodetd tdtime elapsed training modeltd tr tr tdcodeselectedcodetd td1 best performing model 0 resttd tr tr tdcodeaccuracy_functionscodetd tdit defines accuracy function used evaluate model stores r2_score value regression predictions balanced_accuracy_score value classification predictions bounded_ts_accuracy value time series predictions values vary 0 1 1 indicates perfect predictor based results obtained held portion datatd tr tbody table tab tab titledescribe jsonai sql describe model home_rentals_modeljsonai command returns following output column table thead tr thnameth thdescriptionth tr thead tbody tr tdcodeensemblecodetd tdobject json type describing parameters used select best candidate modeltd tr tbody table tab tabs nlp models mindsdb offers nlp models utilize either hugging face openai engines lets see describe models sql describe model sentiment_classifier execution get sql tables name engine project active version status accuracy predict update_status mindsdb_version error select_data_query training_options tag argsmetadata sentiment_classifier openai mindsdb true 1 complete"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/mindsdb_sql/sql/api/describe.mdx",
    "chunk_id": 2,
    "chunk_content": "null sentiment up_to_date 23132 null null target sentiment using prompt_template describe sentiment reviewsn strictly positive neutral negativen love productpositiven scamnegativen review null tables output column lists available options describe model tabs tab titledescribe args sql describe model sentiment_classifierargs command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodekeycodetd tdit stores parameters prompt_template targettd tr tr tdcodevaluecodetd tdit stores parameter valuestd tr tbody table tab tab titledescribe metadata sql describe model sentiment_classifiermetadata command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodekeycodetd tdit stores metadata parameterstd tr tr tdcodevaluecodetd tdit stores parameter valuestd tr tbody table tab tabs nixtla models mindsdb integrates nixtla engines statsforecast neuralforecast hierarchicalforecast lets see describe models based nixtla engines sql describe model quarterly_expenditure_forecaster execution get sql tables name engine project active version status accuracy predict update_status mindsdb_version error select_data_query training_options tag infofeaturesmodel quarterly_expenditure_forecaster statsforecast mindsdb true 1 complete null expenditure up_to_date 23440 null select historical_expenditures target expenditure using timeseries_settings is_timeseries true order_by month horizon 3 group_by category null tables output column lists available options describe model tabs tab titledescribe info sql describe model quarterly_expenditure_forecasterinfo command returns following output columns table thead tr thnameth thdescriptionth tr thead"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/mindsdb_sql/sql/api/describe.mdx",
    "chunk_id": 3,
    "chunk_content": "tbody tr tdcodeaccuraciescodetd tdit lists chosen model name accuracy scoretd tr tr tdcodeoutputscodetd tdthe target columntd tr tr tdcodeinputscodetd tdall feature columnstd tr tbody table tab tab titledescribe features sql describe model quarterly_expenditure_forecasterfeatures command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodedscodetd tdit defines intervals records example weve got monthly expenditure recordstd tr tr tdcodeycodetd tdit stores target columntd tr tr tdcodeunique_idcodetd tdit stores columns listed group clause defines columns dataset split multiple time series track process value different entities groupstd tr tbody table tab tab titledescribe model sql describe model quarterly_expenditure_forecastermodel command returns following output columns table thead tr thnameth thdescriptionth tr thead tbody tr tdcodemodel_namecodetd tdit chosen model nametd tr tr tdcodefrequencycodetd tdit frequencytd tr tr tdcodeseason_lengthcodetd tdit indicates many measurements next season occurs example time series monthly measurements season length 12 means every 12 months new season occurs strong effect final models performancetd tr tr tdcodehierarchycodetd tdit defines whether hierarchicalforecast used true falsetd tr tbody table tab tabs models models utlize langchain brought mindsdb via mlflow described follows sql describe model other_model command returs info first output column sql describe model other_modelinfo command lists basic model information note need information describe"
  },
  {
    "filename": "describe.mdx",
    "path": "docs/mindsdb_sql/sql/api/describe.mdx",
    "chunk_id": 4,
    "chunk_content": "model understand results feel free ask us community slack workspacehttpsmindsdbcomjoincommunity note"
  },
  {
    "filename": "update.mdx",
    "path": "docs/mindsdb_sql/sql/api/update.mdx",
    "chunk_id": 0,
    "chunk_content": "title update table sidebartitle update table description mindsdb provides two ways using update statement 1 regular update statement updates specific column values existing table 2 update select statement updates data existing table subselect query used alternative create table insert store predictions syntax example regular update statement sql update integration_nametable_name set column_name new_value column_name old_value info please replace placeholders follows integration_name name connected data source table_name table name within data source column_name column name within table info example update select statement updates table predictions made within mindsdb sql update integration_to_be_updatedtable_to_be_updated set column_to_be_updated prediction_datapredicted_value_column select ppredicted_value_column pcolumn1 pcolumn2 integration_nametable_name join model_name p prediction_data column1 prediction_datacolumn1 column2 prediction_datacolumn2 alternative update select statement updates table predictions syntax easier write sql update integration_to_be_updatedtable_to_be_updated column1 column2 select ppredicted_value_column column_to_be_updated pcolumn1 pcolumn2 integration_nametable_name join model_name p info steps followed syntax executes query clause get output data example query predictions could simple select another table please note aliased prediction_data updates rows table_to_be_updated table belongs integration_to_be_updated integration match clause criteria rows updated values defined set clause info tip recommended use primary key columns clause column1 column2 primary key columns uniquely identify row otherwise update statement may lead unexpected results altering rows didnt want affect tip"
  },
  {
    "filename": "finetune.mdx",
    "path": "docs/mindsdb_sql/sql/api/finetune.mdx",
    "chunk_id": 0,
    "chunk_content": "title finetune model sidebartitle finetune model description finetune statement lets retrain model additional training data p aligncenter img srchttpsdocsgooglecomdrawingsde2pacx1vtwrr36vercmqyz7amqxs1gzmtgv6urfilkftuqzitszwsuxjizci8tir4yu7nbs3_iugzkgelvq8l9pubw955h460 p imagine model trained certain dataset training data available wish retrain model new dataset finetune statement lets partially retrain model takes less time resources retrainsqlapiretrain statement machine learning literature also referred finetuning model syntax syntax sql finetune model project_namemodel_name integration_name project_name select column_name integration_name project_nametable_name incremental_column last using key value expressions description project_name name project model resides model_name name model retrained integration_name name integration created using create databasesqlcreatedatabases statement file upload select column_name table_name selecting additional data used retraining incremental_column last selecting newly added data used finetune model learn last keyword heremindsdb_sqlsqlcreatejobslast using key value optional using clause lets pass multiple parameters finetune statement info model versions every time model finetuned retrained new version created incremented version number unless overridden recent version becomes active training completes query model versions like sql select project_namemodels information managing model versions check docs heresqlapimanagemodelsversions info warning model generated trained active model becomes active completes generating training warning examples example 1 openai model finetuningfinetuneopenai example 2 llama2 model finetuningfinetuneanyscale example 3 regression model finetuningfinetuneregression example 4 classification model finetuningfinetuneclassification"
  },
  {
    "filename": "manage-models-versions.mdx",
    "path": "docs/mindsdb_sql/sql/api/manage-models-versions.mdx",
    "chunk_id": 0,
    "chunk_content": "title manage model versions sidebartitle manage model versions creating model create model use create modelsqlcreatemodel statement sql create model mindsdbhome_rentals example_db select demo_datahome_rentals predict rental_price using engine lightwood tag model model one version verify querying models table sql describe model home_rentals execution get sql name engine projectactiveversionstatus accuracypredict update_statusmindsdb_versionerror select_data_query training_options current_training_phasetotal_training_phasestag created_at training_time home_rentalslightwoodmindsdbtrue 1 complete0999 rental_priceup_to_date 221132 nullselect demo_datahome_rentalstarget rental_price using 5 5 null20240207 16010499095819946 retraining model retrain model use retrainsqlapiretrain statement sql retrain mindsdbhome_rentals lets query model versions sql describe model home_rentals execution get sql name engine projectactiveversionstatus accuracypredict update_statusmindsdb_versionerror select_data_query training_options current_training_phasetotal_training_phasestag created_at training_time home_rentalslightwoodmindsdbtrue 1 complete0999 rental_priceup_to_date 221132 nullselect demo_datahome_rentalstarget rental_price using 5 5 null20240207 16010499095819946 home_rentalslightwoodmindsdbtrue 2 complete0999 rental_priceup_to_date 221132 nullselect demo_datahome_rentalstarget rental_price using 5 5 null20240207 17010499095821923 using active model version use currently active model version run query sql select mindsdbhome_rentals p join example_dbdemo_datahome_rentals using specific model version use specific model version even set inactive run query sql select mindsdbhome_rentals1 p join example_dbdemo_datahome_rentals setting model version active set specific model version active run statement sql set model_active home_rentals1 deleting specific model version delete specific model version run statement sql drop model home_rentals2 please note delete version active deleting model versions delete models version"
  },
  {
    "filename": "manage-models-versions.mdx",
    "path": "docs/mindsdb_sql/sql/api/manage-models-versions.mdx",
    "chunk_id": 1,
    "chunk_content": "run drop model statement sql drop model mindsdbhome_rentals"
  },
  {
    "filename": "use.mdx",
    "path": "docs/mindsdb_sql/sql/api/use.mdx",
    "chunk_id": 0,
    "chunk_content": "title use data source sidebartitle use data source description use integration_name statement provides option use connected datasources select database tables even connecting mindsdb mysql database still able select database syntax connect database use created datasource sql use integration_name simply select tables sql select table_name use datasourceassetssqlusepng"
  },
  {
    "filename": "jupysql.mdx",
    "path": "docs/mindsdb_sql/connect/jupysql.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb jupyter notebooks sidebartitle jupyter notebooks jupysql full sql client jupyter allows run sql plot large datasets jupyter via sql sql magics also allows users plot data directly db via sqlplot magics jupysql facilitates working databases jupyter download herehttpsgithubcomploomberjupysql run pip install jupysql tip consider option interact mindsdb directly mysql cliconnectmysqlclient postgres cliconnectpostgresclient tip connect prerequisite make sure jupysql installed install run pip install jupysql make sure pymysql installed install run pip install pymysql note easily verify installation jupysql running code python load_ext sql command loads package allows run cell magics top jupyter pymysql validate running command python import pymysql note please follow instructions connect mindsdb via jupysql jupyter tabs tab titlelocal mindsdb use python code connect jupyter notebook lab local mindsdb database via jupysql load extension python load_ext sql connect db python sql mysqlpymysqlmindsdb12700147335mindsdb testing connection listing existing tables pure sql python sql show tables please note use following connection details username mindsdb password left empty host 127001 port 47335 database name mindsdb docker connecting docker might different port tab tabs create database connection execute code success last command lists tables output expected output bash mysqlpymysqlmindsdb12700147335mindsdb 2 rows affected tables_in_mindsdb models whats next set recommend check tutorials community"
  },
  {
    "filename": "jupysql.mdx",
    "path": "docs/mindsdb_sql/connect/jupysql.mdx",
    "chunk_id": 1,
    "chunk_content": "tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "mysql-client.mdx",
    "path": "docs/mindsdb_sql/connect/mysql-client.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb mysql cli sidebartitle mysql cli mindsdb provides powerful mysql api allows users connect using mysql command line client please note connecting mindsdbs mysql api connecting mysql database find information mysql cli herehttpsdevmysqlcomdocrefman80enmysqlhtml tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip connect connect mindsdb mysql use mysql client program bash mysql h hostname port tcpip port number u user p password command allows connect mindsdb bash mysql h 127001 port 47335 u mindsdb execution get bash welcome mariadb monitor commands end g server version 571mindsdb10 mindsdb type help h help type c clear current input statement mysql none whats next set recommend check use casesusecasesoverview section youll find various examples regression classification time series nlp predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages mindsdb sql section explain common sql syntax examples fun"
  },
  {
    "filename": "sql-alchemy.mdx",
    "path": "docs/mindsdb_sql/connect/sql-alchemy.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb sql alchemy sidebartitle sql alchemy sql alchemy python sql toolkit provides objectrelational mapping features python programming language sql alchemy facilitates working databases python download herehttpswwwsqlalchemyorg run pip install sqlalchemy tip consider option interact mindsdb directly mysql cliconnectmysqlclient postgres cliconnectpostgresclient tip connect please follow instructions connect mindsdb sql alchemy tabs tab titlelocal mindsdb use python code connect mindsdb database sql alchemy make sure pymysql module installed executing python code install run pip install pymysql command python sqlalchemy import create_engine user mindsdb password host 127001 port 47335 database def get_connection return create_engine urlmysqlpymysql01234formatuser password host port database __name__ __main__ try engine get_connection engineconnect printfconnection host user user created successfully except exception ex printconnection could made due following error n ex please note use following connection details username mindsdb password left empty host 127001 port 47335 database name left empty create database connection execute code success following output expected bash connection 127001 user mindsdb created successfully tab tabs br note sqlachemy create_engine lazy implies human error entering connection details would undetectable action becomes necessary calling execute method execute sql commands note whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn"
  },
  {
    "filename": "sql-alchemy.mdx",
    "path": "docs/mindsdb_sql/connect/sql-alchemy.mdx",
    "chunk_id": 1,
    "chunk_content": "mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "dbeaver.mdx",
    "path": "docs/mindsdb_sql/connect/dbeaver.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb dbeaver sidebartitle dbeaver dbeaver database tool allows connect work various database engines download herehttpsdbeaverio data setup first create new database connection dbeaver clicking icon shown p aligncenter img srcassetssqldbeaver_1png p next choose mysql database engine click _next_ button warning multiple mysql options choose driver mysql8 later warning p aligncenter img srcassetssqldbeaver_2png p time fill connection details p aligncenter img srcassetssqldbeaver_3png p three options tabs tab titlelocal mindsdb connect local mindsdb please use connection details hostname 127001 port 47334 username mindsdb password leave empty database leave empty tab tabs ready test connection testing connection click test connection button check provided data allows connect mindsdb success see message p aligncenter img srcassetssqldbeaver_4png p lets run queries finally make sure mindsdb database connection works lets run queries sql show full databases execution get sql database type engine information_schema system null mindsdb project null files data files looks dbeaver p aligncenter img srcassetssqldbeaver_5png p tip whitelist mindsdb cloud ip addressfaqswhitelistips tip whats next set recommend check tutorialssqltutorialshousesalesforecasting section youll find various examples regression classification time series predictions mindsdb community tutorialstutorials list learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "postgres-client.mdx",
    "path": "docs/mindsdb_sql/connect/postgres-client.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb postgres cli sidebartitle postgres cli postgres api enables users connect mindsdb using postgresql protocol tip default mindsdb starts http mysql apis define apis start using api flag bash python mindsdb api httpmysqlpostgresmongodb want start mindsdb without graphical user interface gui use no_studio flag bash python mindsdb no_studio tip connect connect mindsdb postgresql use psql client program bash psql h localhost p 55432 mindsdb u mindsdb use local mindsdb installation following parameters host localhost port 55432 database mindsdb user mindsdb command allows connect mindsdb bash psql h 127001 p 47335 mindsdb u mindsdb note please see pagehttpsgithubcommindsdbmindsdbtreemainmindsdbapipostgres implementation details note whats next set recommend check use casesusecasesoverview section youll find various examples regression classification time series nlp predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages mindsdb sql section explain common sql syntax examples fun"
  },
  {
    "filename": "tableau.mdx",
    "path": "docs/mindsdb_sql/connect/tableau.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb tableau sidebartitle tableau iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedeuibvrm85v4 titleconnect using tableau frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe tableau lets visualize data easily intuitively mindsdb supports mysql binary protocol connect tableau see forecasts connect follow steps connect mindsdb tableau first create new workbook tableau open _connectors_ tab _connect data_ window p aligncenter img srcassetsconnect_tableaupng p next choose _mysql_ provide details mindsdb connection ip port database name optionally provide username password click _sign in_ p aligncenter img srcassetsconnect_tableau_2png p connection parameters specific mindsdb installation type tabs tab titlelocal mindsdb text host localhost port 47335 database name mindsdb username mindsdb password leave empty tab tabs youre connected overview mindsdb tableau content mindsdb visible rightside pane p aligncenter img srcassetsconnect_tableau_3png p predictors listed _table_ section also switch integrations _mindsdb_ _files_ _database_ section using dropdown p aligncenter img srcassetsconnect_tableau_4png p lets run examples examples example 1 previewing one tables _mysql_ integration p aligncenter img srcassetsconnect_tableau_5png p example 2 one technical limitation namely join tables different databasesintegrations tableau overcome challenge use either views custom sql queries previewing view joins data table predictor table p aligncenter img srcassetsconnect_tableau_6png p using custom sql query clicking _new custom sql_ button rightside pane p"
  },
  {
    "filename": "tableau.mdx",
    "path": "docs/mindsdb_sql/connect/tableau.mdx",
    "chunk_id": 1,
    "chunk_content": "aligncenter img srcassetsconnect_tableau_7png p whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples tip community check articles video guides created community article predicting visualizing hourly electricity demand us mindsdb tableauhttpsteslimodusmediumcompredictingvisualizinghourlyelectricitydemandintheuswithmindsdbandtableau126d1c74d860 teslim odumuyiwahttpsteslimodusmediumcom article predicting visualizing petroleum production mindsdb tableauhttpsdevtotesprogrampredictingvisualizingpetroleumproductionwithmindsdbandtableau373f teslim odumuyiwahttpsgithubcomtesprogram article predicting visualizing gas prices mindsdb tableauhttpsdevtotesprogrampredictingvisualizinggaspriceswithmindsdbandtableaud1p teslim odumuyiwahttpsgithubcomtesprogram article visualize mindsdb predictions tableauhttpsdevtoephraimxhowtovisualizemindsdbpredictionswithtableau2bpd ephraimxhttpsdevtoephraimx video guide connecting mindsdb tableauhttpswwwyoutubecomwatchveuibvrm85v4 alissa troianohttpsgithubcomalissatroiano video guide visualizing prediction result tableauhttpsyoutube4aio8knboo teslim odumuyiwahttpsgithubcomtesprogram tip fun"
  },
  {
    "filename": "deepnote.mdx",
    "path": "docs/mindsdb_sql/connect/deepnote.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb deepnote sidebartitle deepnote note worked team deepnote built native integration deepnote notebooks please check deepnote demo guidehttpsdeepnotecomprojectmachinelearningwithsql8gdf7bc7szklhblorqoicw2fmindsdb_demoipynb deepnote integration docshttpsdocsdeepnotecomintegrationsmindsdb note whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "metabase.mdx",
    "path": "docs/mindsdb_sql/connect/metabase.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb metabase sidebartitle metabase metabase opensource software facilitates data analysis lets visualize data easily intuitively mindsdb supports mysql binary protocol connect metabase see forecasts creating training models information visit metabasehttpswwwmetabasecom setup mindsdb install mindsdb locally via dockersetupselfhosteddocker docker desktopsetupselfhosteddockerdesktop metabase lets set metabase following one approaches presented metabase open source edition pagehttpswwwmetabasecomstartoss use jar approachhttpswwwmetabasecomdocslatestinstallationandoperationrunningthemetabasejarfilehtml metabase connect follow steps connect mindsdb metabase 1 open metabase navigate _admin settings_ clicking cog bottom left corner 2 click _databases_ top navigation bar 3 click _add database_ top right corner 4 fill form using following data tabs tab titlelocal mindsdb text database type mysql display name mindsdb host localhost port 47335 database name mindsdb username mindsdb password leave empty tab tabs p aligncenter img srcassetsmetabase_add_databasepng p 5 click _save_ youre connected p aligncenter img srcassetsmetabase_connectedpng p example connection mindsdb metabase established lets examples sql statements usually run mindsdb sql editorconnectmindsdb_editor run metabase well lets start something easy metabases home page click _new sql query_ top right corner select mindsdb database lets execute following command editor sql show tables execution get p aligncenter img srcassetsmetabase_run_query_show_tablespng p please note creating database connectionsqltutorialshomerentalsconnectingthedata using create database statement fails curly braces used jdbc escape sequences sql create"
  },
  {
    "filename": "metabase.mdx",
    "path": "docs/mindsdb_sql/connect/metabase.mdx",
    "chunk_id": 1,
    "chunk_content": "database example_db engine postgres parameters user demo_user password demo_password host samplesmindsdbcom port 5432 database demo execution get p aligncenter img srcassetsmetabase_run_query_failurepng p overcome issue using mindsdb sql editorconnectmindsdb_editor create database getting back metabase lets run queries database created help mindsdb sql editorconnectmindsdb_editor sql select example_dbdemo_datahome_rentals limit 10 execution get p aligncenter img srcassetsmetabase_run_query_home_rentalspng p whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "mindsdb_editor.mdx",
    "path": "docs/mindsdb_sql/connect/mindsdb_editor.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb sql editor sidebartitle mindsdb sql editor mindsdb provides sql editor dont need download additional sql clients connect mindsdb use mindsdb sql editor two ways use editor tabs tab titleselfhosted local deployment setting mindsdb using dockersetupselfhosteddocker pip linuxsetupselfhostedpiplinuxwindowssetupselfhostedpipwindowsmacossetupselfhostedpipmacos pip via source codesetupselfhostedpipsource go terminal execute following bash python mindsdb execution get bash 20220506 140704599 info gui available http12700147334 immediately browser automatically opens mindsdb sql editor case doesnt visit url http12700147334http12700147334 browser preference tab tabs sneak peek mindsdb sql editor guiassetscloudgui_querypng whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "connect-mariadb-skysql.mdx",
    "path": "docs/mindsdb_sql/connect/connect-mariadb-skysql.mdx",
    "chunk_id": 0,
    "chunk_content": "title mariadb skysql setup guide mindsdb sidebartitle mariadb skysql find information mariadb sky sql herehttpscloudmariadbcom 1 select service mindsdb havent already identify service enabled mindsdb make sure running otherwise skip step 2 iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedxlj7xd9bru8 titlemariadb skysql service frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe 2 add mindsdb service allowlist access mariadb skysql services restricted perservice basishttpsmariadbcomproductsskysqldocssecurityfirewallsipallowlistservices add following ip addresses allow mindsdb connect mariadb service clicking cog icon navigating security access dialog input prompted one one following ips 1822020595 31915246 521491162 iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedzaum5kw8vjs titlemariadb skysql allowlist frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe 3 download service pem file certificate authority chainhttpsmariadbcomproductsskysqldocsconnectconnectionparametersportalcertificateauthoritychain pem file must provided proper tls certificate validation selected service click world globe icon connect service login credentials section click download aws_skysql_chainpem file download onto machine iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedtbcrpsphkds titlemariadb skysql pem file frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe 4 publically expose service pem file select secure storage aws_skysql_chainpem file allows working public url localpath iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedncpvbmnv7as titlemariadb skysql pem s3 frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe 5 link mindsdb mariadb skysql service print"
  },
  {
    "filename": "connect-mariadb-skysql.mdx",
    "path": "docs/mindsdb_sql/connect/connect-mariadb-skysql.mdx",
    "chunk_id": 1,
    "chunk_content": "query template select add data either top side navigation choose mariadb skysql list fill values run query complete setup iframe classnamewfull h96 width613 height286 srchttpswwwyoutubecomembedyg4qzbdu918 titlemindsdb mariadb skysql step5 connection frameborder0 allowaccelerometer autoplay clipboardwrite encryptedmedia gyroscope pictureinpicture allowfullscreen iframe codes codegroup sql template create database maria_datasource display name database engine mariadb name mindsdb handler parameters host host ip address url port port used make tcpip connection database database name user database user password database password ssl truefalse optional ssl parameter value indicates whether ssl enabled true disabled false ssl_ca optional ssl certificate authority path either path url ssl_cert optional ssl certificates url either path url ssl_key optional ssl keys path either path url sql example mariadb skysql service create database skysql_datasource engine mariadb parameters host mindsdbtestmdb0002956db1skysqlnet port 5001 database mindsdb_data user db00007539 password password ssl certificate required sslca url httpsmindsdbwebbuildss3amazonawscomaws_skysql_chainpem codegroup brbr whats next set recommend check tutorials community tutorials sections youll find various examples regression classification time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  },
  {
    "filename": "grafana.mdx",
    "path": "docs/mindsdb_sql/connect/grafana.mdx",
    "chunk_id": 0,
    "chunk_content": "title mindsdb grafana sidebartitle grafana grafanahttpsgrafanacom opensource analytics interactive visualization web application allows users ingest data various sources query data display customizable charts easy analysis connect begin set grafana following one methods outlined grafana installation documentationhttpsgrafanacomdocsgrafanalatestsetupgrafanainstallationsupportedoperatingsystems grafana successfully set environment navigate connections section click add new connection select mysql plugin shown p aligncenter img srcassetssqlgrafana_1png p time fill connection details p aligncenter img srcassetssqlgrafana_2png p three options tabs tab titlelocal mindsdb connect local mindsdb please use connection details host 12700147335 username mindsdb password leave empty database leave empty tab tabs ready save test connection testing connection click save test button check provided data allows connect mindsdb success see message p aligncenter img srcassetssqlgrafana_3png p examples querying verify functionality mindsdb database connection query data explore view use text edit mode compose queries sql show full databases execution get p aligncenter img srcassetssqlgrafana_4png p visual query builder build dashboard mindsdb database connection example query sql create database mysql_demo_db engine mysql parameters user user password mindsdbuser123 host samplesmindsdbcom port 3306 database public select mysql_demo_dbair_passengers execution get p aligncenter img srcassetssqlgrafana_5png p tip whitelist mindsdb cloud ip addressfaqswhitelistips tip whats next set recommend check tutorials community tutorials sections youll find various examples regression classification"
  },
  {
    "filename": "grafana.mdx",
    "path": "docs/mindsdb_sql/connect/grafana.mdx",
    "chunk_id": 1,
    "chunk_content": "time series predictions mindsdb learn mindsdb follow guide mindsdb database structuresqltablestructure also dont miss remaining pages sql api section explain common sql syntax examples fun"
  }
]